The Connections of Large Perceptrons
 Abstract We derive analytical expressions for the connections of large perceptrons, by studying the fixed points of the perceptron learning rule.  If the training set consists of all possible input vectors, we can calculate (for large systems) the connections as a series expansion in the system size.  The leading term in this expansion turns out to be either the Hebb rule (for unbiased distributions) or the biased Hebb rule (for biased distributions).  The performance of our asymptotic expressions (and finite size corrections) on small systems is studied numerically.  For the more realistic case of having an extensive training set (patterns learned with training noise) we derive a self-consistent set of coupled non-linear equations for the connections.  In the limit of zero training noise, the solution of these equations is shown to give the connections with maximal stability in the Gardner sense.

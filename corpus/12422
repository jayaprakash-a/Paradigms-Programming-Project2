A LEARNING VECTOR QUANTIZATION ALGORITHM FOR PROBABILISTIC MODELS
 ABSTRACT In classification problems, it is preferred to attack the discrimination problem directly rather than indirectly by first estimating the class densities and by then estimating the discrimination function from the generative models through Bayes's rule.  Sometimes, however, it is convenient to express the models as probabilistic models, since they are generative in nature and can handle the representation of high-dimensional data like time-series.  In this paper, we derive a discriminative training procedure based on Learning Vector Quantization (LVQ) where the codebook is expressed in terms of probabilistic models.  The likelihood-based distance measure is justified using the Kullback-Leibler distance.  In updating the winner unit, a gradient learning step is taken with regard to the parameters of the probabilistic model.  The method essentially departs from a prototypical representation and incorporates learning in the parameter space of generative models.  As an illustration, we present experiments in the fraud detection domain, where models of calling behavior are used to classify mobile phone subscribers to normal and fraudulent users.  This is an extension of our earlier work in clustering probabilistic models with the Self-Organizing Map (SOM) algorithm to the classification domain.

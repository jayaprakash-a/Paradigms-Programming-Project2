Bias-Variance Analysis of Support Vector Machines for the Development of SVM-Based Ensemble Methods
 Abstract Bias{variance analysis provides a tool to study learning algorithms and can be used to properly design ensemble methods well-tuned to the properties of a specific base learner.  Indeed the effectiveness of ensemble methods critically depends on accuracy, diversity and learning characteristics of base learners.  We present an extended experimental analysis of bias{variance decomposition of the error in Support Vector Machines (SVMs), considering gaussian, polynomial and dot{product kernels.  A characterization of the error decomposition is provided, by means of the analysis of the relationships between bias, variance, kernel type and its parameters, offering insights into the way SVMs learn.  The results show that the expected trade-off between bias and variance is sometimes observed, but more complex relationships can be detected, especially in gaussian and polynomial kernels.  We show that the bias{variance decomposition offers a rationale to develop ensemble methods using SVMs as base learners, and we outline two directions for developing SVM ensembles, exploiting the SVM bias characteristics and the bias-variance dependence on the kernel parameters.

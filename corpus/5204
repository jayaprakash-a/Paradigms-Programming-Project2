Probabilistic Gaze Imitation and Saliency Learning in a Robotic Head
 Abstract Imitation is a powerful mechanism for transferring knowledge from an instructor to a naive observer.  We first present Bayesian algorithms, based on Meltzoff and Moore's AIM model for imitation in infants, that implement the core of an imitation learning framework.  Next, we present Bayesian algorithms for learning which objects an instructor considers salient in a task.  Finally, we demonstrate the performance of our algorithms in a gaze following and saliency learning task implemented on an active vision robotic head.  Our results suggest that the ability to follow gaze and learn instructor- and task-specific saliency models could play a crucial role in building systems capable of complex forms of humanrobot interaction.  1 Imitation learning and shared attention Imitation is a powerful mechanism for transferring knowledge from a skilled agent (the instructor) to an unskilled agent (or observer) using direct manipulation of the environment.  Several researchers have investigated imitative behavior in apes [21, 5], in children (including infants only 42 minutes old) [15, 16], and in an increasingly diverse selection of machines [9, 14].  The attraction of imitation for robotics is obvious: imitative robots offer drastically reduced programming costs compared to robots requiring programming by an expert.  Imitative robots also offer testbeds for cognitive researchers to test computational theories, and provide modifiable agents for contingent interaction with humans in psychological experiments.  Successful imitation requires that instructor and observer simultaneously attend to the same object or environmental state.  Such simultaneous attention is often referred to as "shared attention" in the psychological literature.  Previous work, notably by Scassellati on the Cog platform [20], has concentrated on deterministic algorithms for shared attention between humans and robots.  Scassellati's work concentrated on tracking the gaze of a human instructor, and on mimicking the motion of the instructor's head in either a vertical or a horizontal direction.  Separately, Movellan and colleagues have used robotic platforms to study shared attention in infants [8].  Although robotic platforms [7, 20] have demonstrated impressive mimicry results, richly contingent human-robot interaction comparable to infant imitation depends on having a model for saliency, i. e. , a model of what components of environmental state are important in a given task.  Ideally, saliency models would be task- or instructor-specific, representing the observer's learned context-dependent knowledge of how to allocate attentional resources.  In this paper, we describe a robotic system that uses probabilistic algorithms to follow the gaze of a human and to identify salient objects in a scene.  Our algorithms employ Bayesian inference because of its robustness to noise and missing data, tractability under large data sets, and unifying mathematical formalism.  Bayesian imitation learning approaches have been proposed to accelerate reinforcement learning [17]; however, that framework chiefly addresses the problem of learning a forward model of the environment [12] via imitation (see Section 3), and its correspondence with cognitive findings in humans is unclear.  Other frameworks have been proposed for imitation learning in machines [3, 20, 1], but most of these are not designed around a coherent probabilistic formalism.  The robotic system described in this paper tracks a human instructor's gaze to an object, then learns a simple instructor-specific, task-specific saliency model.  Our biologicallyinspired, model-based approach extends previous robotic gaze imitation results in three main ways: i) it provides a Bayesian description of imitation in general, and gaze tracking specifically; ii) it incorporates infant imitation findings into a rigorous algorithmic framework; and iii) the system learns simple, context-dependent probabilistic models for saliency.  Preliminary results show how shared attention could be developed between humans and robots.  Sections 2 and 3 respectively discuss our system's modality-independent representation and our Bayesian algorithms for motor planning.  Section 4 describes how our system computes object saliency.  Section 5 concludes with gaze tracking results from our system and a simple example of learning a saliency model.

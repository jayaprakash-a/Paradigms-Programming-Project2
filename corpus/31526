forLearning, Planning, and Reacting Based on Approximating Dynamic Programming
 Abstract This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods.  Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world.  In this pap er, I present and show results for twoDynaarchitectures.  The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas suchasevaluation functions and universal plans (reactive systems).  Using a navigationtask, results are shown for a simple Dyna-PI system that simultaneously learns by trial anderror, learns a world model, and plans optimal routes using the evolving world model.  The Dyna-Q architecture is based on Watkins'sQ-learning, a new kind of reinforcement learning.  Dyna-Q uses a less familiar set of data structures than doesDyna-PI, but is arguably simpler to implement and use.  Weshow that Dyna-Q architectures are easy to adapt for use in changing environments.

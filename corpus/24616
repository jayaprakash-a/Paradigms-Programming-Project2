Mobile Interaction with Remote Worlds: The Acoustic Periscope
 ABSTRACT Strictly speaking, a periscope is an optical device that allows one to view and navigate the external environment.  The acoustic periscope is a metaphor for mobile interaction that transparently exploits audio/speech to navigate and provide an unobstructed scene in a real or virtual world.  We aim at both true mobility -- no strings or devices should be attached to human user to be able navigate -- and at a smart multi modal.  The implementation of our concept highlights un underestimated modality, the acoustic one, for making computers transparent to the actual interaction of the user with a remote world and advancing in the direction of ubiquitous computing.  In this paper we describe the basic principles, architecture and implementation of a system for ubiquitous, multimodal and easy visual accesses to the remote world based on the acoustic periscope idea.  In order to assemble the required functionality we resort to audio signal processing (in particular array signal processing) for location and orientation estimation, speech recognition and text-to-speech synthesis for natural language interaction, mobile computing, communication in a LAN/Bluetooth network, and streaming of data from or control of a remote telerobotic platform with vision capabilities.

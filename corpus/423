Predictive Representations of State
 Abstract We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations.  State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations.  Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models.  Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls).  We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model.  In predicting or controlling a sequence of observations, the concepts of state and state estimation inevitably arise.  There have been two dominant approaches.  The generative-model approach, typified by research on partially observable Markov decision processes (POMDPs), hypothesizes a structure for generating observations and estimates its state and state dynamics.  The history-based approach, typified by k-order Markov methods, uses simple functions of past observations as state, that is, as the immediate basis for prediction and control.  (The data flow in these two approaches are diagrammed in Figure 1. ) Of the two, the generative-model approach is more general.  The model's internal state gives it temporally unlimited memory

Self Supervised Boosting
 Abstract Boosting algorithms and successful applications thereof abound for classification and regression learning problems, but not for unsupervised learning.  We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of "non-data" generated from the model's current estimate of the data density.  Training in each boosting round proceeds in three stages: first we sample non-data from the model's current Boltzmann distribution.  Next, a feature is trained to improve classification performance between data and non-data.  Finally, a coefficient is learned which determines the importance of this feature relative to ones already in the pool.  Non-data only needs to be generated once to learn each new feature.  The validity of the approach is demonstrated on binary digits and continuous synthetic data.

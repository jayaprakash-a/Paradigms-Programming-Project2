Additive Models, Boosting, and Inference for Generalized Divergences
 Abstract We present a framework for designing incremental learning algorithms derived from generalized entropy functionals.  Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform.  A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et al. , as the natural parameter of the family varies.  We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion.  This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani.

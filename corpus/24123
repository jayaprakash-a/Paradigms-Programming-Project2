Bayesian Transductive Learning of the Kernel Matrix
 Abstract This paper addresses the problem of transductive learning of the kernel matrix from a Bayesian perspective.  We consider the target kernel matrix as a random matrix following the Wishart distribution with a positive definite parameter matrix.  This parameter matrix, in turn, has the inverted Wishart distribution (with a positive definite hyperparameter matrix) as its conjugate prior.  By regarding kernel matrix learning as a missing data problem, we propose a Bayesian procedure which uses the Kullback-Leibler (KL) divergence to measure the similarity between the learned kernel matrix and the target kernel matrix.  An expectation-maximization (EM) algorithm is devised to infer the missing data and the model parameters in a maximum a posteriori (MAP) manner.  Using different settings for the target kernel and hyperparameter matrices, our model can be applied to different types of learning problems.  In particular, we consider its application in a semi-supervised learning setting and present two Bayesian nonlinear semi-supervised learning methods, one of which is a nonlinear Gaussian process method and the other is a nonlinear transductive discriminant analysis method.  Classification and clustering experiments are performed on benchmark data sets with encouraging results.

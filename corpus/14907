Generalization Bounds for Convex Combinations of Kernel Functions
 Abstract We derive new bounds on covering numbers for hypothesis classes generated by convex combinations of basis functions.  These are useful in bounding the generalization performance of algorithms such as RBF-networks, boosting and a new class of linear programming machines similar to SV machines.  We show that p-convex combinations with p ? 1 lead to diverging bounds, whereas for p = 1 good bounds in terms of entropy numbers can be obtained.  In the case of kernel expansions, significantly better bounds can be obtained depending on the eigenvalues of the corresponding integral operators.

Leading Best-Response Strategies in Repeated Games
 Abstract In repeated general-sum games, an agent using a \best response" strategy maximizes its own payoff assuming its behavior has no effect on its opponent.  This notion of best response requires some degree of learning to determine the fixed opponent behavior.  Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a \follower," since it adapts to its opponent.  However, pairing two best-response agents in a repeated game can result in suboptimal behavior.  We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy.  We then examine two \leader" strategies that induce better performance from opponent followers via stubbornness and threats.

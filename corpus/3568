Spiking Boltzmann Machines
 Abstract We first showhow to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions.  Then we showhow the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately.  Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generativemodel.  1 Population codes and energy landscapes A perceived object is represented in the brain by the activities of many neurons, but there is no general consensus on how the activities of individual neurons combine to represent the multiple properties of an object.  We start by focussing on the case of a single object that has multiple instantiation parameters such as position, velocity, size and orientation.  We assume that each neuron has an ideal stimulus in the space of instantiation parameters and that its activation rate or probability of activation falls off monotonically in all directions as the actual stimulus departs from this ideal.  The semantic problem is to define exactly what instantiation parameters are being represented when the activities of manysuch neurons are specified.  Hinton, Rumelhart and McClelland (1986) consider binary neurons with receptive fields that are convex in instantiation space.  They assume that when an object is presentitactivates all of the neurons in whose receptive fields its instantiation parameters lie.  Consequently, if it is known that only one object is present, the parameter values of the object must lie within the feasible region formed by the intersection of the receptive fields of the active neurons.  This will be called a conjunctive distributed representation.  Assuming that each receptive field occupies only a small fraction of the whole space, an interesting property of this type of ``coarse coding" is that the bigger the receptive fields, the more accurate the representation.  However, large receptive fields lead to a loss of resolution when several objects are presentsimultaneously.  When the sensory input is noisy,itis impossible to infer the exact parameters of objects so it makes sense for a perceptual system to represent the probabilitydistribution across parameters rather than just a single best estimate or a feasible region.  The full probability distribution is essential for correctly combining inforE(x) P(x) Figure 1: a) Energy landscape over a onedimensional space.  Each neuron adds a dimple (dotted line) to the energy landscape (solid line).  b) The corresponding probability density.  Where dimples overlap the corresponding probability density becomes sharper.  Since the dimples decay to zero, the location of a sharp probability peak is not affected by distantdimples and multimodal distributions can be represented.  mation from different times or different sources.  One obvious way to represent this distribution (Anderson and van Essen, 1994) is to allow each neuron to represent a fairly compact probability distribution over the space of instantiation parameters and to treat the activity levels of neurons as (unnormalized) mixing proportions.  The semantics of this disjunctive distributed representation is precise, but the percepts it allows are not because it is impossible to represent distributions that are sharper than the individual receptive fields and, in high-dimensional spaces, the individual fields must be broad in order to cover the space.  Disjunctive representations are used in Kohonen's self-organizing map which is why it is restricted to very low dimensional latent spaces.  The disjunctive model can be viewed as an attempt to approximate arbitrary smooth probability distributions by adding together probability distributions contributed byeach active neuron.  Coarse coding suggests a multiplicative approachinwhich the addition is done in the domain of energies (negative log probabilities).  Each active neuron contributes an energy landscape over the whole space of instantiation parameters.  The activity level of the neuron multiplies its energy landscape and the landscapes for all neurons in the population are added (Figure 1).  If, for example, each neuron has a full covariance Gaussian tuning function, its energy landscape is a parabolic bowl whose curvature matrix is the inverse of the covariance matrix.  The activitylevel of the neuron scales the inverse covariance matrix.  If there are k instantiation parameters then only k + k(k +1)=2realnumbers are required to span the space of means and inverse covariance matrices.  So the real-valued activities of O(k 2 ) neurons are sufficient to represent arbitrary full covariance Gaussian distributions over the space of instantiation parameters.  Treating neural activities as multiplicative coefficients on additivecontributions to energy landscapes has a number of advantages.  Unlike disjunctive codes, vague distributions are represented bylow activities so significantbiochemical energy is only required when distributions are quite sharp.  A central operation in Bayesian inference is to combine a prior term with a likelihood term or to combine two conditionally independent likelihood terms.  This is trivially achieved by adding two energy landscapes 1 .  1 We thank Zoubin Ghahramani for pointing out that another important operation, convolving a probability distribution with Gaussian noise, is a difficult non-linear operation on the energy landscape.

The wake-sleep algorithm for unsupervised neural networks
 Abstract We describe an unsupervised learning algorithm for a multilayer network of stochastic neurons.  Bottom-up "recognition" connections convert the input into representations in successive hidden layers and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above.  In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below.  In the "sleep" phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.  Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections.  The wake-sleep algorithm finesses both these problems.  When there is no teaching signal to be matched, some other goal is required to force the hidden units to extract underlying structure.  In the wake-sleep algorithm the goal is to learn representations that are economical to describe but allow the input to be reconstructed accurately.  Each input vector could be communicated to a receiver by first sending its hidden representation and then sending the difference between the input vector and its top-down reconstruction from the hidden representation.  The aim of learning is to minimize the "description length" which is the total number of bits that would be required to communicate the input vectors in this way [1].  No communication actually takes place, but minimizing the description length that would be required forces the network to learn economical representations that capture the underlying regularities in the data [2].  The neural network has two quite different sets of connections.  The bottom-up "recognition" connections are used to convert the input vector into a representation in one or more layers of hidden units.  The top-down "generative" connections are then used to reconstruct an approximation to the input vector from its underlying representation.  The training algorithm for these two sets of connections can be used with many different types of stochastic neuron, but for simplicity we use only stochastic binary units that have states of or # .  The state of unit # is ### and the probability that it is on is: ### # ### fiff # 25510 - ##### # ##### # ##### # # (1) where # # is the bias of the unit and # # # is the weight on a connection from unit # .  Sometimes the units are driven by the generative weights and other times by the recognition weights, but the same equation is used in both cases.  In the "wake" phase the units are driven bottom-up using the recognition weights, producing a representation of the input vector in the first hidden layer, a representation of this representation in the second hidden layer and so on.  All of these layers of representation combined are called the "total representation" of the input, and the binary state of each hidden unit, , in total representation ! is fiff" # .  This total representation could be used to communicate the input vector, $ , to a receiver.  According to Shannon's coding theorem, it requires #&%('*),+ bits to communicate an event that has probability + under a distribution agreed by the sender and receiver.  We assume that the receiver knows the top-down generative weights [3] so these can be used to create the agreed probability distributions required for communication.  First, the activity of each unit, - , in the top hidden layer is communicated using the distribution #. # "/10 ### "/ # which is obtained by applying Eq.  1 to the single generative bias weight of unit - .  Then the activities of the units in each lower layer are communicated using the distribution #(# " # 0 #2# " # # obtained by applying Eq.  1 to the already communicated activities in the layer above, # " / , and the generative weights, # / # .  The description length of the binary state of unit is: 3 # # " # # # # # " # %('*)4# " # #5# # # " # # %('*)6# #7# " # # (2) The description length for input vector $ using the total representation ! is simply the cost of describing all the hidden states in all the hidden layers plus the cost of describing the input vector given the hidden states 3 # ! 0 $ # # 3 # ! #8# 3 # $#9 ! # #;:<}=@?A: # =B< 3 # # " # #8# :DC 3 # #DE C 9F! # (3)

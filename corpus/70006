On the Empirical State-Action Frequencies in Markov Decision Processes Under General Policies
 Abstract We consider the empirical state-action frequencies and the empirical reward in weakly communicating finite-state Markov decision processes under general policies.  We define a certain polytope and establish that every element of this polytope is the limit of the empirical frequency vector, under some policy, in a strong sense.  Furthermore, we show that the probability of exceeding a given distance between the empirical frequency vector and the polytope decays exponentially with time, under every policy.  We provide similar results for vector-valued empirical rewards.

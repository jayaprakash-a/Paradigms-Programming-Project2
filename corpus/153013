Variational methods for approximate reasoning in graphical models
 Abstract Exact inference in large and complex graphical models (e. g.  Bayesian networks) is computationally intractable.  Approximate schemes are therefore of great importance for real world computation.  In this paper we consider a general scheme in which the original intractable graphical model is approximated by a model with a tractable structure.  The approximating model is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two models.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  The scheme provides a bridge between naive mean-field theory and exact computation.  Simulation results are provided to illustrate the method.

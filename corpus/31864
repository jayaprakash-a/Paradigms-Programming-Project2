New Distributed Probabilistic Language Models
 Abstract Our previous work on statistical language modeling introduced the use of probabilistic feedforward neural networks with shared parameters in order to help dealing with the curse of dimensionality.  This work started with the motivation to speed up the above model and to take advantage of prior knowledge e. g. , in WordNet or in syntactically labeled data sets, and to better deal with polysemy.  With the objective of reaching these goals, we present here a series of new statistical language models, most of which are yet untested.

Stochastic Encoding and the "Bits-Back" Argument
 Abstract The Minimum Description Length framework is powerful but is often overlooked.  I believe that one reason for this is that methods for attaining efficient encodings are subtle.  In this paper, I discuss one of those techniques, stochastic encoding.  When there are multiple nearly equally valuable choices of a parameter, it is more valuable to choose stochastically---according to a probability distribution--- rather than selecting the single best choice.  Why? Because information can be transmitted in which parameter is chosen.  This is exactly the "bitsback" argument given by Hinton and Zemel in [1].  In the Minimum Description Length (MDL) framework, the objective is to encode the data plus the model with the fewest number of bits possible.  An important advantage to this framework is that the regularizer is simple and innate.  Any complexity of the model must be encoded alongside the data.  Hence, it is of the utmost importance that the model be encoded efficiently.  In particular, the model must be encoded at the proper level of precision.  Some parameters of the model may be encoded with a low degree of precision to achieve the desired benefit, while other parameters will need a high degree of precision.  Designing a code to handle this in a dynamic fashion is not easy.  So, we do something similar to what we do with encoding data.  When encoding data, we don't try to construct a code that actually encodes the data.  That would force us to deal with the discreteness of real codes and the need to adapt the code to different distributions.  Instead, we simply encode based on the uncertainty.  If our model says that a label is highly likely, it takes us little encoding; if our model goofs and declares a label unlikely, we pay by using many bits to encode that label.  We use the encoded model to determine a conditional probability for each label given its example, p(y

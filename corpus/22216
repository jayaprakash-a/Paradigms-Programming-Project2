Sparse Distributed Memories for On-Line Value-Based Reinforcement Learning
 Abstract.  In this paper, we advocate the use of Sparse Distributed Memories (SDMs) for on-line, value-based reinforcement learning (RL).  SDMs provide a linear, local function approximation scheme, designed to work when a very large/ high-dimensional input (address) space has to be mapped into a much smaller physical memory.  We present an implementation of the SDM architecture for on-line, value-based RL in continuous state spaces.  An important contribution of this paper is an algorithm for dynamic on-line allocation and adjustment of memory resources for SDMs, which eliminates the need for choosing the memory size and structure a priori.  In our experiments, this algorithm provides very good performance while efficiently managing the memory resources.

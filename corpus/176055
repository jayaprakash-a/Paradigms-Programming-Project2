Graphical models and variational methods
 1 Abstract We review the use of variational methods of approximating inference and learning in probabilistic graphical models.  In particular, we focus on variational approximations to the integrals required for Bayesian learning.  For models in the conjugate-exponential family, a generalisation of the EM algorithm is derived that iterates between optimising hyperparameters of the distribution over parameters, and inferring the hidden variable distributions.  These approximations make use of available propagation algorithms for probabilistic graphical models.  We give two case studies of how the variational Bayesian approach can be used to learn model structure: inferring the number of clusters and dimensionalities in a mixture of factor analysers, and inferring the dimension of the state space of a linear dynamical system.  Finally, importance sampling corrections to the variational approximations are discussed, along with their limitations.

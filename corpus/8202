Sparse Online Gaussian Processes
 Accepted in Neural Computation Minor corrections included a a The authors acknowledge reader feedbacks Abstract We develop an approach for sparse representations of Gaussian Process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets.  The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the GP model.  By using an appealing parametrisation and projection techniques that use the RKHS norm, recursions for the eective parameters and a sparse Gaussian approximation of the posterior process are obtained.  This allows both for a propagation of predictions as well as of Bayesian error measures.  The significance and robustness of our approach is demonstrated on a variety of experiments.

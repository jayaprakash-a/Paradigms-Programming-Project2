UNSUPERVISED LANGUAGE MODEL ADAPTATION
 ABSTRACT This paper investigates unsupervised language model adaptation, from ASR transcripts.  N-gram counts from these transcripts can be used either to adapt an existing n-gram model or to build an n-gram model from scratch.  Various experimental results are reported on a particular domain adaptation task, namely building a customer care application starting from a general voicemail transcription system.  The experiments investigate the effectiveness of various adaptation strategies, including iterative adaptation and self-adaptation on the test data.  They show an error rate reduction of 3. 9% over the unadapted baseline performance, from 28% to 24. 1%, using 17 hours of unsupervised adaptation material.  This is 51% of the 7. 7% adaptation gain obtained by supervised adaptation.  Self-adaptation on the test data resulted in a 1. 3% improvement over the baseline.

GROSSER SYSTEME ECHTZEITOPTIMIERUNG SCHWERPUNKTPROGRAMM DER DEUTSCHEN FORSCHUNGSGEMEINSCHAFT
 Abstract Unsupervised learning algorithms are designed to extract structure from data without reference to explicit teacher information.  The quality of the infered structure is determined by a quality function which guides the search and validation process of structure in data.  This paper proposes EMPIRICAL RISK APPROXIMATION as a new induction principle for unsupervised learning.  ffl-covers are used to coarsen the hypothesis class of possible structures.  The complexity of the unsupervised learning models are automatically controlled by large deviation bounds.  The maximum entropy principle with deterministic annealing as an efficient search strategy arises from the Empirical Risk Approximation principle as the optimal inference strategy for large learning problems.  Parameter selection of learnable data structures is demonstrated for the case of k-means clustering.  1 What is unsupervised learning? Learning algorithms are designed with the goal in mind that they should extract structure from data.  Two classes of algorithms have been widely discussed in the literature -- supervised and unsupervised learning.  The distinction between the two classes relates to supervision or teacher information which is either available to the learning algorithm or missing in the learning process.  This paper presents a theory of unsupervised learning based on Empirical Risk Approximation (ERA) which generalizes the Empirical Risk Minimization induction principle to describe the approximation of structure in data.  The ERA principle has been developed in analogy to the highly successful statistical learning theory of classification and regression [Vapnik, 1982, Vapnik, 1998].  In supervised learning of classification boundaries or of regression functions the learning algorithm is provided with example points and selects the best candidate function from a set of functions, called the hypothesis class.  Statistical learning theory, developed by Vapnik and Chervonenkis in a series of seminal papers (VC--theory, see [Vapnik, 1982, Vapnik, 1998]), measures the amount of information in a data set which can be used to determine the parameters of the classification or regression models.  Computational learning theory [Valiant, 1984] addresses computational problems of supervised learning in addition to the statistical constraints.  1 In this paper I propose a theoretical framework for unsupervised learning based on optimization of a quality functional R(ff) for structures in the data set (indexed by ff).  The learning algorithm selects the structure which scores best on the sample set X and it averages this structure of minimal empirical risk ^ R(ff; X ) with structures of statistically indistinguishable quality.  This induction principle is refered to as Empirical Risk Approximation and it is summarized by the following inference steps: 1.  Define a hypothesis class H of structures in the data.  The quality of each candidate structure is measured by a loss function h(x; ff).  2.  Find the minimum of the empirical risk ^ R ? := inf ff ^ R(ff; X ).  3.  Determine an approximation accuracy R \Lambda such that induction becomes reliable (in the sense of statistical learning theory).  4.  Average over all structures whose empirical risk does not exceed the minimum of the empirical risk ^ R ? by more than 2R app .  It is important to note that the ERA principle averages over all structures within the approximation accuracy of the minimal empirical risk rather than selecting the loss function with minimal empirical risk.  This averaging prevents the induction algorithm from extracting a structure which is not typical at the given precision level R \Lambda .  Averaging over loss functions is similar in spirit to the Bayesian inference procedure and resembles the averaging of classifiers in bagging or boosting [

WHOLE-SENTENCE EXPONENTIAL LANGUAGE MODELS: A VEHICLE FOR LINGUISTIC-STATISTICAL INTEGRATION
 ABSTRACT We introduce an exponential language model which models a whole sentence or utterance as a single unit.  By avoiding the chain rule, the model treats each sentence as a "bag of features", where features are arbitrary computable properties of the sentence.  The new model is computationally more efficient, and more naturally suited to modeling global sentential phenomena, than the conditional exponential (e. g.  Maximum Entropy) models proposed to date.  Using the model is straightforward.  Training the model requires sampling from an exponential distribution.  We describe the challenge of applying Monte Carlo Markov Chain (MCMC) and other sampling techniques to natural language, and discuss smoothing and step-size selection.  We then present a novel procedure for feature selection, which exploits discrepancies between the existing model and the training corpus.  We demonstrate our ideas by constructing and analyzing competitive models in the Switchboard domain, incorporating lexical and syntactic information.  1.  MOTIVATION AND OUTLINE Conventional statistical language models estimate the probability of a sentence # by using the chain rule to decompose it into a product of conditional probabilities: Pr ##### def # Pr ################# # # # ### # Pr ### ### ########### ### ### def # # # ### # Pr ### # # # # # where # # def ### # ### ##### # # ### # # is the history when predicting word # # .  The vast majority of work in statistical

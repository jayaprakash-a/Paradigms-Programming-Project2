Planning and Acting in Partially Observable Stochastic Domains
 Abstract In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains.  We begin by introducing the theory of Markov decision processes (mdps) and partially observable mdps (pomdps).  We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a pomdp.  We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.  Consider the problem of a robot navigating in a large office building.  The robot can move from hallway intersection to intersection and can make local observations of its world.  Its actions are not completely reliable, however.  Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots.  It has similar problems with observation.  Sometimes a corridor looks like a corner; sometimes a T-junction looks like an L-junction.  How can such an error-plagued robot navigate, even given a map of the corridors?

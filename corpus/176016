The Variational Bayesian EM Algorithm for Incomplete Data: with Application to Scoring Graphical Model Structures
 SUMMARY We present an efficient procedure for estimating the marginal likelihood of probabilistic models with latent variables or incomplete data.  This method constructs and optimises a lower bound on the marginal likelihood using variational calculus, resulting in an iterative algorithm which generalises the EM algorithm by maintaining posterior distributions over both latent variables and parameters.  We define the family of conjugate-exponential models---which includes finite mixtures of exponential family models, factor analysis, hidden Markov models, linear state-space models, and other models of interest---for which this bound on the marginal likelihood can be computed very simply through a modification of the standard EM algorithm.  In particular, we focus on applying these bounds to the problem of scoring discrete directed graphical model structures (Bayesian networks).  Extensive simulations comparing the variational bounds to the usual approach based on the Bayesian Information Criterion (BIC) and to a sampling-based gold standard method known as Annealed Importance Sampling (AIS) show that variational bounds substantially outperform BIC in finding the correct model structure at relatively little computational cost, while approaching the performance of the much more costly AIS procedure.  Using AIS allows us to provide the first serious case study of the tightness of variational bounds.  We also analyse the perfomance of AIS through a variety of criteria, and outline directions in which this work can be extended.

Using the Perceptron Algorithm to Find Consistent Hypotheses
 Abstract The perceptron learning algorithm yields quite naturally an algorithm for finding a linearly separable boolean function consistent with a sample of such a function.  Using the idea of a specifying sample, we give a simple proof that this algorithm is not efficient, in general.  A boolean function t defined on {0, 1} n is linearly separable if there are # 2 R n and # 2 R such that t(x) = ( 1 if h#, xi # # 0 if h#, xi < #, where h#, xi is the standard inner product of # and x.  Given such # and #, we say that t is represented by [#, #] and we write t [#, #].  The vector # is known as the weight-vector, and # is known as the threshold.  This class of functions is the set of functions computable by the simple boolean perceptron (see [8, 9, 6]), and we shall denote it by BP n .  We now give a fleeting description of the perceptron learning algorithm, and refer to [6, 1] for more details.  For any learning constant # } 0, we have the perceptron learning algorithm L # , devised by Rosenblatt [8, 9], which acts sequentially as follows.  Let t be any function in BP n , which may be thought of as the target.  The algorithm L # maintains at each stage a current hypothesis, which is updated on the basis of an example in {0, 1} n , presented together with its classification t(x).  (The initial hypothesis is some fixed `simple' hypothesis.  We shall take the initial hypothesis to have the all-0 vector as weight-vector, and threshold 0. ) Suppose the current hypothesis is h [#, #] and that an example x is presented.  Then the new hypothesis is h 0 [# 0 , # 0 ] where # 0 = # + # (t(x) - h(x)) x, # 0 = # - # (t(x) - h(x)) .  The Perceptron Convergence Theorem [8, 6] asserts that no matter how many examples are presented, the algorithm makes only a finite number of changes, or updates (provided #, which can be a function of n, is small enough).  As indicated in [3], given t 2 BP n and a sample x = (x 1 , x 2 , .  .  .  , xm ) of examples, we may use L # to find a linearly separable boolean function which agrees with t on x---that is, which is consistent with t on x.  We simply keep cycling through x 1 to xm in turn, until no updates are made in a complete cycle.  Thus, the perceptron algorithm (for any learning constant #) can be used as a consistent-hypothesis-finder (using terminology from [3]).  A natural question is whether this is an efficient means of finding a consistent function.  In fact, it is not, in the sense that the number of complete cycles required can be exponential in m, the size of the sample.  This result appears to be accepted, but we have been unable to find a proof of it in the literature.  We note that this is a very different result from those presented by Minsky and Papert[6] and Hampson and Volper [4] in their studies of the perceptron learning algorithm.  Their results show that when the perceptron learning algorithm is used as an exact learning algorithm, the running time can be exponential in n, the domain dimension.  Our result shows that, for fixed n, the running time of the related consistent-hypothesis-finder can be exponential in m, the number of examples presented.  We remark that there is a polynomial time consistent-hypothesis-finder for BP n : rephrase the problem as a linear programme and use Karmarkar's algorithm (see [3]).  Thus the problem of finding a consistent hypothesis has no intrinsic difficulty.  We shall consider the boolean function f 2n of 2n variables with formula f 2n = u 2n ^ (u 2n- 1 _ (u 2n- 2 ^ (u 2n- 3 _ (.

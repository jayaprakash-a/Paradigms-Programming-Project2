Hierarchical Memory-Based Reinforcement Learning
 Abstract A key challenge for reinforcement learning is how to scale up to large partially observable domains.  In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task.  At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time.  We formalize this idea in a framework for solving partially observable, sequential decision tasks called Hierarchical Short-Term Memory (HSM).  HSM uses a memory-based SMDP Qlearning method to rapidly propagate delayed reward across long decision sequences.  We show that the HSM framework outperforms several related reinforcement learning techniques on a realistic corridor navigation task.

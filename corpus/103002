CONSONANT DISCRIMINATION IN ELICITED AND SPONTANEOUS SPEECH: A CASE FOR SIGNAL-ADAPTIVE FRONT ENDS IN ASR
 ABSTRACT The constant frame length in typical ASR front ends is too long to capture transient phenomena in speech, such as stop bursts.  However, current HMM systems have consistently outperformed systems based solely on non-uniform units.  This work investigates an approach to "add back" such transient information to a speech recognizer, without losing the robustness of the standard acoustic models.  We demonstrate a set of phonetically-motivated acoustic features that discriminate a preliminary test set of highly ambiguous voiceless stops in CV contexts.  The features are automatically computed from data that had been hand-marked for consonant burst location and voicing onset (extension to automatic marking is also proposed).  Two corpora are processed using a parallel set of features: conversational speech over the telephone (Switchboard), and a corpus of carefully elicited speech.  The latter provides an upper bound on discrimination, and allows for comparison of feature usage across speaking style.  We explore data-driven approaches to obtaining variable-length time-localized features compatible with an HMM statistical framework.  We also suggest techniques for extension to automatic annotation of burst location, for computation of features at such points, and for augmentation of an HMM system with the added information.

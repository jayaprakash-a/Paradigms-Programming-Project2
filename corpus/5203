Learning Arbitrary State Transition Kernels for Prediction
 Abstract Predicting future states of noisy dynamical systems requires efficient inference algorithms that are robust to uncertainty.  Markovian state-space models are especially effective at such tasks, allowing realtime inference and efficient training from data.  Most prior work focuses on learning state-space models where state densities and dynamics have analytically tractable forms.  We present a model that learns Markov processes with arbitrary state transition kernels and non-Gaussian, multi-modal state densities to predict the temporal evolution of nonlinear Markovian systems.  The model uses radial basis function approximators to represent predictive state transition kernels.  Conjugate gradient forms the basis of a generalized expectation-maximization algorithm for unsupervised learning of the function approximators.  A particle-based state representation admits non-Gaussian state distributions.  Particle smoothing estimates maximum a posteriori state sequences for model inference.  After learning, the model can be used for prediction, filtering, or fixed-lag smoothing.  We demonstrate the model's robustness by predicting nonlinear, non-Gaussian state sequences from artificially generated noisy time-series data.

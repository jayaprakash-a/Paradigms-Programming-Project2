The Information Of Observations And Application For Active Learning With Uncertainty
 Abstract A fundamental problem in learning theory is bounding the information gained by an example about the unknown target concept.  This problem is most critical in the context of active learning, when the learner has to select the most informative examples to be labled in order to minimize the number of lables required for good generalization.  The Mutual Information allows one to measure the average knowledge one gains by knowing the value of one random variable A about a different random variable, B.  However, in concrete learning cases one is interested in a more precise measure, namely, how much does a speci#c value a tells one about B.  Different observations, or examples, give different amount of information about the unknown status of the world, or about the underlying labeling rule.  Here we present an information measure which quantifies the amount of information an observation gives about the state of the world.  We show that with high probability this specific mutual information is bounded by the logarithm of the \covering number of the world" or the \concept class" in the learning context, and establish a version of the Information Processing Inequality suitable for this quantity.  In the second part of the paper we apply our bound to active learning schemes in the presence of uncertainty.  Assume furthermore that the labels one sees are corrupted, either due to noise or by some uncertainty of the model.  We present a generalization of the Query By Committee algorithm and use the information measure presented in the first part to prove its eciency.

A Unifying Review of Linear Gaussian Models
 nonlinearity.  Through the use of other nonlinearities we show how independent component analysis (ICA) is also a variation of the same basic generative model.  We show that factor analysis and mixtures of Gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term.  We introduce a new model for static data known as sensible principal component analysis (SPCA) as well as a novel concept of spatially adaptive observation noise.  We also review some of the literature involving global and local mixtures of the basic models and provide pseudo-code for inference and learning for all the basic models.  1 A Unifying Review Many common statistical techniques for modeling multidimensional static datasets and multidimensional time series can be seen as variants of one underlying model.  As we will show later in the paper, these include factor analysis (FA), principal component analysis (PCA), mixtures of Gaussian clusters, vector quantization (VQ), independent component analysis models (ICA), Kalman filter models (a. k. a.  linear dynamical systems) and hidden Markov models (HMMs).  The relationships between some of these models has been noted in passing in the recent literature.  For example, Hinton et al.  (1995) note that factor analysis and PCA are closely related and Digalakis et al.  (1993) relate the forward--backward algorithm for HMMs to Kalman filtering.  In this paper we unify many of the disparate observations made by previous authors (Rubin and Thayer, 1982; Delyon, 1993; Digalakis et al. , 1993; Hinton et al. , 1995; Elliott et al. , 1995; Ghahramani and Hinton, 1996a, 1996b, 1997; Hinton and Ghahramani, 1997) and present a review of all these algorithms as instances of a single basic generative model.  This unified view allows us to show some interesting relations between previously disparate algorithms.  For example, factor analysis and mixtures of Gaussians can be implemented using autoencoder neural networks with different nonlinearities but learned using a squared error cost penalized by the same regularization term.  ICA can be seen as a nonlinear version of factor analysis.  The framework also makes it possible to derive a new model for static data which is based on PCA but has a sensible probabilistic interpretation as well as a novel concept of spatially adaptive observation noise.  We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode (in the appendix) for inference and learning for all the basic models.  2 The Basic Model The basic models we will work with are discrete time linear dynamical systems with Gaussian noise.  In such models we assume that the state of the process in question can at any time be summarized by a k-vector of state variables or causes x which we cannot observe directly.  However, the system also produces at each time step an output or observable p-vector y to which we do have access.  The state x is assumed to evolve according to simple first-order Markov dynamics; each output vector y is generated from the current state by a simple linear observation process.  Both the state evolution and the observation processes are corrupted by additive Gaussian noise which is also hidden.  If we work with a continuous valued state variable x, the basic generative model can be written 1 as: x t+1 = Ax t +w t = Ax t +w ffl w ffl s N (0; Q) (1a) y t = Cx t + v t = Cx t + v ffl v ffl s N (0; R) (1b) where A is the k \Theta k state transition matrix and C is the p \Theta k observation, measurement, or generative matrix.  The k-vector w and p-vector v are random variables representing the state evolution and observation noises respectively which are independent of each other and of the values of x and y.  1 All vectors are column vectors.  To denote the transpose of a vector or matrix we use the notation x T .  The determinant of a matrix is denoted by jAj and matrix inversion by A\Gamma 1 .  The symbol s means "distributed according to".  A multivariate normal (Gaussian) distribution with mean and covariance matrix \Sigma is written as N (; \Sigma).  The same Gaussian evaluated at the point z is denoted N (; \Sigma) j z .

Learning Color Constancy
 Abstract We decided to test a surprisingly simple hypothesis; namely, that the relationship between an image of a scene and the chromaticity of scene illumination could be learned by a neural network.  The thought was that if this relationship could be extracted by a neural network, then the trained network would be able to determine a scene's illumination from its image, which would then allow correction of the image colors to those relative to a standard illuminant, thereby providing color constancy.  Using a database of surface reflectances and illuminants, along with the spectral sensitivity functions of our camera, we generated thousands of images of randomly selected illuminants lighting `scenes' of 1 to 60 randomly selected reflectances.  During the learning phase the network is provided the image data along with the chromaticity of its illuminant.  After training, the network outputs (very quickly) the chromaticity of the illumination given only the image data.  We obtained surprisingly good estimates of the ambient illumination lighting from the network even when applied to scenes in our lab that were completely unrelated to the training data.  Descriptive Summary Existing color constancy algorithms [1], [2] , [3], [4], [9], [6], [8] generally employ assumptions about either the surface reflectances that will occur in a scene or about the possible spectral power distributions of scene illuminants.  Given the assumptions and 3-band image data (either CIE XYZ specification or camera RGB) these algorithms calculate the chromaticity of the unknown scene illumination.  If the assumptions are satisfied --which generally they are not -- the estimate of the illumination will be correct and can then be used to adjust the image data so that the image would be the same as if had been taken under some standard, known illuminant.  To the extent that the adjusted colors are as they would have been under the standard illuminant, the system can be said to exhibit `color constancy'.  In contrast, the neural network we have developed has no built-in constraints.  It is an adaptive model that makes no explicit assumptions about the input data.  All rules are implicitly learned from the training set, which contains a large number of (artificially generated) scenes.  The experimental results (see below) show that the neural network outperforms the grey-world and white-patch algorithms, especially in the case of scenes containing a small number (1 to 5) of distinct RGB measurements (Since 'color' is a perceptual quality, in what follows we'll avoid using it and instead simply use RGB to mean the response of the camera at a given pixel).  Good performance with only a small number of distinct RGB's means that the network is particularly well suited for processing small, local image regions.  This is important because generally a scene contains more than one source of light, so the assumption that the scene illumination is constant will, at best, hold true only locally within an image.

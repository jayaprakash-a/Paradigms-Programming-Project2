Anytime Interval-Valued Outputs for Kernel Machines: Fast Support Vector Machine Classification via Distance Geometry
 Abstract Classifying M query examples using a support vector machine containing L support vectors traditionally requires exactly M # L kernel computations.  We introduce a computational geometry method for which classi#cation cost becomes roughly proportional to each query's diculty (e. g.  distance from the discriminant hyperplane).  It produces exactly the same classifications, while typically requiring vastly fewer kernel computations.  Related \reduced set" methods (e. g.  (Burges, 1996)) similarly lower the effective L, but provide neither proportionality with diculty nor guaranteed preservation of classi#cations.  Experiments on UCI and NASA data illustrate 2 to 64-fold speedups, across both SVMs and kernel Fisher discriminants.

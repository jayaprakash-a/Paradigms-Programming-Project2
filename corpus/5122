Incorporating Background Invariance into Feature-Based Object Recognition
 Abstract Current feature-based object recognition methods use information derived from local image patches.  For robustness, features are engineered for invariance to various transformations, such as rotation, scaling, or affine warping.  When patches overlap object boundaries, however, errors in both detection and matching will almost certainly occur due to inclusion of unwanted background pixels.  This is common in real images, which often contain significant background clutter, objects which are not heavily textured, or objects which occupy a relatively small portion of the image.

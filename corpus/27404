Knowing What Doesn't Matter: Exploiting Omitted Superfluous Data \Lambda
 Abstract Most inductive inference algorithms (i. e. , "learners") work most effectively when their training data contain completely specified labeled samples.  In many diagnostic tasks, however, the data will include the values of only some of the attributes; we model this as a blocking process that hides the values of those attributes from the learner.  While blockers that remove the values of critical attributes can handicap a learner, this paper instead focuses on blockers that remove only superfluous attribute values, i. e. , values that are not needed to classify an instance, given the values of the other unblocked attributes.  We first motivate and formalize this model of "superfluous-value blocking," and then demonstrate that these omissions can be useful, by showing that certain classes that seem hard to learn in the general PAC model --viz. , decision trees --- are trivial to learn in this setting, and can even be learned in a manner that is very robust to classification noise.  We also discuss how this model can be extended to deal with (1) theory revision (i. e. , modifying an existing decision tree); (2) "complex" attributes (which correspond to combinations of other atomic attributes); (3) blockers that occasionally include superfluous values or exclude required values; and (4) other hypothesis classes (e. g. , DNF formulae).

Ensemble Learning - an alternative Bayesian approach to Neural Networks --DRAFT 1
0.  Abstract.  Bayesian treatments of learning in neural networks are typically based either on local Gaussian approximations to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations.  A third approach, called ensemble learning, was introduced by Hinton and van Camp (1993).  It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution.  However, the derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and so was unable to capture the posterior correlations between parameters.  We show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable.  We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure.  One of the benefits of our approach is that it yields a strict lower bound on the model likelihood, in contrast to other approximate procedures.

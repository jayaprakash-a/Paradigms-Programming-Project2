Meta-learning for text classification
 Abstract Many text classification algorithms, including multinomial and multivariate Bernoulli naive Bayes and most TFIDF variants (such as "probabilistic TFIDF"), make classification predictions by computing an inner product between a test document vector and a parameter vector.  Here, the parameter vector is computed as some simple, closed-form, function g of the training set statistics.  Much research work in text classification consists of manually trying to find better functions g, and indeed, the last few decades have seen numerous heuristic proposals for new g.  In this paper, we propose an algorithm for automatically learning this function from data.  The g found by our algorithm then defines a new learning algorithm for text classification, which we can apply to novel classification tasks.  We show that our learned learning algorithm outperforms naive Bayes and TFIDF on a variety of multiclass text classification tasks.

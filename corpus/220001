Learning, Regularization and Ill-Posed Inverse Problems
 Abstract Many works have recently shown that strong connections relate learning from examples and kernel methods to regularization techniques for ill-posed inverse problems.  Nevertheless by now there was no clear evidence neither that learning from examples could be seen as an inverse problem nor that theoretical results (like consistency) in learning theory could be independently derived using tools from regularization theory for ill-posed inverse problems.  In this paper we provide a positive answer to both the above questions.  First, considering the square loss, we define a linear direct problem and the corresponding linear inverse problem translating the learning problem in the language of regularization theory.  Second, considering the discretization problem of a possibly ill-posed linear inverse problem we show that consistency results and optimal regularization parameter choice can be easily derived.

An Algebraic Approach to Abstraction in Reinforcement Learning
 Abstract To operate effectively in complex environments learning agents have to selectively ignore irrelevant details by forming useful abstractions.  In this article we outline a formulation of abstraction for reinforcement learning approaches to stochastic sequential decision problems modeled as semiMarkov Decision Processes (SMDPs).  Building on existing algebraic approaches, we propose the concept of SMDP homomorphism and argue that it provides a useful tool for a rigorous study of abstraction for SMDPs.  We apply this framework to different classes of abstractions that arise in hierarchical systems and discuss relativized options, a framework for compactly specifying a related family of temporally-extended actions.  Additional details of this work are described in refs.  [1, 2, 3].

Linear regression with errors in both variables: A proper Bayesian approach
 Abstract Linear regression with errors in both variables is a common modeling problem with a 100-year literature, yet we have still not achieved the widespread use of a complete and correct solution.  Much of this is due to a confusion between joint and conditional modeling and an unhealthy aversion to priors.  This paper expands on the proper Bayesian method of Zellner (1971) and Gull (1989), deriving specific parameter estimators and giving an analysis of their performance.  They are shown to perform favorably compared to total least squares.  The main findings are: ffl Linear regression, which is a conditional linear model, is different from principal components analysis, which is a joint linear model, ffl It is important to use a proper prior on both the variance of the noise and the value of the true inputs, ffl Special attention needs to be paid to the case when the scatter of the data is small compared to the noise, which can happen when inputs are correlated, ffl Maximum-likelihood estimation can be unreliable on this problem; it is better to use a Bayesian parameter estimate, and ffl This estimate provides a desirable shrinkage effect that countless penalized regression methods have been searching for.

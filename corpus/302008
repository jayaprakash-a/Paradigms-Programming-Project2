On the Complexity of Learning for a Spiking Neuron
 Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity.  They provide a way of analyzing neural computation that is not captured by the traditional neuron models such as sigmoidal and threshold gates (or "Perceptrons").  We introduce a simple model of a spiking neuron that, in addition to the weights that model the plasticity of synaptic strength, also has variable transmission delays between neurons as programmable parameters.  For coding of input and output values two modes are taken into account: binary coding for the Boolean and analog coding for the real-valued domain.  We investigate the complexity of learning for a single spiking neuron within the framework of PAC-learnability.  With regard to sample complexity, we prove that the VC-dimension is \Theta(n log n) and, hence, strictly larger than that of a threshold gate.  In particular, the lower bound holds for binary coding and even if all weights are kept fixed.  The upper bound is valid for the case of analog coding if weights and delays are programmable.  With regard to computational complexity, we show that there is no polynomial-time PAClearning algorithm, unless RP = NP, for a quite

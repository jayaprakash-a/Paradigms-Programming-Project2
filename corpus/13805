A Probabilistic Upper Bound on Differential Entropy
 Abstract--- The differential entropy is a quantity employed ubiquitously in communications, statistical learning, physics, and many other fields.  We present, to our knowledge, the first non-trivial probabilistic upper bound on the entropy of an unknown one-dimensional distribution, given the support of the distribution and a sample from that distribution.  The bound is completely general in that it does not depend in any way on the form of the unknown distribution (among other things, it does not require that the distribution have a density).  Our bound uses previous distribution-free bounds on the cumulative distribution function of a random variable given a sample of that variable.  We provide a simple, fast, and intuitive algorithm for computing the entropy bound from a sample.

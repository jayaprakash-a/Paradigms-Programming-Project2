Maximum Likelihood and Minimum Classification Error Factor Analysis for Automatic Speech Recognition
 Abstract Hidden Markov models (HMMs) for automatic speech recognition rely on high dimensional feature vectors to summarize the short-time properties of speech.  Correlations between features can arise when the speech signal is non-stationary or corrupted by noise.  We investigate how to model these correlations using factor analysis, a statistical method for dimensionality reduction.  Factor analysis uses a small number of parameters to model the covariance structure of high dimensional data.  These parameters can be chosen in two ways: (i) to maximize the likelihood of observed speech signals, or (ii) to minimize the number of classification errors.  We derive an Expectation-Maximization (EM) algorithm for maximum likelihood estimation and a gradient descent algorithm for improved class discrimination.  Speech recognizers are evaluated on two tasks, one small-sized vocabulary (connected alpha-digits) and one medium-sized vocabulary (New Jersey town names).  We find that modeling feature correlations by factor analysis leads to significantly increased likelihoods and word accuracies.  Moreover, the rate of improvement with model size often exceeds that observed in conventional HMMs.

On the Convergence of Stochastic Iterative Dynamic Programming Algorithms
 Abstract Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments.  These algorithms, including the TD( ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP).  In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem.  The theorem establishes a general class of convergent algorithms to which both TD( ) and Q-learning belong.  An important component of many real world learning problems is the temporal credit assignment problem---the problem of assigning credit or blame to individual components of a temporally-extended plan of action, based on the success or failure of the plan as a whole.  To solve such a problem, the learner must be equipped with the ability to assess the long-term consequences of particular choices of action and must be willing to forego an immediate payoff for the prospect of a longer term gain.  Moreover, because most real world problems involving prediction of the future consequences of actions involve substantial uncertainty, the learner must be prepared to make use of a probability calculus for assessing and comparing actions.  There has been increasing interest in the temporal credit assignment problem, due principally to the development of learning algorithms based on the theory of dynamic programming (DP) (Barto, Sutton,

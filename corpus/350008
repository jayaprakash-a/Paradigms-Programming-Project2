Learning Distance Functions for Image Retrieval
 Abstract Image retrieval critically relies on the distance function used to compare a query image to images in the database.  We suggest to learn such distance functions by training binary classifiers with margins, where the classifiers are defined over the product space of pairs of images.  The classifiers are trained to distinguish between pairs in which the images are from the same class and pairs which contain images from different classes.  The signed margin is used as a distance function.  We explore several variants of this idea, based on using SVM and Boosting algorithms as product space classifiers.  Our main contribution is a distance learning method which combines boosting hypotheses over the product space with a weak learner based on partitioning the original feature space.  The weak learner used is a Gaussian mixture model computed using a constrained EM algorithm, where the constraints are equivalence constraints on pairs of data points.  This approach allows us to incorporate unlabeled data into the training process.  Using some benchmark databases from the UCI repository, we show that our margin based methods significantly outperform existing metric learning methods, which are based on learning a Mahalanobis distance.  We then show comparative results of image retrieval in a distributed learning paradigm, using two databases: a large database of facial images (YaleB), and a database of natural images taken from a commercial CD.  In both cases our GMM based boosting method outperforms all other methods, and its generalization to unseen classes is superior.

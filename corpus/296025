The Structure of Version Space
 Abstract We investigate the generalisation performance of consistent classifiers, i. e.  classifiers that are contained in the so-called version space, both from a theoretical and experimental angle.  In contrast to classical VC analysis --- where no single classifier within version space is singled out on grounds of a generalisation error bound --- the data dependent structural risk minimisation framework suggests that there is one particular classifiers that is to be preferred because it minimises the generalisation error bound.  This is usually taken to provide a theoretical justification for learning algorithms such as the well known support vector machine.  A reinterpretation of a recent PAC-Bayesian result, however, reveals that given a suitably chosen hypothesis space there is a huge number of classifiers with small generalisation error albeit we cannot identify them for a specific learning task.  This result is complemented with an empirical study for kernel classifiers on the task of handwritten digit recognition which demonstrates that even classifiers with a small margin exhibit excellent generalisation.

Natural Language Grammar Induction Using a Constituent-Context Model
 Abstract This paper presents a novel approach to the unsupervised learning of syntactic analyses of natural language text.  Most previous work has focused on maximizing likelihood according to generative PCFG models.  In contrast, we employ a simpler probabilistic model over trees based directly on constituent identity and linear context, and use an EM-like iterative procedure to induce structure.  This method produces much higher quality analyses, giving the best published results on the ATIS dataset.  1 Overview To enable a wide range of subsequent tasks, human language sentences are standardly given tree-structure analyses, wherein the nodes in a tree dominate contiguous spans of words called constituents, as in figure 1(a).  Constituents are the linguistically coherent units in the sentence, and are usually labeled with a constituent category, such as noun phrase (NP) or verb phrase (VP).  An aim of grammar induction systems is to figure out, given just the sentences in a corpus S, what tree structures correspond to them.  In this sense, the grammar induction problem is an incomplete data problem, where the complete data is the corpus of trees T , but we only observe their yields S.  This paper presents a new approach to this problem, which gains leverage by directly making use of constituent contexts.  It is an open problem whether entirely unsupervised methods can produce linguistically accurate parses of sentences.  Due to the difficulty of this task, the vast majority of statistical parsing work has focused on supervised learning approaches to parsing, where one uses a treebank of fully parsed sentences to induce a model which parses unseen sentences [7, 3].  But there are compelling motivations for unsupervised grammar induction.  Building supervised training data requires considerable resources, including time and linguistic expertise.  Investigating unsupervised methods can shed light on linguistic phenomena which are implicit within a supervised parser's supervisory information (e. g. , unsupervised systems often have difficulty correctly attaching subjects to verbs above objects, whereas for a supervised parser, this ordering is implicit in the supervisory information).  Finally, while the presented system makes no claims to modeling human language acquisition, results on whether there is enough information in sentences to recover their structure are important data for linguistic theory, where it has standardly been assumed that the information in the data is deficient, and strong innate knowledge is required for language acquisition [4].  S NP NN 1 Factory NNS payrolls VP VBD fell PP IN in NN 2 September Node Constituent Context S NN NNS VBD IN NN # -- # NP NN NNS # -- VBD VP VBD IN NN NNS -- # PP IN NN VBD -- # NN 1 NN # -- NNS NNS NNS NN -- VBD VBD VBD NNS -- IN IN IN VBD -- NN NN 2 NNS IN -- # Empty Context # 0 # -- NN # 1 NN -- NNS # 2 NNS -- VBD # 3 VBD -- IN # 4 IN -- NN # 5 NN -- # Figure 1: Example parse tree with the constituents and contexts for each tree node.  2 Previous Approaches One aspect of grammar induction where there has already been substantial success is the induction of parts-of-speech.  Several different distributional clustering approaches have resulted in relatively high-quality clusterings, though the clusters' resemblance to classical parts-of-speech varies substantially [9, 15].  For the present work, we take the part-ofspeech induction problem as solved and work with sequences of parts-of-speech rather than words.  In some ways this makes the problem easier, such as by reducing sparsity, but in other ways it complicates the task (even supervised parsers perform relatively poorly with the actual words replaced by parts-of-speech).  Work attempting to induce tree structures has met with much less success.  Most grammar induction work assumes that trees are generated by a symbolic or probabilistic context-free grammar (CFG or PCFG).  These systems generally boil down to one of two types.  Some fix the structure of the grammar in advance [12], often with an aim to incorporate linguistic constraints [2] or prior knowledge [13].  These systems typically then attempt to find the grammar production parameters 2 which maximize the likelihood P(S

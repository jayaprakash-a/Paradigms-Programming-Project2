Approximations of Bayesian Networks through KL Minimisation
 Abstract Exact inference in large, complex Bayesian networks is computationally intractable.  Approximate schemes are therefore of great importance for real world computation.  In this paper we consider an approximation scheme in which the original Bayesian network is approximated by another Bayesian network.  The approximating network is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two networks.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  An important question in this scheme is how to choose the structure of the approximating network.  In this paper we show how redundant structures of the approximating model can be pruned in advance.  Simulation results of model selection and model optimisation are provided to illustrate the methods.

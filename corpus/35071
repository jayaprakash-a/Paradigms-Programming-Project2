Inferring Motor Programs from Images of Handwritten Digits
 Abstract We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program.  We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits.  The inferred motor programs can be used directly for digit classification, but they can also be used in other ways.  By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods.  We can also use the motor programs as additional, highly informative outputs which reduce overfitting when training a feed-forward classifier.  1 Overview The idea that patterns can be recognized by figuring out how they were generated has been around for at least half a century [1, 2] and one of the first proposed applications was the recognition of handwriting using a generative model that involved pairs of opposing springs [3, 4].  The "analysis-by-synthesis" approach is attractive because the true generative model should provide the most natural way to characterize a class of patterns.  The handwritten 2's in figure 1, for example, are very variable when viewed as pixels but they have very similar motor programs.  Despite its obvious merits, analysis-by-synthesis has had few successes, partly because it is computationally expensive to invert non-linear generative models and partly because the underlying parameters of the generative model are unknown for most large data sets.  For example, the only source of information about how the MNIST digits were drawn is the images themselves.  We describe a simple generative model in which a pen is controlled by two pairs of opposing springs whose stiffnesses are specified by a motor program.  If the sequence of stiffnesses is specified correctly, the model can produce images which look just like the MNIST digits.  Using a separate network for each digit class, we show that backpropagation can be used to learn a "recognition" network that maps images to the motor programs required to produce them.  An interesting aspect of this learning is that the network creates its own training data, so it does not require the training images to be labelled with motor programs.  Each recognition network starts with a single example of a motor program and grows an "island of competence" around this example, progressively extending the region over which it can map small changes in the image to the corresponding small changes in the motor program (see figure 2).  Figure 1: An MNIST image of a 2 and the additional images that can be generated by inferring the motor program and then adding random noise to it.  The pixels are very different, but they are all clearly twos.  Fairly good digit recognition can be achieved by using the 10 recognition networks to find 10 motor programs for a test image and then scoring each motor program by its squared error in reconstructing the image.  The 10 scores are then fed into a softmax classifier.  Recognition can be improved by using PCA to model the distribution of motor trajectories for each class and using the distance of a motor trajectory from the relevant PCA hyperplane as an additional score.  Each recognition network is solving a difficult global search problem in which the correct motor program must be found by a single, "open-loop" pass through the network.  More accurate recognition can be achieved by using this open-loop global search to initialize an iterative, closed-loop local search which uses the error in the reconstructed image to revise the motor program.  This requires reconstruction errors in pixel space to be mapped to corrections in the space of spring stiffnesses.  We cannot backpropagate errors through the generative model because it is just a hand-coded computer program.  So we learn "generative" networks, one per digit class, that emulate the generator.  After learning, backpropagation through these generative networks is used to convert pixel reconstruction errors into stiffness corrections.  Our final system gives 1. 82% error on the MNIST test set which is similar to the 1. 7% achieved by a very different generative approach [5] but worse than the 1. 51% produced by the best backpropagation networks or the 1. 4% produced by support vector machines [6].  Recognition of test images is quite slow because it uses ten different recognition networks followed by iterative local search.  There is, however, a much more efficient way to make use of our ability to extract motor programs.  They can be treated as additional output labels when using backpropagation to train a single, multi-layer, discriminative neural network.  These additional labels act as a very informative regularizer that reduces the error rate from 1. 51% to 1. 34% in a network with 500 units in the first hidden layer and 1000 units in the second 1 .  This is a new method of improving performance that can be used in conjunction with other tricks such as preprocessing the images, enhancing the training set or using convolutional neural nets [7, 8].  2 A simple generative model for drawing digits The generative model uses two pairs of opposing springs at right angles.  One end of each spring is attached to a frictionless horizontal or vertical rail that is 39 pixels from the center of the image.  The other end is attached to a "pen" that has significant mass.  The springs themselves are weightless and have zero rest length.  The pen starts at the equilibrium position defined by the initial stiffnesses of the four springs.  It then follows a trajectory that is determined by the stiffness of each spring at each of the 16 subsequent time steps in the motor program.  The mass is large compared with the rate at which the stiffnesses change, so the system is typically far from equilibrium as it follows the smooth trajectory.  On each time step, the momentum is multiplied by 0. 9 to simulate viscosity.  A coarse1 Due to the extreme proximity of the NIPS deadline, only two network architectures were tried and this result was obtained by monitoring the real test error during training.  We promise to do it properly later using a validation set to find a good architecture and stopping criterion.

Sparsity and smoothness via the fused lasso
 Abstract The lasso (Tibshirani 1996) penalizes a least squares regression by the sum of the absolute values (L 1 norm) of the coecients.  The form of this penalty encourages sparse solutions, that is, having many coecients equal to zero.  In this paper we propose the \fused lasso", a generalization of the lasso designed for problems with features that come in a natural order.  The fused lasso penalizes both the L 1 norm of the coecients and their successive differences.  Thus it encourages both sparsity of the coecients and sparsity of their differences, that is, local constancy of the coecient profile.  The fused lasso is especially useful when the number of features p # N , the sample size.  The technique is also extended to the \hinge" loss function, that underlies the support vector classifier.  We illustrate the methods on examples from protein mass spectroscopy and gene expression data.

Query learning for maximum information gain in a multi-layer neural network
 In supervised learning, the redundancy contained in random examples can be avoided by learning from queries, where training examples are chosen to be maximally informative.  Using the tools of statistical mechanics, we analyse query learning in a simple multi-layer network, namely, a large tree-committee machine.  The generalization error is found to decrease exponentially with the number of training examples, providing a significant improvement over the slow algebraic decay for random examples.  Implications for the connection between information gain and generalization error in multi-layer networks are discussed, and a computationally cheap algorithm for constructing approximate maximum information gain queries is suggested and analysed.

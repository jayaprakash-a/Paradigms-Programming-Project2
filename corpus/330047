Probabilistic computation in spiking populations
 Abstract As animals interact with their environments, they must constantly update estimates about their states.  Bayesian models combine prior probabilities, a dynamical model and sensory evidence to perform a statistically optimal updating of the states.  These models have proved to be consistent with the results of many diverse psychophysical studies.  However, little is known about the neural representation and manipulation of such Bayesian information, particularly in populations of spiking neurons.  We consider this issue, suggesting a model based on standard neural architecture and activations.  We illustrate the approach on a simple random walk example, and apply it to a sensorimotor integration task that provides a particularly compelling example of dynamic probabilistic computation.  Bayesian models have been used to explain a gamut of experimental results in tasks which require estimates to be derived from multiple sensory cues.  These include a wide range of psychophysical studies of perception; 13, 16 motor action; 7, 14 and decision-making.  3, 5 Central to Bayesian inference is that computations are sensitive to uncertainties about afferent and efferent quantities, arising from ignorance, noise, or inherent ambiguity (e. g. , the aperture problem), and that these uncertainties change over time as information accumulates and dissipates.  Understanding how neurons represent and manipulate uncertain quantities is therefore key to understanding the neural instantiation of these Bayesian inferences.  Most previous work on representing probabilistic inference in neural populations has focused on the representation of static information.  1, 12, 17 These approaches suggest varying strategies for encoding and decoding uncertain quantities, but they do not readily generalize to real-world dynamic information processing tasks, particularly the most interesting cases in which stimulus information changes over the same timescale as spiking itself.  11 Notable exceptions are the recent models proposed by Gold and Shadlen, 5 Rao, 10 and Deneve, 4 all of which make seminal contributions, but make use of probabilistic representations that we argue are comparatively restricted.  We make two contributions in this paper.  First, we show how probabilistic information varying over time can be represented in a spiking population code.  Second, we show the utility of this method by applying it to a temporal sensorimotor integration task.  1 MODEL FORMULATION We frame the problem and our approach in terms of a two-level network, connecting one population of neurons to another; this construction is intended to apply to any level of processing.  The network maps input population spikes R(t) to output population spikes S(t), where input and output evolve over time (we use R(t) and S(t) to indicate the input and output spike trains from time 0 to t).  The input spikes are assumed to arise stochastically in relation to a trajectory X(t) of an underlying (but hidden) relevant variable.  Therefore, via standard Bayesian inference, R(t) determines a distribution over the hidden variable at time t, P (X(t)jR(t)).  Similarly, output spikes are assumed to determine a distribution over a related hidden variable.  For the recurrent and feedforward computation in the network, we start with the deceptively simple goal 9 of producing output spikes in such a way that the distribution Q(X(t)jS(t)) they imply over the same hidden variable X(t) as the input, faithfully matches P (X(t)jR(t)).  This might seem a strange goal, since one could surely just listen to the input spikes.  However, in order for the output spikes to track the hidden variable, the dynamics of the interactions between the neurons must explicitly capture the dynamics of the process X(t).  Once this `identity mapping' problem has been solved, more general, complex computations can be performed with ease.  We illustrate this on a multisensory integration task, tracking a hidden variable that depends on multiple sensory cues.  We first consider two models of what the input spikes R(t) imply about X(t), and then discuss how the recurrently coupled network captures these dynamics.  Input Coding and Decoding The input spikes R(t) constitute the observations and are assumed to be probabilistically related to the signal by a tuning function f(X; # i ): P (R i (t)jX(t)) / f(X; # i ) (1) for the spike train of the ith input neuron, with parameters # i .  The model described in the following treats X(t) as a continuous variable (although it applies equally to the discrete case).  We consider a version of the dynamics and input coding that permits an analytical examination of the impact of spikes: Let X(t) follow a stationary Gaussian process such that the joint distribution P (X(t 1 ); X(t 2 ); : : : ; X(tm )) is Gaussian for any finite collection of times, with a covariance matrix which depends on the time difference: C ij = c(jt i t j j).  Depending on the function c(j#tj), this can capture random walks of varying forms of smoothness.  We can then write P (X(t)jR(t)) / p(X(t)) R X(t) dX(t)P (R(t)jX(t))P (X(t)jX(t)) (2) where P (X(t)jX(t)) is the distribution over the whole trajectory X(t) conditional on the value of X(t) at its end point.  If R(t) are a set of conditionally independent inhomogeneous Poisson processes, we have P (R(t)jX(t)) / Q ij f(X(t ij ); # i ) exp P i R # d# f(X(#); # i ) # ; (3) where t ij 8j are the spike times j of neuron i in R(t).  Let # = [X(t ij )] be the vector of stimulus positions at the times at which we observed a spike and # = [#(t ij )] be the vector of spike positions.  If the tuning functions are Gaussian f(X; # i ) / exp( (X # i ) 2 =2# 2 ) and sufficiently dense that P i R # d# f(X; # i ) is independent of X (a standard assumption in population coding), then P (R(t)jX(t)) / exp( (# #) T (# #)=2# 2 ) and in Equation 2, we can marginalize out X(t) except at the spike times t ij : P (

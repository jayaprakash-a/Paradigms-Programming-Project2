Towards Context-Based Visual Feedback Recognition for Embodied Agents
 Abstract Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people.  We investigate how contextual information can improve visual recognition of feedback gestures during interactions with embodied conversational agents.  We present a visual recognition model that integrates cues from the spoken dialogue of an embodied agent with direct observation of a user's head pose.  In preliminary experiments using a discriminative framework, contextual information improved the performance of head nod detection.

Formulations of Support Vector Machines: A Note from an Optimization Point of View
 Abstract In this paper, we discuss issues about formulations of support vector machines (SVM) from an optimization point of view.  First, support vector machines map training data into a higher (may be infinite) dimensional space.  Currently primal and dual formulations of SVM are derived in the finite dimensional space and readily extended to the infinite dimensional space.  We rigorously discuss the primal-dual relation in the infinite dimensional spaces.  Second, SVM formulations contain penalty terms which are different from unconstrained penalty functions in optimization.  Traditionally unconstrained penalty functions approximate a constrained problem as the penalty parameter increases.  We are interested in similar properties for SVM formulations.  For two of the most popular SVM formulations, we show that one enjoys properties of exact penalty functions but the other is only like traditional penalty functions which converge when the penalty parameter goes to infinity.

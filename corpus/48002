Distance Metric Learning for Large Margin Nearest Neighbor Classification
 Abstract We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classification by semidefinite programming.  The metric is trained with the goal that k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin.  On seven data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification---for example, achieving a test error rate of 1. 8% on the MNIST handwritten digits.  Our approach has many parallels to support vector machines, including a convex objective function based on the hinge loss and the potential to work in nonlinear feature spaces by using the "kernel trick".  On the other hand, our framework requires no modification for problems with large numbers of classes.

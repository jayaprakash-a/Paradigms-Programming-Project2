Hierarchical Reinforcement Learning Using Graphical Models
 Abstract The graphical models paradigm provides a general framework for automatically learning hierarchical models using ExpectationMaximization, enabling both abstract states and abstract policies to be learned.  In this paper we describe a two-phased method for incorporating policies learned with a graphical model to bias the behaviour of an SMDP Q-learning agent.  In the first reward-free phase, a graphical model is trained from sample trajectories; in the second phase, policies are extracted from the graphical model and improved by incorporating reward information.  We present results from a simulated grid world Taxi task showing that the SMDP Q-learning agent using the learned policies quickly does as well as an SMDP Q-learning agent using hand-coded policies.

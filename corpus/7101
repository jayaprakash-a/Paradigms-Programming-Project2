Uses of Convexity in Numerical Domain Partitioning
 Abstract We study the different manifestations of convexity in classifier-constructing machine learning algorithms.  Two principal ways to apply the consequences of the convexity of the evaluation function to enhance numerical domain partitioning exist: it enables static and dynamic pruning of partition candidates.  Numerical attribute handling is a potential time-consumption bottleneck in classifier learning.  Therefore, speeding it up is important for the practical utility of machine learning algorithms.  In extensive empirical evaluation we review the utility of static and dynamic pruning of partition candidates.  We test for 28 UCI test domains the speed-up gained by both approaches separately and their combined effect.  All pruning methods are able to enhance the efficiency of optimal numerical attribute partitioning.  Best results are obtained by combining static and dynamic pruning.

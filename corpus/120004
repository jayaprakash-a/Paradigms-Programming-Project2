On Learning Monotone Boolean Functions
 Abstract We consider the problem of learning monotone Boolean functions over f0; 1g n under the uniform distribution.  Specifically, given a polynomial number of uniform random samples for an unknown monotone Boolean function f , and given polynomial computing time, we would like to approximate f as well as possible.  We describe a simple algorithm that we prove achieves error at most 1=2 \Gamma \Omega(1= p n), improving on the previous best bound of 1=2 \Gamma \Omega((log 2 n)=n).  We also prove that no algorithm, given a polynomial number of samples, can guarantee error 1=2 \Gamma !((log n)= p n), improving on the previous best hardness bound of O(1= p n).  These lower bounds hold even if the learning algorithm is allowed membership queries.  Thus this paper settles to an O(log n) factor the question of the best achievable error for learning the class of monotone Boolean functions with respect to the uniform distribution.

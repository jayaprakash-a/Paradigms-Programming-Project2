2 The Potential Support Vector Machine 6
 Abstract We describe a new technique for the analysis of data which is given in matrix form.  We consider two sets of objects, the "row" and the "column" objects, and we represent these objects by a matrix of numerical values which describe their mutual relationships.  We then introduce a new technique, the "Potential Support Vector Machine" (P-SVM), as a large-margin based method for the construction of classifiers and regression functions for the "column" objects.  Contrary to standard support vector machine (SVM) approaches, the P-SVM minimizes a scale-invariant capacity measure under a new set of constraints.  As a result, the P-SVM can handle data matrices which are neither positive definite nor square, and leads to a usually sparse expansion of the classification boundary or the regression function in terms of the "row" rather than the "column" objects.  We introduce two complementary regularization schemes in order to avoid overfitting for noisy data sets.  The first scheme improves generalization performance for classification and regression problems, the second scheme leads to the selection of a small and informative set of "row" objects and can be applied to feature selection.  A fast optimization algorithm based on the "Sequential Minimal Optimization" (SMO) technique is provided.  We first apply the new method to two kinds of data representation.  The first representation uses a vectorial representation for the "row" and the "column" objects, and constructs a Gram matrix from feature vectors using a kernel function.  Benchmark results show, that the P-SVM method is competitive with or provides superior classification and regression results compared to standard methods and has the additional advantage that the kernel functions are no longer restricted to be positive definite.  The second representation uses a measured matrix of mutual relations between objects rather than vectorial data.  The new classification and regression method again performs very well compared to standard techniques on the benchmark data sets.  More importantly, however, our experiments show that the P-SVM can be very eectively used for feature selection because the resulting predictions are based on a sparse expansion of the "row" objects.  Benchmarks are performed not only with toy data, but also with several real world data sets including data from the UCI repository, protein classification, web-page classification, and DNA microarray data.

Learning More with Less
 Abstract We investigate the problem of classification using a small number of training examples.  We assume that we have access to a "reference" classification task (and corresponding training examples) that are similar, but not identical, to the main task.  In this paper, we consider the case that the classification problem is similar enough that it is useful to directly incorporate examples from the reference task.  We find that by weighting the reference examples appropriately, they provide regularization for the main task and drastically lower classification error.  On a newsgroup classification task, using training examples from both the main and reference tasks gives error one-fourth that of using either set of examples individually.

Efficient Hyperkernel Learning Using Second-Order Cone Programming
 Abstract.  The kernel function plays a central role in kernel methods.  Most existing methods can only adapt the kernel parameters or the kernel matrix based on empirical data.  Recently, Ong et al.  introduced the method of hyperkernels which can be used to learn the kernel function directly in an inductive setting [12].  However, the associated optimization problem is a semidefinite program (SDP), which is very computationally expensive even with the recent advances in interior point methods.  In this paper, we show that this learning problem can be equivalently reformulated as a second-order cone program (SOCP), which can then be solved more eciently than SDPs.  Experimental results on both toy and real-world data sets show significant speedup.  Moreover, in comparison with the kernel matrix learning method proposed by Lanckriet et al.  [7], our proposed SOCP-based hyperkernel method yields better generalization performance, with a speed that is comparable to their formulation based on quadratically constrained quadratic programming (QCQP).

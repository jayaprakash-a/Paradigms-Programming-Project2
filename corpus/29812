Learning Generic Prior Models for Visual Computation
 Abstract Many generic prior models have been widely used in computer vision ranging from image and surface reconstruction to motion analysis, and these models presume that surfaces of objects be smooth, and adjacent pixels in images have similar intensity values.  However, there is little rigorous theory to guide the construction and selection of prior models for a given application.  Furthermore, images are often observed at arbitrary scales, but none of the existing prior models are scale-invariant.  Motivated by these problems, this article chooses general natural images as a domain of application, and proposes a theory for learning prior models from a set of observed natural images.  Our theory is based on a maximum entropy principle, and the learned prior models are of Gibbs distributions.  A novel information criterion is proposed for model selection by minimizing a KullbackLeibler information distance.  We also investigate scale invariance in the statistics of natural images and study a prior model which has scale invariant property.  In this paper, in contrast with all existing prior models, negative potentials in Gibbs distribution are first reported.  The learned prior models are verified in two ways.  Firstly images are sampled from the prior distributions to demonstrate what typical images they stand for.  Secondly they are compared with existing prior models in experiments of image restoration.

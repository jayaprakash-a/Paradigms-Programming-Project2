Entropy and Inference, Revisited
 Abstract We study properties of popular near--uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results.  However, an Occam--style phase space argument expands the priors into their infinite mixture and resolves most of the observed problems.  This leads to a surprisingly good estimator of entropies of discrete distributions.  Learning a probability distribution from examples is one of the basic problems in data analysis.  Common practical approaches introduce a family of parametric models, leading to questions about model selection.  In Bayesian inference, computing the total probability of the data arising from a model involves an integration over parameter space, and the resulting "phase space volume" automatically discriminates against models with larger numbers of parameters---hence the description of these volume terms as Occam factors [1, 2].  As we move from finite parameterizations to models that are described by smooth functions, the integrals over parameter space become functional integrals and methods from quantum field theory allow us to do these integrals asymptotically; again the volume in model space consistent with the data is larger for models that are smoother and hence less complex [3].  Further, at least under some conditions the relevant degree of smoothness can be determined self--consistently from the data, so that we approach something like a model independent method for learning a distribution [4].  The results emphasizing the importance of phase space factors in learning prompt us to look back at a seemingly much simpler problem, namely learning a distribution on a discrete, nonmetric space.  Here the probability distribution is just a list of numbers {q i },

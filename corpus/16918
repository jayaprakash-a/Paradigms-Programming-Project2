On the Relationship Between Generalization Error, Hypothesis Complexity, and Sample Complexity for Radial Basis Functions
 Abstract Feedforward networks together with their training algorithms are a class of regression techniques that can be used to learn to perform some task from a set of examples.  The question of generalization of network performance from a finite training set to unseen data is clearly of crucial importance.  In this article we first show that the generalization error can be decomposed in two terms: the approximation error, due to the insufficient representational capacity of a finite sized network, and the estimation error, due to insufficient information about the target function because of the finite number of samples.

Regret Bounds for Prediction Problems
 Abstract We present a unified framework for reasoning about worst-case regret bounds for learning algorithms.  This framework is based on the theory of duality of convex functions.  It brings together results from computational learning theory and Bayesian statistics, allowing us to derive new proofs of known theorems, new theorems about known algorithms, and new algorithms.  1 The inference problem We are interested in the following kind of inference problem: on each time step t = 1 : : : T we must choose a prediction vector w t from a set of allowable predictions W .  The interpretation of w t depends on the details of the problem, but for example w t might be our guess at the mean of a sequence of numbers or the coefficients of a linear regression.  Then the loss function l t (w) is revealed to us, and we are penalized l t (w t ).

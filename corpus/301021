Learning Causal Laws
 Abstract Attempts to characterize people's causal knowledge of a domain in terms of causal network structures miss a key level of abstraction: the laws that allow people to formulate meaningful causal network hypotheses, and thereby learn and reason about novel causal systems so effectively.  We outline a preliminary framework for modeling causal laws in terms of generative grammars for causal networks.  We then present an experiment showing that causal grammars can be learned rapidly in a novel domain and used to support one-shot inferences about the unobserved causal properties of new objects.  Finally, we give a Bayesian analysis explaining how causal grammars may be induced from the limited data available in our experiments.  Causal Grammars Recently there has been substantial progress in understanding how people learn causal relations, or causal networks connecting multiple causes and effects.  Here we construe causal network broadly to include any collection of (domain-specific) causal beliefs that can be represented as a set of nodes and a set of (directed) links between nodes.  Nodes may represent objects, properties of or relations between objects, or events.  Links may have different causal semantics depending on the semantics of the nodes.  For instance, the network N 0 (Figure 1) might represent some aspects of a person's knowledge about several common diseases, their effects (symptoms), and causes (risky behaviors).  Our thesis here is that attempts to characterize people's causal knowledge of a domain primarily in terms of such network structures (e. g. , Gopnik & Glymour, 2002; Rehder, in press), while revealing in some important ways, miss a key level of abstraction: the laws that allow people to formulate meaningful causal network hypotheses, and thereby learn and reason about novel causal systems so effectively.  For instance, in Figure 1, there appears to be a common domain theory underlying networks N 0 , N 1 , and N 2 , which distinguishes them from N 3 , but is not explicitly represented in any of them.  We present a framework for representing such abstract causal knowledge, which we call causal grammar.  The framework is surely incomplete and oversimplified; we view it as merely a first pass at a deep and hard problem.  We also describe an experimental study of how people learn and use causal grammars, and briefly sketch a theory of learning based on Bayesian inference.  The networks N 1 and N 2 differ from N 0 in the precise causal links or disease nodes they posit, but they express the same essential regularities: three classes N 0

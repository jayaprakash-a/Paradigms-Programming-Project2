Fast blind separation based on information theory
 ABSTRACT Blind separation is an information theoretic problem, and we have proposed an information theoretic `sigmoid-based' solution [2].  Here we elaborate on several aspects of that solution.  Firstly, we argue that the separation matrix may be exactly found by maximising the joint entropy of the random vector resulting from a linear transformation of the mixtures followed by sigmoidal non-linearities which are the cumulative density functions of the `unknown' sources.  Secondly, we present the learning rule for performing this maximisation.  Thirdly, we discuss the role of prior knowledge of the c. d. f. 's of the sources in customising the learning rule.  We argue that sigmoid-based methods are better able to make use of this prior knowledge than cumulant-based methods, because the optimal non-linearity they should use is just an estimate of the source c. d. f.  We also suggest that they may have the edge in terms of robustness and speed of convergence.  Improvements in convergence speed have been facilitated by the introduction of pre-whitening of the mixture data.  An example result demonstrating this is the perfect separation of ten artificially mixed audio signals in 10 seconds of workstation computing time (4 to prewhiten and 6 to separate).  I.  Blind signal processing .  Statistically independent sources propagating in a medium are subject to several forms of distortion and interference.  They may be (1) mixed with other sources (2) mixed with time delayed versions of themselves, and (3) time-delayed.  The mixing may be linear or non-linear.  The inversion of these three forms of scrambling without any knowledge of their form may be called blind signal processing, or blind identification.  When the mixing is linear, we usually refer to (1) as the problem of blind separation [4], (2) as the problem of blind deconvolution, and (3) as the problem of blind time alignment.  These problems are information theoretic problems in the sense that we are dealing with the removal of statistical dependencies introduced by the medium, and the correct measure of statistical dependency is mutual information (see below).  In the most general information theoretic formalism, no special status is given to noise introduced by the medium or the sensors.  It is regarded as another `source' to be separated out.  It cannot be assumed to be characterised only by secondorder statistics (gaussian).  In fact, if we are lucky (and we usually are), it will not be gaussian, for it is the higher-order statistics which characterise a signal as independent and enable it to be separated out from others.  In [2], an information theoretic approach was outlined to all three of the above problems.  This paper is really a series of footnotes to [2], and should be read in conjunction with it if fuller details, or material of an introductory or tutorial nature are needed.  Here we will concentrate on the blind separation problem in order to show more clearly how it is solved by information theory.  II.  Separation through information theory.  A vector of sources s(t) = [s 1 (t); : : : ; s N (t)] propagates in a medium and mixtures of them, x(t) = [x 1 (t); : : : ; xN (t)] = As(t) 1 , are picked up by sensors.  The mixing is linear and static, there are no time delays and there are the same number (N ) of sensors as sources so that the mixing matrix, A, is square.  The important fact that distinguishes a source, s i , from a mixture, x i , is that it is statistically indepen1 henceforth, for convenience, the time index will be considered as implicit.  dent from the other sources, s j .  Their joint probability density function (p. d. f. ), measured across the time ensemble, factorises: f s (s) = N Y i=1 f s i (s i ) (1) Another way of saying this is that the mutual information between any two sources, i and j, is zero: I(s i ; s j ) = E '' ln f s (s) Q N i=1 f s i (s i ) # = 0 (2) where E[:] denotes expected value across the time ensemble.  Mixtures of sources will be statistically dependent on each other and the mutual information between them, I(x i ; x j ) will in general be positive.  Blind separation then consists in finding a matrix, W, so that the linear transformation u = Wx = WAs reestablishes the condition I(u i ; u j ) = 0, for all i 6= j.  This is the problem of Independent Component Analysis (ICA) [4, 3] One solution to this problem is that W is the inverse of A so that WA=I, the identity matrix.  Any other solution matrix, W, can be shown to be a permutation and rescaling of this one.  See Comon [3] for a fuller discussion of these matters.  To make the u i independent, we need to operate on non-linearly transformed output variables, y i = g(u i ), g() being a sigmoidal function.  2 The sigmoidal function provides, through its Taylor series expansion, all the higher-order statistics necessary to establish independence.  This assertion is justified through the following theorem: Theorem.  3 Independent Component Analysis (blind separation) can be performed exactly, by finding the maximum, with respect to W, of the joint entropy, H(y), of an output vector, y, which is the vector u, except that each element is transformed by a sigmoidal function which is a c. d. f.  of a sources which we are looking for.  In practice, we will often assume that all the sources have the same c. d. f.  and use the same sigmoidal function for each element of u.  To prove this theorem, we develop the following six points: Point 1.  Independent variables cannot become dependent by passing each one through a sigmoid.  Thus 2 a sigmoidal function is defined somewhat generally here as an invertible twice-differentiable function mapping the real line into some interval, often the unit interval: R! [0; 1].  3 Subsequent to the publication of this paper in the NOLTA proceedings, it was pointed out to us by Prof.  Nadal that this theorem is subsumed in the argument given in reference [7].  We would like to apologise for the fact that it appears here as an apparently original result and refer the reader to [7] for a fuller development.  if I(u i ; u j ) = 0 and y=g(u), g() being invertible, then I(y i ; y j ) = 0.  Since g\Gamma 1 is also invertible, the converse also holds.  Point 2.  The entropy, H(y), of a sigmoidally transformed variable has its maximum value (of zero) when the sigmoid function is the cumulative density function (c. d. f. ) of the u-variable.  Proof: H(y) is maximum when f y (y)=1 (the uniform distribution).  Thus by the relation: f y (y) = f u (u) dy=du (3) we have dy=du = fu (u) which means y = Fu (u), the cumulative density.  Point 3.  The joint entropy, H(y 1 ; y 2 ), of two sigmoidally transformed variables has its maximum value (of zero) when y 1 and y 2 are independent and the sigmoid function in each is the c. d. f.  of u 1 and u 2 respectively.  This is a clear consequence of Point 2 and the relation: H(y 1 ; y 2 ) = H(y 1 ) + H(y 2 ) \Gamma I(y 1 ; y 2 ) (4) The N-variable joint entropy, H(y), is similarly maximal when each f y i (y i ) term is maximum and all the I(y i ; y j ) are zero.  Point 4.  When two independent non-gaussian variables, u i and u j are linearly combined, the p. d. f.  of the resulting variable has a different shape from either of f u i (u i ) or f u j (u j ).  In general, the p. d. f.  becomes more gaussian, a trend ultimately enshrined in the Central Limit Theorem.  Gaussian variables are the only ones which retain the form of their p. d. f.  under linear combination.  Point 5.  Consider the joint entropy, H(y), of N sigmoidally transformed variables, where the sigmoid functions are the c. d. f. 's of N independent nongaussian sources (ie: y i = F s i (u i )).  This has its maximal value when u i = s i , in other words when the sources are separated! Any mixing of sources, u i = P j s j , will both: ffl introduce statistical dependencies between the u's, moving I(u i ; u j ) away from zero (and hence also I(y i ; y j ) --- see Point 1), and ffl decrease the individual entropy terms, H(y i ), through deviation of f y i (y i ) from 1.  This latter fact is born out by Points 2 and 4 above.  Taken together, this shows that under the special condition that y i = F s i (u i ), the joint entropy H(y) is maximal when the individual entropies, H(y i ), are

An Analysis of Temporal-Difference Learning withFunction Approximation
 ABSTRACT We discuss thetemporal-di#erence learning algorithm, as applied toapproximatingthe cost-to-go function of an in#nite-horizon discounted Markovchain.  The algorithm weanalyze updates parameters of a linear function approximator on{line, duringasingle endless trajectory of an irreducible aperiodic Markovchain with a finite or in#nitestate space.  We present a proof of convergence (with probability1),acharacterization of the limit of convergence, andaboundonthe resultingapproximation error.  Furthermore, our analysis is based on a new line of reasoningthat provides new intuition aboutthe dynamics of temporal-di#erence learning.  In addition to provingnew and stronger positive resultsthan those previously available, weidentify the significance of on-lineupdatingandpotential hazards associated withthe use of nonlinear function approximators.  First, we provethat divergence may occur when updates are not based on trajectories of the Markovchain.  This fact reconciles positive andnegative resultsthathave been discussed in the literature, regardingthe soundness of temporal-di#erence learning.  Second, we present an example illustratingthe possibilityof divergence when temporal-di#erence learningisusedinthe presence of a nonlinear function approximator.

Planning for Markov Decision Processes with Sparse Stochasticity (Draft Version)
 Abstract Planning algorithms designed for deterministic worlds, such as A* search, usually run much faster than algorithms designed for worlds with uncertain action outcomes, such as value iteration.  Real-world planning problems often exhibit uncertainty, which forces us to use the slower algorithms to solve them.  In particular, planning problems which involve sensing actions always have significant uncertainty: there is no point consulting a sensor if you know the outcome in advance.  However, many sensor planning problems exhibit sparse uncertainty: there are long sequences of deterministic actions which accomplish tasks like moving sensor platforms into place, interspersed with a small number of sensing actions which have uncertain outcomes.  In this paper we describe a new planning algorithm, called MCP (short for MDP Compression Planning), which combines A* search with value iteration.  We present experiments which show that MCP can run substantially faster than competing planners in domains with sparse uncertainty; these experiments are based on a simulation of a ground robot cooperating with a helicopter to fill in a partial map and move to a goal location.  In deterministic planning problems, optimal paths are acyclic: no state is visited more than once.  Because of this property, algorithms like A* search can guarantee that they visit each state in the state space no more than once.  By visiting the states in an appropriate order, it is possible to ensure that we know the exact value of all of a state's possible successors before we visit that state; so, the first time we visit a state we can compute its correct value.  By contrast, if actions have uncertain outcomes, optimal paths may contain cycles: some states will be visited two or more times with positive probability.  Because of these cycles, there is no way to order states so that we determine the values of a state's successors before we visit the state itself.  Instead, the only way to compute state values is to solve a set of simultaneous equations.  In problems with sparse stochasticity, only a small fraction of all states have uncertain outcomes.  It is these few states that cause all of the cycles: while a deterministic state s may participate in a cycle, the only way it can do so is if one of its successors has an action with a stochastic outcome (and only if this stochastic action can lead to a predecessor of s).  In such problems, we would like to build a smaller MDP which contains only states which are related to stochastic actions.  (We will call such an MDP a compressed MDP, and we will call its states distinguished states. ) We could then run fast algorithms like A* (a) Segbot (b) Robotic helicopter (d) Planning map (e) Execution simulation (c) 3D Map Figure 1: Robot-Helicopter Coordination search to plan paths between distinguished states, and reserve slower algorithms like value iteration for deciding how to deal with stochastic outcomes.  There are two problems with such a strategy.  First, there can be a large number of states which are related to stochastic actions, and so it may be impractical to enumerate all of them and make them all distinguished states.  (We would prefer instead to distinguish only states which are likely to be encountered while executing some policy which we are considering. ) Second, there can be a large number of ways to get from one distinguished state to another: edges in the compressed MDP correspond to sequences of actions in the original MDP.  If we knew the values of all of the distinguished states exactly, then we could use A* search to generate optimal paths between them, but since we don't we can't.  In this paper, we will describe an algorithm which incrementally builds a compressed MDP using a sequence of deterministic searches.  It adds states and edges to the compressed MDP only by encountering them along trajectories; so, it never adds irrelevant states or edges to the compressed MDP.  Trajectories are generated by deterministic search, and so undistinguished states are treated only with fast algorithms.  Bellman errors in the values for distinguished states show us where to try additional trajectories, and help us build the relevant parts of the compressed MDP as quickly as possible.

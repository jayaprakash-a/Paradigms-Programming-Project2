RECOGNITION OF OCCLUDED SPEECH BY HIDDEN MARKOV MODELS
 ABSTRACT Previous work at Sheffield on computational models of auditory scene analysis has attempted to separate the acoustic evidence from simultaneous sound sources by techniques grounded in auditory grouping processes.  For this work to be useful in automatic speech recognition, we need to develop recognition techniques which can cope with 'occluded' speech.  The separation stage will group together components of the sound mixture which were produced by the same source, but will contain little or no information for those parts of the spectrum which were obscured by other sources.  We present a method which allows conventional hidden Markov model (HMM) based speech recognition systems, trained on good-quality data, to be applied to occluded speech material.  We show that, for continuous density HMMs, the probability of observing such a partial data vector in a given state can be computed from the appropriate marginal distribution.  We show formally that this distribution can be derived from the full mixture density function associated with the state by striking out the rows and columns from the mean vector and covariance matrix corresponding to the missing elements.  We present empirical evidence for the robustness of this method: on phone models for the TIMIT database it is possible to delete 50% of the observation vector at random without serious degradation in recognition performance (accuracy falls from 58. 9% to 51. 5% for 10 mixture, diagonal covariance models, whilst correctness falls from 69. 6% to 66. 0%).  Even with 80% removed some competence remains (30. 5% accuracy, 55. 0% correct).  We conclude that auditory scene analysis followed by recognition of occluded speech can be seen as an attractive paradigm for robust automatic speech recognition; it makes no assumptions about the nature of the noise or the number of sound sources present.

Estimation of Articulatory Parameters from Speech Acoustics by Kalman Filtering
 Abstract This paper presents our research work of estimating articulatory states and model parameters from speech acoustics.  This work represents a prerequisite for a speech recognition system based on articulatory dynamical models.  The model parameter estimation was based on the vowels obtained from 590 sentences of 59 speakers from TIMIT speech database, whereas the state estimation experiments have been done using the vowels from 100 sentences of 10 speakers from TIMIT.  The 10 English vowels investigated have the following transcription: /AA/, /AE/, /AH/, /AO/, /EH/, /EY/, /IH/, /IY/, /UH/ and /UW/.  For each vowel, articulatory dynamical models were created using secondorder dynamical systems.  The states of these models represent the positions of articulators such as lips, tongue and pharynx.  We used 8 state parameters to represent these articulators and 3 formant frequencies to represent the acoustic observation vectors.  The nonlinear relationship between articulatory state vector and speech acoustics has been approximated using a piecewise linear approximation on small regions in the articulatory space.  This linearization was performed using a codebook of 610,000 pairs of articulatory and acoustic vectors created using the Metropolis algorithm.  The whole codebook was then linearized on about 7,500 regions using a vector quantization method.  The articulatory model parameters and states were estimated using the EM (expectation-maximization) algorithm and Kalman filtering techniques.  The results obtained in estimating the articulatory parameters encourage us to apply this technique to automatic speech recognition.

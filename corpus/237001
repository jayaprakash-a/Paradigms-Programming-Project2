Data Synthesis with Expectation-Maximization
 Abstract A problem of increasing importance in computer graphics is to generate data with the style of some previous training data, but satisfying new constraints.  If we use a probabilistic latent variable model, then learning the model will normally be performed using Expectation-Maximization (EM), or one of its generalizations.  We show that data synthesis for such problems can also be performed using EM, and that this synthesis process closely parallels learning, including identical E-step algorithms.  This observation simplifies the process of developing synthesis algorithms for latent variable models.

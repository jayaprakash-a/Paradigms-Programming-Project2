Leave-one-out Bounds for Support Vector Regression Model Selection
 Abstract Minimizing bounds of leave-one-out (loo) errors is an important and efficient approach for support vector machine (SVM) model selection.  Past research focuses on their use for classification but not regression.  In this article, we derive various loo bounds for support vector regression (SVR) and discuss the difference from those for classification.  Experiments demonstrate that the proposed bounds are competitive with Bayesian SVR for parameter selection.  We also discuss the differentiability of loo bounds.

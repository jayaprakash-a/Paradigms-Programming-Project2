Bounds for the Computational Power and Learning Complexity of Analog Neural Nets
 Abstract It is shown that feedforward neural nets of constant depth with piecewise polynomial activation functions and arbitrary real weights can be simulated for boolean inputs and outputs by neural nets of a somewhat larger size and depth with heaviside gates and weights from f0; 1g.  This provides the first known upper bound for the computational power and VC-dimension of such neural nets.  It is also shown that in the case of piecewise linear activation functions one can replace arbitrary real weights by rational numbers with polynomially many bits, without changing the boolean function that is computed by the neural net.  In addition we improve the best known lower bound for the VC-dimension of a neural net with w weights and gates that use the heaviside function (or other common activation functions such as oe) from \Omega(w) to \Omega(w log w).  This implies the somewhat surprising fact that the Baum-Haussler upper bound for the VC-dimension of a neural net with heaviside gates is asymptotically optimal.  Finally it is shown that neural nets with piecewise polynomial activation functions and a constant number of analog inputs are probably approximately learnable (in Valiant's model for PAC-learning).

High-Zoom Video Hallucination by Exploiting Spatio-Temporal Regularities
 Abstract In this paper, we consider the problem of super-resolving a human face video by a very high (16) zoom factor.  Inspired by recent literature on hallucination and examplebased learning, we formulate this task using a graphical model that encodes 1) spatio-temporal consistencies, and 2) image formation & degradation processes.  A video database of facial expressions is used to learn a domainspecific prior for high-resolution videos.  The problem is posed as one of probabilistic inference, in which we aim to find the high resolution video that best satisfies the constraints expressed through the graphical model.  Traditional approaches to this problem using video data first estimate the relative motion between frames and then compensate for it, effectively resulting in multiple measurements of the scene.  Our use of time is rather direct: We define data structures that span multiple consecutive frames, enriching our feature vectors with a temporal signature.  We then exploit these signatures to find consistent solutions over time.  In our experiments, a 8 6 pixel-wide face video, subject to translational jitter and additive noise, gets magnified to a 128 96 pixel video.  Our results show that by exploiting both space and time, drastic improvements can be achieved in both video flicker artifacts and mean-squared-error.  1 Learning-based Super-Resolution Imagine we are given an extremely low resolution video (Fig.  1, top).  Assuming that there is a human face in these images, can we guess the missing details, and estimate (or "hallucinate") a highly zoomed, super-resolved video that resembles the original (bottom)? In this paper, we present a model for this task, formulate it as an inference problem, and describe an algorithm for solving it.  The problem of estimating high resolution image details is commonly referred to as Super-Resolution (SR) [9], although in practice approaches may differ in their use of a single static image, a sequence of thereof, or a video of a dynamic scene [17, 13, 11, 16].  Mathematically, such problems are highly ill-posed [18], motivating the use of Bayesian techniques and generic smoothness assumptions about high resolution images [8] (Fig.  1, middle).  Figure 1: Given only a low-resolution video (top), how can one estimate (or "hallucinate") the original high-resolution video (bottom)? Unfortunately, simple methods such as bicubic interpolation are insufficient (middle).  In this paper we explore zooming using a database of videos with an inference procedure that enforces spatio-temporal consistency of the resulting hallucinated video.  Recently, learning-based approaches to SR have produced compelling results [12, 2, 5, 15, 6].  The essence of these techniques is to use a training set of high resolution images and their low resolution counterparts to build a cooccurence model (stored either directly as image patches, or as coefficients of alternative representations).  At the time of applying the learnt model, the task is to predict high resolution data from the observed low resolution data.  In [12], an example-based learning scheme was applied to generic images and zooming results up to a factor of 4 were reported.  A direct application of this to video sequences was attempted in [4], but severe video artifacts were found.  As a remedy, an ad-hoc solution was proposed, which consisted of re-using high-resolution solutions for achieving more coherent videos.  An interesting aspect of learning approaches is that they can be made much more powerful when images are limited to a particular domain: For instance, [2] considered superresolving human faces only.  Their recognition algorithm referred to a database of registered face images, and collected best matching image patches given the input, enabling convincing results with zoom factors up to 8.  The model we propose for super-resolving videos is inspired by the following key aspects of earlier work: By limiting our learning task to faces only, and using a spaFigure 2: Model of blur and degradation (See section 2. 1) tially varying prior (as in [2]), we keep the computational requirements relatively low.  Inspired by the use of spatial couplings in [12], we model both spatial and temporal consistencies in the super-resolved videos.  In contrast to [4], we do not resort to re-seeding our high resolution hypothesis space with earlier solutions, but instead model and deal with temporal visual phenomena directly.  2 Modeling the High-Zoom Problem In this section, we present a model for the high-zoom problem, through which we integrate our domain knowledge about the videos of interest with the physical principles of image formation.  2. 1 Generative Image Model A graphical model is a concise tool for expressing causal and statistical dependence relationships between random variables of interest.  We now introduce our graphical model for the formation of low-resolution observations.  For clarity, we describe this generative model for the static image case, then extend it to the temporal dimension for videos in subsection 2. 2.  Our model for low-resolution observations comprises three steps: organized upwards in Fig.  2, 1) Generation of template image T , 2) addition of illumination offset I to generate a noisy high-resolution image H , and 3) downsampling and corruption for forming the low-resolution image L.  We now discuss each of these steps in detail.  The starting point is a high-resolution template image T , generated following a prior model about possible images in the domain.  Building a generative statistical model of T that can account for all possible face images represents a formidable challenge.  In order to circumvent this modeling problem, we will take a non-parametric approach, and draw samples from a large database of examples.  Since capturing all possible variations of facial expressions and features requires a very large number of examples to be stored, one can adopt local models, defined over image patches, and treat Figure 3: Spatial (left) and spatio-temporal (right) coupling between neighboring template patches is shown in the Markov random field graphs for image (2-d, left) and video (3-d, right).  them independently, as in [2].  Such a choice, however, fails to capture those events which span multiple patches, resulting in unrealistic face compositions.  As a computational trade-off between treating these patches all independently and building a full statistical co-occurence model, we will impose compatibility constraints only between neighboring patches.  In particular, we will use a Markov Random Field (MRF) (Fig.  3, left) to model spatial interactions, allowing us to compose face template images without artifacts.  After the template image T is formed, we consider a deviation from the illumination conditions in which the prior model was built: An intensity offset I is added to T to produce the high resolution image H .  Finally, we model the severe blur and downsampling operations for obtaining the low-resolution observation L by a linear, local-averaging operator followed by additive noise [1].  2. 2 Exploiting Time Just as neighboring pixels in natural images tend to be highly correlated, so too are consecutive frames in video sequences.  In our work, we exploit these temporal dependencies in further constraining the space of high resolution solutions.  By extending the MRF framework into the time dimension (Fig.  3, right), we model couplings between consecutive frames.  This results in a three-dimensional network of video patches, defined as data structures spanning multiple consecutive frames.  For instance, as shown in Fig.  4 (bottom), we can choose a temporal support of 2 frames for the nodes in T , and make consecutive nodes overlap by one frame.  This is equivalent to stating that the underlying video sequence is first-order Markov in time.  Our scheme gives the temporal dimension an unconventional role compared to traditional approaches to superresolution: In the literature, the relative motion between frames is estimated, then eliminated via warping or optical flow.  These approches are essentially two-dimensional, treating time, in effect, as a nuisance parameter to be compensated for.  By contrast, we take advantage of the richer local signature that the combination of space and time provides.  In

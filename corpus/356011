REDUCING BIAS IN SUPERVISED LEARNING
 ABSTRACT Nonparametric statistical supervised learning methods often suffer from bias caused by non-uniformity of the probability distribution of training samples.  We discuss this problem and propose a new nonparametric neighborhood method for classification and estimation that significantly reduces the bias.  Simulations exemplify the advantages, and theoretical results are noted.  1.  THE SUPERVISED LEARNING PROBLEM The supervised learning problem, or pattern recognition, arises in many applications of engineering and science, from failure prediction to color management.  Nonparametric pattern classification and estimation can be advantageous when no trusted model exists for the distribution of the observations or for the decision boundaries.  Error rates for simple neighborhood nonparametric methods like k-NN are often competitively low for real data [1].  Even 1-NN for classification and regression achieves at most twice the Bayes risk of the Bayes rule as sample size grows without bound [2].  Neighborhood methods entail estimation of class conditional probability densities based on relative frequencies of samples `near' the test point in question.  Often relative frequencies are weighted, perhaps as distance from the point varies or by a kernel, preassigned or adapted to local data.  The local nature of these algorithms makes nonparametric neighborhood methods sensitive to behavior of the probability distribution of the training sample in neighborhoods where inferences are sought.  There can be bias in classification or regression that owes to the local gradient of the distribution.  We propose and explore the behavior of a method that uses linear interpolation to reduce the bias, while diversifying the weighting of the training samples by maximizing the Shannon entropy of neighborhood weights.  The term `feature vector' denotes a real valued random vector X 2 R d , and the term `observation' denotes a real

Query by Committee
 Abstract We propose an algorithm called query by committee, in which a committee of students is trained on the same data set.  The next query is chosen according to the principle of maximal disagreement.  The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron.  As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain.  This leads to generalization error that decreases exponentially with the number of examples.  This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law.  We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms.

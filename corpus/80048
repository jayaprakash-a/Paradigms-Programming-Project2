Computational Learning Theory: Fourth European Conference,
 Abstract.  Boosting is a general method for improving the accuracy of any given learning algorithm.  Focusing primarily on the AdaBoost algorithm, we briefly survey theoretical work on boosting including analyses of AdaBoost's training error and generalization error, connections between boosting and game theory, methods of estimating probabilities using boosting, and extensions of AdaBoost for multiclass classification problems.  We also briefly mention some empirical work.  Background Boosting is a general method which attempts to "boost" the accuracy of any given learning algorithm.  Kearns and Valiant [21, 22] were the first to pose the question of whether a "weak" learning algorithm which performs just slightly better than random guessing in Valiant's PAC model [34] can be "boosted" into an arbitrarily accurate "strong" learning algorithm.  Schapire [28] came up with the first provable polynomial-time boosting algorithm in 1989.  A year later, Freund [13] developed a much more efficient boosting algorithm which, although optimal in a certain sense, nevertheless suffered from certain practical drawbacks.  The first experiments with these early boosting algorithms were carried out by Drucker, Schapire and Simard [12] on an OCR task.  AdaBoost The AdaBoost algorithm, introduced in 1995 by Freund and Schapire [16], solved many of the practical difficulties of the earlier boosting algorithms, and is the focus of this paper.  Pseudocode for AdaBoost is given in Fig.  1 in the slightly generalized form given by Schapire and Singer [31].  The algorithm takes as input a training set (x 1 ; y 1 ); : : : ; (xm ; ym ) where each x i belongs to some domain or instance space X, and each label y i is in some label set Y .  For most of this paper, we assume Y =f\Gamma 1; +1g; later, we discuss extensions to the multiclass case.  AdaBoost calls a given weak or base learning algorithm repeatedly in a series of rounds t = 1; : : : ; T .  One of the main ideas of the algorithm is to maintain a distribution or set of weights over the training set.  The weight of this distribution on training example i on round t is denoted D t (i).  Initially, all weights are set equally, but on each round, the weights of incorrectly classified Given: (x 1 ; y 1 ); : : : ; (xm ; ym ) where x i 2 X, y i 2 Y = f\Gamma 1; +1g Initialize D 1 (i) = 1=m.  For t = 1; : : : ; T : -- Train weak learner using distribution D t .  -- Get weak hypothesis h t : X ! R.  -- Choose ff t 2 R.  -- Update: D t+1 (i) = D t (i) exp(\Gamma ff t y i h t (x i )) Z t where Z t is a normalization factor (chosen so that D t+1 will be a distribution).  Output the final hypothesis: H(x) = sign / T X t=1 ff t h t (x) ! : Fig.  1.  The boosting algorithm AdaBoost.  examples are increased so that the weak learner is forced to focus on the hard examples in the training set.  The weak learner's job is to find a weak hypothesis h t : X ! R appropriate for the distribution D t .  In the simplest case, the range of each h t is binary, i. e. , restricted to f\Gamma 1; +1g; the weak learner's job then is to minimize the error ffl t = Pr iD t [h t (x i ) 6= y i ] : Once the weak hypothesis h t has been received, AdaBoost chooses a parameter ff t 2 R which intuitively measures the importance that it assigns to h t .  In the figure, we have deliberately left the choice of ff t unspecified.  For binary h t , we typically set ff t = 1 2 ln ` 1 \Gamma ffl t ffl t ' : (1) More on choosing ff t follows below.  The distribution D t is then updated using the rule shown in the figure.  The final hypothesis H is a weighted majority vote of the T weak hypotheses where ff t is the weight assigned to h t .  Analyzing the training error The most basic theoretical property of AdaBoost concerns its ability to reduce the training error.  Specifically, Schapire and Singer [31], in generalizing a theorem of Freund and Schapire [16], show that the training error of the final hypothesis

Neural Networks with Local Receptive Fields and Superlinear VC Dimension
 Abstract Local receptive field neurons comprise such well-known and widely used unit types as radial basis function neurons and neurons with center-surround receptive field.  We study the Vapnik-Chervonenkis (VC) dimension of feedforward neural networks with one hidden layer of these units.  For several variants of local receptive field neurons we show that the VC dimension of these networks is superlinear.  In particular, we establish the bound \Omega(W log k) for any reasonably sized network with W parameters and k hidden nodes.  This bound is shown to hold for discrete center-surround receptive field neurons, which are physiologically relevant models of cells in the mammalian visual system, for neurons computing a difference of Gaussians, which are popular in computational vision, and for standard radial basis function (RBF) neurons,

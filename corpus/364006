STUDENT PAPER: A Multiagent Reinforcement Learning Algorithm by Dynamically Merging Markov Decision Processes
 ABSTRACT One general strategy for accelerating the learning of cooperative multiagent tasks is to reuse (good or optimal) solutions to the task when each agent is acting alone.  In this paper, we formalize this approach as dynamically merging solutions to multiple Markov decision processes (MDPs), each representing an individual agent's solution when acting alone, to obtain (good or optimal) solutions to the overall multiagent MDP when all the agents act together.  We present a new temporal-di#erence learning algorithm called MAPLE (MultiAgent Policy LEarning) that uses Q-learning and dynamic merging to efficiently construct global solutions to the overall multiagent problem from solutions to the individual MDPs.  We illustrate the efficiency of MAPLE by comparing its performance with standard Q-learning applied to the overall multiagent MDP.  We also describe a corresponding planning algorithm that, given complete knowledge of the underlying single agent MDPs, uses dynamic merging to ef#ciently solve the multiagent MDP.  We also illustrate how the dynamic merging framework can be extended to the case when agents use temporally extended actions, by using semiMarkov decision processes (SMDPs) to represent variablelength decision epochs.

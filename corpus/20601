On Finding Good State Aggregation Functions
 Abstract We describe a novel algorithm that learns to perform a Heuristic Embedding of Markov Processes (HEMP) into a low dimensional Euclidean space (Engel & Mannor, 2001) Learning is performed online by observing actual state transitions and gradually constructing a map of the Markov state space.  In this map the distance between any two states is related to the transition probabilities between them.  Such maps may be useful as an alternative representation for Markov Decision Processes (MDPs) in cases where the state space is of high dimensionality and the number of states is large.  These are exactly the conditions under which one would have to resort to hierarchical RL schemes.  One of the fundamental questions in the field of hierarchical RL is that of state aggregation and dimensional reduction - namely, how can state information be safely abstracted away.  We motivate the algorithm from the stateaggregation and dimensional reduction perspectives, and demonstrate its operation in a maze test-bed.

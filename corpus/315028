Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales
 Abstract Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key challenges for AI.  In this paper we develop an approach to these problems based on the mathematical framework of reinforcement learning and Markov decision processes (MDPs).  We extend the usual notion of action to include options---whole courses of behavior that may be temporally extended, stochastic, and contingent on events.  Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques.  Options may be given a priori, learned by experience, or both.  They may be used interchangeably with actions in a variety of planning and learning methods.  The theory of semi-Markov decision processes (SMDPs) can be applied to model the consequences of options and as a basis for planning and learning methods using them.  In this paper we develop these connections, building on prior work by Bradtke and Duff (1995), Parr (1998) and others.  Our main novel results concern the interface between the MDP and SMDP levels of analysis.  We show how a set of options can be altered by changing only their termination conditions to improve over SMDP methods with no additional cost.  We also introduce intra-option temporal-difference methods that are able to learn from fragments of an option's execution.  Finally, we propose a notion of subgoal which can be used to improve the options themselves.  Overall, we argue that options and their models provide hitherto missing aspects of a powerful, clear, and expressive framework for representing and organizing knowledge.  1.  Temporal Abstraction To make everyday decisions, people must foresee the consequences of their possible courses of action at multiple levels of temporal abstraction.  Consider a traveler deciding to undertake a journey to a distant city.  To decide whether or not to go, the benefits of the trip must be weighed against the expense.  Having decided to go, choices must be made at each leg, e. g. , whether to fly or to drive, whether to take a taxi or to arrange a ride.  Each of these steps involves foresight and decision, all the way down to the smallest of actions.  For example, just to call a taxi may involve finding a telephone, dialing each digit, and the individual muscle contractions to lift the receiver to the ear.  Human decision making routinely

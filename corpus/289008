Radius Margin Bounds for Support Vector Machines with the RBF Kernel
 Abstract An important approach for efficient support vector machine (SVM) model selection is to use differentiable bounds of the leave-one-out (loo) error.  Past efforts focused on finding tight bounds of loo, for example, radius margin bounds, span bounds, etc.  However, their practical viability is still not very satisfactory.  In (Duan et al. , 2003), it has been shown that radius margin bound gives good prediction for L2-SVM.  In this paper, through the analyses why this bound performs well for L2-SVM, we show that finding a bound whose minima are in a region with small loo values may be more important than its tightness.  Based on this principle we propose modified radius margin bounds for L1-SVM where the original bound is only applicable to the hard-margin case.  Our modification for L1-SVM achieves comparable performance to L2-SVM.  To study whether L1- or L2-SVM should be used, we further analyze other properties such as their differentiability, number of support vectors, and number of free support vectors.  In this aspect, L1-SVM possesses the advantage of having fewer support vectors.  Their implementations are also different so we discuss related issues in detail.

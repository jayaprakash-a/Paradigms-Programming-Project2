A New View of the EM Algorithm that Justifies Incremental and Other Variants
 Abstract We present a new view of the EM algorithm for maximum likelihood estimation in situations with unobserved variables.  In this view, both the E and the M steps of the algorithm are seen as maximizing a joint function of the model parameters and of the distribution over unobserved variables.  From this perspective, it is easy to justify an incremental variant of the algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step.  This variant is shown empirically to give faster convergence in a mixture estimation problem.  A wide range of other variant algorithms are also seen to be possible.

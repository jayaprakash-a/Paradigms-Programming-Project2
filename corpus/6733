MENTAL STATE DETECTION OF DIALOGUE SYSTEM USERS VIA SPOKEN LANGUAGE
 ABSTRACT This paper presents an approach to simulate the mental activities of children during their interaction with computers through their spoken language.  The mental activities are categorized into three states: confidence, confusion and frustration.  Two knowledge sources are used in the detection.  One is prosody, which indicates utterance type and user's attitude.  The other is embedded key words/phrases which help interpret the utterances.  Moreover, it is found that children's speech exhibits very different acoustic characteristics from adults.  Given the uniqueness of children's speech, this paper applies a vocal-tract-length-normalization (VTLN)-based technique to compensate for both inter-speaker variability and intraspeaker variability in children's speech.  The detected key words/phrases are then integrated with prosodic information as the cues for the MAP decision of mental states.  Tests on a set of 50 utterances collected from the project experiment showed the classification accuracy was 74%.

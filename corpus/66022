A Universal Noise Cleaning Procedure via the Dual Learning Problem
 Abstract We present a new scheme for cleaning a sample corrupted by labeling noise.  Our scheme is universal in the sense that we only make general assumptions on the dual learning problem and therefore it is completely detached from the specifics of the primal problem itself.  In a nutshell, we turn to the dual learning problem to exploit valuable information about the underlying structure of the primal one, which in turn provides the means to device a simple "noise cleaning" mechanism, using Membership Queries.  We demonstrate the strength and applicability of the suggested method with a few learning problems of different nature.  Of particular interest is the problem of learning in the restricted class of parity functions, where only k out of n bits are active.  We show that in the MQ model we can outperforme the recent result by Blum et al.  [3] and handle k = O (n \Gamma c log (n) log log (n)).  This also provides a sharp separation between our method and the SQ model.  The suggested procedure works not only for classification problems but for regression problems as well.  To this end, we present a uniform upper bound on the fat-shattering dimension of both the primal and dual problems, which is derived from a geometric property of classes of real-valued functions - called type.

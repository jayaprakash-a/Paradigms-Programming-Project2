Distributed Speech Processing in MiPad's Multimodal User Interface
 Abstract---This paper describes the main components of MiPad (Multimodal Interactive PAD) and especially its distributed speech processing aspects.  MiPad is a wireless mobile PDA prototype that enables users to accomplish many common tasks using a multimodal spoken language interface and wireless-data technologies.  It fully integrates continuous speech recognition and spoken language understanding, and provides a novel solution for data entry in PDAs or smart phones, often done by pecking with tiny styluses or typing on minuscule keyboards.  Our user study indicates that the throughput of MiPad is significantly superior to that of the existing pen-based PDA interface.  Acoustic modeling and noise robustness in distributed speech recognition are key components in MiPad's design and implementation.  In a typical scenario, the user speaks to the device at a distance so that he or she can see the screen.  The built-in microphone thus picks up a lot of background noise, which requires MiPad be noise robust.  For complex tasks, such as dictating e-mails, resource limitations demand the use of a client--server (peer-to-peer) architecture, where the PDA performs primitive feature extraction, feature quantization, and error protection, while the transmitted features to the server are subject to further speech feature enhancement, speech decoding and understanding before a dialog is carried out and actions rendered.  Noise robustness can be achieved at the client, at the server or both.  Various speech processing aspects of this type of distributed computation as related to MiPad's potential deployment are presented in this paper.  Recent user interface study results are also described.  Finally, we point out future research directions as related to several key MiPad functionalities.

Syntactic Features for Evaluation of Machine Translation
 Abstract Automatic evaluation of machine translation, based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems.  We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.  Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.

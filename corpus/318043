METRIC-BASED MODEL SELECTION FOR TIME-SERIES FORECASTING
 Abstract.  Metric-based methods, which use unlabeled data to detect gross differences in behavior away from the training points, have recently been introduced for model selection, often yielding very significant improvements over alternatives (including cross-validation).  We introduce extensions that take advantage of the particular case of time-series data in which the task involves prediction with a horizon h.  The ideas are (i) to use at t the h unlabeled examples that precede t for model selection, and (ii) take advantage of the different error distributions of cross-validation and the metric methods.  Experimental results establish the effectiveness of these extensions in the context of feature subset selection.  MODEL SELECTION AND REGULARIZATION Supervised learning algorithms take input/output training pairs {(x 1 , y 1 ) (x l , y l )} sampled (usually independently) from an unknown joint distribution P (X, Y ) and attempt to infer a function f 2 F that minimizes the expected value of the loss L(f(X), Y ) (also called the generalization error).  In many cases one faces the dilemma that if F is too "rich" then the average training set loss (training error) will be low but the expected out-of-sample loss may be large (overfitting), and viceversa if F is not "rich" enough (underfitting).  In many cases one can define a collection of increasingly complex function classes F 0 # F 1 # # F (although some methods studied here work as well with a partial order).  Model selection methods attempt to choose one of these function classes to avoid both overfitting and underfitting.  For example, in the case of variable subset selection, these subsets may correspond to the number of input variables that are allowed (e. g.  F i is the set of linear regressions with i input variables).  One approach to model selection is based on complexity penalization [5, 3].  Another approach to model selection is based on held-out data: one selects the model with the lowest generalization error, estimated by repeatedly training on a subset of the data and testing on the rest, e. g.  using the bootstrap, leave-one-out or K-fold crossvalidation (XVT).  The metric-based methods introduced by Schuurmans [6, 7] are somewhat in between in that they take advantage of unlabeled data not used for training (but only the input part) in order to introduce a complexity penalty.  These methods take advantage of unlabeled data: the behavior of functions corresponding to different choices of complexity are compared on the training data and on the unlabeled data, and differences in behavior that would indicate overfitting are exploited to perform model selection.  An overview of advances in model selection and feature selection methods can be found in a recent Machine Learning special issue [1].  After a review of metric-based model selection methods, we introduce the extensions proposed in this paper that deal specifically with time-series data.

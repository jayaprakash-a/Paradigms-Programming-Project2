Accelerating Reinforcement Learning through the Discovery of Useful Subgoals
 Abstract An ability to adjust to changing environments and unforeseen circumstances is likely to be an important component of a successful autonomous space robot.  This paper shows how to augment reinforcement learning algorithms with a method for automatically discovering certain types of subgoals online.  By creating useful new subgoals while learning, the agent is able to accelerate learning on a current task and to transfer its expertise to related tasks through the reuse of its ability to attain subgoals.  Subgoals are created based on commonalities across multiple paths to a solution.  We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions.  We introduced this approach in [10] and here we present additional results for a simulated mobile robot task.

Joint Learning Improves Semantic Role Labeling
 Abstract Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.  This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.  We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models.  This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank.

Improved Switching among Temporally Abstract Actions
 Abstract In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps hand-crafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible.  In this paper we present a framework based on Markov decision processes and semi-Markov decision processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem.  In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers, with negligible additional cost and no re-planning.  In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.  In many applications, solutions to parts of a task are known, either because they were handcrafted by people or because they were previously learned or planned.  For example, in robotics applications, there may exist controllers for moving joints to positions, picking up objects, controlling eye movements, or navigating along hallways.  More generally, an intelligent system may have available to it several temporally extended courses of action to choose from.  In such cases, a key challenge is to take full advantage of the existing temporally extended actions, to choose or switch among them effectively, and to plan at their level rather than at the level of individual actions.  Recently, several researchers have begun to address these challenges within the framework of reinforcement learning and Markov decision processes (e. g. , Singh, 1992; Kaelbling, 1993; Dayan & Hinton, 1993; Thrun and Schwartz, 1995; Sutton, 1995; Dietterich, 1998; Parr & Russell, 1998; McGovern, Sutton & Fagg, 1997).  Common to much of this recent work is the modeling of a temporally extended action as a policy (controller) and a condition for terminating, which we together refer to as an option (Sutton, Precup & Singh, 1998).  In this paper we consider the problem of effectively combining given options into one overall policy, generalizing prior work by Kaelbling (1993).  Sections 1--3 introduce the framework; our new results are in Sections 4 and 5.  1 Reinforcement Learning (MDP) Framework In a Markov decision process (MDP), an agent interacts with an environment at some discrete, lowest-level time scale t = 0; 1; 2; : : : On each time step, the agent perceives the state of the environment, s t 2 S, and on that basis chooses a primitive action, a t 2 A.  In response to each action, a t , the environment produces one step later a numerical reward, r t+1 , and a next state, s t+1 .  The one-step model of the environment consists of the one-step statetransition probabilities and the one-step expected rewards, p a ss 0 = Prfs t+1 = s 0 j s t = s; a t = ag and r a s = Efr t+1 j s t = s; a t = ag; for all s; s 0 2 S and a 2 A.  The agent's objective is to learn an optimal Markov policy, a mapping from states to probabilities of taking each available primitive action, : S \Theta A ! [0; 1], that maximizes the expected discounted future reward from each state s: V (s) = E n r t+1 + flr t+2 + \Delta \Delta \Delta fi fi fi s t = s; o = X a2As (s; a)[r a s + fl X s 0 p a ss 0 V (s 0 )]; where (s; a) is the probability with which the policy chooses action a 2 A s in state s, and fl 2 [0; 1] is a discount-rate parameter.  V (s) is called the value of state s under policy , and V is called the state-value function for .  The optimal state-value function gives the value of a state under an optimal policy: V \Lambda (s) = max V (s) = max a2As [r a s + fl P s 0 p a ss 0 V \Lambda (s 0 )].  Given V \Lambda , an optimal policy is easily formed by choosing in each state s any action that achieves the maximum in this equation.  A parallel set of value functions, denoted Q and Q \Lambda , and Bellman equations can be defined for state-action pairs, rather than for states.  Planning in reinforcement learning refers to the use of models of the environment to compute value functions and thereby to optimize or improve policies.  2 Options We use the term options for our generalization of primitive actions to include temporally extended courses of action.  Let h t;T = s t ; a t ; r t+1 ; s t+1 ; a t+1 ; : : : ; r T ; s T be the history sequence from time t T to time T , and let \Omega denote the set of all possible histories in the given MDP.  Options consist of three components: an initiation set I ` S, a policy : \Omega \Theta A ! [0; 1], and a termination condition fi : \Omega ! [0; 1].  An option o = hI; ; fii can be taken in state s if and only if s 2 I.  If o is taken in state s t , the next action a t is selected according to (s t ; \Delta).  The environment then makes a transition to s t+1 , where o terminates with probability fi(h t;t+1 ), or else continues, determining a t+1 according to (h t;t+1 ; \Delta), and transitioning to state s t+2 , where o terminates with probability fi(h t;t+2 ) etc.  We call the general options defined above semi-Markov because and fi depend on the history sequence; in Markov options and fi depend only on the current state.  Semi-Markov options allow "timeouts", i. e. , termination after some period of time has elapsed, and other extensions which cannot be handled by Markov options.  The initiation set and termination condition of an option together limit the states over which the option's policy must be defined.  For example, a hand-crafted policy for a mobile robot to dock with its battery charger might be defined only for states I in which the battery charger is within sight.  The termination condition fi would be defined to be 1 outside of I and when the robot is successfully docked.  We can now define policies over options.  Let the set of options available in state s be denoted O s ; the set of all options is denoted O = S s2S O s .  When initiated in a state s t , the Markov policy over options : S \Theta O ! [0; 1] selects an option o 2 O s t according to the probability distribution (s t ; \Delta).  The option o is then taken in s t , determining actions until it terminates in s t+k , at which point a new option is selected, according to (s t+k ; \Delta), and so on.  In this way a policy over options, , determines a (non-stationary) policy over actions, or flat policy, = f().  We define the value of a state s under a general flat policy as the expected return

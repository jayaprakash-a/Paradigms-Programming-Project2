Driver Behavior Recognition and Prediction in a SmartCar
 ABSTRACT This paper presents our SmartCar testbed platform: a real-time data acquisition and playback system and a machine learning --dynamical graphical models-- framework for modeling and recognizing driver maneuvers at a tactical level, with particular focus on how contextual information affects the driver's performance.  The SmartCar's perceptual input is multi-modal: four video signals capture the surrounding traffic, the driver's head position and the driver's viewpoint; and a real-time data acquisition system records the car's brake, gear, steering wheel angle, speed and acceleration throttle signals.  We have carried out driving experiments with the instrumented car over a period of 2 months.  Over 70 drivers have driven the SmartCar for 1. 25 hours in the greater Boston area.  Dynamical Graphical models, HMMs and potentially extensions (CHMMs), have been trained using the experimental driving data to create models of seven different driver maneuvers: passing, changing lanes right and left, turning right and left, starting and stopping.  These models are essential to build more realistic automated cars in car simulators, to improve the human-machine interface in driver assistance systems, to prevent potential dangerous situations and to create more realistic automated cars in car simulators.

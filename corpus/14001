Neural Networks for Predicting Kaposi's Sarcoma
 often very limited.  This applies especially to the study of Kaposi's Sarcoma (KS), a vascular tumour which is common and often aggressive in patients with underlying immunosuppression (e. g.  AIDS).  The objective of this project is to deploy neural networks to predict the development of KS and to determine factors that influence the variable progression rate in HIV infected individuals by multi-variable analysis in order to define clinical end-points and provide guidelines for better patient management.  II.  Methodology A.  Neural network model and training scheme As the total number of KS+ patients available in the study is limited to about 30 cases (out of a total of about 120 AIDS patients), backpropagation-like training schemes give, in general, a poor generalisation performance for the following reasons: (1) The minimum of the cost function corresponds to the mode of the posterior weight distribution, which for sparse data is unrepresentative of the distribution as a whole (over-fitting).  (2) Cross-validation-based regularisation schemes are not applicable due to the sparseness of the data.  We therefore adopt a Bayesian approach and integrate over the weights w and weight-decay parameters ff (the `hyperparameters') to obtain the probability of the target y (binary variable indicating KS+ if y = 1 or KS-- if y = 0) conditional only on the input vector x and the training data D = fxn ; yng, as suggested by Neal [4]: P (yjx; D) = Z P (yjw; x)P (w; ffjD)dwdff (1) where P (yjw; x) is the output of an appropriate neural network (employing a sigmoidal transfer function at the output node to ensure that P (yjw; x) 2 [0; 1]) with weights w.  This integral can be solved numerically by Markov chain Monte Carlo (MCMC), that is, by constructing a Markov chain f(w t ; ff t )g which converges to a sample drawn from the posterior distribution P (w; ffjD) and then, after a sufficiently long equilibration period TE , approximating the integral by a sum over this sample, P (yjx; D) 1 T TE+T X t=TE+1 P (yjx; w t ) (2) By combining all weights exiting the same input node into a distinct weight group and introducing a separate weight decay parameter ff k for any such group, the relevance of an input should be reflected, in principle, by the posterior distribution of the respective weight decay parameter ff k (the method of automatic relevance determination (ARD); see [4] and [3]): For an irrelevant input this posterior distribution will be concentrated on large values of ff k such that predominantly large values will be sampled.  This will suppress the weights exiting the respective input unit and thus effectively switch its contribution off.  B.  Performance assessment Note that the objective of the neural network is the prediction of the conditional probability P (KS + jx); that is, the probability that a patient is KS+ conditional on the vector of measurements x.  This is different from the actual classification of a patient as KS+ or KS--, which is effected by subjecting the neural network output to a threshold function and classifying a patient as KS+ if the output is larger than ` 2 [0; 1]: Patient KS+ if P (KS + jx) ? ` Patient KS\Gamma if P (KS + jx) ` While the threshold parameter is commonly set to a fixed value of ` = 0:5, this would be suboptimal in our application due to the strongly unbalanced data set.  Also, any fixed choice of ` does not reflect the operational procedure in a medical decision process, where various risk factors have to be considered and the costs of wrong positive decisions have to be weighed against those of wrong negative decisions.  We therefore vary the threshold parameter ` over its entire range [0; 1] and plot the ensuing receiver operating characteristic (ROC) curves of true positive predictions, P (predict KS+jpatient is KS+), against false alarms, P (predict KS+jpatient is KS\Gamma ), as suggested, for instance, in [5].  Note that the area under the ROC curve (AUROC) is a value between 0 and 1, where AUROC=0. 5 is equal to the performance of a random classifier, whereas AUROC=1 indicates a perfect prediction.  This value is more informative than a percent misclassification rate for a fixed threshold ` and will be used for model assessment in the remainder of this paper.  (In what follows, we multiply the AUROC values always by factor of 100 to give a number that is similar to percent classification, with 50 indicating random and 100 indicating perfect classification) III.  Details of the experiments We trained a one-hidden-layer MLP with m input units, H = 8 hidden tanh units 1 and all-to-all connections between adjacent layers on the cross-entropy cost function (the likelihood term) E = ln P (Djw): E = X n h yn ln P (yn jxn ; w)+(1 \Gamma yn ) ln i 1 \Gamma P (yn jxn ; w) ji (3) The network parameters were divided into m+3 weight groups as follows: (1; : : : ; m) : separate weight groups for all the weights exiting the same input unit; (m+1) : hidden-unit biases; (m+ 2) : hidden-to-output weights; (m + 3) : output-unit biases.  We put a Gaussian prior (which is equivalent to a linear weight-decay term) on each weight group: p(w gi jff g ) = exp i \Gamma ff g 2 w 2 gi j (4) where ff g is the inverse variance or weight-decay hyperparameter of the gth weight group, and w gi represents the ith weight in the gth weight group.  The hyperparameters ff g were drawn from a gamma distribution 1 Note that the number of hidden units is not critical due to the Bayesian paradigm; see [4].

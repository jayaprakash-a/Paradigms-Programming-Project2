Evaluation of formant-like features for automatic speech recognition
 Abstract This study investigates possibilities to find a low-dimensional, formant-related physical representation of speech signals, which is suitable for automatic speech recognition.  This aim is motivated by the fact that formants are known to be discriminant features for speech recognition.  Combinations of automatically extracted formant-like features and state-of-the-art, noise-robust features have previously been shown to be more robust in adverse conditions than state-of-the-art features alone.  However, it is not clear how these automatically extracted formant-like features behave in comparison with true formants.  The purpose of this paper is to investigate two methods to automatically extract formant-like features, i. e.  robust formants and HMM2 features, and to compare these features to hand-labeled formants as well as to mel-frequency cepstral coecients in terms of their performance on a vowel classification task.  The speech data and hand-labeled formants that were used in this study are a subset of the American English vowels database presented in [Hillenbrand et al. , J.  Acoust.  Soc.  Am.  97, 3099-3111 (1995)].  Classification performance was measured on the original, clean data as well as in (simulated) adverse conditions.  In combination with standard automatic speech recognition methods, the classification performance of the robust formant and HMM2 features compare very well to the performance of the hand-labeled formants.

Learning Nonlinear Dynamical Systems Using an EM Algorithm
 Abstract The Expectation{Maximization (EM) algorithm is an iterativeprocedure for maximum likelihood parameter estimation from data sets with missing or hidden variables [2].  It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9].  We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems.  The \expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the \maximization" step re-estimates the parameters using these uncertain state estimates.  In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states.  However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.  1 Stochastic Nonlinear Dynamical Systems We examine inference and learning in discrete-time dynamical systems with hidden state x t , inputs u t , and outputs y t .  1 The state evolves according to stationary nonlinear dynamics driven by the inputs and by additive noise x t+1 = f(x t ;u t )+w (1) 1 All lowercase characters (except indices) denote vectors.  Matrices are represented by uppercase characters.  where w is zero-mean Gaussian noise with covariance Q.  2 The outputs are nonlinearly related to the states and inputs by y t = g(x t ;u t )+v (2) where v is zero-mean Gaussian noise with covariance R.  The vector-valued nonlinearities f and g are assumed to be differentiable, but otherwise arbitrary.  Models of this kind have been examined for decades in various communities.  Most notably, nonlinear state-space models form one of the cornerstones of modern systems and control engineering.  In this paper, we examine these models within the framework of probabilistic graphical models and deriveanovel learning algorithm for them based on EM.  With one exception, 3 this is to the best of our knowledge the first paper addressing learning of stochastic nonlinear dynamical systems of the kind wehave described within the framework of the EM algorithm.  The classical approach to system identification treats the parameters as hidden variables, and applies the Extended Kalman Filtering algorithm (described in section 2) to the nonlinear system with the state vector augmented by the parameters [5].  4 This approach is inherently on-line, whichmay be important in certain applications.  Furthermore, it provides an estimate of the covariance of the parameters at each time step.  In contrast, the EM algorithm we presentisabatch algorithm and does not attempt to estimate the covariance of the parameters.  There are three important advantages the EM algorithm has over the classical approach.  First, the EM algorithm provides a straightforward and principled method for handing missing inputs or outputs.  Second, EM generalizes readily to more complex models with combinations of discrete and real-valued hidden variables.  For example, one can formulate EM for a mixture of nonlinear dynamical systems.  Third, whereas it is often very difficult to prove or analyze stability within the classical on-line approach, the EM algorithm is always attempting to maximize the likelihood, which acts as a Lyapunov function for stable learning.  In the next sections we will describe the basic components of the learning algorithm.  For the expectation step of the algorithm, we infer the conditional distribution of the hidden states using Extended Kalman Smoothing (section 2).  For the maximization step we#rstdiscuss the general case (section 3) and then describe the particular case where the nonlinearities are represented using Gaussian radial basis function (RBF; [6]) networks (section 4).  2 Extended Kalman Smoothing Given a system described by equations (1) and (2), we need to infer the hidden states from a history of observed inputs and outputs.  The quantity at the heart of this inference problem is the conditional density P (x t ju 1 ;:::;u T ;y 1 ;:::;y T ), for 1 # t # T,which captures the fact that the system is stochastic and therefore our inferences about x will be uncertain.  2 The Gaussian noise assumption is less restrictive for nonlinear systems than for linear systems since the nonlinearity can be used to generate non-Gaussian state noise.  3 The authors have just become aware that Briegel and Tresp (this volume) have applied EM to essentially the same model.  Briegel and Tresp's method uses multilayer perceptrons (MLP) to approximate the nonlinearities, and requires sampling from the hidden states to fit the MLP. We use Gaussian radial basis functions (RBFs) to model the nonlinearities, which can be fit analytically without sampling (see section 4).

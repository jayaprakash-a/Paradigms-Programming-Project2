INTERNATIONAL COMPUTER SCIENCE INSTITUTE
 Abstract Whereas most state-of-the-art speech recognition systems use spectral or cepstral representations of the speech signal, there have also been some promising attempts at using articulatory information.  These attempts have been motivated by two major assumptions: first, coarticulation can be modeled more naturally due to the inherently asynchronous nature of articulatory information.  Second, it is assumed that the overall patterns in the speech signal caused by articulatory gestures are more robust to noise and speaker-dependent acoustic variation than spectral parameters.  A third assumption can be made, viz.  that acoustic and articulatory representations of speech can supply mutually complementary information to a speech recognizer, in which case the combination of these representations might be beneficial.  Previously, articulatory-based speech recognizers have exclusively been developed for clean speech; the potential of an articulatory representation of the speech signal for noisy test conditions, by contrast, has not been explored.  Moreover, there have barely been attempts at systematically combining articulatory information with standard acoustic recognizers.  This paper investigates the second and third of the above assumptions by reporting speech recognition results on a variety of acoustic test conditions for individual acoustic and articulatory speech recognizers, as well as for a combined system.  On a continuous numbers recognition task, the acoustic system generally performs equal to, or slightly better than, the articulatory system, whereas the articulatory system shows a statistically significant improvement on noisy speech with a low signal-to-noise ratio.  The combined system nearly always performs significantly better than either of the individual systems.

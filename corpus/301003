The Role of Causal Models in Reasoning Under Uncertainty
 Abstract Numerous studies of how people reason with statistical data suggest that human judgment often fails to approximate rational probabilistic (Bayesian) inference.  We argue that a major source of error in these experiments may be misunderstanding causal structure.  Most laboratory studies demonstrating probabilistic reasoning deficits fail to explain the causal relationships behind the statistics presented, or they suggest causal mechanisms that are not compatible with people's prior theories.  We propose that human reasoning under uncertainty naturally operates over causal mental models, rather than pure statistical representations, and that statistical data typically support correct Bayesian inference only when they can be incorporated into a causal model consistent with people's theory of the relevant domain.  We show that presenting people with questions that clearly explain an intuitively natural causal structure responsible for a set of statistical data significantly improves their performance.  In particular, we describe two modifications to the standard medical diagnosis scenario that each eliminates the phenomenon of base-rate neglect, merely by clarifying the causal structure behind false-positive test results.

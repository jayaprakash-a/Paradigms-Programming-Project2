Machine Learning: Proceedings of the Fourteenth International Conference
 Abstract.  This paper describes a new technique for solving multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Dietterich and Bakiri's method of error-correcting output codes (ECOC).  Boosting is a general method of improving the accuracy of a given base or "weak" learning algorithm.  ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems.  We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning algorithm work on binary-labeled data.  Like boosting, we prove that the method comes with strong theoretical guarantees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing.  Althoughprevious methodswere known for boostingmulticlass problems, the new method may be significantly faster and require less programming effort in creating the base learning algorithm.  We also compare the new algorithm experimentally to other voting methods.

Task Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks
 Abstract A novel modular connectionist architecture is presented in which the networks composing the architecture compete to learn the training patterns.  An outcome of the competition is that different networks learn different training patterns and, thus, learn to compute different functions.  The architecture performs task decomposition in the sense that it learns to partition a task into two or more functionally independent tasks and allocates distinct networks to learn each task.  In addition, the architecture tends to allocate to each task the network whose topology is most appropriate to that task.  The architecture's performance on "what" and "where" vision tasks is presented and compared with the performance of two multi--layer networks.  Finally, it is noted that function decomposition is an underconstrained problem and, thus, different modular architectures may decompose a function in different ways.  We argue that a desirable decomposition can be achieved if the architecture is suitably restricted in the types of functions that it can compute.  Appropriate restrictions can be found through the application of domain knowledge.  A strength of the modular architecture is that its structure is well--suited for incorporating domain knowledge.  Although many biologists and psychologists share the view that the brain has a modular architecture, there is no general agreement on the number of modules, the function of the modules, the nature of the interaction between modules, or the manner in which the modules develop.  One reason for this diversity of opinion is that answering questions about the modular nature of the brain involves the difficult task of reasoning about a system with a large number of interacting components.  Even systems of interacting components with a small fraction of the brain's complexity present formidable conceptual and analytical difficulties.  In many cases, mathematical and computer models provide essential tools for understanding aspects of these systems.  One class of models that has the potential for helping to answer questions about modular systems is the class of connectionist models, also known as artificial neural network models.  A hierarchical classification of the components of connectionist models may be defined in which a unit is the finest level of classification, a layer is a coarser level, and a network is a still coarser level.  Connectionist researchers typically design systems that are modular at the level of units or layers.  In this paper we argue that there are significant practical and theoretical advantages to be realized by considering modularity at the level of networks.  In other words, we argue that connectionist architectures should consist of multiple networks, and that connectionist learning algorithms should be designed to take advantage of this modular structure.  Although terms such as layer or network are imprecise, it is generally agreed that they provide a convenient language for discussing connectionist architectures.  An analogous situation occurs in the neurosciences where researchers debate whether nervous systems are

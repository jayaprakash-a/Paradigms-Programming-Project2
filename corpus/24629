The Variable Bandwidth Mean Shift and Data-Driven Scale Selection
 Abstract We present two solutions for the scale selection problem in computer vision.  The first one is completely nonparametric and is based on the the adaptive estimation of the normalized density gradient.  Employing the sample point estimator, we define the Variable Bandwidth Mean Shift, prove its convergence, and show its superiority over the fixed bandwidth procedure.  The second technique has a semiparametric nature and imposes a local structure on the data to extract reliable scale information.  The local scale of the underlying density is taken as the bandwidth which maximizes the magnitude of the normalized mean shift vector.  Both estimators provide practical tools for autonomous image and quasi real-time video analysis and several examples are shown to illustrate their effectiveness.  1 Motivation for Variable Bandwidth The efficacy of Mean Shift analysis has been demonstrated in computer vision problems such as tracking and segmentation in [5, 6].  However, one of the limitations of the mean shift procedure as defined in these papers is that it involves the specification of a scale parameter.  While results obtained appear satisfactory, when the local characteristics of the feature space differs significantly across data, it is difficult to find an optimal global bandwidth for the mean shift procedure.  In this paper we address the issue of locally adapting the bandwidth.  We also study an alternative approach for data-driven scale selection which imposes a local structure on the data.  The proposed solutions are tested in the framework of quasi real-time video analysis.  We review first the intrinsic limitations of the fixed bandwidth density estimation methods.  Then, two of the most popular variable bandwidth estimators, the balloon and the sample point, are introduced and their advantages discussed.  We conclude the section by showing that, with some precautions, the performance of the sample point estimator is superior to both fixed bandwidth and balloon estimators.  1. 1 Fixed Bandwidth Density Estimation The multivariate fixed bandwidth kernel density estimate is defined by ^ f(x) = 1 nh d n X i=1 K ` x\Gamma x i h ' : (1) where the d-dimensional vectors fx i g i=1:::n represent a random sample from some unknown density f and the kernel, K, is taken to be a radially symmetric, nonnegative function centered at zero and integrating to one.  The terminology fixed bandwidth is due to the fact that h is held constant across x 2 R d .  As a result, the fixed bandwidth procedure (1) estimates the density at each point x by taking the average of identically scaled kernels centered at each of the data points.  For pointwise estimation, the classical measure of the closeness of the estimator ^ f to its target value f is the mean squared error (MSE), equal to the sum of the variance and squared bias MSE(x) = E h ^ f(x) \Gamma f(x) i 2 = Var i ^ f(x) j + h Bias i ^ f(x) ji 2 : (2) Using the multivariate form of the Taylor theorem, the bias and the variance are approximated by [20, p. 97] Bias(x) 1 2 h 2 2 (K )\Deltaf (x) (3) and Var(x) n\Gamma 1 h \Gamma d R(K)f(x) ; (4) where 2 (K) = R z 2 1 K(z)dz and R(K) = R K(z)dz are kernel dependent constants, z 1 is the first component of the vector z, and \Delta is the Laplace operator.  The tradeoff of bias versus variance can be observed in (3) and (4).  The bias is proportional to h 2 , which means that smaller bandwidths give a less biased estimator.  However, decreasing h implies an increase in the variance which is proportional to n \Gamma 1 h \Gamma d .  Thus for a fixed bandwidth estimator we should choose h that achieves an optimal compromise between the bias and variance over all x 2 R d , i. e. , minimizes the mean integrated squared error (MISE) MISE(x) = E Z i ^ f(x) \Gamma f(x) j 2 dx : (5) Nevertheless, the resulting bandwidth formula (see [17, p. 85], [20, p. 98]) is of little practical use, since it depends on the Laplacian of the unknown density being estimated.  The best of the currently available data-driven methods for bandwidth selection seems to be the plug-in rule [15], which was proven to be superior to least squares cross validation and biased cross-validation [11], [16, p. 46].  A practical one dimensional algorithm based on this method is described in the Appendix.  For the multivariate case, see [20, p. 108].  Note that these data-driven bandwidth selectors work well for multimodal data, their only assumption being a certain smoothness in the underlying density.  However, the fixed bandwidth affects the estimation performance, by undersmoothing the tails and oversmoothing the peaks of the density.  The performance also decreases when the data exhibits local scale variations.  1. 2 Balloon and Sample Point Estimators According to expression (1), the bandwidth h can be varied in two ways.  First, by selecting a different bandwidth h = h(x) for each estimation point x, one can define the balloon density estimator ^ f 1 (x) = 1 nh(x) d n X i=1 K ` x\Gamma x i h(x) ' : (6) In this case, the estimate of f at x is the average of identically scaled kernels centered at each data point.  Second, by selecting a different bandwidth h = h(x i ) for each data point x i we obtain the sample point density estimator ^ f 2 (x) = 1 n n X i=1 1 h(x i ) d K ` x \Gamma x i h(x i ) ' : (7) for which the estimate of f at x is the average of differently scaled kernels centered at each data point.  While the balloon estimator has more intuitive appeal, its performance improvement over the fixed bandwidth estimator is insignificant.  When the bandwidth h(x) is chosen as a function of the k-th nearest neighbor, the bias and variance are still proportional to h 2 and n\Gamma 1 h \Gamma d , respectively [8].  In addition, the balloon estimators usually fail to integrate to one.  The sample point estimators, on the other hand, are themselves densities, being non-negative and integrating to one.  Their most attractive property is that a particular choice of h(x i ) reduces considerably the bias.  Indeed, when h(x i ) is taken to be reciprocal to the square root of f(x i ) h(x i ) = h 0 f(x i ) 1=2 (8) the bias becomes proportional to h 4 , while the variance remains unchanged, proportional to n \Gamma 1 h \Gamma d [1, 8].  In (8), h 0 represents a fixed bandwidth and is a proportionality constant.  Since f(x i ) is unknown it has to be estimated from the data.  The practical approach is to use one of the methods described in Section 1. 1 to find h 0 and an initial estimate (called pilot) of f denoted by ~ f .  Note that by using ~ f instead of f in (8), the nice properties of the sample point estimators (7) remain unchanged [8].  Various authors [16, p. 56], [17, p. 101] remarked that the method is insensitive to the fine detail of the pilot estimate.  The only provision that should be taken is to bound the pilot density away from zero.  The final estimate (7) is however influenced by the choice of the proportionality constant , which divides the range of density values into low and high densities.  When the local density is low, i. e. , ~ f(x i ) ! , h(x i ) increases relative to h 0 implying more smoothing for the point x i .  For data points that verify ~ f(x i ) ? , the bandwidth becomes narrower.  A good initial choice [17, p. 101] is to take as the geometric mean of n ~ f(x i ) o i=1:::n .  Our experiments have shown that for superior results, a certain degree of tuning is required for .  Nevertheless, the sample point estimator proved to be almost all the time much better than the fixed bandwidth estimator.

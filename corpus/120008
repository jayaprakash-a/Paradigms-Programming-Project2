FeatureBoost: A Meta-Learning Algorithm that Improves Model Robustness
 Abstract Most machine learning algorithms are lazy: they extract from the training set the minimum information needed to predict its labels.  Unfortunately, this often leads to models that are not robust when features are removed or obscured in future test data.  For example, a backprop net trained to steer a car typically learns to recognize the edges of the road, but does not learn to recognize other features such as the stripes painted on the road which could be useful when road edges disappear in tunnels or are obscured by passing trucks.  The net learns the minimum necessary to steer on the training set.  In contrast, human driving is remarkably robust as features become obscured.  Motivated by this, we propose a framework for robust learning that biases induction to learn many different models from the same inputs.  We present a meta algorithm for robust learning called FeatureBoost, and demonstrate it on several problems using backprop nets, k-nearest neighbor, and decision trees.  1.  Motivation Consider a backprop net learning to steer a car.  In the ALVINN system (Pomerleau, 1993) the principal internal features learned by ALVINN nets detect the left and right edges of the road.  Typically, ALVINN nets do not learn internal features that detect other road phenomena that could be useful for steering such as road centerlines, roadway signs, trees, other traffic, people, etc.  This creates a problem when the left or right edges of the road are obstructed by passing vehicles, or are missing as on bridges and in tunnels.  Yet human steering is remarkably robust to the loss of these features.  Human drivers can fall back on a number of alternate features as different subsets of road features come in and out of view.  Backprop nets can learn to steer better if they learn to recognize other road features such as centerlines (Caruana, 1997).  How can we force backprop nets to learn to use a variety of road features when learning to steer? A related problem arises in health care (Cooper et al. , 1997).  Basic inputs such as age, gender, and blood pressure are available for most patients before they enter the hospital.  Other measurements such as RBC counts, oxygenation, and Albumin become available after patients are hospitalized.  As you would expect, models trained to predict patient risk from both the pre and in-hospital features usually outperform models trained to predict risk from only the prehospital inputs.  But these models perform poorly on patients not yet admitted to the hospital when one marginalizes over the missing in-hospital features.  Models that use only the pre-hospital inputs are more accurate for patients not yet admitted to the hospital than marginalized models trained on all the features.  How can we force learning to learn models that make better predictions when some input features (such as the in-hospital attributes) are missing for some test cases? If the edges of the road, or the in-hospital features are always available, models learned the usual way perform well.  In the ALVINN and health care problems above, the difficulty arises when features are missing or obscured in the test cases.  Boosting algorithms such as AdaBoost are one way to make learned models more robust to feature obscuration.  If the main features such as the edges of the road are obscured or missing from a few training cases, boosting places more emphasis on these cases because they are predicted poorly.  This emphasis forces the learning algorithm to use other features such as road centerlines for these cases.  Unfortunately, boosting learns about centerlines by strongly emphasizing the cases that are missing road edges, even though centerlines may be visible in all images.  Boosting could learn about other features better if it used all of the training data containing those features to learn about them.  How can we make boosting take full advantage of all the redundant information in the training set? This paper introduces a general framework for induction called robust learning, which is motivated by our desire to model situations where features may be corrupted or missing in ways not adequately represented in the training set.  Guided by the framework, we devise a meta-learning algorithm called FeatureBoost, that trains models to use different subsets of features.  Because the final prediction from FeatureBoost combines the predictions of models that depend on different (often overlapping) subsets of features, it is more robust to missing or obscured features.  We develop the paper as follows: 1 Present a general framework for robust learning.  2 Examine a specialization of this framework that suggests one way to improve robustness.  3 Develop a meta-learning algorithm (FeatureBoost) inspired by this model.  4 Test FeatureBoost on a variety of learning problems and machine learning algorithms.

Bayesian Model Comparison by Monte Carlo Chaining
 Abstract The techniques of Bayesian inference have been applied with great success to many problems in neural computing including evaluation of regression functions, determination of error bars on predictions, and the treatment of hyper-parameters.  However, the problem of model comparison is a much more challenging one for which current techniques have significant limitations.  In this paper we show how an extended form of Markov chain Monte Carlo, called chaining, is able to provide effective estimates of the relative probabilities of different models.  We present results from the robot arm problem and compare them with the corresponding results obtained using the standard Gaussian approximation framework.  1 Bayesian Model Comparison In a Bayesian treatment of statistical inference, our state of knowledge of the values of the parameters w in a model M is described in terms of a probability distribution function.  Initially this is chosen to be some prior distribution p(wjM), which can be combined with a likelihood function p(Djw; M) using Bayes' theorem to give a posterior distribution p(wjD; M) in the form p(wjD; M) = p(Djw; M)p(wjM) p(DjM) (1) where D is the data set.  Predictions of the model are obtained by performing integrations weighted by the posterior distribution.  The comparison of different models M i is based on their relative probabilities, which can be expressed, again using Bayes' theorem, in terms of prior probabilities p(M i ) to give p(M i jD) p(M j jD) = p(DjM i )p(M i ) p(DjM j )p(M j ) (2) and so requires that we be able to evaluate the model evidence p(DjM i ), which corresponds to the denominator in (1).  The relative probabilities of different models can be used to select the single most probable model, or to form a committee of models, weighed by their probabilities.  It is convenient to write the numerator of (1) in the form expf\Gamma E(w)g, where E(w) is an error function.  Normalization of the posterior distribution then requires that p(DjM) = Z expf\Gamma E(w)g dw: (3) Generally, it is straightforward to evaluate E(w) for a given value of w, although it is extremely difficult to evaluate the corresponding model evidence using (3) since the posterior distribution is typically very small except in narrow regions of the high-dimensional parameter space, which are unknown a-priori.  Standard numerical integration techniques are therefore inapplicable.  One approach is based on a local Gaussian approximation around a mode of the posterior (MacKay, 1992).  Unfortunately, this approximation is expected to be accurate only when the number of data points is large in relation to the number of parameters in the model.  In fact it is for relatively complex models, or problems for which data is scarce, that Bayesian methods have the most to offer.  Indeed, Neal, R.  M.  (1996) has argued that, from a Bayesian perspective, there is no reason to limit the number of parameters in a model, other than for computational reasons.  We therefore consider an approach to the evaluation of model evidence which overcomes the limitations of the Gaussian framework.  For additional techniques and references to Bayesian model comparison, see Gilks et al.  (1995) and Kass and Raftery (1995).  2 Chaining Suppose we have a simple model M 0 for which we can evaluate the evidence analytically, and for which we can easily generate a sample w l (where l = 1; : : : ; L) from the corresponding distribution p(wjD; M 0 ).  Then the evidence for some other model M can be expressed in the form p(DjM) p(DjM 0 ) = Z expf\Gamma E(w) +E 0 (w)gp(wjD; M 0 ) dw ' 1 L L X l=1 expf\Gamma E(w l ) +E 0 (w l )g: (4) Unfortunately, the Monte Carlo approximation in (4) will be poor if the two error functions are significantly different, since the exponent is dominated by regions where E is relatively small, for which there will be few samples unless E 0 is also small in those regions.  A simple Monte Carlo approach will therefore yield poor results.  This problem is equivalent to the evaluation of free energies in statistical physics,

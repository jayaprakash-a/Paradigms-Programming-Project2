Learning Domain Structures
 Psychologists have argued that cognition in different domains draws on qualitatively different mental representations.  Tree structures appear well-suited to representing relationships between animal species [1, 2, 10], while a one-dimensional structure (the liberalconservative spectrum) seems better for representing people's political views.  The possibility of different structures raises a fundamental question: how do people learn what kind of structure is appropriate in each domain? The standard approach to this question is to reject one of its assumptions.  Nativists deny that core structures are learned, at least for evolutionarily important domains like folkbiology.  Instead, infants come equipped with innate knowledge about which structures are appropriate for which domains.  Atran [1], for example, argues that folkbiology is a core domain of human knowledge, and that the tendency to group living kinds into hierarchies reflects an "innately determined cognitive structure. " More generally, Keil [8] has argued that ontological knowledge obeys an innate "M-constraint", requiring the extensions of predicates to conform to rigidly treestructured hierarchies of objects.  Alternatively, empiricists generally deny that structured representations are present at all.  Domain-specific representations are merely emergent properties of unstructured, domain-general associative learning architectures.  McClelland and Rogers [12], for example, have recently suggested that the acquisition of semantic knowledge in domains such as intuitive biology can be explained as learning in a generic connectionist network.  Their architecture never explicitly represents any tree structure, although with repeated training, its hidden unit representations may implicitly come to approximate the taxonomic relations between biological species.  This paper proposes an alternative approach -- structure learning -- that combines important insights from both of these traditions.  Our key contribution is to show how structured domain representations can be acquired within a domain-general framework for Bayesian inference.  Like nativists, we suggest that different domains are represented with qualitatively different structures, and we show how these structured representations serve as critical constraints on inductive generalization.  Like empiricists, though, we emphasize the importance of learning, and attempt to show how domain structures can be acquired through domain-general statistical inference.  This is not only more parsimonious than the nativist position, but allows us to explain the origin of structured representations in novel domains, where the prior existence of domain-specific innate structure is highly implausible.  After describing our structure learning framework, we present two empirical tests of its performance.  First, we show that it chooses the appropriate domain structure for both synthetic and real-world data sets.  It correctly chooses a tree structure for a biological domain (animal feature judgments), and a linear structure for a political domain (US Supreme Court decisions).  Second, we model two classic data sets of inductive judgments in biology [13] and show that our framework performs better than an unstructured connectionist approach.  Bayesian structure learning Our proposal takes the form of a rational analysis.  We aim to demonstrate the computational plausibility and explanatory value of Bayesian structure learning, but leave for future work the question of how these computations might be implemented or approximated by cognitive processes.  Assume the learner's data consist of a binary-valued object-feature matrix D specifying the features of each object in a given domain.  In biology, for instance, the rows of D might correspond to species, and the columns to anatomical and behavioral attributes.  The entry in row i and column j would then specify the value of feature j for species i.  Structurelearning includes computational problems at two levels.  First, which structure class is most appropriate for the domain? Second, given a structure class, which structure in that class provides the best account of the data? For instance, suppose that a learner exposed to biological data ends up organizing animal species into a taxonomic tree.  The first problem asks how she knew to use a tree rather than some other kind of structure.  The second problem asks why she settled on one specific tree instead of the many other trees she might have chosen.  Our focus here is on the first problem -- the problem of inferring the right structure class for a domain.  A solution to the second problem, however, falls out of our probabilistic approach.  We assume that learners come to a domain equipped with a hypothesis space of structure classes, either constructed from innate primitives or based on analogies with previously learned domains.  For simplicity, this paper considers a hypothesis space of just three canonical classes: taxonomic trees, one-dimensional (linear) spaces, and independent feature models.  People surely have access to other classes, including higherdimensional spaces, flat (non-hierarchical) clusterings, and causal networks.  We leave it to future work to characterize the full range of structure classes accessible to human cognition.  In particular, it is an open question whether this space is small enough to be explicitly enumerated as we do here, or is so large (perhaps infinite or uncountable) that it can be specified only implicitly through some generating mechanism.  Future work should also consider the possibility that multiple structures may apply within a single domain.  Given a set of probabilistic models, Bayesian techniques can be used to evaluate which of the models is most likely to have generated some data [7].  Before these techniques can be applied to inferring domain structures, we need to associate each structure class in our hypothesis space with a probabilistic generative model for the features of objects.  The next section defines these models, but here we show how Bayesian inference can be used to choose between them.  Let D be an object-feature matrix generated from one of several structure classes.  The posterior probability of each class C i is proportional to the product of the likelihood p(D

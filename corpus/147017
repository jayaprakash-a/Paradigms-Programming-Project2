Metric Learning by Collapsing Classes
 Abstract We present an algorithm for learning a quadratic Gaussian metric (Mahalanobis distance) for use in classification tasks.  Our method relies on the simple geometric intuition that a good metric is one under which points in the same class are simultaneously near each other and far from points in different classes.  We construct a convex optimization problem which generates such metrics by trying to collapse all examples in the same class to a single point and push examples in other classes infinitely far away.  We show that when the metric we learn is used in a simple classifier, it yields substantial improvements over standard alternatives on a variety of problems; we also discuss how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space allowing more efficient classification with very little reduction in performance.

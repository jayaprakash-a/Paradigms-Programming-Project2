Partially Observable Semi-Markov Decision Processes: Theory and Applications in Engineering and Cognitive Science
 Abstract In this paper we argue that many application areas, both in engineering as well as in biological modeling and cognitive science, require broadening the class of sequential decision-making models from synchronous discrete-time models to asynchronous discrete-event models.  In the latter, actions can be modeled as persisting for extended time periods, and can involve a complex set of lower-level routines or behaviors.  In addition, decision-making epochs are longer strictly in correspondence with state changes.  Rather, decisions are made only in states satisfying one of a set of discrete conditions (or events).  Finally, the environment can change state independently of the agent, due to a set of asynchronous state-changing processes running in parallel.  Although there has been past research in reinforcement learning on discrete event-based models, these studies assume states are complete observable.  We focus in this paper on modeling asynchronous discreteevent systems using partially observable semi-Markov decision processes (POSMDPs).  We describe the basic state estimation and planning algorithms for POSMDPs.  We examine the applicability of the POSMDP model to several engineering problems, in particular robot navigation and flexible manufacturing.  We also discuss specific applications of POSMDPs in modeling the behavior of biological agents, specifically the human foveated visual control system and honeybee navigation.  Motivation Decision-making in many domains can be (rather abstractly) viewed as follows.  At each step, the agent perceives (perhaps imperfectly) the underlying environment as being in one of a (possibly very large, but finite) set of states.  The agent chooses one of a set of finite actions in a given state, and carries it out.  The action modifies the environment in some way (or transports the agent around), thereby modifying the perceived state into a new state.  One time unit later, the agent repeats the process.  Much recent work in control of autonomous agents, including reinforcement learning (Sutton & Barto 1998), decision-theoretic planning (Boutilier, Dearden, & Goldszmidt 1995), and robot navigation (Koenig & Simmons 1997), has adopted this framework.  Although this discrete-time framework has led to some successful applications, it is a significant simplification of the actual process of decision making in both engineering and biological systems.  For example, consider building a robotic agent (or modeling a human agent (Liu 1998)) that has to drive to a designated location.  There is a great deal of complexity that underlies this task which is not being explicitly modeled in the discrete-time synchronous model.  For example, actions (be they information gathering steps, such as looking in the driving mirror, or state altering steps, such as lane changing) usually invoke a complex set of lower level procedures (moving the head and eyes to a designated target location, turning the steering wheel at a given rate and then a reverse action to recenter).  Actions persist for extended time periods, during which the state is continually changing (the location of the car is changing while the driver is saccading to the driving mirror, and other cars in neighboring lanes are also moving independently).  The agent has to explicitly reason about time: expected arrival time is not solely a function of distance, but depends on traffic conditions, number of traffic lights, stop signs, and turns along the way.  In this paper we extend this discrete-time framework to a discrete-event framework, thereby generalizing it in several ways.  Time is explicitly modeled as a continuous variable, but the agent observes the environment and makes decisions only at certain discrete points (or decision epochs).  In between these epochs, the state of the system can be changing in some complex way, but these changes may (or may not) provide the agent with any additional information.  Furthermore, actions take non-constant time periods, modeled by some arbitrary time distribution.  We use the framework of generalized semi-Markov decision processes (Puterman 1994) as a formal model of event-based state dynamics, but also combine it with the well-known partially observable Markov decision process (POMDP) model (Littman, Cassandra, & Kaelbling 1992).  The POSMDP model makes a critical distinction between the natural process, the actual (possibly unobservable) state trajectory resulting from the combination of (an arbitrarily complex set of) asynchronous process, versus the (partially observable) state of the system at a decision making epoch.  The state transitions at decision-making epochs is termed a semi-Markov model, because the transition depends not only on the current state and action, but also on how long the system has been in the current state.  This is critical in both engineering and biological applications.  For example, whether a machine will transition to a failed state depends not just on the current state, but on the entire history of past states since the last time it was maintained or repaired.  Similarly, the transition from one foveated point in a scene to another depends not only on the past state, but on gaze duration as well.  We discuss a number of applications that motivate the need for considering POSMDP models.  These range from engineering domains, including robot navigation and flexible manufacturing, to biological and cognitive modeling problems, specifically human visual information gathering and honeybee navigation.  The POSMDP model Formally, we model the evolution of the environment at decision epochs as a generalized semi-Markov decision process (SMDP) (Puterman 1994).  An SMDP is defined as a five tuple (S; A; P; R; F ), where S is a finite set of states, A is a set of actions, P is a set of state and action dependent transition probabilities, R is a reward function, and F specifies the probability distribution of transition times for each state-action pair.  P (y j x; a) denotes the probability that action a will cause the system to transition from state x 2 S to y 2 S.  This transition probability function describes the transitions at decision epochs only.  F is a function where F (t j s; a) is the probability that the next decision epoch occurs within t time units, after the agent chooses action a in state s at a decision epoch.  From F , and P , we can compute Q by Q(t; y j x; a) = P (y j x; a)F (t j x; a) where Q denotes the probability that the system will be in state y for the next decision epoch, at or before t time units after choosing action a in state s, at the last decision epoch.  Q can be used to calculate the expected transition time between decision epochs.

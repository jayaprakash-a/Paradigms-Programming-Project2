Training Algorithms for Hidden Markov Models using Entropy Based Distance Functions
 Abstract We present new algorithms for parameter estimation of HMMs.  By adapting a framework used for supervised learning, we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay "close" to the current estimated parameters.  We use a bound on the relative entropy between the two HMMs as a distance measure between them.  The result is new iterative training algorithms which are similar to the EM (Baum-Welch) algorithm for training HMMs.  The proposed algorithms are composed of a step similar to the expectation step of Baum-Welch and a new update of the parameters which replaces the maximization (re-estimation) step.  The algorithm takes only negligibly more time per iteration and an approximated version uses the same expectation step as Baum-Welch.  We evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data.  For sparse models, i. e.  models with relatively small number of non-zero parameters, the proposed algorithms require significantly fewer iterations.  1 Preliminaries We use the numbers from 0 to N to name the states of an HMM.  State 0 is a special initial state and state N is a special final state.  Any state sequence, denoted by s, starts with the initial state but never returns to it and ends in the final state.  Observations symbols are also numbers in f1; : : : ; Mg and observation sequences are denoted by x.  A discrete output hidden Markov model (HMM) is parameterized by two matrices A and B.  The first matrix is of dimension [N; N ] and a i;j (0 i N \Gamma 1; 1 j N) denotes the probability of moving from state i to state j.  The second matrix is of dimension [N +1;M ] and b i;k is the probability of outputting symbol k at state i.  The set of parameters of an HMM is denoted by ` = (A; B).  (The initial state distribution vector is represented by the first row of A. ) An HMM is a probabilistic generator of sequences.  It starts in the initial state 0.  It then iteratively does the following until the final state is reached.  If i is the current state then a next state j is chosen according to the transition probabilities out of the current state (row i of matrix A).  After arriving at state j a symbol is output according to the output probabilities of that state (row j of matrix B).  Let P (x; sj`) denote the probability (likelihood) that an HMM ` generates the observation sequence x on the path s starting at state 0 and ending at state N : P (x; sj jsj = jxj + 1; s 0 = 0; s jsj = N; `) def = Q jxj t=1 a s t\Gamma 1 ;s t b s t ;x t .  For the sake of brevity we omit the conditions on s and x.  Throughout the paper we assume that the HMMs are absorbing, that is from every state there is a path to the final state with a non-zero probability.  Similar parameter estimation algorithms can be derived for ergodic HMMs.  Absorbing HMMs induce a probability over all state-observation sequences, i. e.  P x;s P (x; sj`) = 1.  The likelihood of an observation sequence x is obtained by summing over all possible hidden paths (state sequences), P (xj`) = P s P (x; sj`).  To obtain the likelihood for a set X of observations we simply multiply the likelihood values for the individual sequences.  We seek an HMM ` that maximizes the likelihood for a given set of observations X , or equivalently, maximizes the log-likelihood, LL(X j`) = 1 jX j P x2X ln P (xj`).  To simplify our notation we denote the generic parameter in ` by ` i , where i ranges from 1 to the total number of parameters in A and B (There might be less if some are clamped to zero).  We denote the total number of parameters of ` by I and leave the (fixed) correspondence between the ` i and the entries of A and B unspecified.  The indices are naturally partitioned into classes corresponding to the rows of the matrices.  We denote by [i] the class of parameters to which ` i belongs and by ` [i] the vector of all ` j s. t.  j 2 [i].  If j 2 [i] then both ` i and ` j are parameters from the same row of one of the two matrices.  Whenever it is clear from the context, we will use [i] to denote both a class of parameters and the row number (i. e.  state) associated with the class.  We now can rewrite P (x; sj`) as Q I i=1 ` n i (x;s) i , where n i (x; s) is the number of times parameter i is used along the path s with observation sequence x.  (Note that this value does not depend on the actual parameters `. ) We next compute partial derivatives of the likelihood and the log-likelihood using this notation.  @ @` i P (x; sj`) = ` n 1 (x;s) 1 \Delta \Delta \Delta ` n i\Gamma 1 (x;s) i\Gamma 1 n i (x; s) ` n i (x;s)\Gamma 1 i \Delta \Delta \Delta ` n I (x;s) I = n i (x; s) ` i I Y i=1 ` n i (x;s) i = n i (x; s) ` i P (x; sj`): (1) @LL(X j`) @` i = 1 jX j X x2X X s @ @` i P (

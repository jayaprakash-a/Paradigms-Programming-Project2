The Use of Transfer in Natural Language Processing
 Abstract Many NLP researchers have noted that a parser trained on one domain (e. g.  WSJ articles) may not perform as well on another domain (e. g.  New Yorker articles).  Experiments have shown that replacing in-domain (target) training data with out-of-domain (reference) training data degrades parsing accuracy [Rat99].  Also, adding reference data to a set of target training data improves performance minimally [Gil01].  But, these results do not eliminate the possiblity of gaining useful information from reference data.  [Hwa99] shows that a main contribution of target data is high-level phrase structure information.  Low-level sentence structure can be learned from reference data.  [RB03] shows that reference data is more useful when (1) there is less target data, and (2) when each reference example is given 1/5 th the weight of target example.  While a parser trained on one domain is unlikely to perform well on a different domain, the reference data contains much useful information.  The challenge is finding the right way to synthesize reference and target training data.  The work we discuss shows initial progress, and also shows that there is much left to be done.

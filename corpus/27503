An Alternate Objective Function for Markovian Fields
 Abstract In labelling or prediction tasks, a trained model's test performance is often based on the quality of its single-time marginal distributions over labels rather than its joint distribution over label sequences.  We propose using a new cost function for discriminative learning that more accurately reflects such test time conditions.  We present an ecient method to compute the gradient of this cost for Maximum Entropy Markov Models, Conditional Random Fields, and for an extension of these models involving hidden states.  Our experimental results show that the new cost can give significant improvements and that it provides a novel and effective way of dealing with the 'label-bias' problem.

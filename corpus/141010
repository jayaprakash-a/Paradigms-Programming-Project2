Bayesian Classifiers Are Large Margin Hyperplanes in a Hilbert Space
 simply output one hypothesis, but rather an entire distribution of probability over an hypothesis set: the Bayes posterior.  An alternative perspective is that they output a linear combination of classifiers, whose coefficients are given by Bayes theorem.  One of the concepts used to deal with thresholded convex combinations is the `margin' of the hyperplane with respect to the training sample, which is correlated to the predictive power of the hypothesis itself.  We provide a novel theoretical analysis of such classifiers, based on DataDependent VC theory, proving that they can be expected to be large margin hyperplanes in a Hilbert space.  We then present experimental evidence that the predictions of our model are correct, i. e.  that Bayesian classifiers really find hypotheses which have large margin on the training examples.  This not only explains the remarkable resistance to overfitting exhibited by such classifiers, but also co-locates them in the same class of other systems, like Support Vector machines and Adaboost, which have a similar performance.

On L2-norm Regularization and the Gaussian Prior
 Abstract We show how the regularization used for classification can be seen from the MDL viewpoint as a Gaussian prior on weights.  We consider the problem of transmitting classification labels; we select as our model class logistic regression with perfect precision where we specify a weight for each feature.  This is unrealistic since the encoding length of any such model is infinite, but if we use a Gaussian prior on weights and ignore constant factors, we find that the encoding length objective exactly matches that of logistic regression with an L2-norm regularization penalty.  Through this understanding, we see that the tradeoff parameter is the variance of the Gaussian prior.  It also delineates steps for improved regularization

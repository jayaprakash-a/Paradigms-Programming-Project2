INTERNATIONAL COMPUTER SCIENCE INSTITUTE
 Abstract Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i. e. , pairs with one element from either set.  This includes event co-occurrences, histogram data, and single stimulus preference data as special cases.  Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision.  In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models.  Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery.  Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure.  We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.

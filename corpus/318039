Scaling Large Learning Problems with Hard Parallel Mixtures
 A challenge for statistical learning is to deal with large data sets, e. g.  in data mining.  The training time of ordinary Support Vector Machines is at least quadratic, which raises a serious research challenge if we want to deal with data sets of millions of examples.  We propose a "hard parallelizable mixture" methodology which yields significantly reduced training time through modularization and parallelization: the training data is iteratively partitioned by a "gater" model in such a way that it becomes easy to learn an "expert" model separately in each region of the partition.  A probabilistic extension and the use of a set of generative models allows representing the gater so that all pieces of the model are locally trained.  For SVMs, time complexity appears empirically to locally grow linearly with the number of examples, while generalization performance can be enhanced.  For the probabilistic version of the algorithm, the iterative algorithm provably goes down in a cost function that is an upper bound on the negative log-likelihood.

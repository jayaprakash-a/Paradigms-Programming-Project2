Noise Tolerant Learnability via the Dual Learning Problem
 Abstract.  Much of the research in machine learning and neural computation assumes that the teacher gives the correct answers to the learning algorithm.  However, in many cases this assumption is doubtful, since different sorts of noise may disable the learner from getting the correct answers all the time.  The noise could be caused by noisy communication, human errors, measuring equipment and many other causes.  In some cases, problems which are eciently learnable without noise, become hard to learn when noise is introduced [3].  Partial answers to the noisy learning problem are known for specific classes, ([11]), but in general, no simple parameters are known which distinguish between classes that are learnable in the presence of noise and those which become hard to learn.  The goal of this work is to introduce such parameters.  We use the Membership Query model to show that if the VC-dimension of the dual learning problem is moderate and if the dual learning problem is dense in some sense, then the class is learnable in the presence of persistent random classification noise.  We show that our assumptions hold for a variety of learning problems, such as smooth functions, general geometrical concepts, and monotone monomials.  We are particularly interested in the analysis of smooth function classes.  We show uniform upper bound on the fat-shattering dimension of both the primal and dual learning problem which is derived by a geometric property of the class called type.

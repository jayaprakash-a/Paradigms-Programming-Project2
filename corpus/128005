A bridge between mean field theory and exact inference in probabilistic graphical models
 Abstract Exact inference in large and complex probabilistic graphical models (e. g.  Bayesian networks, Boltzmann machines) is computationally intractable.  Approximate inference methods are therefore of great importance.  In this paper we provide a general scheme in which the original intractable graphical model is approximated by a model with a tractable structure.  The approximating model is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two models.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  The scheme provides a bridge between mean-field theory and exact computation.  Simulation results are provided to illustrate the method.

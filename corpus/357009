Sequential Prediction of Individual Sequences Under General Loss Functions
 Abstract We consider adaptive sequential prediction of arbitrary binary sequences when the performance is evaluated using a general loss function.  The goal is to predict on each individual sequence nearly as well as the best prediction strategy in a given comparison class of (possibly adaptive) prediction strategies, called experts.  By using a general loss function, we generalize previous work on universal prediction, forecasting, and data compression.  However, here we restrict ourselves to the case when the comparison class is finite.  For a given sequence, we define the regret as the total loss on the entire sequence suffered by the adaptive sequential predictor, minus the total loss suffered by the predictor in the comparison class that performs best on that particular sequence.  We show that for a large class of loss functions, the minimax regret is either \Theta(log N) or \Omega( p ` log N ), depending on the loss function, where N is the number of predictors in the comparison class and ` is the length of the sequence to be predicted.  The former case \Lambda Preliminary results have appeared in Computational Learning Theory: EuroCOLT '93,

Simultaneous Localization and Surveying with Multiple Agents
 Abstract We apply constrained Hidden Markov Model architecture to the problem of simultaneous localization and surveying from sensor logs of mobile agents navigating in unknown environments.  We show the solution of this problem for the case of one robot and extend our model to the more interesting case of multiple agents, that interact with each other through proximity sensors.  Since exact learning in this case becomes exponentially expensive, we develop an approximate method for inference using loopy belief propagation and apply it to the localization and surveying problem with multiple interacting robots.  In support of our analysis, we report experimental results showing that with the same amount of data, approximate learning with the interaction signals outperforms exact learning ignoring interactions.  1 Constrained HMMs for Map Learning By identifying each state in a hidden Markov model with some small spatial region of a continuous space, it is possible to naturally define neighboring states as those which correspond to connected regions in the underlying space.  The transition matrix of the HMM can then be constrained to allow transitions only between neighbors; this means that all valid state sequences correspond to connected paths in the continuous space.  The transition matrix does not need to be explicitly stored or learned, it is merely computed by a function that respects the state topology; the remaining parameters of the model scale only linearly with the number of states.  We apply this constrained HMM architecture[10] to the problem of simultaneous localization and surveying from sensor logs of mobile agents navigating in unknown environments.  The surveying problem is distinct from the mapping problem: we are not trying to learn the occupancy grid of a world.  Rather, we are trying to learn the values that various sensors (e. g.  altitude, temperature, light level, beacon signals) take on as a function of position in the unknown environment.  Our problem is motivated by mobile planetary rovers, which generally operate on open plains, collect temporal histories of multiple sensors, and cannot rely on the odometry of self-locomotion because they are navigating extremely rough terrain, often not using conventional wheels.  Figure 1 shows the typical input to a single agent (robot) in the scenario we are studying.  Each robot moves through the environment under the control of an external navigation algorithm that we cannot influence.  As it proceeds, it logs readings from multiple noisy sensors, some of which may be smoothly varying and others of which may be intermittent and discontinuous.  Crucially, the agents can also detect each other when in close proximity.  No odometry or other information about navigational control signals (either intended

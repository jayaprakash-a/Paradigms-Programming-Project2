AUTOMATIC GENERATION AND SELECTION OF MULTIPLE PRONUNCIATIONS FOR DYNAMIC VOCABULARIES
 ABSTRACT In this paper, we present a new scheme for the acoustic modeling of speech recognition applications requiring dynamic vocabularies.  It applies especially to the acoustic modeling of out-of-vocabulary words which need to be added to a recognition lexicon based on the observation of a few (say one or two) speech utterances of these words.  Standard approaches to this problem derive a single pronunciation from each speech utterance by combining acoustic and phone transition scores.  In our scheme, multiple pronunciations are generated from each speech utterance of a word to enroll by varying the relative weights assigned to the acoustic and phone transition models.  In our experiments, the use of these multiple baseforms dramatically outperforms the standard approach with a relative decrease of the word error rate ranging from 20% to 40% on all our test sets.  1.  MOTIVATION Speech recognition systems usually rely on a fixed lexicon where the pronunciations of the vocabulary words are given by hand-crafted phonetic baseforms, i. e.  sequences of phones written by a phonetician.  However, many applications require new words to be dynamically added to the recognition vocabulary, or new pronunciations of invocabulary words to be added to the lexicon.  Hence the need for techniques which can automatically derive phonetic baseforms.  This occurs for example in dictation systems that allow personalized vocabularies, in name dialer applications where the user enrolls the names he wants to dial, and in any application where actual pronunciations differ from canonic pronunciations (like for non-native speakers), so that the robustness of linguist-written pronunciations needs to be improved.  In situations where the speech recognition engine is embedded in a small device, there may not be any interface media, such as a keyboard, to allow the user to enter the spelling of the words he wants to add to his/her personalized vocabulary [1].  And even if such an interface were to be available, the spellings may not be of very much help as these applications typically involve words the pronunciation of which is highly unpredictable, like proper names for example.  In this context, it is difficult to use a priori knowledge, such as letter-to-sound rules in a reliable way.  Consequently, the user is asked to utter once or twice the words to add to his/her personalized vocabulary, and phonetic baseforms for these words are derived from the acoustic evidence provided by the user's utterances.  These approaches ( [2], [3], [4], [5]) usually rely on the combined use of: (i) an existing set of speaker-independent acoustic models of subphone units, and (ii) a model of transition between these subphone units.  The way to optimally combine these models is an open issue as it is not known in advance which of the models can most reliably describe the acoustic evidence observed for each new word to enroll.  For example, when the enrolled words are proper names, the reliability of the model of transition between the subphones is questionable since proper names do not follow strict phonotactic rules.  Current techniques of automatic baseform generation do not take into consideration the relative degree of confidence that should be put in either component.  The scheme presented in this paper deviates from standard approaches in that: (i) the acoustic model and the transition model which are combined to generate the baseforms are assigned a weight, (ii) multiple baseforms are derived from a single speech utterance by varying the relative weights of the models.  The basic idea behind this approach is twofold.  First, since we have to guess the pronunciation of the enrolled words from just one or two speech examples, we may as well use multiple guesses to maximize the chance of guessing right.  Second, since we do not know a priori how reliable each of the two models is relative to the other model, we avoid arbitrarily favoring either one of the models by varying their relative weights when generating the guesses.  The distinct baseforms obtained from the speech utterance of a word are added to the recognition lexicon as pronunciation variants of that word.  It has been extensively investigated recently how, in standard recognition frameworks, adding pronunciation variants to the canonic pronunciations of a static lexicon can significantly improve the recognition accuracy [7] [8].  We show that this applies also in the context of dynamic vocabularies, where no canonic pronunciations at all are available.  On the other hand, as multiple baseforms are added to the recognition lexicon, we can expect the acoustic confusability between the entries of the lexicon to increase with the risk of hurting the recognition accuracy.  In this paper, we report on an extensive set of speech recognition experiments showing the influence of the number of automatically generated baseforms on the decoding accuracy.  The structure of this paper is as follows.  In section 2 and 3, we describe our scheme to generate multiple baseforms and build variable-size lexicons.  In section 4, we present speech recognition experiments comparing lexicons of automatically generated baseforms on test data consisting of either isolated or in-context words, and in both quiet and noisy environments.  Section 5 concludes on this work.  2.  GENERATION OF MULTIPLE BASEFORMS In this section, we present a scheme to derive multiple baseforms from acoustic evidence, where it is attempted to make the best possible use of our a priori knowledge, where our a priori knowledge comprises a set of speaker independent acoustic models of subphone units and a statistical model of transitions between subphone units 1 .  In the standard way, the problem of deriving a baseform from acoustic evidence is usually stated as the problem of retrieving the most likely string U # of T subphone units, given the string O of T acoustic observations: U # = arg max fUg log P (U; O) = arg max fUg log P (O j U) + log P (U) The string U # is retrieved with a Viterbi algorithm.  The conditional probability of the acoustic observations given the string of subphone units is computed as: P (O j U) = t=T Y t=1 p(o (t) j u (t) ) The conditional probability of each acoustic observationp(o j u i ) is computed with the acoustic model - in our experiments speaker independent mixtures of gaussians.  The probability of observing the string of subphone units U is computed with the transition model assumed between the subphones - in our experiments a bigram model: P (U) = p(u (1) ) t=T Y t=2 p(u (t) j u (t 1) ) 1 Each subphone unit corresponds to roughly one third of a phone.  The bigram model of subphone units is estimated off-line by aligning a large dataset of speech with a known transcription on the acoustic models of the subphone units.  The probabilities fp(u j j u i )g are computed from the observed relative counts of the subphone models in the alignment (in our experiments, no backoff is used to smooth the bigram probabilities).  Note that both the duration of the units and the transition between the units are modeled.  The modification that we introduce to this baseline approach is to compute the log-likelihood of a baseform as a weighted sum of the log-scores of the acoustic model and of the transition model, with weights respectively of (1 #) and #: U # # = arg max fUg (1 #) log P (O j U) + # log P (U) Each value of # defines a distinct log-likelihood function wich reaches its maximum value for possibly distinct strings of subphone units.  The parameter # can be seen as reflecting the confidence put into each model.  In a context where it is not known which of the two models can most relevantly account for the observations, the generation of multiple strings U # # for various values of # allows to compensate for a possible mismatch.  3.  BUILDING LEXICONS WITH MULTIPLE BASEFORMS In our experiments, we define a set of # values by scanning an interval [#1; #2] (0 # #1 # #2 # 1), with a step of 0:1.  Each string U # # is converted into a phonetic baseform by replacing the subphone units with their phone counterpart and by merging together repeated phones.  All the distinct phonetic baseforms obtained from the speech utterance of a word by scanning a set of values of # are added as pronunciation variants in the recognition lexicon.  Each interval [#1; #2] thus results in a specific recognition lexicon, hence raising the question of how to select a priori the best performing lexicon.  We can expect that accumulating multiple baseforms for each enrollment speech utterance will improve the recognition accuracy by allowing a broader modelling of the pronunciation of the new words.  However it is well known that increasing the number of pronunciation variants increases the acoustic confusability in the recognition lexicon, which eventually hurts the accuracy.  In our experiments, we noticed for example that the baseforms obtained with # equal to or more than 0. 8 tended to look more and more alike, which we attributed to the prevailing influence of the subphone transition model.  As a result, cumulating baseforms with # values higher than 0. 8 was resulting in higher word error rates.  In the following section, we report on experiments where lexicons are build for each interval [#1; #2] with #

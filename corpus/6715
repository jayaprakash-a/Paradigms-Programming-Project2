Updating an NLP System to Fit New Domains: an empirical study on the sentence segmentation problem
 Abstract Statistical machine learning algorithms have been successfully applied to many natural language processing (NLP) problems.  Compared to manually constructed systems, statistical NLP systems are often easier to develop and maintain since only annotated training text is required.  From annotated data, the underlying statistical algorithm can build a model so that annotations for future data can be predicted.  However, the performance of a statistical system can also depend heavily on the characteristics of the training data.  If we apply such a system to text with characteristics different from that of the training data, then performance degradation will occur.  In this paper, we examine this issue empirically using the sentence boundary detection problem.  We propose and compare several methods that can be used to update a statistical NLP system when moving to a different domain.

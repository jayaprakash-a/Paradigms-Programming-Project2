The Importance of Convexity in Learning with Squared Loss
 Abstract We show that if the closure of a function class F under the metric induced by some probability distribution is not convex, then the sample complexity for agnostically learning F with squared loss (using only hypotheses in F ) is \Omega(ln(1=ffi)=ffl 2 ) where 1\Gamma ffi is the probability of success and ffl is the required accuracy.  In comparison, if the class F is convex and has finite pseudo-dimension, then the sample complexity is O\Gamma 1 ffl\Gamma ln 1 ffl + ln 1 ffi \Delta\Delta .  If a non-convex class F has finite pseudodimension, then the sample complexity for agnostically learning the closure of the convex hull of F , is O \Gamma 1 ffl \Gamma 1 ffl ln 1 ffl + ln 1 ffi \Delta\Delta .  Hence, for agnostic learning, learning the convex hull provides better approximation capabilities with little sample complexity penalty.

Category: Control, Navigation and Planning
 Abstract Undoubtedly, efficient exploration is crucial for the success of a learning agent.  Previous approaches to exploration in reinforcement learning exclusively address exploration in Markovian domains, i. e.  domains in which the state of the environment is fully observable.  If the environment is only partially observable, they cease to work because exploration statistics are confounded between aliased world states.  This paper presents Fringe Exploration, a technique for efficient exploration in partially observable domains.  The key idea, (applicable to many exploration techniques), is to keep statistics in the space of possible shortterm memories, instead of in the agent's current state space.  Experimental results in a partially observable maze and in a difficult driving task with visual routines show dramatic performance improvements.  1 The Problem Efficient exploration is of fundamental importance for autonomous systems that learn to act.  In recent years, a variety of exploration techniques have been proposed for reinforcement learning (RL).  Many researchers use undirected techniques---approaches closely related to random walks, e. g.  [Mozer and Bachrach, 1989; Barto et al. , 1995] .  These include the common "random action with probability e" and "Boltzmann distribution based on utility and decreasing temperature. " While easy to implement, these approaches are often unbearably inefficient; Whitehead has proved that undirected exploration can cause the learning time to scale exponentially with the size of the state space [Whitehead, 1991] .  To improve learning speed, several researchers have proposed directed exploration techniques---approaches that use statistics from the learning experience to more efficiently guide the search.  Statistics used include action counts, e. g.  [Sato et al. , 1988; Thrun, 1992] , action recency [Sutton, 1990] , and confidence intervals on utility data [Kaelbling, 1990] .  It has been shown both analytically and empirically that directed exploration reduces worst-case learning time from exponential to only a low-degree polynomial [Thrun, 1992; Koenig and Simmons, 1993] .  Directed exploration is clearly superior.  A second important issue in reinforcement learning is hidden state.  Hidden state occurs whenever sensory limitations hide features of the world from the agent; this "partial observability" can make different world states appear as identical to the agent.  State identification techniques use memory of past percepts and actions to distinguish states that are aliased by these non-Markovian dependencies.  Several reinforcement learning algorithms augment their state representations on-line by adding memory; examples include techniques based on partially observable Markov decision processes [Chrisman, 1992; McCallum, 1993] , recurrent neural networks [Lin and Mitchell, 1992] and suffix trees [McCallum, 1995] .  The problem is that directed exploration techniques all hinge on the assumption that the state of the world is observable.  That is, they all depend on associating unique exploration statistics with each world state.  This assumption is broken by hidden state.  For example, since different world states may appear as identical to the agent, a hospital navigation robot may arrive at a hallway intersection it has experienced only seldomly, yet, due to hidden state, choose its next action using statistics confounded with a different intersection it has visited many times; this mistake can cause the robot to uselessly repeat past experience, and miss opportunities to visit unexplored regions.  No previous research addresses the combination of directed exploration with hidden state in reinforcement learning; all previous hidden state work has used undirected exploration.  Efficient exploration in non-Markovian domains has long been understood as a special difficulty, (Chrisman refers to it as the `Exacerbated Exploration Problem' [Chrisman, 1992] ), but little is known about it.  As demonstrated in this paper, the straightforward application of directed approaches fails badly.  2 A Solution The central contribution of this paper is Fringe Exploration, a new methodology that differs from previous approaches in that it addresses directed exploration specifically in non-Markovian domains.  This section presents the idea in the abstract; the next section contributes a specific implementation.  The abstract idea of Fringe Exploration is that, instead of exploring only the space of observations, the agent explores the space of memories.  That is, instead of associating exploration statistics with agent states, statistics are associated with entire sequences of observations and actions.  This can be understood as maintaining "high resolution" exploration statistics---the agent keeps the statistics associated with states that make "extra" distinctions based on short-term memory; (in many contexts, these extra, hypothesis distinctions are called the "fringe. ") As a result of using memory, the danger of accidentally confounding two different world states is reduced.  Fringe Exploration avoids assuming that the agent's current internal states are Markovian, but it does assume that the fringe states are Markovian.  Note that, depending on the task and the depth of the fringe memory, the fringe states may not in fact be Markovian, but algorithms

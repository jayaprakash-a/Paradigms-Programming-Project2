The interaction of stability and weakness in AdaBoost
 Abstract We provide an analysis of AdaBoost within the framework of algorithmic stability.  In particular, we show that AdaBoost is a stabilitypreserving operation: if the \input" (the weak learner) to AdaBoost is stable, then the \output" (the strong learner) is almost-everywhere stable.  Because classifier combination schemes such as AdaBoost have greatest effect when the weak learner is weak, we discuss weakness and its implications.  We also show that the notion of almost-everywhere stability is sucient for good bounds on generalization error.  These bounds hold even when the weak learner has infinite VC dimension.

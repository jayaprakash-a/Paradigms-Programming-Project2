Integrating Topics and Syntax
 Abstract Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words.  We present a generative model that uses both kinds of dependencies, and is capable of simultaneously finding syntactic classes and semantic topics despite having no knowledge of syntax or semantics beyond statistical dependency.  This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively.

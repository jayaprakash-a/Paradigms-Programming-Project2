On-line learning processes in artificial neural networks
 Abstract We study on-line learning processes in artificial neural networks from a general point of view.  On-line learning means that a learning step takes place at each presentation of a randomly drawn training pattern.  It can be viewed as a stochastic process governed by a continuous-time master equation.  On-line learning is necessary if not all training patterns are available all the time.  This occurs in many applications when the training patterns are drawn from a time-dependent environmental distribution.  Studying learning in a changing environment, we encounter a conflict between the adaptability and the confidence of the network's representation.  Minimization of a criterion incorporating both effects yields an algorithm for on-line adaptation of the learning parameter.  The inherent noise of on-line learning makes it possible to escape from undesired local minima of the error potential on which the learning rule performs (stochastic) gradient descent.  We try to quantify these often made claims by considering the transition times between various minima.  We apply our results on the transitions from "twists" in twodimensional self-organizing maps to perfectly ordered configurations.  Finally, we discuss the capabilities of on-line learning for global optimization.

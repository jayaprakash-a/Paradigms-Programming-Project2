Bagging belief structures in Dempster-Shafer K-NN rule
 Abstract This paper introduces bagging in the evidence-theoretic K-nearest neighbor rule (K-NN).  It is known that bagging decreases the variability of classifiers, so the main idea here is to build stable belief structures associated to query samples, before decisions are made.  In order to compare bagged K-NN with the classical algorithm in a precisely controlled environment, data sets were generated according to known distributions.  Results show that bagging improves classi#cation, especially for ambiguous cases and outliers.  Moreover, bagged belief structures give pignistic probabilities closer to the a posteriori class probabilities than the original method.

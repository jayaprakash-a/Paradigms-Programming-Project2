Surprising Outcome While Benchmarking Statistical Tests
 Abstract Although non-parametric tests have already been proposed for that purpose, statistical significance tests for non-standard measures (different from the classification error) are less often used in the literature.  This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions.  More precisely, using a very large dataset to estimate the whole "population", we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size.  A surprising conclusion is that paired tests such as McNemar badly fail when comparing models which are similar (such as SVMs with different kernels).  On the other hand, non-parametric tests were relatively robust to class unbalance and the closeness of the models.

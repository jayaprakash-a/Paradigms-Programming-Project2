Sharing Resources: Buy Attention, Get Object Recognition
 Abstract Inspired by nature's policy of sharing resources, we have enhanced our attention model with minimal extra hardware to enable the twin powers of object detection and recognition.  With just the elementary information available at the preattentive stage in the form of low-level feature maps tuned to color, intensity and orientation, our model learns representations of objects in diverse, complex backgrounds.  The representation starts with simple vectors of low-level feature values computed at different locations on the object (views of the object).  We then recursively combine these views to form instances, in turn combined into simple objects, composite objects, and so on, taking into account feature values and their variance.  Given any new scene, our model uses the learnt representation of the target object to perform top-down biasing on the attention system such as to render this object more salient by enhancing those features which are characteristic of the object.  Experimental results verified the null hypothesis that the enhanced model would take half the number of fixations as that taken by the naive bottom-up model to detect all targets in a scene.  Our model is also able to recognize a wide variety of simple objects ranging from geometrical objects to soda cans, handicap signs, and many others under noisy conditions.  There are few false negatives and false positives.  The good performance of our lightweight model suggests that the human visual system may indeed be sharing resources extensively and attention and object recognition may be so intimately related that if we buy attention, we might get the other with minimal effort!

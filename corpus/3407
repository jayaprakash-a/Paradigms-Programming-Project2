Bayesian Conditional Random Fields
 Abstract We propose Bayesian Conditional Random Fields (BCRFs) for classifying interdependent and structured data, such as sequences, images or webs.  BCRFs are a Bayesian approach to training and inference with conditional random fields, which were previously trained by maximizing likelihood (ML) (Lafferty et al. , 2001).  Our framework eliminates the problem of overfitting, and offers the full advantages of a Bayesian treatment.  Unlike the ML approach, we estimate the posterior distribution of the model parameters during training, and average over this posterior during inference.  We apply an extension of EP method, the power EP method, to incorporate the partition function.  For algorithmic stability and accuracy, we flatten the approximation structures to avoid two-level approximations.  We demonstrate the superior prediction accuracy of BCRFs over conditional random fields trained with ML or MAP on synthetic and real datasets.

Thesis Proposal: Learning from Labeled and Unlabeled Data with Gaussian Random Fields
 Abstract One hurdle in supervised learning for many tasks is the diculty in obtaining labeled data.  Unlabeled data may be relatively easy to collect, but traditionally it was ignored in supervised learning.  Recent studies show benefit of including unlabeled data.  I propose to study the statistical learning frameworks that use both labeled and unlabeled data in so called semi-supervised learning, and its application to natural language processing.  I start with a short survey of the state-of-the-art approaches in learning from labeled and unlabeled data, analyzing the assumptions made by various methods.  Next the main mathematical model is introduced, which represents labeled and unlabeled data as vertices in a weighted graph, with edge weights encoding the similarity between instances.  The learning problem is then formulated in terms of a Gaussian random field on this graph.  This model is able to use the distribution of unlabeled data to improve classification.  Moreover, it leads to a natural strategy of active learning, which is also important when labeled data is scarce.  The model has several nice properties.  It has intimate connections with random walks, electric networks, and spectral graph theory.  The mean of the field is a harmonic functions, and can be eciently computed using matrix methods or belief propagation.  Class priors and the predictions of classifiers obtained by supervised learning can be incorporated with the Gaussian field model to improve prediction.  Hyperparameter learning is also feasible by entropy minimization, which can be shown to perform feature selection.  Preliminary experimental results on synthetic data, OCR handwritten digit recognition, and text classification tasks are presented.  Several open questions are posed at the end.

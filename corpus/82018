Mutual Information, Metric Entropy, and Risk in Estimation of Probability Distributions
 Abstract Assume fP ` : ` 2 \Thetag is a set of probability distributions with a common dominating measure on a complete separable metric space Y .  A state ` \Lambda 2 \Theta is chosen by Nature.  A statistician gets n independent observations Y 1 ; : : : ; Y n from Y distributed according to P ` \Lambda .  For each time t between 1 and n, based on the observations Y 1 ; : : : ; Y t\Gamma 1 , the statistician produces an estimated distribution ^ P t for P ` \Lambda , and suffers a loss L(P ` \Lambda ; ^ P t ).  The cumulative risk for the statistician is the average total loss up to time n.  Of special interest in information theory, data compression, mathematical finance, computational learning theory and statistical mechanics is the special case when the loss L(P ` \Lambda ; ^ P t ) is the relative entropy between the true distribution P ` \Lambda and the estimated distribution ^ P t .  Here the cumulative Bayes risk from time 1 to n is the mutual information between the random parameter \Theta \Lambda and the observations Y 1 ; : : : ; Y n .  New bounds on this mutual information are given in terms of the Laplace transform of the Hellinger distance between pairs of distributions indexed by parameters in \Theta.

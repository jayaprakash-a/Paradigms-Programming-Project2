Text Categorization Based on Regularized Linear Classification Methods
 Abstract A number of linear classification methods such as the linear least squares fit (LLSF), logistic regression, and support vector machines (SVM's) have been applied to text categorization problems.  These methods share the similarity by finding hyperplanes that approximately separate a class of document vectors from its complement.  However, support vector machines are so far considered special in that they have been demonstrated to achieve the state of the art performance.  It is therefore worthwhile to understand whether such good performance is unique to the SVM design, or if it can also be achieved by other linear classification methods.  In this paper, we compare a number of known linear classification methods as well as some variants in the framework of regularized linear systems.  We will discuss the statistical and numerical properties of these algorithms, with a focus on text categorization.  We will also provide some numerical experiments to illustrate these algorithms on a number of datasets.  1 Background The text categorization problem is to determine predefined categories for an incoming unlabeled message or document containing text based on information extracted from a training set of labeled messages or documents.  Text categorization is an important practical problem for companies that wish to use computers to categorize incoming email, thereby either enabling an automatic machine response to the email or simply assuring that the email reaches the correct human recipient.  But, beyond email, text items to be categorized may come from many sources, including the output of voice recognition software, collections of documents (e. g. , news stories, patents, or case summaries), and the contents of web pages.  The text categorization problem can be reduced to a set of binary classification problems { one for each category { where for each one wishes to determine a method for predicting inclass versus out-of-class membership.  Such supervised learning problems have been widely studied in the past.  Recently, many methods developed for classification problems have been applied to text categorization.  For example, Apte, Damerau, and Weiss [1] applied an inductive rule learning algorithm, SWAP1, to the text categorization problem.  In [25], Yang and Chute proposed a linear least squares fit algorithm to train linear classifiers.  Yang also compared a number of statistical methods for text categorization in [24].  The best performances previously reported in the literature are from weighted resampled decision trees in [23] and (linear) support vector machines in [12, 4].  Integral parts of all these approaches are tokenization, feature selection, and creating numeric vector representations of documents.  The first step, tokenization, is laid out in detail in Figure 1.  This functionality is common to most methods of text categorization.  In the tokenization procedure depicted in Figure 1, one or both of Steps 4 and 5 may be omitted, although keeping them may improve performance.  If both steps are retained, elimination of stopwords (Step 5) may also be done before stemming (Step 4).  Also, the elimination of stopwords (Step 5) may in some instances be subsumed by subsequent feature selection, to be discussed below.  For consistency, the same tokenization procedure would be used both (1) for documents used in training to build categorization rules and (2) for incoming documents to be categorized by a system employing the classifiers obtained in the training phase.  After tokenization, each document is represented by a list of word occurrences.  A program should then be used to carry out feature selection.  (However, feature selection could also be skipped entirely, so that tokens were to be taken by themselves to be the sole features of interest. ) We will not take up the specifics of feature selection, but a number of methods of varying degrees of sophistication have been studied in [27].  Feature selection might be done only once for the entire data set, but experience indicates that better results will be obtained by doing feature selection separately for each category, reflecting the intuition that features indicative of category membership will differ as one moves from category to category.  Feature selection, under the assumption that a separate set of features is to be selected for each category, is laid out in Figure 2.  The output of feature selection would normally be a specification of a separate list of selected features (words) for each category for which we intend to generate a classifier.  The specification would necessarily be detailed enough to permit a computer to identify each occurrence of each feature in the tokenized representation of a document.  After feature selection, each document is represented by a vector of word occurrences for each category where each vector component corresponds to a word feature selected for the category in the previous step.  Figure 3 shows the steps necessary to use a list of features selected for relevance to a particular category to convert a tokenized representation of a document to a numeric vector representing the document.  Because of the vast numbers of different words that may appear in text, generally the numerical vectors of world occurrences one gets are sparse vectors of very high dimensionality.  Thus, text categorization necessitates

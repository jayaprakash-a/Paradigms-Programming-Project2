Worst-case Bounds for the Logarithmic Loss of Predictors
 Using Shtarkov's theorem and tools from empirical process theory, we prove a general upper bound on the best possible (minimax) regret.  The bound depends on certain metric properties of the class of predictors.  We apply the bound to both parametric and nonparametric classes of predictors.  Finally, we point out a suboptimal behavior of the popular Bayesian weighted average algorithm.

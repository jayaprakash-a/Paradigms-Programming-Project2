Learning Default Concepts
 Abstract Classical concepts, based on necessary and sufficient defining conditions, cannot classify logically insufficient object descriptions.  Many reasoning systems avoid this limitation by using ``default concepts" to classify incompletely described objects.  This paper addresses the task of learning such default concepts from observational data.  We first model the underlying performance task --- classifying incomplete examples --- as a probabilistic process that passes random test examples through a "blocker" that can hide object attributes from the classifier.  We then address the task of learning accurate default concepts from random training examples.  After surveying the learning techniques that have been proposed for this task in the machine learning and knowledge representation literatures, and investigating their relative merits, we present a more data-efficient learning technique, developed from well-known statistical principles.  Finally, we extend Valiant's paclearning framework to this context and obtain a number of useful learnability results.

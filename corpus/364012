Incremental Learning of Factorial Markov Decision Processes
 Abstract We investigate a general approach to approximately learning a compact and structured representation of the transition model for Factorial Markov Decision Processes (FMDPs).  FMDPs are based on mixed memory Markov models, in which the transition probabilities are factored into a mixture of terms depending on each state variable.  We develop an incremental ExpectationMaximization (EM) procedure for learning the transition probabilities, more suited to an online reinforcement learning approach for learning real-time control than the traditional batch-mode EM.  A key advantage of FMDPs is that the agent is able to rapidly generalize observed transition experience to unseen regions of the environment.  We describe detailed experiments in which a modelbased reinforcement learning agent incrementally learns the transition model of an underlying FMDP while simultaneously learning the optimal policy in a certainty-equivalent control regime.  Our results show that exploiting the structure of an FMDP results in much faster model and control learning compared to the default tabular maximumlikelihood approach.

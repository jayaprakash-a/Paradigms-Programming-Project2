A NEW VIEW OF ICA
 ABSTRACT We present a new way of interpreting ICA as a probability density model and a new way of fitting this model to data.  The advantage of our approach is that it suggests simple, novel extensions to overcomplete, undercomplete and multilayer non-linear versions of ICA.  1.  ICA AS A CAUSAL GENERATIVE MODEL Factor analysis is based on a causal generative model in which an observation vector is generated in three stages.  First, the activities of the factors (also known as latent or hidden variables) are chosen independently from one dimensional Gaussian priors.  Next, these hidden activities are multiplied by a matrix of weights (the "factor loading" matrix) to produce a noise-free observation vector.  Finally, independent Gaussian "sensor noise" is added to each component of the noise-free observation vector.  Given an observation vector and a factor loading matrix, it is tractable to compute the posterior distribution of the hidden activities because this distribution is a Gaussian, though it generally has off-diagonal terms in the covariance matrix so it is not as simple as the prior distribution over hidden activities.  ICA can also be viewed as a causal generative model [1, 2] that differs from factor analysis in two ways.  First, the priors over the hidden activities remain independent but they are non-Gaussian.  By itself, this modification would make it intractable to compute the posterior distribution over hidden activities.  Tractability is restored by eliminating sensor noise and by using the same number of factors as input dimensions.  This ensures that the posterior distribution over hidden activities collapses to a point.  Interpreting ICA as a type of causal generative model suggests a number of ways in which it might be generalized, for instance to deal with more hidden units than input dimensions.  Most of these generalizations retain marginal independence of the hidden activities and add sensor noise, but fail to preserve the property that the posterior distribution collapses to a point.  As # Funded by the Wellcome Trust and the Gatsby Charitable Foundation.  a result inference is intractable and crude approximations are needed to model the posterior distribution, e. g. , a MAP estimate in [3], a Laplace approximation in [4, 5] or more sophisticated variational approximations in [6].

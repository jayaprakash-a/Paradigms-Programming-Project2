The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length
 Abstract.  We propose and analyze a distribution learning algorithm for variable memory length Markov processes.  These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata.  The learning algorithm is motivated by real applications in man-machine interaction such as handwriting and speech recognition.  Conventionally used fixed memory Markov and hidden Markov models have either severe practical or theoretical drawbacks.  Though general hardness results are known for learning distributions generated by sources with similar structure, we prove that our algorithm can indeed efficiently learn distributions generated by our more restricted sources.  In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time and sample complexity.  We present two applications of our algorithm.  In the first one we apply our algorithm in order to construct a model of the English language, and use this model to correct corrupted text.  In the second application we construct a simple stochastic model for E. coli DNA.

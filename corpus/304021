Some Tests of an Unsupervised Model of Language Acquisition
 Abstract We outline an unsupervised language acquisition algorithm and offer some psycholinguistic support for a model based on it.  Our approach resembles the Construction Grammar in its general philosophy, and the Tree Adjoining Grammar in its computational characteristics.  The model is trained on a corpus of transcribed child-directed speech (CHILDES).  The model's ability to process novel inputs makes it capable of taking various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability.  We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli.  1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al. , 1996).  Computational models of language acquisition or "grammar induction" are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke "general-purpose" statistical learning mechanisms.  We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework.  On the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation and probabilistic learning, yet generic "connectionist" architectures are ill-suited to the abstraction and processing of symbolic information.  On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difficult to train.  We are developing an approach to the acquisition of distributional information from raw input (e. g. , transcribed speech corpora) that also supports the distillation of structural regularities comparable to those captured by Context Sensitive Grammars out of the accrued statistical knowledge.  In thinking about such regularities, we adopt Langacker's notion of grammar as "simply an inventory of linguistic units" ((Langacker, 1987), p. 63).  To detect potentially useful units, we identify and process partially redundant sentences that share the same word sequences.  We note that the detection of paradigmatic variation within a slot in a set of otherwise identical aligned sequences (syntagms) is the basis for the classical distributional theory of language (Harris, 1954), as well as for some modern work (van Zaanen, 2000).  Likewise, the pattern --- the syntagm and the equivalence class of complementary-distribution symbols that may appear in its open slot --- is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure).  Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists.  Thus, the main computational principles behind the ADIOS model are outlined here only briefly.  The algorithmic details of our approach and accounts of its learning from CHILDES corpora appear elsewhere (Solan et al. , 2003a; Solan et al. , 2003b; Solan et al. , 2004; Edelman et al. , 2004).  2 The principles behind the ADIOS algorithm The representational power of ADIOS and its capacity for unsupervised learning rest on three principles: (1) probabilistic inference of pattern significance, (2) context-sensitive generalization, and (3) recursive construction of complex patterns.  Each of these is described briefly below.  P84 that P58 P63 E63 E64 P48 E64 Beth 

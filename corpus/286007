Learning the higher-order structure of a natural sound
 Abstract.  Unsupervised learning algorithms paying attention only to second-order statistics ignore the phase structure (higher-order statistics) of signals, which contains all the informative temporal and spatial coincidences which we think of as `features'.  Here we discuss how an Independent Component Analysis (ICA) algorithm may be used to elucidate the higher-order structure of natural signals, yielding their independent basis functions.  This is illustrated with the ICA transform of the sound of a fingernail tapping musically on a tooth.  The resulting independent basis functions look like the sounds themselves, having the same temporal envelopes and the same musical pitches.  Thus they reflect both the phase and frequency information inherent in the data.  Short title: Learning higher order structure.  February 2, 1996 1.  The poverty of second-order statistics.  Natural signals have characteristic statistical dependencies across space and time.  One view of sensory systems is that they must uncover these dependencies by processing them with filters whose form depends on the characteristic statistics of the ensemble of signals to which they are exposed (Barlow 1989, Atick & Redlich 1990).  A considerable effort has gone into finding unsupervised learning algorithms able to self-organise to produce such filters (Linsker 1988, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990, Intrator 1992, Atick & Redlich 1993 and many others).  These efforts have been criticised by David Field 1987, 1994.  A major component of Field's argument is that the above methods are sensitive only to second-order statistics, since they all use correlation-based learning rules (ie: Hebbian and/or antiHebbian rules. ) Most of the methods bear some relation to Principal Components Analysis (the Karhunen-Loeve Transform), a second-order statistical technique.  The most informative features of natural signals, however, require higher-order statistics for their characterisation.  An edge in an image, or the transient attack or decay of a sound waveform, are examples of `features' which involve relationships between not just two, but many tens or even hundreds of pixels or time-points.  The failure of correlation-based learning is most clearly shown by the filters they produce when trained on stationary ensembles of signals.  The filters are typically global (see Figure 2a-c), sensitive to different spatio- or temporal frequencies, but with non-zero weights extending throughout the filter.  They reflect only the amplitude spectrum of the signal and ignore the phase spectrum where most of the suspicious local coincidences in natural signals take place.  An edge in an image, for example, is a coincidence in the phase spectrum, since if we were to Fourier analyse it, we would see many sine waves of different frequencies, all aligned in phase where the edge occurred.  Correlation-based methods cannot learn edge-detectors, though they often may seem to be doing so by local-windowing of the learnt Fourier components, turning them into Gabor-like filters (see Daugman 1985 for an analysis of the pertinence of Gabor filters to vision. ).  To illustrate formally that second order statistics only carry information about the amplitude spectrum, consider the autocorrelation function of a signal, which contains all its second order structure.  The Fourier transform of this is the power spectrum, which is the square of the amplitude spectrum.  Thus the two carry identical information.  To demonstrate intuitively that what we consider as the informative part of a natural signal is captured in the phase spectrum, Fourier transform the signal, remove the phase information, and transform it back to the space or time domain.  It will then look or sound like noise, typically with a 1/f amplitude spectrum.  All the visual features or sounds that our perceptual system thinks of as of as `signal' will be lost.  On the other hand, if we remove the amplitude information, and preserve the phase, the signal will be distorted but remain recognisable.  This points to a curious paradox: correlation-based learning algorithms are sensitive to exactly the part of natural signals which we regard as least meaningful (amplitude), and ignore the part of the signal which we find most meaningful (phase).  To encode the phase of signals, we need an algorithm sensitive to higher-order staistics.  The problem with higher-order statistics is that there are an infinite number of them.  Deciding which to measure a priori would be a difficult task.  Looking for horizontal bars in an image, for example, we may decide to estimate the average

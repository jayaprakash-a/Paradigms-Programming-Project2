On the generalization ability of support vector machines
 Abstract In this article we study the generalization abilities of the 1-norm soft margin classifier.  We show for several standard kernels, like the Gaussian RBF kernel, that this algorithm yields arbitrarily good generalization results provided that the factor which weighs the sum of the slack variables is chosen well.  This kind of result is completely new.  Indeed, for the first time it can now be explained without a-priori assumptions on the classification problem why the support vector approach may provide good generalization performance.  Our considerations are firstly based on an approximation property of the used kernels which also gives new insight into the role of kernels in these and other algorithms.  Thus it may also be of independent interest.  Secondly, the result is archieved by a precise investigation of the optimization problem which underlies the 1-norm soft margin algorithm.

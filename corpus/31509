Off-policy Learning with Recognizers
 Abstract We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods.  We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections.  We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance.  This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence.  Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators.  Off-policy learning is learning about one way of behaving while actually behaving in another way.  For example, Q-learning is an off- policy learning method because it learns about the optimal policy while taking actions in a more exploratory fashion, e. g. , according to an e-greedy policy.  Off-policy learning is of interest because only one way of selecting actions can be used at any time, but we would like to learn about many different ways of behaving from the single resultant stream of experience.  For example, the options framework for temporal abstraction involves considering a variety of different ways of selecting actions.  For each such option one would like to learn a model of its possible outcomes suitable for planning and other uses.  Such option models have been proposed as fundamental building blocks of grounded world knowledge (Sutton, Precup & Singh, 1999; Sutton, Rafols & Koop, 2005).  Using off-policy learning, one would be able to learn predictive models for many options at the same time from a single stream of experience.  Unfortunately, off-policy learning using temporal-difference methods has proven problematic when used in conjunction with function approximation.  Function approximation is essential in order to handle the large state spaces that are inherent in many problem domains.  Q-learning, for example, has been proven to converge to an optimal policy in the tabular case, but is unsound and may diverge in the case of linear function approximation (Baird, 1996).  Precup, Sutton, and Dasgupta (2001) introduced and proved convergence for the first off-policy learning algorithm with linear function approximation.  They addressed the problem of learning the expected value of a target policy based on experience generated using a different behavior policy.  They used importance sampling techniques to reduce the off-policy case to the on-policy case, where existing convergence theorems apply (Tsitsiklis & Van Roy, 1997; Tadic, 2000).  There are two important difficulties with that approach.  First, the behavior policy needs to be stationary and known, because it is needed to compute the importance sampling corrections.  Second, the importance sampling weights are often ill-conditioned.  In the worst case, the variance could be infinite and convergence would not occur.  The conditions required to prevent this were somewhat awkward and, even when they applied and asymptotic convergence was assured, the variance could still be high and convergence could be slow.  In this paper we address both of these problems in the context of off-policy learning for options.  We introduce the notion of a recognizer.  Rather than specifying an explicit target policy (for instance, the policy of an option), about which we want to make predictions, a recognizer specifies a condition on the actions that are selected.  For example, a recognizer for the temporally extended action of picking up a cup would not specify which hand is to be used, or what the motion should be at all different positions of the cup.  The recognizer would recognize a whole variety of directions of motion and poses as part of picking the cup.  The advantage of this strategy is not that one might prefer a multitude of different behaviors, but that the behavior may be based on a variety of different strategies, all of which are relevant, and we would like to learn from any of them.  In general, a recognizer is a function that recognizes or accepts a space of different ways of behaving and thus, can learn from a wider range of data.  Recognizers have two advantages over direct specification of a target policy: 1) they are a natural and easy way to specify a target policy for which importance sampling will be well conditioned, and 2) they do not require the behavior policy to be known.  The latter is important because in many cases we may have little knowledge of the behavior policy, or a stationary behavior policy may not even exist.  We show that for the case of state aggregation, even if the behavior policy is unknown, convergence to a good model is achieved.  1 Non-sequential example The benefits of using recognizers in off-policy learning can be most easily seen in a nonsequential context with a single continuous action.  Suppose you are given a sequence of sample actions a i 2 [0, 1], selected i. i. d.  according to probability density b : [0, 1] 7! + (the behavior density).  For example, suppose the behavior density is of the oscillatory form shown as a red line in Figure 1.  For each each action, a i , we observe a corresponding outcome, z i 2 , a random variable whose distribution depends only on a i .  Thus the behavior density induces an outcome density.  The on-policy problem is to estimate the mean m b of the outcome density.  This problem can be solved simply by averaging the sample outcomes: ^ m b = i z i .  The off-policy problem is to use this same data to learn what the mean would be if actions were selected in some way other than b.  For example, if the actions were restricted to a designated range, such as between 0. 7 and 0. 9.  There are two natural ways to pose this off-policy problem.  The most straightforward way is to be equally interested in all actions within the designated region.  One professes to be interested in actions selected according to a target density p : [0, 1] 7! + , which in the example would be 5. 0 between 0. 7 and 0. 9, and zero elsewhere, as in the dashed line in Figure 1 (left).  The importance- sampling estimate of the mean outcome is ^ m p = i p(a i ) b(a i ) z i .  (1)

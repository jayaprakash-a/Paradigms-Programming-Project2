IS EM REALLY BETTER?
 Abstract Unsupervised learning plays an important role in many areas of machine learning.  Two major research fields in unsupervied learning are density estimation, where one seeks to find a descriptive model of data, and dimensionality reduction, where one tries to discover a compact representation of data.  Both of those classic problems often involve fitting models with unobserved or latent variables.  A commonly used technique for Maximum Likelihood Learning of model parameters in the presence of latent variables is Expectation-Maximization (EM) algorithm We propose a new approach for Maximum Likelihood Estimation of the model parameters using the method of conjugate gradients, which we call the Expectation Conjuagte Gradient (ECG) algorithm.  The key idea is that is it possible to find exact gradients of the likelihood function in many latent variable models.  Computing the gradient often involves finding the posterior distribution over latent variables, which corresponds to the E-step of the EM algorithm.  The goal of the present project is to compare the performance and convergence properties of EM and ECG as optimization techniques; 3 both of which require inference as a subroutine as well as to study the relationship between EM and gradient based algorithms.  General Framework A lot of probablistic models have unobserved or hidden variables.  Hidden variables are usually introduced to help model correlations in observed variables.  .  Let X ) observed random variable Z ) hidden, or latent variable.

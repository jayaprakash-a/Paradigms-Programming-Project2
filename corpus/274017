Efficient exploration for optimizing immediate reward
 Abstract We consider the problem of learning an effective behavior strategy from reward.  Although much studied, the issue of how to use prior knowledge to scale optimal behavior learning up to real-world problems remains an important open issue.  We investigate the inherent data-complexity of behavior-learning when the goal is simply to optimize immediate reward.  Although easier than reinforcement learning, where one must also cope with state dynamics, immediate reward learning is still a common problem and is fundamentally harder than supervised learning.  For optimizing immediate reward, prior knowledge can be expressed either as a bias on the space of possible reward models, or a bias on the space of possible controllers.  We investigate the two paradigmatic learning approaches of indirect (reward-model) learning and direct-control learning, and show that neither uniformly dominates the other in general.  Model-based learning has the advantage of generalizing reward experiences across states and actions, but direct-control learning has the advantage of focusing only on potentially optimal actions and avoiding learning irrelevant world details.  Both strategies can be strongly advantageous in different circumstances.  We introduce hybrid learning strategies that combine the benefits of both approaches, and uniformly improve their learning efficiency.

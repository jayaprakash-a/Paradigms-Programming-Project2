The Sample Complexity of Exploration in the Multi-Armed Bandit Problem
 Abstract We consider the multi-armed bandit problem under the PAC ("probably approximately correct") model.  It was shown by Even-Dar et al.  (2002) that given n arms, a total of O (n/e 2 )log(1/d) # trials suffices in order to find an e-optimal arm with probability at least 1- d.  We establish a matching lower bound on the expected number of trials under any sampling policy.  We furthermore generalize the lower bound, and show an explicit dependence on the (unknown) statistics of the arms.  We also provide a similar bound within a Bayesian setting.  The case where the statistics of the arms are known but the identities of the arms are not, is also discussed.  For this case, we provide a lower bound of Q (1/e 2 )(n + log(1/d)) # on the expected number of trials, as well as a sampling policy with a matching upper bound.  If instead of the expected number of trials, we consider the maximum (over all sample paths) number of trials, we establish a matching upper and lower bound of the form Q (n/e 2 )log(1/d) # .  Finally, we derive lower bounds on the expected regret, in the spirit of Lai and Robbins.

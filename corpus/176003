Learning from incomplete data
 Abstract Real-world learning tasks often involve high-dimensional data sets with complex patterns of missing features.  In this paper we review the problem of learning from incomplete data from two statistical perspectives---the likelihood-based and the Bayesian.  The goal is two-fold: to place current neural network approaches to missing data within a statistical framework, and to describe a set of algorithms, derived from the likelihood-based framework, that handle clustering, classification, and function approximation from incomplete data in a principled and efficient manner.  These algorithms are based on mixture modeling and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al.

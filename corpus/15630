AUC Optimization vs
 Error Rate Minimization.  Abstract The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classification algorithm.  However, the objective function optimized in most of these algorithms is the error rate and not the AUC value.  We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate.  Our results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable.  Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values.  We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC.  We report the results of our experiments with RankBoost in several datasets demonstrating the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.  1 Motivation In many applications, the overall classification error rate is not the most pertinent performance measure, criteria such as ordering or ranking seem more appropriate.  Consider for example the list of relevant documents returned by a search engine for a specific query.  That list may contain several thousand documents, but, in practice, only the top fifty or so are examined by the user.  Thus, a search engine's ranking of the documents is more critical than the accuracy of its classification of all documents as relevant or not.  More generally, for a binary classifier assigning a real-valued score to each object, a better correlation between output scores and the probability of correct classification is highly desirable.  A natural criterion or summary statistic often used to measure the ranking quality of a classifier is the area under an ROC curve (AUC) [8].  1 However, the objective function optimized by most classification algorithms is the error rate and not the AUC.  Recently, several algorithms have been proposed for maximizing the AUC value locally [4] or maximizing some approximations of the global AUC value [9, 15], but, in general, these algorithms do not obtain AUC values significantly better than those obtained by an algorithm designed to minimize the error rates.  Thus, it is important to determine the relationship between the AUC values and the error rate.  # This author's new address is: Google Labs, 1440 Broadway, New York, NY 10018, corinna@google. com.  1 The AUC value is equivalent to the Wilcoxon-Mann-Whitney statistic [8] and closely related to the Gini index [1].  It has been re-invented under the name of L-measure by [11], as already pointed out by [2], and slightly modified under the name of Linear Ranking by [13, 14].  (1,1) (0,0) False positive rate True positive rate ROC Curve.  AUC=0. 718 True positive rate = correctly classified positive total positive False positive rate = incorrectly classified negative total negative Figure 1: An example of ROC curve.  The line connecting (0, 0) and (1, 1), corresponding to random classification, is drawn for reference.  The true positive (negative) rate is sometimes referred to as the sensitivity (resp.  specificity) in this context.  In the following sections, we give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate.  2 We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC.  We report the results of our experiments with RankBoost in several datasets and demonstrate the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.  2 Definition and properties of the AUC The Receiver Operating Characteristics (ROC) curves were originally developed in signal detection theory [3] in connection with radio signals, and have been used since then in many other applications, in particular for medical decision-making.  Over the last few years, they have found increased interest in the machine learning and data mining communities for model evaluation and selection [12, 10, 4, 9, 15, 2].  The ROC curve for a binary classification problem plots the true positive rate as a function of the false positive rate.  The points of the curve are obtained by sweeping the classification threshold from the most positive classification value to the most negative.  For a fully random classification, the ROC curve is a straight line connecting the origin to (1, 1).  Any improvement over random classification results in an ROC curve at least partially above this straight line.  Fig.  (1) shows an example of ROC curve.  The AUC is defined as the area under the ROC curve and is closely related to the ranking quality of the classification as shown more formally by Lemma 1 below.  Consider a binary classification task with m positive examples and n negative examples.  We will assume that a classifier outputs a strictly ordered list for these examples and will denote by 1X the indicator function of a set X .  Lemma 1 ([8]) Let c be a fixed classifier.  Let x 1 , .  .  .  , xm be the output of c on the positive examples and y 1 , .  .  .  , yn its output on the negative examples.  Then, the AUC, A, associated to c is given by: A = P m i=1 P n j=1 1 x i }y j mn (1) that is the value of the Wilcoxon-Mann-Whitney statistic [8].  Proof.  The proof is based on the observation that the AUC value is exactly the probability P (X } Y ) where X is the random variable corresponding to the distribution of the outputs for the positive examples and Y the one corresponding to the negative examples [7].  The Wilcoxon-Mann-Whitney statistic is clearly the expression of that probability in the discrete case, which proves the lemma [8].  Thus, the AUC can be viewed as a measure based on pairwise comparisons between classifications of the two classes.  With a perfect ranking, all positive examples are ranked higher than the negative ones and A = 1.  Any deviation from this ranking decreases the AUC.

Neural Nets with Superlinear VC-Dimension
 Abstract It has been known for quite a while that the Vapnik-Chervonenkis dimension (VC-dimension) of a feedforward neural net with linear threshold gates is at most O(w \Delta log w), where w is the total number of weights in the neural net.  We show in this paper that this bound is in fact asymptotically optimal.  More precisely, we exhibit for any depth d 3 a large class of feedforward neural nets of depth d with w weights that have VC-dimension \Omega(w \Delta log w).  This lower bound holds even if the inputs are restricted to boolean values.  The proof of this result relies on a new method that allows us to encode more "program-bits" in the weights of a neural net than previously thought possible.  The Vapnik-Chervonenkis dimension VC-dimension(N ) of a neural net N with n input nodes is defined as the size of the largest set S ` R n which is "shattered" by N in the sense that every function F : S ! f0; 1g can be computed by N with some assignment of real numbers to its weights.  The VC-dimension of a neural net N is an important measure for the expressiveness for N , i. e.  for the variety of functions that can be computed by N with different choices for its weights.  In particular it has been shown in [BEHW] and [EHKV] that the VC-dimension of N essentially determines the number of training examples that are needed to train N in Valiant's model [V] for probably approximately correct learning ("PAC-learning").  It has been known for quite a while that the VC-dimension of a neural net with linear threshold gates and w edges (respectively w weights) is at most O(w \Delta log w).  This result, which holds for arbitrary real valued input patterns, was first shown by Cover in [C 64] (see also [C 68]), and later by Baum and Haussler [BH].  It has frequently been conjectured that the "true" upper bound is O(w).  This conjecture is quite plausible, since a single linear threshold gate with w edges has VC-dimension w + 1.  Furthermore it is hard to imagine that the VC-dimension of a network of linear threshold gates can be larger than the sum of the VC-dimensions of the individual linear threshold gates in the network.  We disprove this popular conjecture by showing that for any depth d 3 quite a number of neural nets N of depth d have a VC-dimension that is superlinear in the number w of edges in N .  In particular, we exhibit for arbitrarily large w 2 N neural nets N of depth 3 (i. e.  with 2 hidden layers) with w weights that have VC-dimension \Omega(w \Delta log w).  This shows that the quoted upper bound of O(w log w) is in fact asymptotically optimal.  It is of some interest to note that the upper bound O(w \Delta log w) for the VCdimension of a neural net with w weights holds even for the case of real valued inputs, whereas our matching lower bound \Omega(w \Delta log w) for the VC-dimension of certain neural nets Nw with w weights holds already for the restriction of Nw to boolean inputs.  Our lower bound also shows that the well-known upper bound 2w log(eN) for the VC-dimension of a neural net with w weights and N computation nodes (due to Baum and Haussler [BH]) is asymptotically optimal.  The result of this paper may also be viewed as mathematical evidence for a certain type of "connectionism thesis": that a network of neuron-like elements is more than just the sum of its elements.  We show that in a large neural net a single edge may add more than a constant to the VC-dimension of the neural net: its contribution may increase with the logarithm of the total size of the neural net.  Although we consider in this paper only neural nets with linear threshold gates, it is obvious that the same lower bound can also be derived for neural nets with other activation functions such as oe(y) = 1 1+e\Gamma y (see [RM]) or piecewise linear (resp.  polynomial) functions of a similar type (see [S], [MSS], [M 93]).  This paper improves our earlier results from [M 92] (see [M 93] for an extended abstract), where we had exhibited neural nets of depth 4 with superlinear VC-dimension.  Both our preceding results and the proof of our new result employ classical circuit construction methods due to Neciporuk [N] and

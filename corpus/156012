Generalized Algorithms for Constructing Statistical Language Models
 Abstract Recent text and speech processing applications such as speech mining raise new and more general problems related to the construction of language models.  We present and describe in detail several new and efficient algorithms to address these more general problems and report experimental results demonstrating their usefulness.  We give an algorithm for computing efficiently the expected counts of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton; describe a new technique for creating exact representations of # -gram language models by weighted automata whose size is practical for offline use even for a vocabulary size of about 500,000 words and an # -gram order ##### ; and present a simple and more general technique for constructing class-based language models that allows each class to represent an arbitrary weighted automaton.  An efficient implementation of our algorithms and techniques has been incorporated in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities.  1 Motivation Statistical language models are crucial components of many modern natural language processing systems such as speech recognition, information extraction, machine translation, or document classification.  In all cases, a language model is used in combination with other information sources to rank alternative hypotheses by assigning them some probabilities.  There are classical techniques for constructing language models such as #gram models with various smoothing techniques (see Chen and Goodman (1998) and the references therein for a survey and comparison of these techniques).  In some recent text and speech processing applications, several new and more general problems arise that are related to the construction of language models.  We present new and efficient algorithms to address these more general problems.  Counting.  Classical language models are constructed by deriving statistics from large input texts.  In speech mining applications or for adaptation purposes, one often needs to construct a language model based on the output of a speech recognition system.  But, the output of a recognition system is not just text.  Indeed, the word error rate of conversational speech recognition systems is still too high in many tasks to rely only on the one-best output of the recognizer.  Thus, the word lattice output by speech recognition systems is used instead because it contains the correct transcription in most cases.  A word lattice is a weighted finite automaton (WFA) output by the recognizer for a particular utterance.  It contains typically a very large set of alternative transcription sentences for that utterance with the corresponding weights or probabilities.  A necessary step for constructing a language model based on a word lattice is to derive the statistics for any given sequence from the lattices or WFAs output by the recognizer.  This cannot be done by simply enumerating each path of the lattice and counting the number of occurrences of the sequence considered in each path since the number of paths of even a small automaton may be more than four billion.  We present a simple and efficient algorithm for computing the expected count of any given sequence in a WFA and report experimental results demonstrating its efficiency.  Representation of language models by WFAs.  Classical sical # -gram language models admit a natural representation by WFAs in which each state encodes a left context of width less than # .  However, the size of that representation makes it impractical for offline optimizations such as those used in large-vocabulary speech recognition or general information extraction systems.  Most offline representations of these models are based instead on an approximation to limit their size.  We describe a new technique for creating an exact representation of # -gram language models by WFAs whose size is practical for offline use even in tasks with a vocabulary size of about 500,000 words and for ##### .  Class-based models.  In many applications, it is natural and convenient to construct class-based language models, that is models based on classes of words (Brown et al. , 1992).  Such models are also often more robust since they may include words that belong to a class but that were not found in the corpus.  Classical class-based models are based on simple classes such as a list of words.  But new clustering algorithms allow one to create more general and more complex classes that may be regular languages.  Very large and complex classes can also be defined using regular expressions.  We present a simple and more general approach to class-based language models based on general weighted context-dependent rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996).  Our approach allows us to deal efficiently with more complex classes such as weighted regular languages.  We have fully implemented the algorithms just mentioned and incorporated them in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities (Allauzen et al. , 2003).  In the following, we will present in detail these algorithms and briefly describe the corresponding GRM utilities.  2 Preliminaries Definition 1 A system ############# ### ### is a semiring (Kuich and Salomaa, 1986) if: ######### ### is a commutative monoid with identity element # ; ######### ### is a monoid with identity element # ; # distributes over # ; and # is an annihilator for # : for all ####### #!# #"# #####$# # .  Thus, a semiring is a ring that may lack negation.  Two semirings often used in speech processing are: the log semiring %&#'##(*),+. -0/1####2 3 45##6###-0# #1# (Mohri, 2002) which is isomorphic to the familiar real or probability semiring ##(879##6##5:;# #<#=### via a }@?1A morphism with, for all #B# C##D(E)F+. -0/ : #;# 2 3G4 CH##ID}@?1AJ#LKNMPOQ#RIS#P#Q6,KTMPOQ#UI#CT#U# and the convention that: KTMPOQ#UI#-## # # and ID}@?1A####1#V#W- , and the tropical semiring XY#Y#Z([7\) +. -0/1#G]!^@_`##6###-##G#1# which can be derived from the log semiring using the Viterbi approximation.  Definition 2 A weighted finite-state transducer a over a semiring # is an 8-tuple ab#c#ed###f###g!#Gh##GiH# jk# lQ#UmP# where: d is the finite input alphabet of the transducer; f is the finite output alphabet; g is a finite set of states; h0nog the set of initial states; ipnqg the set of final states; jrn0gs:D#td#)u+#v=/w#x:y##fz)u+wvN/. #{:k#\:Vg a finite set of transitions; l#

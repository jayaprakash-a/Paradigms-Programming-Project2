Tree Classification Software
 ABSTRACT This paper introduces the IND Tree Package to prospective users.  IND does supervised learning using classification trees.  This learning task is a basic tool used in the development of diagnosis, monitoring and expert systems.  IND was developed as part of a NASA project to semi-automate the development of data analysis and modelling algorithms using artificial intelligence techniques.  IND integrates features from Breiman et al. 's CART and Quinlan's C4. 5 with newer Bayesian and minimum encoding methods for growing classification trees and graphs.  IND also provides an experimental control suite on top.  The newer features give improved probability estimates often required in diagnostic and screening tasks.  The package comes with a manual, Unix "man" entries, and a guide to tree methods and research.  IND is implemented in C under Unix, and has been beta-tested at university and commercial research laboratories in the United States.  DIAGNOSIS AND CLASSIFICATION A common inference task is where we learn to make a discrete prediction about some case given other details about the case.  For instance, in financial credit assessment we wish to decide whether to accept or reject a customer's application for a loan given particular personal information.  In monitoring a subsystem of the space shuttle, measurements such as flow rates and temperature are continuously recorded and we need to screen those measurements to decide if the system is in normal or abnormal operation.  If the system is in abnormal operation we might further wish to try and predict the type of abnormality present.  This prediction task is the basic task of many expert systems, health monitoring systems, diagnostic systems, etc.  Furthermore, more complex problems can often be broken down into a sequence of simple prediction problems.  For instance, speech understanding, converting the spoken word into written text, is a sequence of prediction tasks about each phoneme.  In medical diagnosis, or diagnosis of equipment subsystems, we need more than just a prediction, we need a careful probabilistic assessment.  A simplistic medical example will bring this point home.  Suppose your doctor suspects you have a cyst in your abdomen.  The options (1 or 2) and outcomes (A or B) give the following set of possibilities: (1A) operates, discovers a cyst, removes it, and you're grateful; (1B) operates, no cyst found, but you're left with the medical bill and a day recovery in hospital; (2A), doesn't operate but the cyst exists and causes medical complications due to lack of treatment; (2B), doesn't operate, no cyst exists.  Each case has important implications to you both financially and in quality of life.  With a careful probabilistic assessment of the existence of a cyst, you can weigh up the options and decide which option (1 or 2) is the most beneficial to you.  For instance, if the medical bill is insignificant compared to the potential medical complications, then you would decide to have the operation even if there was a small chance of having the cyst.  If the potential medical complications were insignificant, you would only decide to operate if there was a very high probability of having the cyst.  This process of decision analysis requires as input probabilities about the new case in question.  In health monitoring and diagnosis, these probability assessments are needed when the system is being used to screen cases, i. e.  the computer systems scans the on-line monitoring data and at certain time points alerts a human expert that a potentially anomalous situation has arisen.  Probability assessments such as the "probability of equipment failure" can be used to determine which of the many cases scanned should be forwarded to the human expert for the more costly process of manual inspection.  I will refer to this prediction problem as classification, where the aim is to classify each new case.  One common technique for developing a system to do prediction or probability assessment about new cases is to examine a database of cases, for instance collected historically.  Assume that hindsight tells us which is the correct classification for each case in the data base, so for each we know which prediction was optimal.  From the data base we use statistical techniques to "discover" or "learn" how to do the predictions for new unseen cases.  This learning technique is represented in Figure 1.  The process requires three main forms of input: an expert who is able to advise on the problem, help configure the system, etc. , a data base of correctly classified cases to use in the learning process, and a model family from which the learning algorithm is to select a "good" model for doing prediction or probabilistic assessment.  expert guidance and intuition data base of cases correctly classified skeletal model for system to build on model discovery (data analysis) system model developed from the data base feedback feedback Figure 1.  Learning prediction models from data.  This model learning or discovery process is a useful technique in almost every industry, finance, manufacturing, etc. , wherever on-line databases are stored and important predictions have to be made on a regular basis about new cases before they enter the data base.  Not surprisingly, there are many different fields of science that address this problem as one of their central concerns.  In artificial intelligence it is referred to as the classification or induction problem.  Techniques include tree and rule learning algorithms of the form I will present in this paper.  In statistics it is referred to as the discrimination problem, and common techniques are the linear models used in the finance and banking industry for credit assessment.  In pattern recognition it is referred to as supervised learning.  In neural networks it is the classification and generalization problem and is routinely investigated using a number of network architectures.  These diverse fields are all studying the same problem, "learning to predict", and present a confusing array of methodologies and paradigms for addressing that problem.  They differ in the following aspects: Model family: Which class of models are being used to do prediction? In Figure 1 this corresponds to the "skeletal model".  I present classification tree and classification graph model families in this paper.  Statistical philosophy: How is learning to occur? That is, what statistical principles if any are used to develop the central box in Figure 1? Computational and optimization methods: What are the basic computational methods used in terms of efficiency, optimality, search method, etc. ? Methodological support: What methodology does the analyst use to go about applying the technique to a real problem? For statisticians this is the "consultancy phase" rarely covered in university courses.  In artificial intelligence this is the process of "knowledge engineering".  I will refer to the general task of learning how to predict (or estimate probabilities) from data as the classification task.  The next section discusses the design of tools for this task.  After this, the model family considered in this paper is addressed, and the IND program presented.

Bayesian Learning via Stochastic Dynamics
 Abstract The attempt to find a single "optimal" weight vector in conventional network training can lead to overfitting and poor generalization.  Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data.  This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution.  1 CONVENTIONAL AND BAYESIAN LEARNING I view neural networks as probabilistic models, and learning as statistical inference.  Conventional network learning finds a single "optimal" set of network parameter values, corresponding to maximum likelihood or maximum penalized likelihood inference.  Bayesian inference instead integrates the predictions of the network over all possible values of the network parameters, weighting each parameter set by its posterior probability in light of the training data.  1. 1 NEURAL NETWORKS AS PROBABILISTIC MODELS Consider a network taking a vector of real-valued inputs, x, and producing a vector of real-valued outputs, y, perhaps computed using hidden units.  Such a network architecture corresponds to a function, f , with y = f(x; w), where w is a vector of connection weights.  If we assume the observed outputs, y, are equal to y plus Gaussian noise of standard deviation oe, the network defines the conditional probability for an observed output vector given an input vector as follows: P (y j x; oe) / exp\Gamma \Gamma jy \Gamma f(x; w)j 2 ffi 2oe 2 \Delta (1) The probability of the outputs in a training set (x 1 ; y 1 ); .  .  .  ; (xn ; yn ) given this fixed noise level is therefore P (y 1 ; .  .  .  ; yn j x 1 ; .  .  .  ; xn ; oe) / exp\Gamma \Gamma P c jy c \Gamma f(x c ; w)j 2 ffi 2oe 2 \Delta (2) Often oe is unknown.  A Bayesian approach to handling this is to assign oe a vague prior distribution and then integrating it away, giving the following probability for the training set (see (Buntine and Weigend, 1991) or (Neal, 1992) for details):

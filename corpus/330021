Encyclopedia of Cognitive Science, in press Reinforcement Learning
 Glossary Markov chain a model for a random process that evolves over time such that the states (like locations in a maze) occupied in the future are independent of the states in the past given the current state.  Markov decision problem (MDP) a model for a controlled random process in which an agent's choice of action determines the probabilities of transitions of a Markov chain and lead to rewards (or costs) that need to be maximised (or minimised).  policy a deterministic or stochastic scheme for choosing an action at every state or location.  reward an immediate, possibly stochastic, payoff that results from performing an action in a state.  In an MDP, the immediate reward depends on the current state and action only.  The agent's aim is to optimise a sum or average of (possibly discounted) future rewards.  value function a function defined over states, which gives an estimate of the total (possibly discounted) reward expected in the future, starting from each state, and following a particular policy.  discounting if rewards received in the far future are worth less than rewards received sooner, they are described as being discounted.  Humans and animals appear to discount future rewards hyperbolically; exponential discounting is common in engineering and finance.  dynamic programming a collection of calculation techniques (notably policy and value iteration) for finding a policy that maximises reward or minimises costs.  temporal difference prediction error a measure of the inconsistency between estimates of the value function at two successive states.  This prediction error can be used to improve the predictions and also to choose good actions.  Reinforcement Learning Secondary Computation dynamic programming#value function#policy#actor-critic#Q

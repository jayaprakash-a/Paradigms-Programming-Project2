Multiplicative Updates for Classification by Mixture Models
 Abstract We investigate a learning algorithm for the classification of nonnegative data by mixture models.  Multiplicative update rules are derived that directly optimize the performance of these models as classifiers.  The update rules have a simple closed form and an intuitive appeal.  Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm---its guarantee of monotonic improvement, and its absence of tuning parameters---with the added advantage of optimizing a discriminative objective function.  The algorithm reduces as a

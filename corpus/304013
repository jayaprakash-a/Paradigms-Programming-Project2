Rich Syntax from a Raw Corpus: Unsupervised Does It
 Abstract We compare our model of unsupervised learning of linguistic structures, ADIOS [1], to some recent work in computational linguistics and in grammar theory.  Our approach resembles the Construction Grammar in its general philosophy (e. g. , in its reliance on structural generalizations rather than on syntax projected by the lexicon, as in the current generative theories), and the Tree Adjoining Grammar in its computational characteristics (e. g. , in its apparent affinity with Mildly Context Sensitive Languages).  The representations learned by our algorithm are truly emergent from the (unannotated) corpus data, whereas those found in published works on cognitive and construction grammars and on TAGs are hand-tailored.  Thus, our results complement and extend both the computational and the more linguistically oriented research into language acquisition.  We conclude by suggesting how empirical and formal study of language can be best integrated.  1 Unsupervised learning through redundancy reduction Reduction of redundancy is a general (and arguably the only conceivable) approach to unsupervised learning [2, 3].  Written natural language (or transcribed speech) is trivially redundant to the extent it relies on a fixed lexicon.  This property of language makes possible the unsupervised recovery of words from a text corpus with all the spaces omitted, through a straightforward minimization of per-letter entropy [4].  Pushing entropy minimization to the limit would lead to an absurd situation in which the agglomeration of words into successively longer "primitive" sequences renders the resulting representation useless for dealing with novel texts (that is, incapable of generalization; cf.  [5], p. 188).  We observe, however, that a word-based representation is still redundant to the extent that different sentences share the same word sequences.  Such sequences need not be contiguous; indeed, the detection of paradigmatic variation within a slot in a set of otherwise identical aligned sequences (syntagms) is the basis for the classical distributional theory of language [6], as well as for some modern NLP methods [7].  The pattern --- the syntagm and the equivalence class of complementary-distribution symbols that may appear in its open slot --- is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure) [1].

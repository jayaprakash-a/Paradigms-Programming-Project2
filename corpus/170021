Using Temporal Neighborhoods to Adapt Function Approximators in Reinforcement Learning
 Abstract To avoid the curse of dimensionality, function approximators are used in reinforcement learning to learn value functions for individual states.  In order to make better use of computational resources (basis functions) many researchers are investigating ways to adapt the basis functions during the learning process so that they better fit the value-function landscape.  Here weintroduce temporal neighborhoods as small groups of states that experience frequentintragroup transitions during on-line sampling.  We then form basis functions along these temporal neighborhoods.  Empirical evidence is provided which demonstrates the effectiveness of this scheme.  We discuss a class of RL problems for which this method mightbe plausible.  1 Overview In reinforcement learning an agentnavigates an environment (a state space) by selecting various actions in each state.  As the agent makes actions, it receives rewards indicating the \goodness" of the action.  Reinforcement learning is a methodology which allows the agent to discover which actions to select in order to optimize the rewards in each state.  The value of a state is the immediate reward an agent will receive from that state and the discounted sum of all future rewards encountered by the agent.  Detailed reviews of reinforcement learning are available [2, 3].  On-line algorithms use the experience of the agent as it moves about the state space to learn the values of each state.  Tables are often employed to \memorize" the value for each individual state.  However, many RL problems involvevery large state spaces, especially when the state space is multidimensional.  The curse of dimensionality arises because state spaces grow too large to store all individual state values in a single table.  To lessen the curse of dimensionality, function approximators are commonly utilized: they require far fewer resources than a table look-up method, and they generalize over other parts of the state space so that learning experience can be shared among states.  Function approximators commonly use fixed basis functions (such as CMACs and Radial Basis Functions) which have shown to be stable in both theory and in practice [8, 9].  Despite the proofs of convergence for fixed basis function approximators, these RL algorithms are often slow to converge in practice.  Research indicates that different types of basis functions are better suited to different problems, and they often need to be \#ne-tuned" to the particular task [4].  Fixed basis functions also tend to be somewhat wasteful of computational resources because they do not accommodate the pecularities of the value function landscape; one needs to be certain to employ enough fixed basis functions of adaquately fine resolution to learn a value function well.  There have been many attempts to adapt basis functions during learning to better fit the value function landscape.  The most common methods perform gradient descent on an error metric but these techniques are generally slow to converge and are overly sensativetovarious parameters.  Singh's soft state aggregation demonstrates success using a gradient descent technique to shape the basis functions [7].  There are also various other adaptive approaches.  Anderson's Hidden Restart Method [1] relocates basis functions to regions of the state space which are not adequately modelled.  Whitehead and Choate employ genetic algorithms to position and form basis functions [10].  Moore's Parti-Game Algorithm learns value functions by dynamically creating variable resolution \basis functions" [6].  Here we develop a novel approach in which the basis functions are adapted according to the perceived state transition probabilities.  McCallum has shown successful results in using Transitional Proximity for a faster Q-value update scheme [5].  We use the same information in much different manner.  Additionally,we provide a theoretical discussion regarding the types of RL tasks that would benefit from using state transitions.  In Section 2 we define temporal neighborhoods and discuss their role in forming basis functions for function approximation.  Section 3 shows a simple example illustrating the advantage of temporal neighborhoods.  The details of the algorithm are presented in Section 4.  In Section 5 we apply the algorithm to the more complex Mountain Car task.  A summary and discussion of future work are presented in Section 6.  2 Temporal Neighborhoods With most local function approximators, basis functions are created to span a small neighborhood of physically adjacent states.  By \physically adjacent states" we mean states which are near each other in Euclidean distance.  Although states maybephysically adjacent, in many control problems it may be unlikely (or even impossible) that the agent can transition between them.  A better notion of nearness is temporal adjacency.  As the agentinteracts with the environment, there tend to be pathways or trajectories through the state space which the agent uses with high frequency.  The states which lie along these trajectories are temporaly adjacent.  Two states are temporally adjacent if when the agent currently occupies one state, there exists a high probability that the agent will transition to the other state on the next move.

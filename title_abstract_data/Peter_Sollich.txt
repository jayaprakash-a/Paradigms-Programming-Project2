RHEOLOGY AND GLASSY DYNAMICS OF FOAMS| Abstract.  After a brief `warm-up' discussion of osmotic pressure of foams, the basic phenomena of foam rheology are reviewed, focusing on linear viscoelastic spectra (elastic and loss moduli) with brief mention of nonlinear effects.  Theoretical models for some of these properties are then described, starting with Princen's model for the elastic modulus G 0 of an ordered foam in two dimensions.  There is a basic conflict between this model, which predicts a step-function onset of the modulus when droplets first contact at volume fraction OE = OE 0 , and the experimental data (which show G 0 OE \Gamma OE 0 ).  The three dimensional ordered case is reviewed next, focusing on anharmonic deformation theory which predicts a logarithmic softening of the modulus near OE 0 ; this is still not soft enough to explain the observations.  The 3D disordered case is then addressed; a combination of disorder and the anharmonic effect finally seems able to explain the data.  We then consider the problem of the frequency-dependent loss modulus G 00 (!) which describes dissipation in a foam.  Somewhat alarmingly, the data suggest behaviour incompatible with linear response theory; reconciliation is possible if one invokes some very slow relaxation processes at timescales longer than experiment.  We briefly describe the search for foam-specific slow relaxation mechanisms of surfactant and water transport, which so far has yielded no viable candidates.  Since similar anomalies in G 00 (!) are observed in several other systems, they are instead tentatively ascribed to a generic phenomenon: glassy dynamics.  A recent model for the rheology of "soft glassy matter" is then reviewed; though phenomenological, this suggests that glassy dynamics may be a useful concept in foam rheology. 
Online Learning from Finite Training Sets and Robustness to Input Bias| We analyse online gradient descent learning from finite training sets at noninfinitesimal learning rates j.  Exact results are obtained for the time-dependent generalization error of a simple model system: a linear network with a large number of weights N , trained on p = ffN examples.  This allows us to study in detail the effects of finite training set size ff on, for example, the optimal choice of learning rate j.  We also compare online and offline learning, for respective optimal settings of j at given final learning time.  Online learning turns out to be much more robust to input bias and actually outperforms offline learning when such bias is present; for unbiased inputs, online and offline learning perform almost equally well. 
Gaussian Process Regression with Mismatched Models| Abstract Learning curves for Gaussian process regression are well understood when the `student' model happens to match the `teacher' (true data generation process).  I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly.  In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples.  Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly.  All predictions are confirmed by simulations. 
Probabilistic methods for Support Vector Machines| Abstract I describe a framework for interpreting Support Vector Machines (SVMs) as maximum a posteriori (MAP) solutions to inference problems with Gaussian Process priors.  This can provide intuitive guidelines for choosing a `good' SVM kernel.  It can also assign (by evidence maximization) optimal values to parameters such as the noise level C which cannot be determined unambiguously from properties of the MAP solution alone (such as cross-validation error).  I illustrate this using a simple approximate expression for the SVM evidence.  Once C has been determined, error bars on SVM predictions can also be obtained. 
Probabilistic interpretations and Bayesian methods for Support Vector Machines| Abstract Support Vector Machines (SVMs) can be interpreted as maximum a posteriori solutions to inference problems with Gaussian Process (GP) priors and appropriate likelihood functions.  Focussing on the case of classification, I show first that such an interpretation gives a clear intuitive meaning to SVM kernels, as covariance functions of GP priors; this can be used to guide the choice of kernel.  Second, a probabilitistic interpretation allows Bayesian methods to be used for SVMs: Using a local approximation of the posterior around its maximum (the standard SVM solution), I discuss how the evidence for a given kernel and noise parameter can be estimated, and how approximate error bars for the classification of test points can be calculated.  1 SVMs Support Vector Machines (SVMs) have been the subject of intense research activity from within the neural networks community over the last few years; for tutorial introductions and overviews of recent developments see [1, 2, 3].  One of the open questions that remains is how to set the `tunable' parameters of an SVM algorithm: While methods for choosing the width of the kernel function and the noise parameter (which controls how closely the training data are fitted) have recently been proposed [4, 5], the effect of the overall shape of the kernel function remains imperfectly understood [1].  In this paper I suggest that a probabilistic interpretation of SVMs might be useful in tackling this problem.  It has two main benefits: First, it clarifies the role of the kernel as specifying an SVM `prior' over functions on the input space, avoiding the need to think in terms of an abstract feature space.  I illustrate this with samples from some typical SVM priors and discuss general guidelines for the choice of kernels that emerge.  Second, a probabilistic interpretation also allows Bayesian methods to be applied to SVMs.  As an example, I sketch how the loglikelihood (or `evidence') for an SVM model -- specified by a kernel and noise parameter -can be evaluated approximately; this could be a useful alternative to generalization error bounds in guiding the search for optimal kernels.  I also outline how approximate error bars for the predictions of a trained SVM can be obtained.  These could be of use in safety-critical applications, for example, where the conventional purely deterministic SVM predictions are undesirable.  I will focus mainly on (two-class) classification problems.  Suppose we are given a set D of n training examples, each of the form (x; y x ) with a binary output y x = \Sigma1 classifying to which of the two possible classes the input x belongs.  The basic SVM idea is then to map the inputs x onto vectors OE(x) in some high-dimensional feature space; ideally, in this feature space, the problem should be linearly separable.  Suppose first that this is true.  Among all decision hyperplanes w \Delta OE(x) + b = 0 which separate the training examples (i. e. , which obey y x (w \Delta OE(x) + b) ? 0 for all x 2 X , X being the set of training inputs), the SVM solution is then chosen as the one with the largest margin, i. e. , the largest minimal distance from any of the training examples.  In practice, it is easier to specify the margin instead (the conventional value is unity) and minimize the squared length of the weight vector jjwjj 2 [1].  This leads to the following optimization problem: Find a weight vector w and an offset b such that 1 2 jjwjj 2 is minimized, subject to the constraint that y x (w\DeltaOE(x) +b) 1 for all training examples.  What happens if the problem is not linearly separable, even in the highdimensional feature space? Then `slack variables' x 0 are introduced which measure how much the margin constraint is violated for training input x; one writes y x (w \Delta OE(x) + b) 1 \Gamma x .  To control the amount of slack allowed (which determines how closely the training data are fitted), a penalty term must then be added to the objective function 1 2 jjwjj 2 .  The most widely used choice is C P x x , with a penalty coefficient C.  Training examples for which y x (w\DeltaOE(x) + b) 1 (and hence x = 0) then incur no penalty, while for the other examples there is a penalty of C[1\Gamma y x (w\DeltaOE(x)+b)] each.  Altogether, the SVM optimization problem can therefore be expressed as follows: Find w and b to minimize 1 2 jjwjj 2 + C X x2X l(y x [w \Delta OE(x) + b]) (1) where l(z) is the (shifted) `hinge loss', l(z) = (1 \Gamma z)\Theta(1\Gamma z).  The probabilistic interpretation of SVMs that I discuss below hinges on the fact that (1) can be regarded as defining a (negative) log-posterior probability for the parameters w and b of the SVM, given a training set D.  The first term gives us the prior P (w) exp(\Gamma 1 2 jjwjj 2 ).  This is a Gaussian prior on w; the components of w are uncorrelated with each other and have unit variance.  The prior on b is flat (uninformative, improper) and I will not write it explicitly.  The weights w occur in the second term of (1) -- to be identified as the log-likelihood shortly -- only through a(x) = w \Delta OE(x).  It is therefore useful to express the prior directly as a distribution over the functions a.  The function values a(x) have a joint Gaussian distribution because the components of w do, and their covariances are given by ha(x)a(x 0 )i = h(OE(x) \Delta w)(w \Delta OE(x 0 ))i = OE(x)\DeltaOE(x 0 ).  In other words, the SVM prior is simply a Gaussian process (GP) prior over the functions a, with covariance function K(x; x 0 ) = OE(x) \Delta OE(x 0 ) (and zero mean).  To interpret the second term in (1) as a (negative) log-likelihood, one only has to define the probability of obtaining output y for a given x (and a, b) as P (y = \Sigma1jx; a; b) = (C)e \Gamma Cl(y[a(x)+b]) (2) The proportionality constant (C) here needs to be chosen such that the probabilities for y = \Sigma1 never add up to a value larger than one; it is sensible to choose the largest value that achieves this, which is (C) = 1=[1 + exp(\Gamma 2C)]: (3) For a generic value of a(x) + b, however, the probabilities P (y = \Sigma1jx; a; b) then still add to a value ! 1.  It is therefore necessary to introduce a `don't-know' class (labelled by y = 0, say), with probability P (y = 0jx; a; b) = 1 \Gamma X y=\Sigma1 P (y = \Sigma1jx; a; b): (4) While this may seem surprising at first sight, it has the appealing feature that the probability for the `don't-know' class is largest in the `gap',\Gamma 1 ! a(x) + b ! 1, where one would intuitively expect the output of the SVM to be least certain 1 .  Up to the trivial additive constant n ln(1 + e \Gamma 2C ), the second term in (1) can now be identified as the negative log-likelihood of the observed data D.  To summarize, the task of training an SVM for classification can be reformulated as follows: Given a zero-mean Gaussian process prior over functions a with covariance function K(x; x 0 ), a flat prior over offsets b, and the likelihood function defined by (2--4), find the maximum a posteriori (MAP) values of a and b.  (These are the values that maximize P (a; bjD) P (a)P (Dja; b). ) The feature space has disappeared entirely from this formulation 2 ; all its relevant properties are encoded in the GP prior, specified by the kernel K(x; x 0 ).  There is nothing profound about this correspondence between SVMs and GPs.  It is related to the common link to reproducing kernel Hilbert spaces [7], and can be extended from SVMs to more general 1 This is based on the reasonable assumption that C is not too small (C 1, roughly).  Very small values of C would give a large range of values of a(x) + b where the probabilities for y \Sigma 1 are comparable, producing essentially random outputs; this is an unrealistic situation.  2 The case of SVM regression is completely analogous.  The only modification is in the likelihood function, which then determines the probability of a real-valued output, P (yjx; a; b) expf\Gamma Cl ffl (y \Gamma [a(x)+b])g, in terms of the ffl-insensitive loss function l ffl (z) = (jzj \Gamma ffl)\Theta(jzj \Gamma ffl). 
Generalization of Plaskota's bound for Gaussian process learning curves| Abstract [Note: This paper is an extended version of the manuscript Learning curves for Gaussian process regression: Approximations and bounds by Sollich and Halees.  The only difference is the addition of Appendix B, which gives the derivation of the generalized version of Plaskota's bound.  The remainder of the paper has been left in place to provide the proper context. ] We consider the problem of calculating learning curves (i. e. , average generalization performance) of Gaussian processes used for regression.  On the basis of a simple expression for the generalization error, in terms of the eigenvalue decomposition of the covariance function, we derive a number of approximation schemes.  We identify where these become exact, and compare with existing bounds on learning curves; the new approximations, which can be used for any input space dimension, generally get substantially closer to the truth.  We also study possible improvements to our approximations.  Finally, we use a simple exactly solvable learning scenario to show that there are limits of principle on the quality of approximations and bounds expressible solely in terms of the eigenvalue spectrum of the covariance function. 
Using the Equivalent Kernel to Understand Gaussian Process Regression| Abstract The equivalent kernel [1] is a way of understanding how Gaussian process regression works for large sample sizes based on a continuum limit.  In this paper we show (1) how to approximate the equivalent kernel of the widely-used squared exponential (or Gaussian) kernel and related kernels, and (2) how analysis using the equivalent kernel helps to understand the learning curves for Gaussian processes.  Consider the supervised regression problem for a dataset D with entries (x i , y i ) for i = 1, .  .  .  , n.  Under Gaussian Process (GP) assumptions the predictive mean at a test point x # is given by f(x # ) = k } (x # )(K + # 2 I) - 1 y, (1) where K denotes the n n matrix of covariances between the training points with entries k(x i , x j ), k(x # ) is the vector of covariances k(x i , x # ), # 2 is the noise variance on the observations and y is a n 1 vector holding the training targets.  See e. g.  [2] for further details.  We can define a vector of functions h(x # ) = (K + # 2 I) - 1 k(x # ) .  Thus we have f(x # ) = h } (x # )y, making it clear that the mean prediction at a point x # is a linear combination of the target values y.  Gaussian process regression is thus a linear smoother, see [3, section 2. 8] for further details.  For a fixed test point x # , h(x # ) gives the vector of weights applied to targets y.  Silverman [1] called h } (x # ) the weight function.  Understanding the form of the weight function is made complicated by the matrix inversion of K + # 2 I and the fact that K depends on the specific locations of the n datapoints.  Idealizing the situation one can consider the observations to be "smeared out" in x-space at some constant density of observations.  In this case analytic tools can be brought to bear on the problem, as shown below.  By analogy to kernel smoothing Silverman [1] called the idealized weight function the equivalent kernel (EK).  The structure of the remainder of the paper is as follows: In section 1 we describe how to derive the equivalent kernel in Fourier space.  Section 2 derives approximations for the EK for the squared exponential and other kernels.  In section 3 we show how use the EK approach to estimate learning curves for GP regression, and compare GP regression to kernel regression using the EK. 
Learning in large linear perceptrons and why the thermodynamic limit is relevant to the real world| Abstract We present a new method for obtaining the response function G and its average G from which most of the properties of learning and generalization in linear perceptrons can be derived.  We first rederive the known results for the `thermodynamic limit' of infinite perceptron size N and show explicitly that G is self-averaging in this limit.  We then discuss extensions of our method to more general learning scenarios with anisotropic teacher space priors, input distributions, and weight decay terms.  Finally, we use our method to calculate the finite N corrections of order 1=N to G and discuss the corresponding finite size effects on generalization and learning dynamics.  An important spin-off is the observation that results obtained in the thermodynamic limit are often directly relevant to systems of fairly modest, `real-world' sizes. 
Learning with ensembles: How overfitting can be useful| Abstract We study the characteristics of learning with ensembles.  Solving exactly the simple model of an ensemble of linear students, we find surprisingly rich behaviour.  For learning in large ensembles, it is advantageous to use under-regularized students, which actually over-fit the training data.  Globally optimal performance can be obtained by choosing the training set sizes of the students appropriately.  For smaller ensembles, optimization of the ensemble weights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process.  Choosing students with a wide range of regularization parameters makes this improvement robust against changes in the unknown level of noise in the training data. 
Finite size effects in learning and generalization in linear perceptrons| Abstract.  Most properties of learning and generalization in linear perceptrons can be derived from the average response function G.  We present a method for calculating G using only simple matrix identities and partial differential equations.  Using this method, we first rederive the known result for G in the thermodynamic limit of perceptrons of infinite size N , which has previously been calculated using replica and diagrammatic methods.  We also show explicitly that the response function is self-averaging in the thermodynamic limit.  Extensions of our method to more general learning scenarios with anisotropic teacher space priors, input distributions, and weight decay terms are discussed.  Finally, finite size effects are considered by calculating the O(1=N) correction to G.  We verify the result by computer simulations and discuss the consequences for generalization and learning dynamics in linear perceptrons of finite size. 
On-line Learning from Finite Training Sets in Nonlinear Networks| Abstract Online learning is one of the most common forms of neural network training.  We present an analysis of online learning from finite training sets for non-linear networks (namely, soft-committee machines), advancing the theory to more realistic learning scenarios.  Dynamical equations are derived for an appropriate set of order parameters; these are exact in the limiting case of either linear networks or infinite training sets.  Preliminary comparisons with simulations suggest that the theory captures some effects of finite training sets, but may not yet account correctly for the presence of local minima. 
Finite size effects in on-line learning of multi-layer neural networks| Abstract We complement recent advances in thermodynamic limit analyses of mean on-line gradient descent learning dynamics in multi-layer networks by calculating fluctuations possessed by finite dimensional systems.  Fluctuations from the mean dynamics are largest at the onset of specialisation as student hidden unit weight vectors begin to imitate specific teacher vectors, increasing with the degree of symmetry of the initial conditions.  In light of this, we include a term to stimulate asymmetry in the learning process, which typically also leads to a significant decrease in training time.  An attractive feature of neural networks is their ability to learn a parametrised rule from a set of input/output training examples, by which the parameters of the network are adapted to minimise an error measuring the misfit of the network mapping on the training examples.  Different approaches to the learning process are typically evaluated by the expected error that the network will make on a randomly presented input example.  In on-line learning, statistical mechanics plays a strong role in calculating this generalisation error (see [1, 2, 4] and refs.  within) through self-averaging in the thermodynamic limit, for which an understanding of finite size effects would benefit further advances.  Connections to alternative finite dimensional methods (see [3] and refs.  within) will be pointed to in the course of our analysis.  In on-line learning, the weights parametrising the student network are successively updated according to the error incurred on a single example from a stream of input/output examples, f; ()g, generated by a teacher network (\Delta).  We assume that the teacher network the student attempts to learn is a soft committee machine[1, 4] of N inputs, and M hidden units, this being a one hidden layer network with weights connecting each hidden to output unit set to +1, and with each hidden unit n connected to all input units by B n (n = 1::M).  Explicitly, for the N dimensional training input vector , the output of the teacher is given by, i = M X n=1 g(B n \Delta ); (1) where g(x) is the activation function of the hidden units, and we take g(x) = erf(x= p 2).  The teacher generates a stream of training examples (; i ), with input components drawn from a normal distribution of zero mean, unit variance.  The student network that attempts to learn the teacher, by fitting the training examples, is also a soft committee machine, but with K hidden units.  For input , the student output is, oe(J; ) = K X i=1 g(J i \Delta ); (2) where the student weights J = fJ i g(i = 1::K) are sequentially modified to reduce the error that the student makes on an input , ffl(J; ) = 1 2 (oe(J; ) \Gamma i ) 2 = 1 2 / K X i=1 g(x i ) \Gamma M X n=1 g(y n ) ! 2 ; (3) where the activations are defined x i = J i \Delta , and y n = B n \Delta .  Gradient descent on the error(3) results in an update of the student weight vectors, J +1 = J \Gamma j N ffi i ; (4) where, ffi i = g 0 (x i ) 2 4 M X n=1 g(y n ) \Gamma K X j=1 g(x j ) 3 5 ; (5) and g 0 is the derivative of the activation function g.  The typical performance of the student on a randomly selected input example is given by the generalisation error, ffl g = hffl(J; )i, where h::i represents an average over the gaussian input distribution.  One finds that ffl g depends only on the order-parameters, R in = J i \Delta Bn , Q ij = J i \Delta J j , and Tnm = Bn \Delta Bm (i; j = 1::K; n; m = 1::M)[4], for which, using (4), we derive (stochastic) update equations, R +1 in\Gamma R in = j N ffi i y n ; (6) Q +1 ik\Gamma Q ik = j N \Gamma ffi i x j + ffi k x i \Delta + j 2 N 2 ffi i ffi k \Delta : (7) We average over the input distribution to obtain deterministic equations for the mean values of the order parameters, which are self-averaging in the thermodynamic limit, N!1.  The order-parameter approach contrasts with approaches which analyze the dynamics of the individual weight components, based upon approximate Fokker-Plank equations (see [3] and refs.  within).  The advantage of the order-parameter approach is that the system is modelled exactly in the thermodynamic limit, with only a small number of equations.  In this work we present a more realistic treatment by calculating the dynamic fluctuations induced
Learning from queries for maximum information gain in imperfectly learnable problems| Abstract In supervised learning, learning from queries rather than from random examples can improve generalization performance significantly.  We study the performance of query learning for problems where the student cannot learn the teacher perfectly, which occur frequently in practice.  As a prototypical scenario of this kind, we consider a linear perceptron student learning a binary perceptron teacher.  Two kinds of queries for maximum information gain, i. e. , minimum entropy, are investigated: Minimum student space entropy (MSSE) queries, which are appropriate if the teacher space is unknown, and minimum teacher space entropy (MTSE) queries, which can be used if the teacher space is assumed to be known, but a student of a simpler form has deliberately been chosen.  We find that for MSSE queries, the structure of the student space determines the efficacy of query learning, whereas MTSE queries lead to a higher generalization error than random examples, due to a lack of feedback about the progress of the student in the way queries are selected. 
Gaussian Fields for approximate Inference in Sigmoid Belief Networks| Abstract We are interested in Sigmoid Belief Networks.  Have a layered structure.  Gaussian Field assumption.  Good for inference in large layered networks. 
Model Selection for Support Vector Machine Classification| Abstract We address the problem of model selection for Support Vector Machine (SVM) classification.  For fixed functional form of the kernel, model selection amounts to tuning kernel parameters and the slack penalty coefficient C.  We begin by reviewing a recently developed probabilistic framework for SVM classification.  An extension to the case of SVMs with quadratic slack penalties is given and a simple approximation for the evidence is derived, which can be used as a criterion for model selection.  We also derive the exact gradients of the evidence in terms of posterior averages and describe how they can be estimated numerically using Hybrid Monte Carlo techniques.  Though computationally demanding, the resulting gradient ascent algorithm is a useful baseline tool for probabilistic SVM model selection, since it can locate maxima of the exact (unapproximated) evidence.  We then perform extensive experiments on several benchmark data sets.  The aim of these experiments is to compare the performance of probabilistic model selection criteria with alternatives based on estimates of the test error, namely the so-called "span estimate" and Wahba's Generalized Approximate Cross-Validation (GACV) error.  We find that all the "simple" model criteria (Laplace evidence approximations, and the Span and GACV error estimates) exhibit multiple local optima with respect to the hyperparameters.  While some of these give performance that is competitive with results from other approaches in the literature, a significant fraction lead to rather higher test errors.  The results for the evidence gradient ascent method show that also the exact evidence exhibits local optima, but these give test errors which are much less variable and also consistently lower than for the simpler model selection criteria. 
On-Line Learning with Restricted Training Sets: Exact Solution as Benchmark for General Theories| Abstract We solve the dynamics of on-line Hebbian learning in perceptrons exactly, for the regime where the size of the training set scales linearly with the number of inputs.  We consider both noiseless and noisy teachers.  Our calculation cannot be extended to nonHebbian rules, but the solution provides a nice benchmark to test more general and advanced theories for solving the dynamics of learning with restricted training sets. 
Query learning for maximum information gain in a multi-layer neural network| In supervised learning, the redundancy contained in random examples can be avoided by learning from queries, where training examples are chosen to be maximally informative.  Using the tools of statistical mechanics, we analyse query learning in a simple multi-layer network, namely, a large tree-committee machine.  The generalization error is found to decrease exponentially with the number of training examples, providing a significant improvement over the slow algebraic decay for random examples.  Implications for the connection between information gain and generalization error in multi-layer networks are discussed, and a computationally cheap algorithm for constructing approximate maximum information gain queries is suggested and analysed. 
Learning Curves for Gaussian Process Regression: Approximations and Bounds| Abstract We consider the problem of calculating learning curves (i. e. , average generalization performance) of Gaussian processes used for regression.  On the basis of a simple expression for the generalization error, in terms of the eigenvalue decomposition of the covariance function, we derive a number of approximation schemes.  We identify where these become exact, and compare with existing bounds on learning curves; the new approximations, which can be used for any input space dimension, generally get substantially closer to the truth.  We also study possible improvements to our approximations.  Finally, we use a simple exactly solvable learning scenario to show that there are limits of principle on the quality of approximations and bounds expressible solely in terms of the eigenvalue spectrum of the covariance function. 
Ageing and Rheology in Soft Materials| Abstract We study theoretically the role of ageing in the rheology of soft materials.  We define several generalized rheological response functions suited to ageing samples (in which time translation invariance is lost).  These are then used to study ageing effects within a simple scalar model (the "soft glassy rheology" or SGR model) whose constitutive equations relate shear stress to shear strain among a set of elastic elements, with distributed yield thresholds, undergoing activated dynamics governed by a "noise temperature", x.  (Between yields, each element follows affinely the applied shear. ) For 1 ! x ! 2 there is a power-law fluid regime in which transients occur, but no ageing.  For x ! 1, the model has a macroscopic yield stress.  So long as this yield stress is not exceeded, ageing occurs, with a sample's apparent relaxation time being of order its own age.  The (age-dependent) linear viscoelastic loss modulus G 00 (!; t) rises as frequency is lowered, but falls with age t, so as to always remain less than G 0 (!; t) (which is nearly constant).  Significant ageing is also predicted for the stress overshoot in nonlinear shear startup and for the creep compliance.  Though obviously oversimplified, the SGR model may provide a valuable paradigm for the experimental and theoretical study of rheological ageing phenomena in soft solids. 
On-Line Learning with Restricted Training Sets: An Exactly Solvable Case| Abstract We solve the dynamics of on-line Hebbian learning in large perceptrons exactly, for the regime where the size of the training set scales linearly with the number of inputs.  We consider both noiseless and noisy teachers.  Our calculation cannot be extended to non-Hebbian rules, but the solution provides a convenient and welcome benchmark with which to test more general and advanced theories for solving the dynamics of learning with restricted training sets. 
Finite size effects and optimal test set size in linear perceptrons|
Bayesian Methods for Support Vector Machines: Evidence and Predictive Class Probabilities|
Rheology of soft glassy materials|
Exact constitutive equation for soft glassy rheology|
Bayesian methods for support vecotr machines: Evidence and predictive class probabilities|
Statistical Mechanics of Ensemble Learning|
Query construction, entropy and generalization in neural network models|
Probabilistic Methods for Support Vector Machines|
Neural Information Processing Systems 11,|
Neural Information Processing Systems 14,|
Gaussian Fields for Approximate Inference in Layered Sigmoid Belief Networks|
], Test Error fluctuations in finite linear perceptrons,|
Learning from minimum entropy queries in a large committee machine|
In NIPS 14,|
Rheological constitutive equation for a model of soft glassy materials|
Aging and rheology in soft materials|
Bayesian methods for support vector machines: Evidence and error bars,|
Observable dependence of Fluctuation dissipation relation and effective temperature,|
Learning with ensembles: how over-fitting can be useful| In: Touretzky D, Mozer M, Hasselmo M editors. 
Learning unrealizable tasks from minimum entropy queries|
On-line learning from finite training sets|
Online Learning from Finite Training Sets: An Analytical Case Study|
Learning Curves for Gaussian Processes|
Approximate learning curves for Gaussian processes|

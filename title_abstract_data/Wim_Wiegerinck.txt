Lab-Test Selection in Diagnosis of Anaemia| Abstract.  Two problems are discussed.  The first one is how to deal with incomplete data, and what to do if the data is insufficient to make a reliable diagnosis.  The second problem is how to combine training data and expert knowledge if either of them is insufficient for good modelling.  We show that both problems can be dealt with when probabilistic models are used.  The proposed methods are applied in an advisory sytem for anaemia. 
Probability Assessment with Maximum Entropy in Bayesian Networks| Abstract Bayesian networks are widely accepted as tools for probabilistic modeling in the medical domain.  In modeling Bayesian networks in collaboration with domain experts, the definition of the network structure is relatively easy.  The assessment of the conditional probability tables (CPT) is often a much more dicult task, even though there is a lot of statistical information available in the medical literature.  The problem is twofold.  In the first place it is usually not possible to use this information directly to fill in the CPTs.  In the second place, the information is usually insucient for a unique definition of the CPTs.  A standard approach to define a probabilistic model on the basis of insucient statistical information is to apply the Maximum Entropy Method (MaxEnt).  MaxEnt searches for the unique model that maximizes the entropy under the constraints that it satisfies the given statistical information.  In standard applications of MaxEnt for models defined by one joint probability table, these constraints are linear in the table entries.  However, if MaxEnt is applied to a Bayesian network, i. e.  the joint distribution is factorized into a product of CPTs, these constraints are typically nonlinear in the CPTs.  In this paper we show how these nonlinear constraints can be dealt with, and we describe an algorithm that (locally) maximizes entropy under constraints in Bayesian networks.  The method is illustrated by an example. 
Quintessential Quiz Questions| Mastermind, Weekend Millionairs, Een tegen Honderd: they are successful quizzes that captivate television audiences in many countries.  The idea is well known: contestants are to answer a number of short questions that have short, straightforward answers.  The contestant who answers the most questions correctly wins a large cash prize -- or eternal fame.  Answering quiz-type questions -- also called factoid questions -- is the current goal of the AI branch called question answering.  Previous questions-answering research relied on explicit domain knowledge, which resulted in systems that only worked for specialized domains.  With the advent of large document repositories, and especially the Web, a new approach has become possible: exploiting the redundancy that is present in all the documents, using shallow lexical techniques such as stemming and pattern matching.  The result is open-domain question answering.  Quite a few groups are tackling question answering for the English language, as the 34 participants of the question-answering track at the Text Retrieval conference 2003 showed.  For other languages, including Dutch, there is less research activity.  Last summer there was only one participant of the pilot question-answering track at CLEF, the European information-retrieval conference.  This participant was the Quartz system, built by ILLC at the Universiteit van Amsterdam.  Quartz comes in two flavours: Quartz-d for Dutch and Quartz-e for English question answering.  Below some typical questions and answers are shown for Quartz-e.  Quartz provides a list of possible answers, ranked by confidence or no answer if the confidence is too low. 
Approximations of Bayesian Networks through KL Minimisation| Abstract Exact inference in large, complex Bayesian networks is computationally intractable.  Approximate schemes are therefore of great importance for real world computation.  In this paper we consider an approximation scheme in which the original Bayesian network is approximated by another Bayesian network.  The approximating network is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two networks.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  An important question in this scheme is how to choose the structure of the approximating network.  In this paper we show how redundant structures of the approximating model can be pruned in advance.  Simulation results of model selection and model optimisation are provided to illustrate the methods. 
A bridge between mean field theory and exact inference in probabilistic graphical models| Abstract Exact inference in large and complex probabilistic graphical models (e. g.  Bayesian networks, Boltzmann machines) is computationally intractable.  Approximate inference methods are therefore of great importance.  In this paper we provide a general scheme in which the original intractable graphical model is approximated by a model with a tractable structure.  The approximating model is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two models.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  The scheme provides a bridge between mean-field theory and exact computation.  Simulation results are provided to illustrate the method. 
Modeling Bayesian Networks by Learning from Experts| Abstract Bayesian network modeling by domain experts is still mainly a process
Approximations with Reweighted Generalized Belief Propagation| Abstract In (Wainwright et al. , 2002) a new general class of upper bounds on the log partition function of arbitrary undirected graphical models has been developed.  This bound is constructed by taking convex combinations of tractable distributions.  The experimental results published so far concentrates on combinations of tree-structured distributions leading to a convexi#ed Bethe free energy, which is minimized by the tree-reweighted belief propagation algorithm.  One of the favorable properties of this class of approximations is that increasing the complexity of the approximation is guaranteed to increase the precision.  The lack of this guarantee is notorious in standard generalized belief propagation.  We increase the complexity of the approximating distributions by taking combinations of junction trees, leading to a convexi#ed Kikuchi free energy, which is minimized by reweighted generalized belief propagation.  Experimental results for Ising grids as well as for fully connected Ising models are presented illustrating advantages and disadvantages of the reweighting method in approximate inference. 
EM Algorithms for Self-Organizing Maps| Abstract Self-organizing maps are popular algorithms for unsupervised learning and data visualization.  Exploiting the link between vector quantization and mixture modeling, we derive EM algorithms for selforganizing maps with and without missing values.  We compare self-organizing maps with the elastic-net approach and explain why the former is better suited for the visualization of high-dimensional data.  Several extensions and improvements are discussed. 
Generative Vector Quantisation| Abstract -- Based on the assumption that a pattern is constructed out of features which are either fully present or absent, we propose a vector quantisation method which constructs patterns using binary combinations of features.  For this model there exists an efficient EM-like learning algorithm which learns a set of representative codebook vectors.  In terms of a generative model, the collection of allowed binary states `generates' the set of codebook vectors.  The method, therefore, provides not only a compact description of the data in terms of clusters, but also an explanation of the individual clusters in terms of common elementary features.  Preliminary results on image compression and handwritten digit analysis indicate that our approach is an interesting and computationally inexpensive alternative to more complex probabilistic generative graphical models. 
Fractional Belief Propagation| Abstract We consider approximate inference in probabilistic graphical models with approximate free energy methods.  By considering equivalent factor-graph representations of a probabilistic model, we write down a family of different approximate tree-like free energies.  We show that this family interpolates between the naive mean-field free energy and the Bethe free energy.  We derive fixed-point equations that lead to fractional belief propagation algorithms, which include standard mean-field equations and loopy belief propagation as special cases.  Using a cavity-#eld argument, we compute the fractional algorithm that gives, in lowest order, a correction around the Bethe (loopy belief) approximation for the means.  Simulation results illustrate the potential merits of the approach. 
Variational methods for approximate reasoning in graphical models| Abstract Exact inference in large and complex graphical models (e. g.  Bayesian networks) is computationally intractable.  Approximate schemes are therefore of great importance for real world computation.  In this paper we consider a general scheme in which the original intractable graphical model is approximated by a model with a tractable structure.  The approximating model is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two models.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  The scheme provides a bridge between naive mean-field theory and exact computation.  Simulation results are provided to illustrate the method. 
Second Order Approximations for Probability Models| Abstract In this paper, we derive a second order mean field theory for directed graphical probability models.  By using an information theoretic argument it is shown how this can be done in the absense of a partition function.  This method is the direct generalisation of the well-known TAP approximation for Boltzmann Machines.  In a numerical example, it is shown that the method greatly improves the first order mean field approximation.  The computational complexity of the first (second) order method is linear (quadratic) in the network size and is exponential in the potential size.  For a restricted class of graphical models, so-called single overlap graphs, the second order method has comparable complexity to the first order method. 
Making Decisions with Probability Models| Abstract This paper considers the problem of classification based on a minimum number of measurements.  We compare two different approaches.  The first approach is to determine the optimal measurements as it is done in decision tree algorithms.  The second is to estimate a probability distribution from a database and to determine the optimal measurements from this distribution.  Test results indicate that the method using probability models results in better classifying trees. 
The actual modeling effort is done by specialists in internal medicine| Their knowledge is crucial to obtain the correct graphical structure and probabilities. 
Stochastic Dynamics of Learning with Momentum in Neural Networks| Abstract We study on-line learning with momentum term for nonlinear learning rules.  Through introduction of auxiliary variables, we show that the learning process can be described by a Markov process.  For small learning parameters j and momentum parameters ff close to 1, such that fl = j=(1\Gamma ff) 2 is finite, the time scales for the evolution of the weights and the auxiliary variables are the same.  In this case Van Kampen's expansion can be applied in a straightforward manner.  We obtain evolution equations for the average network state and the fluctuations around this average.  These evolution equations depend (after rescaling of time and fluctuations) only on fl: all combinations (j; ff) with the same value of fl give rise to similar behaviour.  The case ff constant and j small requires a completely different analysis.  There are two different time scales: a fast time scale on which the auxiliary variables equilibrate and a slow time scale for the change of the weights.  By projection on the space of slow variables the fast variables can be eliminated.  We find that for small learning parameters j and finite momentum parameters ff learning with momentum is equivalent to learning without momentum term with rescaled learning parameter ~ j = j=(1\Gamma ff).  Simulations with the nonlinear Oja learning rule confirm the theoretical results. 
A theoretical comparison of batch-mode, on-line, cyclic, and almost cyclic learning| Abstract We study and compare different neural network learning strategies: batch-mode learning, on-line learning, cyclic learning, and almost cyclic learning.  Incremental learning strategies require less storage capacity than batch-mode learning.  However, due to the arbitrariness in the presentation order of the training patterns, incremental learning is a stochastic process, whereas batch-mode learning is deterministic.  In zeroth order, i. e. , as the learning parameter j tends to zero, all learning strategies approximate the same ordinary differential equation, for convenience referred to as the "ideal behavior".  Using stochastic methods valid for small learning parameters j, we derive differential equation describing the evolution of the lowest order deviations from this ideal behavior.  We compute how the asymptotic misadjustment, measuring the average asymptotic distance from a stable fixed point of the ideal behavior, scales as a function of the learning parameter and the number of training patterns.  Knowing the asymptotic misadjustment, we calculate the typical number of learning steps necessary to generate a weight within order ffl of this fixed point, both with fixed and time-dependent learning parameters.  We conclude that almost cyclic learning (learning with random cycles) is a better alternative for batch-mode learning than cyclic learning (learning with a fixed cycle). 
Approximate Explanation of Reasoning in Bayesian Networks| Abstract We address the question of explaining the computation of the posterior probability of a node in a Bayesian network.  In polytrees, this probability can be explained by decomposing it into a product of causal terms (from the ancestors) and diagnostic terms (from the descendants).  In models with loops, this decomposition is in general not valid.  To proceed, we propose a scheme to approximate the posterior of a loopy model locally by a polytree.  In this approximating model, the node probability can again be decomposed into causal and diagnostic terms.  This decomposition can then be used as a rough explanation of reasoning for a user.  The method is illustrated by numerical examples. 
Variational Approximations in a Broad and Detailed Probabilistic Model for Medical Diagnosis| Abstract Exact inference in large, detailed probabilistic models for medical diagnosis is typically computationally infeasible, and approximate schemes are therefore of great importance.  In this paper, we consider variational methods, which provide bounds on the probabilities of interest.  We sketch some characteristics of a typical broad and detailed probabilistic model (BDPM) for medical diagnosis and describe how recently developed variational techniques can be applied for approximate inference in such a model.  Currently we are developing a BDPM to study the practical feasibility and the usefulness of a system based on such a model in medical practice. 
Approximate Expectation Maximization| Abstract We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference.  Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm.  This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge.  Simulations illustrate the merits of such an approach. 
On-line Learning with Time-Correlated Examples| Abstract We study the dynamics of on-line learning with time-correlated patterns.  In this, we make a distinction between "small" networks and "large" networks.  "Small" networks have a finite number of input units and are usually studied using tools from stochastic approximation theory in the limit of small learning parameters.  "Large" networks have an extensive number of input units.  A description in terms of individual weights is no longer useful and tools from statistical mechanics can be applied to compute the evolution of macroscopic order parameters.  We give general derivations for both cases, but in the end focus on the effect of correlations on plateaus.  Plateaus are long time spans in which the performance of the networks hardly changes.  Learning in both "small" and "large" multi-layered perceptrons is often hampered by the presence of plateaus.  The effect of correlations, however, appears to be quite different: they can have a huge beneficial effect in small networks, but seem to have only marginal effects in large networks. 
Variational Approximations between Mean Field Theory and the Junction Tree Algorithm| Abstract Recently, variational approximations such as the mean field approximation have received much interest.  We extend the standard mean field method by using an approximating distribution that factorises into cluster potentials.  This includes undirected graphs, directed acyclic graphs and junction trees.  We derive generalised mean field equations to optimise the cluster potentials.  We show that the method bridges the gap between the standard mean field approximation and the exact junction tree algorithm.  In addition, we address the problem of how to choose the structure and the free parameters of the approximating distribution.  From the generalised mean field equations we derive rules to simplify the approximation in advance without affecting the potential accuracy of the model class.  We also show how the method fits into some other variational approximations that are currently popular. 
IPF for Discrete Chain Factor Graphs| Abstract Iterative Proportional Fitting (IPF), combined with EM, is commonly used as an algorithm for likelihood maximization in undirected graphical models.  In this paper, we present two iterative algorithms that generalize upon IPF.  The first one is for likelihood maximization in discrete chain factor graphs, which we define as a wide class of discrete variable models including undirected graphical models and Bayesian networks, but also chain graphs and sigmoid belief networks.  The second one is for conditional likelihood maximization in standard undirected models and Bayesian networks.  In both algorithms, the iteration steps are expressed in closed form.  Numerical simulations show that the algorithms are competitive with state of the art methods. 
Improving classi cation with latent variable models by sequential constraint optimization| Abstract In this paper we propose a method to use multiple generative models with latent variables for classi cation tasks. 
The Connections of Large Perceptrons| Abstract We derive analytical expressions for the connections of large perceptrons, by studying the fixed points of the perceptron learning rule.  If the training set consists of all possible input vectors, we can calculate (for large systems) the connections as a series expansion in the system size.  The leading term in this expansion turns out to be either the Hebb rule (for unbiased distributions) or the biased Hebb rule (for biased distributions).  The performance of our asymptotic expressions (and finite size corrections) on small systems is studied numerically.  For the more realistic case of having an extensive training set (patterns learned with training noise) we derive a self-consistent set of coupled non-linear equations for the connections.  In the limit of zero training noise, the solution of these equations is shown to give the connections with maximal stability in the Gardner sense. 
Classification with Multiple Latent Variable Models using Maximum Entropy Discrimination| Abstract In this paper we propose a method to use multiple generative models for classification tasks.  The standard approach to use generative models for classification is to train a separate model for each class.  A novel data point is then classified by the model that gives it the highest probability.  The algorithm we propose alters the parameters of the models to improve the classification accuracy.  The algorithm is based on the maximum entropy discrimination method, recently introduced by T.  Jaakkola.  Our approach is made computationally tractable by assuming that each of the models is deterministic, by which we mean that a data-point is associated to only a single latent or hidden state.  The resulting algorithm is an interesting variant of the support vector machine learning algorithm.  We show and compare test results on a handwritten digit recognition problem. 
Variational Belief Networks for Approximate Inference| Abstract Exact inference in large, densely connected probabilistic networks is computationally intractable, and approximate schemes are therefore of great importance.  One approach is to use mean field theory, in which the exact log-likelihood is bounded from below using a simpler approximating distribution.  In the standard mean field theory, the approximating distribution is factorial.  We propose instead to use a (tractable) belief network as an approximating distribution.  The resulting compact framework is analogous to standard mean field theory and no additional bounds are required, in contrast to other recently proposed extensions.  We derive mean field equations which provide an efficient iterative algorithm to optimize the parameters of the approximating belief network.  Simulation results indicate a considerable improvement on previously reported methods. 
Central extensions and physics,|
Tractable Variational Structures for Approximating Graphical Models|
A Hybrid Network for Medical Diagnosis|
Inference and advisory system for medical diagnosis|
Mean field theory based on belief networks for approximate inference|
Mean field theory for graphical models|
Tractable undirected approximations for graphical models|
Deterministic Generative Models for Fast Feature Discovery|
Central Extensions in Physics,|
Tractable undirected approximations for graphical models| In International Conference Artificial Neural Networks '98. 
Stimulation initiative for european neural applications (SIENA),|

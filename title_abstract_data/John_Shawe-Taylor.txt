A Recurrent Network with Stochastic Weights| ABSTRACT Stochastic neural networks for global optimization are usually built by introducing random fluctuations into the network.  A natural method is to use stochastic weights rather than stochastic activation functions.  We propose a new model in which each neuron has very simple functionality but all the weights are stochastic.  It is shown that the stationary distribution of the network uniquely exists and it is approximately a Boltzmann-Gibbs distribution when the size of the network is not too small.  A new technique to implement simulated annealing is proposed.  Simulation results on the graph bisection problem show that the power of the network is comparable with that of a Boltzmann machine. 
Canonical correlation analysis; An overview with application to learning methods| Abstract We present a general method using kernel Canonical Correlation Analysis to learn a semantic representation to web images and their associated text.  The semantic space provides a common representation and enables a comparison between the text and images.  In the experiments we look at two approaches of retrieving images based only on their content from a text query.  We compare the approaches against a standard cross-representation retrieval technique known as the Generalised Vector Space Model. 
On the Eigenspectrum of the Gram Matrix and Its Relationship to the Operator Eigenspectrum| Abstract.  In this paper we analyze the relationships between the eigenvalues of the m # m Gram matrix K for a kernel k(#; #) corresponding to a sample x1 ; : : : ; xm drawn from a density p(x) and the eigenvalues of the corresponding continuous eigenproblem.  We bound the differences between the two spectra and provide a performance bound on kernel PCA. 
The Complexity of Learning Minor Closed Graph Classes| Abstract The paper considers the problem of learning classes of graphs closed under taking minors.  It is shown that any such class can be properly learned in polynomial time using membership and equivalence queries.  The representation of the class is in terms of a set of minimal excluded minors (obstruction set).  Moreover, a negative result for learning such classes using only equivalence queries is also provided, after introducing a notion of reducibility among query learning problems. 
Generalisation Error Bounds for Sparse Linear Classifiers| Abstract We provide small sample size bounds on the generalisation error of linear classifiers that are sparse in their dual representation given by the expansion coefficients of the weight vector in terms of the training data.  These results theoretically justify algorithms like the Support Vector Machine, the Relevance Vector Machine and K-nearest-neighbour.  The bounds are a-posteriori bounds to be evaluated after learning when the attained level of sparsity is known.  In a PAC-Bayesian style prior knowledge about the expected sparsity is incorporated into the bounds.  The proofs avoid the use of double sample arguments by taking into account the sparsity that leaves unused training points for the evaluation of classifiers.  We furthermore give a PAC-Bayesian bound on the average generalisation error over subsets of parameter space that may pave the way combining sparsity in the expansion coefficients and margin in a single bound.  Finally, reinterpreting a mistake bound for the classical perceptron algorithm due to Novikoff we demonstrate that our new results put classifiers found by this algorithm on a firm theoretical basis. 
A unifying framework for invariant pattern recognition| Abstract We introduce a group-theoretic model of invariant pattern recognition, the Group Representation Network.  We show that many standard invariance techniques can be viewed as GRNs, including the DFT power spectrum, higher order neural network and fast translation-invariant transform. 
KCCA for fMRI Analysis| Abstract.  We use Kernel Canonical Correlation Analysis (KCCA) to infer brain activity in functional MRI by learning a semantic representation of fMRI brain scans and their associated activity signal.  The semantic space provides a common representation and enables a comparison between the fMRI and the activity signal.  We compare the approach against Canonical Correlation Analysis (CCA) by localising "activity" on a simulated null data set.  Finally we present an approach to reconstruct an activity signal from a testing-set fMRI scans (both simulated and real), a method which allows us to validate our initial analysis. 
Confidence Estimates of Classification Accuracy on New Examples| Abstract Following recent results [5] showing the importance of the fat shattering dimension in explaining the beneficial effect of a large margin on generalization performance, the current paper investigates how the margin on a test example can be used to give greater certainty of correct classification in the distribution independent model.  The results show that even if the classifier does not classify all of the training examples correctly, the fact that a new example has a larger margin than that on the misclassified examples, can be used to give very good estimates for the generalization performance in terms of the fat shattering dimension measured at a scale proportional to the excess margin.  The estimate relies on a sufficiently large number of the correctly classified training examples having a margin roughly equal to that used to estimate generalization, indicating that the corresponding output values need to be `well sampled'. 
Linear Programming Boosting for Uneven Datasets| Abstract The paper extends the notion of linear programming boosting to handle uneven datasets.  Extensive experiments with text classification problem compare the performance of a number of different boosting strategies, concentrating on the problems posed by uneven datasets. 
Bayesian Classifiers Are Large Margin Hyperplanes in a Hilbert Space| simply output one hypothesis, but rather an entire distribution of probability over an hypothesis set: the Bayes posterior.  An alternative perspective is that they output a linear combination of classifiers, whose coefficients are given by Bayes theorem.  One of the concepts used to deal with thresholded convex combinations is the `margin' of the hyperplane with respect to the training sample, which is correlated to the predictive power of the hypothesis itself.  We provide a novel theoretical analysis of such classifiers, based on DataDependent VC theory, proving that they can be expected to be large margin hyperplanes in a Hilbert space.  We then present experimental evidence that the predictions of our model are correct, i. e.  that Bayesian classifiers really find hypotheses which have large margin on the training examples.  This not only explains the remarkable resistance to overfitting exhibited by such classifiers, but also co-locates them in the same class of other systems, like Support Vector machines and Adaboost, which have a similar performance. 
Sample Sizes for Sigmoidal Neural Networks| Abstract This paper applies the theory of Probably Approximately Correct (PAC) learning to feedforward neural networks with sigmoidal activation functions.  Despite the best known upper bound on the VC dimension of such networks being O((WN ) 2 ), for W parameters and N computational nodes, it is shown that the asymptotic bound on the sample size required for learning with increasing accuracy 1\Gamma ffl and decreasing probability of failure ffi is O((1=ffl)(W log(1=ffl) + (WN ) 2 + log(1=ffi)).  For practical values of ffl and ffi the formula obtained for the sample sizes is a factor 2 log(2e=ffl) smaller than a naive use of the VC dimension result would give.  Similar results are obtained for learning where the hypothesis is only guaranteed to correctly classify a given proportion of the training sample.  The results are formulated in general terms and show that for many learning classes defined by smooth functions thresholded at the output, the sample size for a class with VC-dimension d and ` parameters is O((1=ffl)(` log(1=ffl) + o(log(1=ffl))d + log(1=ffi)). 
Latent Semantic Kernels| Abstract.  Kernel methods like Support Vector Machines have successfully been used for text categorization.  A standard choice of kernel function has been the inner product between the vector-space representation of two documents, in analogy with classical information retrieval (IR) approaches.  Latent Semantic Indexing (LSI) has been successfully used for IR purposes as a technique for capturingsemantic relations between terms and inserting them into the similarity measure between two documents.  One of its main drawbacks, in IR, is its computational cost.  In this paper we describe how the LSI approach can be implemented in a kernel-defined feature space.  We provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it. 
Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis| Abstract The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation.  This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting.  By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space.  The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis.  A set of directions is found in the first and in the second space that are maximally correlated.  Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reflect some semantic similarity.  Certain patterns of English words that relate to a specific meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus.  Using the semantic representation obtained in this way we first demonstrate that the correlations detected between the two versions of the corpus are significantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reflect semantic information.  Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and significantly superior to LSI on the same data. 
A Column Generation Algorithm For Boosting| Abstract We examine linear program (LP) approaches to boosting and demonstrate their efficient solution using LPBoost, a column generation simplex method.  We prove that minimizing the soft margin error function (equivalent to solving an LP) directly optimizes a generalization error bound.  LPBoost can be used to solve any boosting LP by iteratively optimizing the dual classification costs in a restricted LP and dynamically generating weak learners to make new LP columns.  Unlike gradient boosting algorithms, LPBoost converges finitely to a global solution using well defined stopping criteria.  Computationally, LPBoost finds very sparse solutions as good as or better than those found by ADABoost using comparable computation. 
A Result of Vapnik with Applications| Abstract A new proof of a result due to Vapnik is given.  Its implications for the theory of PAC learnability are discussed, with particular reference to the learnability of functions taking values in a countable set.  An application to the theory of artificial neural networks is then given. 
Margin Distribution Bounds on Generalization| Abstract A number of results have bounded generalization of a classifier in terms of its margin on the training points.  There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization.  Freund and Schapire [6] have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound.  We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size. 
Spectral Kernel Methods for Clustering| Abstract In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix.  All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices.  We use two different but related cost functions, the Alignment and the `cut cost'.  The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts.  Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels.  We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions.  We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information.  The resulting simple algorithms are tested on real world data with positive results. 
On Maximum Margin Hierarchical Multilabel Classification| Abstract We present work in progress towards maximum margin hierarchical classification where the objects are allowed to belong to more than one category at a time.  The classification hierarchy is represented as a Markov network equipped with an exponential family defined on the edges.  We present a variation of the maximum margin multilabel learning framework, suited to the hierarchical classification task and allows efficient implementation via gradient-based methods.  We compare the behaviour of the proposed method to the recently introduced hierarchical regularized least squares classifier as well as two SVM variants in Reuter's news article classification.  Often in hierarchical classification, the object to be classified is assumed to belong to exactly one (leaf) node in the hierarchy (c. f.  [5, 2, 4]).  Following [3], in this paper we consider the more general case where a single object can be classified into several categories in the hierarchy, to be specific, the multilabel is a union of partial paths in the hierarchy.  For example, a news article about David and Victoria Beckham could belong to partial paths sport, football and entertainment, music but might not belong to any leaf categories such as champions league or jazz.  In our setting the training data ((x i , y(x i ))) m i=1 consists of pairs (x, y) of vector x 2 R n and a multilabel y 2 {+1,- 1} k consisting of k microlabels.  As the model class we use the exponential family P (y|x) / Y e2E exp w T e # # # e (x, y e ) # = exp w T # # #(x, y) # (1) defined on the edges of a Markov network G = (V, E), where node j 2 V corresponds to the j'th component of the multilabel and the edges e = (j, j 0 ) 2 E correspond # Corresponding author.  to the classification hierarchy given as input.  By y e = (y j , y 0 j ) we denote the restriction of the multilabel y = (y 1 , .  .  .  , y k ) to the edge e = (j, j 0 ).  The edgefeature vector # # # e , in turn, is a concatenation of 'class-sensitive' feature vectors # # # u e (x, y e ) = Jy e = uK# # #(x), where JK denotes an indicator function (c. f.  [1]), and w e is the weight vector for edge e.  The vector # # #(x) could be a bag of words---as in the experiments reported here---or any other feature representation of the document x.  Also, note that although the same feature vector # # #(x) is duplicated for each edge and edge-labeling, in the weight vector w = (w ue e ) e2E,ue we still have a separate weights to represent importance differences of a given feature in different contexts.  There are many ways to define loss functions for hierarchical classification setting (c. f [5, 2, 4, 3]).  Zero-one loss ` 0/1 (y, u) = Jy 6= uK is not very well suited to the task as it ignores the severity of the discrepancy between y and u.  Symmetric difference loss ` # (y, u) = P j Jy i 6= u i K does not suffer from this deficiency.  However, it fails to take the dependency structure of the microlabels into account.  A more appealling choice is the hierarchical loss function of [3].  It penalizes the first mistake along a path, ` PATH (y, u) = P j c j Jy j 6= u j & y k = u k 8k 2 anc(j)K, where the coefficients c root = 1, c j = c pa(j) /|sibl(j)| down-weight mistakes made deeper in the hierarchy.  Here we denoted by anc(j) an ancestor, by pa(j) the immediate parent, and by sibl(j) the set of siblings of node j.  In this paper, we consider a simplified version of ` PATH , namely ` EDGE (y, u) = X j c j Jy j 6= u j & y pa(j) = u pa(j) K, that penalizes a mistake in child if the label of the parent was correct.  This choice lets the loss function to capture some of the hierarchical dependencies (between the parent and the child) but allows us define the loss in terms of edges, which is crucial for the efficiency of our learning algorithm.  This is achieved by dividing the microlabel loss of each node among the edges adjacent to it.  As in [7, 8], our goal is to learn a weight vector w that maximizes the minimum margin on training data the between the correct multilabel y(x i ) and the incorrect multilabels y 6= y(x i ).  Also, we would like the margin to scale as a function of the loss.  Alloting a single slack variable for each training example results in the following soft-margin optimization problem: min w 1 2 ||w|| 2 + C m X i=1 # i s. t.  w T fiff # #(x i , y) # ` ` `(y i , y) - # i , 8i,
Valid Generalisation from Approximate Interpolation| Abstract Let H and C be sets of functions from domain X to !.  We say that H validly generalises C from approximate interpolation if and only if for each j ? 0 and ffl; ffi 2 (0; 1) there is m 0 (j; ffl; ffi ) such that for any function t 2 C and any probability distribution P on X, if m m 0 then with P m probability at least 1 \Gamma ffi , a sample x = (x 1 ; x 2 ; : : : ; xm ) 2 X m satisfies 8h 2 H; jh(x i )\Gamma t(x i )j ! j; (1 i m) =) P(fx : jh(x)\Gamma t(x)j jg) ! ffl: We find conditions that are necessary and sufficient for H to validly generalise C from approximate interpolation, and we obtain bounds on the sample length m 0 (j; ffl; ffi ) in terms of various parameters describing the expressive power of H. 
Using String Kernels to Identify Famous Performers from Their Playing Style| Abstract.  In this paper we show a novel application of string
The Decision List Machine| Abstract We introduce a new learning algorithm for decision lists to allow features that are constructed from the data and to allow a tradeobetween accuracy and complexity.  We bound its generalization error in terms of the number of errors and the size of the classifier it finds on the training data.  We also compare its performance on some natural data sets with the set covering machine and the support vector machine. 
Text Classification using String Kernels| Abstract We propose a novel approach for categorizing text documents based on the use of a special kernel.  The kernel is an inner product in the feature space generated by all subsequences of length k.  A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously.  The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous.  A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k.  The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique.  Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel Joachims (1998) show positive results on modestly sized datasets.  The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors.  For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets. 
Using the Perceptron Algorithm to Find Consistent Hypotheses| Abstract The perceptron learning algorithm yields quite naturally an algorithm for finding a linearly separable boolean function consistent with a sample of such a function.  Using the idea of a specifying sample, we give a simple proof that this algorithm is not efficient, in general.  A boolean function t defined on {0, 1} n is linearly separable if there are # 2 R n and # 2 R such that t(x) = ( 1 if h#, xi # # 0 if h#, xi < #, where h#, xi is the standard inner product of # and x.  Given such # and #, we say that t is represented by [#, #] and we write t [#, #].  The vector # is known as the weight-vector, and # is known as the threshold.  This class of functions is the set of functions computable by the simple boolean perceptron (see [8, 9, 6]), and we shall denote it by BP n .  We now give a fleeting description of the perceptron learning algorithm, and refer to [6, 1] for more details.  For any learning constant # } 0, we have the perceptron learning algorithm L # , devised by Rosenblatt [8, 9], which acts sequentially as follows.  Let t be any function in BP n , which may be thought of as the target.  The algorithm L # maintains at each stage a current hypothesis, which is updated on the basis of an example in {0, 1} n , presented together with its classification t(x).  (The initial hypothesis is some fixed `simple' hypothesis.  We shall take the initial hypothesis to have the all-0 vector as weight-vector, and threshold 0. ) Suppose the current hypothesis is h [#, #] and that an example x is presented.  Then the new hypothesis is h 0 [# 0 , # 0 ] where # 0 = # + # (t(x) - h(x)) x, # 0 = # - # (t(x) - h(x)) .  The Perceptron Convergence Theorem [8, 6] asserts that no matter how many examples are presented, the algorithm makes only a finite number of changes, or updates (provided #, which can be a function of n, is small enough).  As indicated in [3], given t 2 BP n and a sample x = (x 1 , x 2 , .  .  .  , xm ) of examples, we may use L # to find a linearly separable boolean function which agrees with t on x---that is, which is consistent with t on x.  We simply keep cycling through x 1 to xm in turn, until no updates are made in a complete cycle.  Thus, the perceptron algorithm (for any learning constant #) can be used as a consistent-hypothesis-finder (using terminology from [3]).  A natural question is whether this is an efficient means of finding a consistent function.  In fact, it is not, in the sense that the number of complete cycles required can be exponential in m, the size of the sample.  This result appears to be accepted, but we have been unable to find a proof of it in the literature.  We note that this is a very different result from those presented by Minsky and Papert[6] and Hampson and Volper [4] in their studies of the perceptron learning algorithm.  Their results show that when the perceptron learning algorithm is used as an exact learning algorithm, the running time can be exponential in n, the domain dimension.  Our result shows that, for fixed n, the running time of the related consistent-hypothesis-finder can be exponential in m, the number of examples presented.  We remark that there is a polynomial time consistent-hypothesis-finder for BP n : rephrase the problem as a linear programme and use Karmarkar's algorithm (see [3]).  Thus the problem of finding a consistent hypothesis has no intrinsic difficulty.  We shall consider the boolean function f 2n of 2n variables with formula f 2n = u 2n ^ (u 2n- 1 _ (u 2n- 2 ^ (u 2n- 3 _ (. 
Semi-Definite Programming by Perceptron Learning| Abstract We present a modified version of the perceptron learning algorithm (PLA) which solves semidefinite programs (SDPs) in polynomial time.  The algorithm is based on the following three observations: (i) Semidefinite programs are linear programs with infinitely many (linear) constraints; (ii) every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints; (iii) in general, the perceptron learning algorithm solves a constraint satisfaction problem with linear constraints in finitely many updates.  Combining the PLA with a probabilistic rescaling algorithm (which, on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time.  We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior point methods. 
Estimating the Moments of a Random Vector with Applications| Abstract -- A general result about the quality of approximation of the mean of a distribution by its empirical estimate is proven that does not involve the dimension of the feature space.  Using the kernel trick this gives also bounds the quality of approximation of higher order moments.  A number of applications are derived of interest in learning theory including a new novelty detection algorithm and rigorous bounds on the Robust Minimax Classification algorithm. 
Generalisation of A Class of Continuous Neural Networks| Abstract We propose a way of using boolean circuits to perform real valued computation in a way that naturally extends their boolean functionality.  The functionality of multiple fan in threshold gates in this model is shown
A New Incremental Learning Technique| Abstract We present a new type of constructive algorithm for incremental learning.  The algorithm overcomes many of the problems associated with standard back propagation such as speed and optimum network size.  We investigate the ability of the network to learn and test the resulting generalisation of the network. 
Enlarging the Margins in Perceptron Decision Trees| Abstract Capacity control in perceptron decision trees is typically performed by controlling their size.  We prove that other quantities can be as relevant to reduce their flexibility and combat overfitting.  In particular, we provide an upper bound on the generalization error which depends both on the size of the tree and on the margin of the decision nodes.  So enlarging the margin in perceptron decision trees will reduce the upper bound on generalization error.  Based on this analysis, we introduce three new algorithms, which can induce large margin perceptron decision trees.  To assess the effect of the large-margin bias, OC1 [18] of Murthy, Kasif and Salzberg, a well-known system for inducing perceptron decision tree, is used as the baseline algorithm.  An extensive experimental study on real world data showed that all three new algorithms perform better or at least not significantly worse than OC1 on almost every dataset with only one exception.  OC1 did worse than the best margin-based method on every dataset. 
Syllables and other String Kernel Extensions| Abstract Recently, the use of string kernels that compare documents as a string of letters has been shown to achieve good results on text classification problems.  In this paper we introduce the application of the string kernel in conjunction with syllables.  Using syllables shortens the representation of documents and as a result reduces computation time.  Moreover syllables provide a more natural representation of text; rather than the traditional coarse representation given by the bag-of-words, or the too fine one resulting from considering individual letters only.  We give some experimental results which show that syllables can be effectively used in textcategorisation problems.  In this paper we also propose two extensions to the string kernel.  The first introduces a new lambdaweighting scheme, where different symbols can be given differing decay weightings.  This may be useful in text and other applications where the insertion of certain symbols may be known to be less significant.  We also introduce the concept of `soft matching', where symbols can match (possibly weighted by relevance) even if they are not identical.  Again, this provides a method of incorporating prior knowledge where certain symbols can be regarded as a partial or exact match and contribute to the overall similarity measure for two data items. 
Robust Bounds on Generalization from the Margin Distribution| Abstract A number of results have bounded generalization of a classifier in terms of its margin on the training points.  There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization.  Freund and Schapire [8] have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound.  We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size.  Algorithms arising from the approach are related to those of Cortes and Vapnik [5].  We generalise the basic result to function classes with bounded fat-shattering dimension and the 1-norm of the slack variables which gives rise to Vapnik's box constraint algorithm.  We also extend the results to the regression case and obtain bounds on the probability that a randomly chosen test point will have error greater than a given value.  The bounds apply to the fflinsensitive loss function proposed by Vapnik for Support Vector Machine regression.  A special case of this bound gives a bound on the probabilities in terms of the least squares error on the training set showing a quadratic decline in probability with margin. 
Latent Semantic Kernels for Feature Selection| Abstract Latent Semantic Indexing is a method for selecting informative subspaces of feature spaces.  It was developed for information retrieval to reveal semantic information from document co-occurrences.  The paper demonstrates how this method can be implemented implicitly to a kernel defined feature space and hence adapted for application to any kernel based learning algorithm and data.  Experiments with text and UCI data show the technique can improve generalisation performance by focussing attention of a Support Vector Machine onto informative subspaces of the feature space. 
Sparsity vs| Large Margins for Linear Classifiers.  Abstract We provide small sample size bounds on the generalisation error of linear classifiers that take advantage of large observed margins on the training set and sparsity in the data dependent expansion coefficients.  It is already known from results in the luckiness framework that both criteria independently have a large impact on the generalisation error.  Our new results show that they can be combined which theoretically justifies learning algorithms like the Support Vector Machine [4] or the Relevance Vector Machine [12].  In contrast to previous studies we avoid using the classical technique of symmetrisation by a ghost sample but directly using the sparsity for the estimation of the generalisation error.  We demonstrate that our result leads to practical useful results even in case of small sample size if the training set witnesses our prior belief in sparsity and large margins. 
Sample Based Generalization Bounds| Abstract It is known that the covering numbers of a function class on a double sample (length 2m, where m is the number of points in the sample) can be used to bound the generalization performance of a classifier by using a margin based analysis.  Traditionally this has been done using a ``Sauer-like" relationship involving a combinatorial dimension such as the fat-shattering dimension.  In this paper we show that one can utilize an analogous argument in terms of the observed covering numbers on a single m-sample (being the actual observed data points).  The significance of this is that for certain interesting classes of functions, such as support vector machines, one can readily estimate the empirical covering numbers quite well.  We show how to do so in terms of the eigenvalues of the Gram matrix created from the data.  These covering numbers can be much less than a priori bounds indicate in situations where the particular data received is ``easy".  The work can be considered an extension of previous results which provided generalization performance bounds in terms of the VC-dimension of the class of hypotheses restricted to the sample, with the considerable advantage that the covering numbers can be readily computed, and they often are small. 
On the Generalisation of Soft Margin Algorithms| Abstract| Generalisation bounds depending on the margin of a classifier are a relatively recent development.  They provide an explanation of the performance of state-of-theart learning systems such as Support Vector Machines (SVM) [1] and Adaboost [2].  The diculty with these bounds has been either their lack of robustness or their looseness.  The question of whether the generalisation of a classifier can be more tightly bounded in terms of a robust measure of the distribution of margin values has remained open for some time.  The paper answers this open question in the armative and furthermore the analysis leads to bounds that motivate the previously heuristic soft margin SVM algorithms as well as justifying the use of the quadratic loss in neural network training algorithms.  The results are extended to give bounds for the probability of failing to achieve a target accuracy in regression prediction, with a statistical analysis of Ridge Regression and Gaussian Processes as a special case.  The analysis presented in the paper has also lead to new boosting algorithms described elsewhere [3]. 
String Kernels, Fisher Kernels and Finite State Automata| Abstract In this paper we show how the generation of documents can be thought of as a k-stage Markov process, which leads to a Fisher kernel from which the n-gram and string kernels can be re-constructed.  The Fisher kernel view gives a more flexible insight into the string kernel and suggests how it can be parametrised in a way that reflects the statistics of the training corpus.  Furthermore, the probabilistic modelling approach suggests extending the Markov process to consider sub-sequences of varying length, rather than the standard fixed-length approach used in the string kernel.  We give a procedure for determining which sub-sequences are informative features and hence generate a Finite State Machine model, which can again be used to obtain a Fisher kernel.  By adjusting the parametrisation we can also influence the weighting received by the features.  In this way we are able to obtain a logarithmic weighting in a Fisher kernel.  Finally, experiments are reported comparing the different kernels using the standard Bag of Words kernel as a baseline. 
A Generalized Kernel Approach to Dissimilarity-based| Abstract Usually, objects to be classified are represented by features.  In this paper, we discuss an alternative object representation based on dissimilarity values.  If such distances separate the classes well, the nearest neighbor method oers a good solution.  However, dissimilarities used in practice are usually far from ideal and the performance of the nearest neighbor rule suers from its sensitivity to noisy examples.  We show that other, more global classification techniques are preferable to the nearest neighbor rule, in such cases.  For classification purposes, two dierent ways of using generalized dissimilarity kernels are considered.  In the first one, distances are isometrically embedded in a pseudo-Euclidean space and the classification task is performed there.  In the second approach, classifiers are built directly on distance kernels.  Both approaches are described theoretically and then compared using experiments with dierent dissimilarity measures and datasets including degraded data simulating the problem of missing values. 
PAC-Bayes & Margins| Abstract We show two related things: (1) Given a classifier which consists of a weighted sum of features with a large margin, we can construct a stochastic classifier with negligibly larger training error rate.  The stochastic classifier has a future error rate bound that depends on the margin distribution and is independent of the size of the base hypothesis class.  (2) A new true error bound for classifiers with a margin which is simpler, functionally tighter, and more data-dependent than all previous bounds. 
Learning Semantic Similarity| Abstract The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms.  Attempts to incorporate some notion of term similarity include latent semantic indexing [8], the use of semantic networks [9], and probabilistic methods [5].  In this paper we propose two methods for inferring such similarity from a corpus.  The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure.  The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information.  Both approaches produce valid kernel functions parametrised by a real number.  The paper shows how the alignment measure can be used to successfully perform model selection over this parameter.  Combined with the use of support vector machines we obtain positive results. 
PUBLICATIONS DE L'INSTITUT MATH #| Abstract.  1-factorability of the composition of graphs is studied.  The followings sucient conditions are proved: G[H] is 1-factorable if G and H are regular and at least one of the following holds: (i) Graphs G and H both contain a 1-factor, (ii) G is 1-factorable (iii) H is 1-factorable.  It is also shown that the tensor product G # H is 1-factorable, if at least one of two graphs is 1-factorable.  This result in turn implies that the strong tensor product G# 0 H is 1-factorable, if G is 1-factorable. 
On Specifying Boolean Functions by Labelled Examples| Abstract We say a function t in a set H of f0; 1g-valued functions defined on a set X is specified by S ` X if the only function in H which agrees with t on S is t itself.  The specification number of t is the least cardinality of such an S.  For a general finite class of functions, we show that the specification number of any function in the class is at least equal to a parameter from [21] known as the testing dimension of the class.  We investigate in some detail the specification numbers of functions in the set of linearly separable Boolean functions of n variables---those functions f such that f\Gamma 1 (f0g) and f \Gamma 1 (f1g) can be separated by a hyperplane.  We present general methods for finding upper bounds on these specification numbers and we characterise those functions which have largest specification number.  We obtain a general lower bound on the specification number and we show that for all nested functions, this lower bound is attained.  We give a simple proof of the fact that for any linearly separable Boolean function, there is exactly one set of examples of minimal cardinality which specifies the function.  We discuss those functions which have limited dependence, in the sense that some of the variables are redundant (that is, there are irrelevant attributes), giving tight upper and lower bounds on the specification numbers of such functions.  We then bound the average, or expected, number of examples needed to specify a linearly separable Boolean function.  In the final section of the paper, we address the complexity of computing specification numbers and related parameters. 
The Perceptron Algorithm with Uneven Margins| Abstract The perceptron algorithm with margins is a simple, fast and effective learning algorithm for linear classifiers; it produces decision hyperplanes within some constant ratio of the maximal margin.  In this paper we study this algorithm and a new variant: the perceptron algorithm with uneven margins, tailored for document categorisation problems (i. e.  problems where classes are highly unbalanced and performance depends on the ranking of patterns).  We discuss the interest of these algorithms from a theoretical point of view, provide a generalisation of Noviko#'s theorem for uneven margins, give a geometrically description of these algorithms and show experimentally that both algorithms yield equal or better performances than support vector machines, while reducing training time and sparsity, in classification (USPS) and document categorisation (Reuters) problems. 
Composite Kernels for Hypertext Categorisation| Abstract Kernels are problem-specific functions that act as an interface between the learning system and the data.  While it is well-known when the combination of two kernels is again a valid kernel, it is an open question if the resulting kernel will perform well.  In particular, in which situations can a combination of kernel be expected to perform better than its components considered separately? We investigate this problem by looking at the task of designing kernels for hypertext classi#cation, where both words and links information can be exploited.  We provide sucient conditions that indicate when an improvement can be expected, highlighting and formalising the notion of \independent kernels".  Experimental results confirm the predictions of the theory in the hypertext domain. 
A Sufficient Condition for Polynomial Distribution-dependent Learnability| Abstract We investigate upper bounds on the sample-size sufficient for `solid' learnability with respect to a probability distribution.  We obtain a sufficient condition for feasible (polynomially bounded) sample-size bounds for distributionspecific (solid) learnability. 
A stochastic neural architecture that exploits dynamically reconfigurable FPGAs| Abstract In this paper we present an expandable digital architecture that provides an efficient real time implementation platform for large neural networks.  The architecture makes heavy use of the techniques of bit serial stochastic computing to carry out the large number of required parallel synaptic calculations.  In this design all real valued quantities are encoded on to stochastic bit streams in which the `1' density is proportional to the given quantity.  The actual digital circuitry is simple and highly regular thus allowing very efficient space usage of fine grained FPGAs.  Another feature of the design is that the large number of weights required by a neural network are generated by circuitry tailored to each of their specific values, thus saving valuable cells.  Whenever one of these values is required to change, the appropriate circuitry must be dynamically reconfigured.  This may always be achieved in a fixed and minimum number of cells for a given bit stream resolution. 
Large Margin Trees for Induction and Transduction| Abstract The problem of controlling the capacity of decision trees is considered for the case where the decision nodes implement linear threshold functions.  In addition to the standard early stopping and pruning procedures, we implement a strategy based on the margins of the decision boundaries at the nodes.  The approach is motivated by bounds on generalization error obtained in terms of the margins of the individual classifiers.  Experimental results are given which demonstrate that considerable advantage can be derived from using the margin information.  The same strategy is applied to the problem of transduction, where the positions of the testing points are revealed to the training algorithm.  This information is used to generate an alternative training criterion motivated by transductive theory.  In the transductive case, the results are not as encouraging, suggesting that little, if any, consistent advantage is culled from using the unlabelled data in the proposed fashion.  This conclusion does not contradict theoretical results, but leaves open the theoretical and practical question of whether more effective use can be made of the additional information. 
Estimating the Support of a High-Dimensional Distribution|
Covering Numbers for Support Vector Machines|
Further Results on the Margin Distribution|
Classification Accuracy Based on Observed Margin|
Large Margin DAGs for Multiclass Classification|
A Framework for Structural Risk Minimisation|
The Learnability of Formal Concepts|
SV estimation of a distribution's support|
Optimizing Classifers for Imbalanced Training Sets|
On Exact Specification by Examples|
Generalization Performance of Classifiers in Terms of Observed Covering Numbers|
Dynamically Adapting Kernels in Support Vector Machines|
On the Concentration of Spectral Properties|
On the generalization of soft margin algorithms|
Support Vector Method for Novelty Detection|
A PAC Analysis of a Bayesian Estimator|
The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum|
Composite kernels for hypertext categorization,|
An Approximate String-Matching Algorithm|
Sample Sizes for Threshold Networks with Equivalences|
Characterizing Graph Drawing with Eigenvectors|
Representation Theory and Invariant Neural Networks|
Margin Distribution Bounds on Generalization|
Bayesian Classifiass are Large Margin Hyperplanes in a Hilbert Space,|
Martin Anthony, Structural Risk Minimization over Data-Dependent Hierarchies,|
The Set Covering Machine|
Linear Programming Boosting via Column Generation|
Using kcca for japanese-english cross-language information retrieval and classification|
Learning Minor Closed Graph Classes with Membership and Equivalence Queries,|
An introduction to supDataset s = 1 (SVM) s = 2 s = p n/m s = max|
Support vector and kernel methods|
Boosting the Margin Distribution|
Fast Multiple Keyword Searching|
Learning with the Set Covering Machine|
A column generation approach to boosting|
Kernel Methods for Document Filtering|

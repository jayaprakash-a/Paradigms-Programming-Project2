Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data| Abstract In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when longrange dependencies exist.  We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices.  Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP).  On a natural-language chunking task, we show that a DCRF performs better than a series of linearchain CRFs, achieving comparable performance using only half the training data. 
Learning in Markov Random Fields with Contrastive Free Energies| Abstract Learning Markov random field (MRF) models is notoriously hard due to the presence of a global normalization factor.  In this paper we present a new framework for learning MRF models based on the contrastive free energy (CF) objective function.  In this scheme the parameters are updated in an attempt to match the average statistics of the data distribution and a distribution which is (partially or approximately) "relaxed" to the equilibrium distribution.  We show that maximum likelihood, mean field, contrastive divergence and pseudo-likelihood objectives can be understood in this paradigm.  Moreover, we propose and study a new learning algorithm: the "kstep Kikuchi/Bethe approximation".  This algorithm is then tested on a conditional random field model with "skip-chain" edges to model long range interactions in text data.  It is demonstrated that with no loss in accuracy, the training time is brought down on average from 19 hours (BP based learning) to 83 minutes, an order of magnitude improvement. 
Dynamic Conditional Random Fields for Jointly Labeling Multiple Sequences| Abstract Conditional random fields (CRFs) for sequence modeling have several advantages over joint models such as HMMs, including the ability to relax strong independence assumptions made in those models, and the ability to incorporate arbitrary overlapping features.  Previous work has focused on linear-chain CRFs, which correspond to finite-state machines, and have efficient exact inference algorithms.  Often, however, we wish to label sequence data in multiple interacting ways---for example, performing part-of-speech tagging and noun phrase segmentation simultaneously, increasing joint accuracy by sharing information between them.  We present dynamic conditional random fields (DCRFs), which are CRFs in which each time slice has a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks---and parameters are tied across slices.  (They could also be called conditionallytrained Dynamic Markov Networks. ) Since exact inference can be intractable in these models, we perform approximate inference using the tree-based reparameterization framework (TRP).  We also present empirical results comparing DCRFs with linear-chain CRFs on naturallanguage data. 
Joint Parsing and Semantic Role Labeling| Abstract A striking feature of human syntactic processing is that it is context-dependent, that is, it seems to take into account semantic information from the discourse context and world knowledge.  In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses.  To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.  Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates. 
Information Theory and Representation in Associative Word Learning| Abstract A significant portion of early language learning can be viewed as an associative learning problem.  We investigate the use of associative language learning based on the principle that words convey Shannon information about the environment.  We discuss the shortcomings in representation used by previous associative word learners and propose a functional representation that not only denotes environmental categories, but serves as the basis for activities and interaction with the environment.  We present experimental results with an autonomous agent acquiring language. 
Collective segmentation and labeling of distant entities in information extraction|

Limits of Spectral Clustering| Abstract An important aspect of clustering algorithms is whether the partitions constructed on finite samples converge to a useful clustering of the whole data space as the sample size increases.  This paper investigates this question for normalized and unnormalized versions of the popular spectral clustering algorithm.  Surprisingly, the convergence of unnormalized spectral clustering is more difficult to handle than the normalized case.  Even though recently some first results on the convergence of normalized spectral clustering have been obtained, for the unnormalized case we have to develop a completely new approach combining tools from numerical integration, spectral and perturbation theory, and probability.  It turns out that while in the normalized case, spectral clustering usually converges to a nice partition of the data space, in the unnormalized case the same only holds under strong additional assumptions which are not always satisfied.  We conclude that our analysis gives strong evidence for the superiority of normalized spectral clustering.  It also provides a basis for future exploration of other Laplacian-based methods. 
Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering| Abstract Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space.  The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering.  Several applications are considered.  In many areas of artificial intelligence, information retrieval and data mining, one is often confronted with intrinsically low dimensional data lying in a very high dimensional space.  For example, gray scale n \Theta n images of a fixed object taken with a moving camera yield data points in R n 2 .  However, the intrinsic dimensionality of the space of all images of the same object is the number of degrees of freedom of the camera -- in fact the space has the natural structure of a manifold embedded in R n 2 .  While there is a large body of work on dimensionality reduction in general, most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside.  Recently, there has been some interest (Tenenbaum et al, 2000; Roweis and Saul, 2000) in the problem of developing low dimensional representations of data in this particular context.  In this paper, we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction.  The core algorithm is very simple, has a few local computations and one sparse eigenvalue problem.  The solution reflects the intrinsic geometric structure of the manifold.  The justification comes from the role of the Laplacian operator in providing an optimal embedding.  The Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace-Beltrami operator defined on the manifold.  The embedding maps for the data come from approximations to a natural map that is defined on the entire manifold.  The framework of analysis presented here makes this connection explicit.  While this connection is known to geometers and specialists in spectral graph theory (for example, see [1, 2]) to the best of our knowledge we do not know of any application to data representation yet.  The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner.  The locality preserving character of the Laplacian Eigenmap algorithm makes it relatively insensitive to outliers and noise.  A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data.  Connections to spectral clustering algorithms developed in learning and computer vision (see Shi and Malik, 1997) become very clear.  Following the discussion of Roweis and Saul (2000), and Tenenbaum et al (2000), we note that the biological perceptual apparatus is confronted with high dimensional stimuli from which it must recover low dimensional structure.  One might argue that if the approach to recovering such low-dimensional structure is inherently local, then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception.  1 The Algorithm Given k points x 1 ; : : : ; x k in R l , we construct a weighted graph with k nodes, one for each point, and the set of edges connecting neighboring points to each other.  1.  Step 1.  [Constructing the Graph] We put an edge between nodes i and j if x i and x j are "close".  There are two variations: (a) ffl-neighborhoods.  [parameter ffl 2 R] Nodes i and j are connected by an edge if jjx i \Gamma x j jj 2 ! ffl.  Advantages: geometrically motivated, the relationship is naturally symmetric.  Disadvantages: often leads to graphs with several connected components, difficult to choose ffl.  (b) n nearest neighbors.  [parameter n 2 N] Nodes i and j are connected by an edge if i is among n nearest neighbors of j or j is among n nearest neighbors of i.  Advantages: simpler to choose, tends to lead to connected graphs.  Disadvantages: less geometrically intuitive.  2.  Step 2.  [Choosing the weights] Here as well we have two variations for weighting the edges: (a) Heat kernel.  [parameter t 2 R].  If nodes i and j are connected, put W ij = e\Gamma jjx i \Gamma x j jj 2 t The justification for this choice of weights will be provided later.  (b) Simple-minded.  [No parameters].  W ij = 1 if and only if vertices i and j are connected by an edge.  A simplification which avoids the necessity of choosing t.  3.  Step 3.  [Eigenmaps] Assume the graph G, constructed above, is connected, otherwise proceed with Step 3 for each connected component. 
Regression and Regularization on Large Graphs| Abstract We consider the problem of labeling a partially labeled graph.  This setting may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings.  It is also of potential practical importance, when the data is abundant, but labeling is expensive or requires human assistance.  Our approach develops a framework for regularization on such graphs.  Within this framework two algorithms are developed.  The algorithms are very simple and involve solving a single, usually sparse, system of linear equations.  Using the notion of algorithmic stability, we derive bounds on the generalization error and relate it to the structural invariants of the graph.  Some experimental comparisons to existing algorithms suggest that this approach is competitive. 
Laplacian Eigenmaps for Dimensionality Reduction and Data Representation| Abstract One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data.  We consider the problem of constructing a representation for data lying on a low dimensional manifold embedded in a high dimensional space.  Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high dimensional data.  The algorithm provides a computationally ecient approach to non-linear dimensionality reduction that has locality preserving properties and a natural connection to clustering.  Some potential applications and illustrative examples are discussed. 
Linear Manifold Regularization for Large Scale Semi-supervised Learning| Abstract The enormous wealth of unlabeled data in many applications of machine learning is beginning to pose challenges to the designers of semi-supervised learning methods.  We are interested in developing linear classification algorithms to efficiently learn from massive partially labeled datasets.  In this paper,
Semi-supervised learning on manifolds| Abstract We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy.  Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner.  The central idea of our approach is that classification functions are naturally defined only on the submanifold in question rather than the total ambient space.  Using the Laplace Beltrami operator one produces a basis for a Hilbert space of square integrable functions on the submanifold.  To recover such a basis, only unlabeled examples are required.  Once such a basis is obtained, training can be performed using the labeled data set.  Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace Beltrami operator by the graph Laplacian.  We provide details of the algorithm, its theoretical justification, and several practical applications for image, speech, and text classification. 
Using eigenvectors of the bigram graph to infer morpheme identity| Abstract This paper describes the results of some experiments exploring statistical methods to infer syntactic categories from a raw corpus in an unsupervised fashion.  It shares certain points in common with Brown et at (1992) and work that has grown out of that: it employs statistical techniques to derive categories based on what words occur adjacent to a given word.  However, we use an eigenvector decomposition of a nearest-neighbor graph to produce a two-dimensional rendering of the words of a corpus in which words of the same syntactic category tend to form clusters and neighborhoods.  We exploit this technique for extending the value of automatic learning of morphology.  In particular, we look at the suffixes derived from a corpus by unsupervised learning of morphology, and we ask which of these suffixes have a consistent syntactic function (e. g. , in English, -ed is primarily a mark of verbal past tense, does but --s marks both noun plurals and 3 rd person present on verbs). 
Manifold Regularization: A Geometric Framework for Learning from Examples| Abstract We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution.  We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. 
Regularization and Semi-supervised Learning on Large Graphs|
Using Manifold Stucture for Partially Labeled Classification|
On the Convergence of Spectral Clustering on Random Samples: The Normalized Case|

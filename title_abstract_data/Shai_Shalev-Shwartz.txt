Robust temporal and spectral modeling for query By melody| ABSTRACT Query by melody is the problem of retrieving musical performances from melodies.  Retrieval of real performances is complicated due to the large number of variations in performing a melody and the presence of colored accompaniment noise.  We describe a simple yet effective probabilistic model for this task.  We describe a generative model that is rich enough to capture the spectral and temporal variations of musical performances and allows for tractable melody retrieval.  While most of previous studies on music retrieval from melodies were performed with either symbolic (e. g.  MIDI) data or with monophonic (single instrument) performances, we performed experiments in retrieving live and studio recordings of operas that contain a leading vocalist and rich instrumental accompaniment.  Our results show that the probabilistic approach we propose is effective and can be scaled to massive datasets. 
The Forgetron: A Kernel-Based Perceptron on a Fixed Budget| Abstract The Perceptron algorithm, despite its simplicity, often performs well in online classification tasks.  The Perceptron becomes especially effective when it is used in conjunction with kernels.  However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly.  In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget.  To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores and, on the other hand, entertains a relative mistake bound.  In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach. 
The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees| Abstract Prediction suffix trees (PST) provide a popular and effective tool for tasks such as compression, classification, and language modeling.  In this paper we take a decision theoretic view of PSTs.  Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a mistake bound for it.  We then describe a self-bounded enhancement of our learning algorithm for which the learning process automatically grows a bounded-depth PST.  We also prove a similar mistake-bound for the self-bounded algorithm.  The result is an efficient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters.  To our knowledge, this is the first provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any fixed PST determined in hindsight. 
Online and batch learning of pseudo-metrics| Abstract We describe and analyze an online algorithm for supervised learning of pseudo-metrics.  The algorithm receives pairs of instances and predicts their similarity according to a pseudo-metric.  The pseudo-metrics we use are quadratic forms parameterized by positive semi-definite matrices.  The core of the algorithm is an update rule that is based on successive projections onto the positive semi-definite cone and onto half-space constraints imposed by the examples.  We describe an efficient procedure for performing these projections, derive a worst case mistake bound on the similarity predictions, and discuss a dual version of the algorithm in which it is simple to incorporate kernel operators.  The online algorithm also serves as a building block for deriving a large-margin batch algorithm.  We demonstrate the merits of the proposed approach by conducting experiments on MNIST dataset and on document filtering. 
Smooth "-Insensitive Regression by Loss Symmetrization| Abstract We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions.  These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss.  The resulting symmetric logistic loss can be viewed as a smooth approximation to the "-insensitive hinge loss used in support vector regression.  We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses.  The first family employs an iterative log-additive update which provides a regression counterpart for recent boosting algorithms.  The second family utilizes an iterative additive update step.  We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the logistic loss.  A byproduct of our work is a new simple form of regularization for boosting-based classification and regression algorithms.  Our regression framework also has implications on classification algorithms, namely, a new additive batch algorithm for the log-loss and exp-loss used in boosting.  We demonstrate the merits of our algorithms in a series of experiments including an experiment that boosts the accuracy of support vector regressors on a benchmark dataset. 
LEARNING TO ALIGN POLYPHONIC MUSIC| ABSTRACT We describe an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acoustic counterpart.  Our method employs a supervised learning approach by using a training set of aligned symbolic and acoustic representations.  The alignment function we devise is based on mapping the input acousticsymbolic representation along with the target alignment into an abstract vector-space.  Building on techniques used for learning support vector machines (SVM), our alignment function distills to a classifier in the abstract vectorspace which separates correct alignments from incorrect ones.  We describe a simple iterative algorithm for learning the alignment function and discuss its formal properties.  We use our method for aligning MIDI and MP3 representations of polyphonic recordings of piano music.  We also compare our discriminative approach to a generative method based on a generalization of hidden Markov models.  In all of our experiments, the discriminative method outperforms the HMM-based method. 
Learning to align polyphonic music| Long version. 
Smooth e-Intensive Regression by Loss Symmetrization|
A new perspective on an old perceptron algorithm|

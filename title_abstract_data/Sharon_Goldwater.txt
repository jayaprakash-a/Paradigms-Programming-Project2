Building a Robust Dialogue System with Limited Data \Lambda| Abstract We describe robustness techniques used in the CommandTalk system at the recognition level, the parsing level, and the dialogue level, and how these were influenced by the lack of domain data.  We used interviews with subject matter experts (SME's) to develop a single grammar for recognition, understanding, and generation, thus eliminating the need for a robust parser.  We broadened the coverage of the recognition grammar by allowing word insertions and deletions, and we implemented clarification and correction subdialogues to increase robustness at the dialogue level.  We discuss the applicability of these techniques to other domains. 
A Type System for Statically Detecting Spreadsheet Errors| Abstract We describe a methodology for detecting user errors in spreadsheets, using the notion of units as our basic elements of checking.  We define the concept of a header and discuss two types of relationships between headers, namely is-a and has-a relationships.  With these, we develop a set of rules to assign units to cells in the spreadsheet.  We check for errors by ensuring that every cell has a well-formed unit.  We describe an implementation of the system that allows the user to check Microsoft Excel spreadsheets.  We have run our system on practical examples, and even found errors in published spreadsheets. 
Compiling Language Models from a Linguistically Motivated Unification Grammar| Abstract Systems now exist which are able to compile unification grammars into language models that can be included in a speech recognizer, but it is so far unclear whether non-trivial linguistically principled grammars can be used for this purpose.  We describe a series of experiments which investigate the question empirically, by incrementally constructing a grammar and discovering what problems emerge when successively larger versions are compiled into finite state graph representations and used as language models for a medium-vocabulary recognition task. 
Interpolating Between Types and Tokens by Estimating Power-Law Generators| Abstract Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens.  We present a framework for developing statistical models that generically produce power-laws, augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies.  We show that taking a particular stochastic process -- the Pitman-Yor process -- as an adaptor justifies the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology. 
Statically Finding Errors in Spreadsheets| Abstract We describe a methodology for detecting user errors in spreadsheets, using the notion of units as our basic elements of checking.  We define the concept of a header and discuss two types of relationships between headers, namely is-a and has-a relationships.  With these, we develop a set of rules to assign units to cells in the spreadsheet.  We check for errors by ensuring that every cell has a well-formed unit.  We describe an implementation of the system that allows the user to check Microsoft Excel spreadsheets. 
Priors in Bayesian Learning of Phonological Rules| Abstract This paper describes a Bayesian procedure for unsupervised learning of phonological rules from an unlabeled corpus of training data.  Like Goldsmith's Linguistica program (Goldsmith, 2004b), whose output is taken as the starting point of this procedure, our learner returns a grammar that consists of a set of signatures, each of which consists of a set of stems and a set of suffixes.  Our grammars differ from Linguistica's in that they also contain a set of phonological rules, specifically insertion, deletion and substitution rules, which permit our grammars to collapse far more words into a signature than Linguistica can.  Interestingly, the choice of Bayesian prior turns out to be crucial for obtaining a learner that makes linguistically appropriate generalizations through a range of different sized training corpora. 
Best-first edgebased chart parsing|
Compiling Language Models from a Linquistically Motivated Unification Grammar|
Interpreting Language in Context in CommandTalk|

Generalization Error and Algorithmic Convergence of Median Boosting| Abstract We have recently proposed an extension of ADABOOST to regression that uses the median of the base regressors as the final regressor.  In this paper we extend theoretical results obtained for ADABOOST to median boosting and to its localized variant.  First, we extend recent results on efficient margin maximizing to show that the algorithm can converge to the maximum achievable margin within a preset precision in a finite number of steps.  Then we provide confidence-interval-type bounds on the generalization error. 
A Polygonal Line Algorithm for Constructing Principal Curves| Abstract Principal curves have been defined as "self consistent" smooth curves which pass through the "middle" of a d-dimensional probability distribution or data cloud.  Recently, we [1] have offered a new approach by defining principal curves as continuous curves of a given length which minimize the expected squared distance between the curve and points of the space randomly chosen according to a given distribution.  The new definition made it possible to carry out a theoretical analysis of learning principal curves from training data.  In this paper we propose a practical construction based on the new definition.  Simulation results demonstrate that the new algorithm compares favorably with previous methods both in terms of performance and computational complexity. 
Confidence-rated Regression by Localized Median Boosting| Abstract In this paper we describe and analyze LOCMEDBOOST, an algorithm that boosts regressors with input dependent weights.  The algorithm is a synthesis of median boosting [1] and localized boosting [2, 3, 4], and unifies the advantages of the two approaches.  We prove boostingtype convergence of the algorithm and give clear conditions for the convergence of the robust training error, where robustness is measured in terms of the expert population and with respect to the underlying confidence estimate.  We extend Ratsch and Warmuth's results [5] on efficient margin maximization to show that the algorithm can converge to maximum achievable margin in a finite number of steps.  We also extend probabilistic bounds on the generalization error derived for ADABOOST.  The results provide bounds on the confidence-interval-type error and qualitatively justify the algorithmic objective of the minimization of the robust error.  Finally, we present some promising experimental results on synthetic and benchmark data sets. 
Intrinsic Dimension Estimation Using Packing Numbers| Abstract We propose a new algorithm to estimate the intrinsic dimension of data sets.  The method is based on geometric properties of the data and requires neither parametric assumptions on the data generating model nor input parameters to set.  The method is compared to a similar, widelyused algorithm from the same family of geometric techniques.  Experiments show that our method is more robust in terms of the data generating distribution and more reliable in the presence of noise. 
Piecewise linear skeletonization using principal curves|
Principle Curves: Learning, Design, and Applications,"|
Robust regression by boosting the median,|
Principal curves: Learning and convergence,"|
Date-dependent margin-based bounds for classification|

Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data| Abstract In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when longrange dependencies exist.  We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices.  Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP).  On a natural-language chunking task, we show that a DCRF performs better than a series of linearchain CRFs, achieving comparable performance using only half the training data. 
Toward Optimal Active Learning through Sampling Estimation of Error Reduction| Abstract This paper presents an active learning method that directly optimizes expected future error.  This is in contrast to many other popular techniques that instead aim to reduce version space size.  These other methods are popular because for many learning models, closed form calculation of the expected future error is intractable.  Our approach is made feasible by taking a sampling approach to estimating the expected reduction in error due to the labeling of a query.  In experimental results on two real-world data sets we reach high accuracy very quickly, sometimes with four times fewer labeled examples than competing methods. 
Learning to Extract Symbolic Knowledge from the World Wide Web| Abstract The World Wide Web is a vast source of information accessible to computers, but understandable only to humans.  The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web.  Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledgebased inference and problem solving.  Our approachistodevelop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations.  Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web.  This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system. 
Linking Shared Segments| ABSTRACT As an alternative to communication via messages or files, shared memory has the potential to be simpler, faster, and less wasteful of space.  Unfortunately, the mechanisms available for sharing in Unix are not very easy to use.  As a result, shared memory tends to appear primarily in self-contained parallel applications, where library or compiler support can take care of the messy details.  We have developed a system, called Hemlock, for transparent sharing of variables and/or subroutines across application boundaries.  Our system is backward compatible with existing versions of Unix.  It employs dynamic linking in conjunction with the Unix mmap facility and a kernel-maintained correspondence between virtual addresses and files.  It introduces the notion of scoped linking to avoid naming conflicts in the face of extensive sharing. 
Learning to Classify Text from Labeled and Unlabeled Documents| Abstract In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap.  This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents.  We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function.  We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier.  The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence.  Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%. 
Gene Prediction with Conditional Random Fields| Abstract Given a sequence of DNA nucleotide bases, the task of gene prediction is to find subsequences of bases that encode proteins.  Reasonable performance on this task has been achieved using generatively trained sequence models, such as hidden Markov models.  We propose instead the use of
Overcoming Incomplete Perception with Utile Distinction Memory| Abstract This paper presents a method by which a reinforcement learning agent can solve the incomplete perception problem using memory.  The agent uses a hidden Markov model (HMM) to represent its internal state space and creates memory capacity by splitting states of the HMM.  The key idea is a test to determine when and how a state should be split: the agent only splits a state when doing so will help the agent predict utility.  Thus the agent can create only as much memory as needed to perform the task at hand---not as much as would be required to model all the perceivable world.  I call the technique UDM, for Utile Distinction Memory. 
Maximum Entropy Markov Models for Information Extraction and Segmentation| Abstract Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction.  In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations.  This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences.  It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state.  We present positive experimental results on the segmentation of FAQ's. 
A Comparison of Event Models for Naive Bayes Text Classification| Abstract Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption.  Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e. g.  Larkey and Croft 1996; Koller and Sahami 1997).  Others use a multinomial model, that is, a uni-gram language model with integer word counts (e. g.  Lewis and Gale 1994; Mitchell 1997).  This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora.  We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes---providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size. 
Learning to construct knowledge bases from the World Wide Web| Abstract The World Wide Web is a vast source of information accessible to computers, but understandable only to humans.  The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web.  Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving.  Our approach is to develop a trainable information extraction system that takes two inputs.  The first is an ontology that defines the classes (e. g. , company, person, employee, product) and relations (e. g. , employed by, produced by) of interest when creating the knowledge base.  The second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations.  Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web.  This article describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects. 
Improving Text Classification by Shrinkage in a Hierarchy of Classes| Abstract When documents are organized in a large number of topic categories,
Predictive Random Fields: Latent Variable Models Fit by Multiway Conditional Probability with Applications to Document Analysis| Abstract We introduce predictive random fields, a framework for learning undirected graphical models based not on joint, generative likelihood, or on conditional likelihood, but based on a product of several conditional likelihoods each relying on common sets of parameters and predicting different subsets of variables conditioned on other subsets.  When applied to models with latent variables, such as the Harmonium, this approach results in powerful clustering models that combine the advantages of conditional random fields with the unsupervised clustering ability of popular topic models, such as latent Dirichlet allocation and its successors.  We present new algorithms for parameter estimation based on contrastive divergence.  Experimental results show significant improvement in inferring hidden document categories, and learning models of authors, words, topics and time. 
The Author-Recipient-Topic Model for Topic and Role Discovery in Social Networks: Experiments with Enron and Academic Email| Abstract Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links.  We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the the directionsensitive messages sent between entities.  The model builds on Latent Dirichlet Allocation and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient---steering the discovery of topics according to the relationships between people.  We give results on both the Enron email corpus and a researcher's email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people's roles. 
A Note on the Unification of Information Extraction and Data Mining using Conditional-Probability, Relational Models| Abstract Although information extraction and data mining appear together in many applications, their interface in most current systems would better be described as serial juxtaposition than as tight integration.  Information extraction populates slots in a database by identifying relevant subsequences of text, but is usually not aware of the emerging patterns and regularities in the database.  Data mining methods begin from a populated database, and are often unaware of where the data came from, or its inherent uncertainties.  The result is that the accuracy of both suffers, and significant mining of complex text sources is beyond reach.  This position paper proposes the use of unified, relational, undirected graphical models for information extraction and data mining, in which extraction decisions and data-mining decisions are made in the same probabilistic "currency," with a common inference procedure---each component thus being able to make up for the weaknesses of the other and therefore improving the performance of both.  For example, data mining run on a partiallyfilled database can find patterns that provide "topdown" accuracy-improving constraints to information extraction.  Information extraction can provide a much richer set of "bottom-up" hypotheses to data mining if the mining is set up to handle additional uncertainty information from extraction.  We outline an approach and describe several models, but provide no experimental results. 
Learning to Create Customized Authority Lists| Abstract The proliferation of hypertext and the popularity of Kleinberg's HITS algorithm have brought about an increased interest in link analysis.  While HITS and its older relatives from the Bibliometrics provide a method for finding authoritative sources on a particular topic, they do not allow individual users to inject their own opinions on what sources are authoritative.  This paper presents a technique for learning a user's internal model of authority.  We present experimental results based on Cora on-line index, a database of approximately one million on-line computer science literature references. 
An Integrated, Conditional Model of Information Extraction and Coreference with Application to Citation Matching| Abstract Although information extraction and coreference resolution appear together in many applications, most current systems perform them as independent steps.  This paper describes an approach to integrated inference for extraction and coreference based on conditionally-trained undirected graphical models.  We discuss the advantages of conditional probability training, and of a coreference model structure based on graph partitioning.  On a data set of research paper citations, we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy, and using coreference to improve the accuracy of the extracted fields. 
Chinese Segmentation and New Word Detection using Conditional Random Fields| Abstract Chinese word segmentation is a difficult, important and widely-studied sequence modeling problem.  This paper demonstrates the ability of linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the integration of domain knowledge in the form of multiple lexicons of characters and words.  We also present a probabilistic new word detection method, which further improves performance.  Our system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition.  State-of-the-art performance is obtained. 
Group and Topic Discovery from Relations and Their Attributes| Abstract We present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes.  Block-models of relationship data have been studied in social network analysis for some time.  Here we simultaneously cluster in several modalities at once, incorporating the attributes (here, words) associated with certain relationships.  Significantly, joint inference allows the discovery of topics to be guided by the emerging groups, and vice-versa.  We present experimental results on two large data sets: sixteen years of bills put before the U. S.  Senate, comprising their corresponding text and voting records, and thirteen years of similar data from the United Nations.  We show that in comparison with traditional, separate latent-variable models for words or Blockstructures for votes, the Group-Topic model's joint inference discovers more cohesive groups and improved topics. 
Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data| Abstract We present conditional random fields, a framework for building probabilistic models to segment and label sequence data.  Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models.  Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states.  We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 
Using Maximum Entropy for Text Classification| Abstract This paper proposes the use of maximum entropy techniques for text classification.  Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation.  The underlying principle of maximum entropy is that without external knowledge, one should prefer distributions that are uniform.  Constraints on the distribution, derived from labeled training data, inform the technique where to be minimally non-uniform.  The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm.  In this paper, maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document.  In experiments on several text datasets we compare accuracy to naive Bayes and show that maximum entropy is sometimes significantly better, but also sometimes worse.  Much future work remains, but the results indicate that maximum entropy is a promising technique for text classification. 
An Exploration of Entity Models, Collective Classification and Relation Description| ABSTRACT Traditional information retrieval typically represents data using a bag of words; data mining typically uses a highly structured database representation.  This paper explores the middle ground using a representation which we term entity models, in which questions about structured data may be posed and answered, but the complexities and task-specific restrictions of ontologies are avoided.  An entity model is a language model or word distribution associated with an entity, such as a person, place or organization.  Using these perentity language models, entities may be clustered, links may be detected or described with a short summary, entities may be collectively classified, and question answering may be performed.  On a corpus of entities extracted from newswire and the Web, we group entities by profession with 90% accuracy, improve accuracy further on the task of classifying politicians as liberal or conservative using collective classification and conditional random fields, and answer questions about "who a person is" with mean reciprocal rank (MRR) of 0. 52. 
Practical Markov Logic Containing First-Order Quantifiers with Application to Identity Uncertainty| Abstract Markov logic is a highly
Rapid development of Hindi named entity recognition using conditional random fields and feature induction| This paper describes our application of Conditional Random Fields (CRFs) with feature induction to a Hindi named entity recognition task.  With only five days development time and little knowledge of this language, we automatically discover relevant features by providing a large array of lexical tests and using feature induction to automatically construct the features that most increase conditional likelihood.  In an effort to reduce overfitting, we use a combination of a Gaussian prior and early-stopping based on the results of 10-fold cross validation. 
Joint Deduplication of Multiple Record Types in Relational Data| ABSTRACT Record deduplication is the task of merging database records that refer to the same underlying entity.  In relational databases, accurate deduplication for records of one type is often dependent on the decisions made for records of other types.  Whereas nearly all previous approaches have merged records of different types independently, this work models these inter-dependencies explicitly to collectively deduplicate records of multiple types.  We construct a conditional random field model of deduplication that captures these relational dependencies, and then employ a novel relational partitioning algorithm to jointly deduplicate records.  For two citation matching datasets, we show that collectively deduplicating paper and venue records results in up to a 30% error reduction in venue deduplication, and up to a 20% error reduction in paper deduplication. 
BETWEENAND WITHIN FIELD VARIABILITY OF NEW ZEALAND INDIGENOUS FLOWER VISITORS TO ONIONS| ABSTRACT In New Zealand, few studies have documented the occurrence of indigenous flower visitors within crops.  A five-year survey is examining the distribution, diversity and abundance of flower visitors in onion (Allium cepa) fields located throughout New Zealand.  Day-time observations from the first year of the survey recorded nine arthropod orders visiting onion flowers in the South Island, with Diptera (flies) and Hymenoptera being the most abundant.  Over six fields the proportion of bees that were indigenous species (Apoidea) ranged from <1% to 63. 6%.  Common bee genera and fly families varied considerably in their presence and abundance, even over 17 km.  Moreover, counts of Lasioglossum spp.  at five points within one field ranged from 0 to 576, demonstrating that the distribution of some flower visitors can be highly variable.  Understanding crop flower visitors may help develop systems to reduce transgene flow should genetically modified crops be commercially produced in New Zealand. 
Dynamic Sharing and Backward Compatibility on 64-Bit Machines| Abstract As an alternative to communication via messages or files, shared memory has the potential to be simpler, faster, and less wasteful of space.  Unfortunately, the mechanisms available for sharing in most multi-user operating systems are difficult to use.  As a result, shared memory tends to appear primarily in self-contained parallel applications, where library or compiler support can take care of the messy details.  We see a tremendous opportunity to extend the advantages of sharing across application boundaries.  We believe that these advantages can be realized without introducing major changes to the Unix programming model.  In particular, we believe that it is both possible and desirable to incorporate shared memory segments into the hierarchical file system name space.  Our approach has two components: First, we use dynamic linking to allow programs to access shared data and code in the same way they access ordinary (private) variables and functions.  Second, we unify memory and files into a single-level store that facilitates the sharing of pointers.  This second component is made feasible by the 64-bit addresses of emerging microprocessors. 
Bootstrapping for Text Learning Tasks| Abstract When applying text learning algorithms to complex tasks, it is tedious and expensive to hand-label the large amounts of training data necessary for good performance.  This paper presents bootstrapping as an alternative approach to learning from large sets of labeled data.  Instead of a large quantity of labeled data, this paper advocates using a small amount of seed information and a large collection of easily-obtained unlabeled data.  Bootstrapping initializes a learner with the seed information; it then iterates, applying the learner to calculate labels for the unlabeled data, and incorporating some of these labels into the training input for the learner.  Two case studies of this approach are presented.  Bootstrapping for information extraction provides 76% precision for a 250-word dictionary for extracting locations from web pages, when starting with just a few seed locations.  Bootstrapping a text classifier from a few keywords per class and a class hierarchy provides accuracy of 66%, a level close to human agreement, when placing computer science research papers into a topic hierarchy.  The success of these two examples argues for the strength of the general bootstrapping approach for text learning tasks. 
Disambiguating Web Appearances of People in a Social Network| ABSTRACT Say you are looking for information about a particular person.  A search engine returns many pages for that person's name but which pages are about the person you care about, and which are about other people who happen to have the same name? Furthermore, if we are looking for multiple people who are related in some way, how can we best leverage this social network? This paper presents two unsupervised frameworks for solving this problem: one based on link structure of the Web pages, another using Agglomerative/Conglomerative Double Clustering (A/CDC)---an application of a recently introduced multi-way distributional clustering method.  To evaluate our methods, we collected and hand-labeled a dataset of over 1000 Web pages retrieved from Google queries on 12 personal names appearing together in someones in an email folder.  On this dataset our methods outperform traditional agglomerative clustering by more than 20%, achieving over 80% F-measure. 
Employing EM and Pool-Based Active Learning for Text Classification| Abstract This paper shows how a text classifier's need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents.  We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling.  Then active learning is combined with ExpectationMaximization in order to "fill in" the class labels of those documents that remain unlabeled.  Experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or EM alone. 
Text Classification by Bootstrapping with Keywords, EM and Shrinkage| Abstract When applying text classification to complex tasks, it is tedious and expensive to hand-label the large amounts of training data necessary for good performance.  This paper presents an alternative approach to text classification that requires no labeled documents; instead, it uses a small set of keywords per class, a class hierarchy and a large quantity of easilyobtained unlabeled documents.  The keywords are used to assign approximate labels to the unlabeled documents by termmatching.  These preliminary labels become the starting point for a bootstrapping process that learns a naive Bayes classi#er using Expectation-Maximization and hierarchical shrinkage.  When classifying a complex data set of computer science research papers into a 70-leaf topic hierarchy, the keywords alone provide 45% accuracy.  The classifier learned by bootstrapping reaches 66% accuracy, a level close to human agreement. 
Accurate Information Extraction from Research Papers using Conditional Random Fields| Abstract With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance.  This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers.  The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration.  This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-6 priors for improved regularization, and several classes of features and Markov order.  On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results.  Accuracy compares even more favorably against HMMs. 
Conditional Models of Identity Uncertainty with Application to Noun Coreference| Abstract Coreference analysis, also known as record linkage or identity uncertainty, is a difficult and important problem in natural language processing, databases, citation matching and many other tasks.  This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models.  Unlike many historical approaches to coreference, the models presented here are relational---they do not assume that pairwise coreference decisions should be made independently from each other.  Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies---paralleling the advantages of conditional random fields over hidden Markov models.  We present positive results on noun phrase coreference in two standard text data sets. 
Using Reinforcement Learning to Spider the Web Efficiently| Abstract Consider the task of exploring the Web in order to find pages of a particular kind or on a particular topic.  This task arises in the construction of search engines and Web knowledge bases.  This paper argues that the creation of ecient web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making.  One strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future.  We present an algorithm for learning a value function that maps hyperlinks to future discounted reward using a naive Bayes text classifier.  Experiments on two real-world spidering tasks show a threefold improvement in spidering eciency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only. 
Classification with Hybrid Generative/Discriminative Models| Abstract Although discriminatively trained classifiers are usually more accurate when labeled training data is abundant, previous work has shown that when training data is limited, generative classifiers can out-perform them.  This paper describes a hybrid model in which a high-dimensional subset of the parameters are trained to maximize generative likelihood, and another, small, subset of parameters are discriminatively trained to maximize conditional likelihood.  We give a sample complexity bound showing that in order to fit the discriminative parameters well, the number of training examples required depends only on the logarithm of the number of feature occurrences and feature set size.  Experimental results show that hybrid models can provide lower test error and can produce better accuracy/coverage curves than either their purely generative or purely discriminative counterparts.  We also discuss several advantages of hybrid models, and advocate further work in this area. 
Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference| Abstract Coreference analysis, also known as record linkage or identity uncertainty, is a difficult and important problem in natural language processing, databases, citation matching and many other tasks.  This paper introduces several discriminative, conditionalprobability models for coreference analysis, all examples of undirected graphical models.  Unlike many historical approaches to coreference, the models presented here are relational---they do not assume that pairwise coreference decisions should be made independently from each other.  Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies--paralleling the advantages of conditional random fields over hidden Markov models.  We present experiments on proper noun coreference in two text data sets, showing results in which we reduce error by nearly 28% or more over traditional thresholded record-linkage, and by up to 33% over an alternative coreference technique previously used in natural language processing. 
A Conditional Random Field for Discriminatively-trained Finite-state String Edit Distance| Abstract The need to measure sequence similarity arises in information extraction, object identity, data mining, biological sequence analysis, and other domains.  This paper presents discriminative string-edit CRFs, a finitestate conditional random field model for edit sequences between strings.  Conditional random fields have advantages over generative approaches to this problem, such as pair HMMs or the work of Ristad and Yianilos, because as conditionally-trained methods, they enable the use of complex, arbitrary actions and features of the input strings.  As in generative models, the training data does not have to specify the edit sequences between the given string pairs.  Unlike generative models, however, our model is trained on both positive and negative instances of string pairs.  We present positive experimental results on several data sets. 
Dynamic Conditional Random Fields for Jointly Labeling Multiple Sequences| Abstract Conditional random fields (CRFs) for sequence modeling have several advantages over joint models such as HMMs, including the ability to relax strong independence assumptions made in those models, and the ability to incorporate arbitrary overlapping features.  Previous work has focused on linear-chain CRFs, which correspond to finite-state machines, and have efficient exact inference algorithms.  Often, however, we wish to label sequence data in multiple interacting ways---for example, performing part-of-speech tagging and noun phrase segmentation simultaneously, increasing joint accuracy by sharing information between them.  We present dynamic conditional random fields (DCRFs), which are CRFs in which each time slice has a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks---and parameters are tied across slices.  (They could also be called conditionallytrained Dynamic Markov Networks. ) Since exact inference can be intractable in these models, we perform approximate inference using the tree-based reparameterization framework (TRP).  We also present empirical results comparing DCRFs with linear-chain CRFs on naturallanguage data. 
Information Extraction with HMM Structures Learned by Stochastic Optimization| Abstract Recent research has demonstrated the strong performance of hidden Markov models applied to information extraction|the task of populating database slots with corresponding phrases from text documents.  A remaining problem, however, is the selection of statetransition structure for the model.  This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization.  Our algorithm begins with a simple model and then performs hill-climbing in the space of possible structures by splitting states and gauging performance on a validation set.  Experimental results show that this technique finds HMM models that nearly always out-perform a fixed model, and have superior average performance across tasks. 
Short-Term Memory in Visual Routines for "Off-Road Car Chasing"| Abstract Using "off-road car chasing" as an example task, this paper explores the need for short-term memory (or ``internal state") in visual routines.  The driving domain involves dynamics, uncertainty and requires a tight sense-act loop.  Some previous demonstrations of visual routines avoided the need for short-term memory by providing their agent with a bird's-eye view or a 360 ffi view [ Agre and Chapman, 1987; Chapman, 1990; Reece, 1992 ] .  In contrast, our SGI-rendered simulation provides a realistic, robot-centered view of the environment in which limited field of view and occlusions necessitate memory of past perceptions and actions.  Our agent's action policy maintains a minimal, task-specific amounts of memory using a finite state machine. 
Collective Multi-Label Text Classification| Abstract Multi-label classification, the task of assigning one or more class labels to a document, arises in many domains.  In the multi-label domain the categories need not be independent.  As examples, a news article may be about multiple related topics and a medical journal article may pertain to multiple medical conditions.  A common approach to multi-label classification is to train independent binary classifiers for each label, but this approach fails to exploit dependencies between labels.  This paper explores conditional random field models for classifying documents that may have multiple labels.  Single-label CRF models maintain features for word occurrence patterns; the models that this paper describes additionally maintain features corresponding to label co-occurrence patterns.  These models outperform their independently trained singlelabel counterparts using several evaluation metrics on widely used corpora having varying characteristics.  For example, even for sparsely multi-labeled corpora the models reduce subset classification error by as much as 30%.  In addition, the models exhibit comparable F1-scores to the best performing classifiers under similar training conditions. 
Interactive Information Extraction with Constrained Conditional Random Fields| Abstract Information Extraction methods can be used to automatically "fill-in" database forms from unstructured data such as Web documents or email.  State-of-the-art methods have achieved low error rates but invariably make a number of errors.  The goal of an interactive information extraction system is to assist the user in filling in database fields while giving the user confidence in the integrity of the data.  The user is presented with an interactive interface that allows both the rapid verification of automatic field assignments and the correction of errors.  In cases where there are multiple errors, our system takes into account user corrections, and immediately propagates these constraints such that other fields are often corrected automatically.  Linear-chain conditional random fields (
Learning Hidden Markov Model Structure for Information Extraction| Abstract Statistical machine learning techniques, while well proven in fields such as speech recognition, are just beginning to be applied to the information extraction domain.  We explore the use of hidden Markov models for information extraction tasks, specifically focusing on how to learn model structure from data and how to make the best use of labeled and unlabeled data.  We show that a manually-constructed model that contains multiple states per extraction field outperforms a model with one state per field, and discuss strategies for learning the model structure automatically from data.  We also demonstrate that the use of distantly-labeled data to set model parameters provides a significant improvement in extraction accuracy.  Our models are applied to the task of extracting important fields from the headers of computer science research papers, and achieve an extraction accuracy of 92. 9%. 
Learning to Understand the Web| Abstract In a traditional information retrieval system, it is assumed that queries can be posed about any topic.  In reality, a large fraction of web queries are posed about a relatively small number of topics, like products, entertainment, current events, and so on.  One way of exploiting this sort of regularity in web search is to build, from the information found on the web, comprehensive databases about specific topics.  An appropriate interface to such a database can support complex structured queries which are impossible to answer with traditional topic-independent query methods.  Here we discuss three case studies for this "data-centric" approach to web search.  A common theme in this discussion is the need for very robust methods for finding relevant information, extracting data from pages, and integrating information taken from multiple sources, and the importance of statistical learning methods as a tool for creating such robust methods. 
Extracting social networks and contact information from email and the Web| Abstract We present an end-to-
Efficient clustering of high-dimensional data sets with application to reference matching| ABSTRACT Many important problems involve clustering large datasets.  Although naive implementations of clustering are computationally expensive, there are established ecient techniques for clustering when the dataset has either (1) a limited number of clusters, (2) a low feature dimensionality, or (3) a small number of data points.  However, there has been much less work on methods of eciently clustering datasets that are large in all three ways at once|for example, having millions of data points that exist in many thousands of dimensions representing many thousands of clusters.  We present a new technique for clustering these large, highdimensional datasets.  The key idea involves using a cheap, approximate distance measure to eciently divide the data into overlapping subsets we call canopies.  Then clustering is performed by measuring exact distances only between points that occur in a common canopy.  Using canopies, large clustering problems that were formerly impossible become practical.  Under reasonable assumptions about the cheap distance metric, this reduction in computational cost comes without any loss in clustering accuracy.  Canopies can be applied to many domains and used with a variety of clustering approaches, including Greedy Agglomerative Clustering, K-means and Expectation-Maximization.  We present experimental results on grouping bibliographic citations from the reference sections of research papers.  Here the canopy approach reduces computation time over a traditional clustering approach by more than an order of magnitude and decreases error in comparison to a previously used algorithm by 25%. 
Distributional Clustering of Words for Text Classification| Abstract This paper describes the application of Distributional Clustering [20] to document classification.  This approach clusters words into groups based on the distribution of class labels associated with each word.  Thus, unlike some other unsupervised dimensionalityreduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy.  Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy---significantly better than Latent Semantic Indexing [6], class-based clustering [1], feature selection by mutual information [23], or Markov-blanket-based feature selection [13].  We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering. 
Category: Control, Navigation and Planning| Abstract Undoubtedly, efficient exploration is crucial for the success of a learning agent.  Previous approaches to exploration in reinforcement learning exclusively address exploration in Markovian domains, i. e.  domains in which the state of the environment is fully observable.  If the environment is only partially observable, they cease to work because exploration statistics are confounded between aliased world states.  This paper presents Fringe Exploration, a technique for efficient exploration in partially observable domains.  The key idea, (applicable to many exploration techniques), is to keep statistics in the space of possible shortterm memories, instead of in the agent's current state space.  Experimental results in a partially observable maze and in a difficult driving task with visual routines show dramatic performance improvements.  1 The Problem Efficient exploration is of fundamental importance for autonomous systems that learn to act.  In recent years, a variety of exploration techniques have been proposed for reinforcement learning (RL).  Many researchers use undirected techniques---approaches closely related to random walks, e. g.  [Mozer and Bachrach, 1989; Barto et al. , 1995] .  These include the common "random action with probability e" and "Boltzmann distribution based on utility and decreasing temperature. " While easy to implement, these approaches are often unbearably inefficient; Whitehead has proved that undirected exploration can cause the learning time to scale exponentially with the size of the state space [Whitehead, 1991] .  To improve learning speed, several researchers have proposed directed exploration techniques---approaches that use statistics from the learning experience to more efficiently guide the search.  Statistics used include action counts, e. g.  [Sato et al. , 1988; Thrun, 1992] , action recency [Sutton, 1990] , and confidence intervals on utility data [Kaelbling, 1990] .  It has been shown both analytically and empirically that directed exploration reduces worst-case learning time from exponential to only a low-degree polynomial [Thrun, 1992; Koenig and Simmons, 1993] .  Directed exploration is clearly superior.  A second important issue in reinforcement learning is hidden state.  Hidden state occurs whenever sensory limitations hide features of the world from the agent; this "partial observability" can make different world states appear as identical to the agent.  State identification techniques use memory of past percepts and actions to distinguish states that are aliased by these non-Markovian dependencies.  Several reinforcement learning algorithms augment their state representations on-line by adding memory; examples include techniques based on partially observable Markov decision processes [Chrisman, 1992; McCallum, 1993] , recurrent neural networks [Lin and Mitchell, 1992] and suffix trees [McCallum, 1995] .  The problem is that directed exploration techniques all hinge on the assumption that the state of the world is observable.  That is, they all depend on associating unique exploration statistics with each world state.  This assumption is broken by hidden state.  For example, since different world states may appear as identical to the agent, a hospital navigation robot may arrive at a hallway intersection it has experienced only seldomly, yet, due to hidden state, choose its next action using statistics confounded with a different intersection it has visited many times; this mistake can cause the robot to uselessly repeat past experience, and miss opportunities to visit unexplored regions.  No previous research addresses the combination of directed exploration with hidden state in reinforcement learning; all previous hidden state work has used undirected exploration.  Efficient exploration in non-Markovian domains has long been understood as a special difficulty, (Chrisman refers to it as the `Exacerbated Exploration Problem' [Chrisman, 1992] ), but little is known about it.  As demonstrated in this paper, the straightforward application of directed approaches fails badly.  2 A Solution The central contribution of this paper is Fringe Exploration, a new methodology that differs from previous approaches in that it addresses directed exploration specifically in non-Markovian domains.  This section presents the idea in the abstract; the next section contributes a specific implementation.  The abstract idea of Fringe Exploration is that, instead of exploring only the space of observations, the agent explores the space of memories.  That is, instead of associating exploration statistics with agent states, statistics are associated with entire sequences of observations and actions.  This can be understood as maintaining "high resolution" exploration statistics---the agent keeps the statistics associated with states that make "extra" distinctions based on short-term memory; (in many contexts, these extra, hypothesis distinctions are called the "fringe. ") As a result of using memory, the danger of accidentally confounding two different world states is reduced.  Fringe Exploration avoids assuming that the agent's current internal states are Markovian, but it does assume that the fringe states are Markovian.  Note that, depending on the task and the depth of the fringe memory, the fringe states may not in fact be Markovian, but algorithms
Reducing labeling effort for structured prediction tasks| Abstract A common obstacle preventing the rapid deployment of supervised machine learning algorithms is the lack of labeled training data.  This is particularly expensive to obtain for structured prediction tasks, where each training instance may have multiple, interacting labels, all of which must be correctly annotated for the instance to be of use to the learner.  Traditional active learning addresses this problem by optimizing the order in which the examples are labeled to increase learning efficiency.  However, this approach does not consider the difficulty of labeling each example, which can vary widely in structured prediction tasks.  For example, the labeling predicted by a partially trained system may be easier to correct for some instances than for others.  We propose a new active learning paradigm which reduces not only how many instances the annotator must label, but also how difficult each instance is to annotate.  The system also leverages information from partially correct predictions to efficiently solicit annotations from the user.  We validate this active learning framework in an interactive information extraction system, reducing the total number of annotation actions by 22%. 
Automatic Categorization of Email into Folders: Benchmark Experiments on Enron and SRI Corpora| Abstract Oce workers everywhere are drowning in email---not only spam, but also large quantities of legitimate email to be read and organized for browsing.  Although there have been extensive investigations of automatic document categorization, email gives rise to a number of unique challenges, and there has been relatively little study of classifying email into folders.  This paper presents an extensive benchmark study of email foldering using two large corpora of real-world email messages and foldering schemes: one from former Enron employees, another from participants in an SRI research project.  We discuss the challenges that arise from dierences between email foldering and traditional document classification.  We show experimental results from an array of automated classification methods and evaluation methodologies, including a new evaluation method of foldering results based on the email timeline, and including enhancements to the exponential gradient method Winnow, providing top-tier accuracy with a fraction the training time of alternative methods.  We also establish that classification accuracy in many cases is relatively low, confirming the challenges of email data, and pointing toward email foldering as an important area for further research. 
Ecient Web Spidering with Reinforcement Learning| Abstract Consider the task of exploring the Web in order to
Joint Parsing and Semantic Role Labeling| Abstract A striking feature of human syntactic processing is that it is context-dependent, that is, it seems to take into account semantic information from the discourse context and world knowledge.  In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses.  To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.  Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates. 
Learning with Scope, with Application to Information Extraction and Classification| Abstract In probabilistic approaches to text classi#cation and information extraction, one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data.  In many text data sets however, there are additional, scope-limited features whose predictive power is applicable only to a certain subset of the data.  For example, in information extraction from web pages, word formatting may be indicative of extraction category in different ways on different web pages.  The central diculty with using such features is capturing and exploiting the new regularities encountered in previously unseen data.  In this work, we propose a hierarchical probabilistic model that uses both local/scopelimited features (e. g. , formatting) and global features (e. g. , word content).  The local regularities are represented as an unobserved random parameter for each local data set, and these regularities are captured in the inference process.  This process is akin to automatically retuning our classifier to the local regularities on each newly encountered web page.  Exact inference is intractable, and we present approximations via point estimates and variational methods.  Empirical results on large collections of web data show this method significantly improving performance over traditional models of global features alone. 
Confidence Estimation for Information Extraction| Abstract Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents.  Despite the successes of these systems, accuracy will always be imperfect.  For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field.  The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model.  We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records. 
Table extraction using conditional random fields| ABSTRACT The ability to find tables and extract information from them is a necessary component of data mining, question answering, and other information retrieval tasks.  Documents often contain tables in order to communicate densely packed, multi-dimensional information.  Tables do this by employing layout patterns to efficiently indicate fields and records in two-dimensional form.  Their rich combination of formatting and content present difficulties for traditional language modeling techniques, however.  This paper presents the use of conditional random fields (CRFs) for table extraction, and compares them with hidden Markov models (HMMs).  Unlike HMMs, CRFs support the use of many rich and overlapping layout and language features, and as a result, they perform significantly better.  We show experimental results on plain-text government statistical reports in which tables are located with 92% F1, and their constituent lines are classified into 12 table-related categories with 94% accuracy.  We also discuss future work on undirected graphical models for segmenting columns, finding cells, and classifying them as data cells or label cells. 
Information Extraction with HMMs and Shrinkage| Abstract Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling time series data, and have been applied with success to many language-related tasks such as part of speech tagging, speech recognition, text segmentation and topic detection.  This paper describes the application of HMMs to another language related task|information extraction|the problem of locating textual sub-segments that answer a particular information need.  In our work, the HMM state transition probabilities and word emission probabilities are learned from labeled training data.  As in many machine learning problems, however, the lack of sucient labeled training data hinders the reliability of the model.  The key contribution of this paper is the use of a statistical technique called \shrinkage" that significantly improves parameter estimation of the HMM emission probabilities in the face of sparse training data.  In experiments on seminar announcements and Reuters acquisitions articles, shrinkage is shown to reduce error by up to 40%, and the resulting HMM outperforms a state-of-the-art rule-learning system. 
Learning to Use Selective Attention and Short-Term Memory in Sequential Tasks| Abstract This paper presents U-Tree, a reinforcement learning algorithm that uses selective attention and shortterm memory to simultaneously address the intertwined problems of large perceptual state spaces and hidden state.  By combining the advantages of work in instance-based (or "memory-based") learning and work with robust statistical tests for separating noise from task structure, the method learns quickly, creates only task-relevant state distinctions, and handles noise well.  U-Tree uses a tree-structured representation, and is related to work on Prediction Suffix Trees [Ron et al. , 1994] , Parti-game [Moore, 1993] , G-algorithm [Chapman and Kaelbling, 1991] , and Variable Resolution Dynamic Programming [Moore, 1991] .  It builds on Utile Suffix Memory [McCallum, 1995c] , which only used short-term memory, not selective perception.  The algorithm is demonstrated solving a highway driving task in which the agent weaves around slower and faster traffic.  The agent uses active perception with simulated eye movements.  The environment has hidden state, time pressure, stochasticity, over 21,000 world states and over 2,500 percepts.  From this environment and sensory system, the agent uses a utile distinction test to build a tree that represents depththree memory where necessary, and has just 143 internal states---far fewer than the 2500 3 states that would have resulted from a fixed-sized history-window approach. 
Object Consolodation by Graph Partitioning with a Conditionally-Trained Distance Metric| ABSTRACT Coreference analysis, also known as record linkage, object consolidation or identity uncertainty, is a difficult and important problem in natural language processing, databases, citation matching and many other tasks.  This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models.  Unlike many historical approaches to coreference, the models presented here are relational---they do not assume that pairwise coreference decisions should be made independently from each other.  Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies--paralleling the advantages of conditional random fields over hidden Markov models.  We present positive results on noun coreference in two standard text data sets. 
Building Domain-Specific Search Engines with Machine Learning Techniques| Abstract Domain-specific search engines are growing in popularity because they offer increased accuracy and extra functionality not possible with the general, Web-wide search engines.  For example, www. campsearch. com allows complex queries by age-group, size, location and cost over summer camps.  Unfortunately these domain-specific search engines are difficult and timeconsuming to maintain.  This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines.  We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, identifying informative text segments, and populating topic hierarchies.  Using these techniques, we have built a demonstration system: a search engine for computer science research papers.  It already contains over 50,000 papers and is publicly available at www. cora. justresearch. com. 
Semi-supervised Clustering with User Feedback| Abstract We present a new approach to clustering based on the observation that \it is easier to criticize than to construct. " Our approach of semi-supervised clustering allows a user to iteratively provide feedback to a clustering algorithm.  The feedback is incorporated in the form of constraints which the clustering algorithm attempts to satisfy on future iterations.  These constraints allow the user to guide the clusterer towards clusterings of the data that the user finds more useful.  We demonstrate semi-supervised clustering with a system that learns to cluster news stories from a Reuters data set. 
Text Classification from Labeled and Unlabeled Documents using EM|
Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering|
Automating the Construction of Internet Portals with Machine Learning|
Reinforcement Learning with Selective Perception and Hidden State|
Mallet: A machine learning for language toolkit|
Learning to use selective attention and short-term memory in sequential tasks|
Using transitional proximity for faster reinforcement learning|
Efficiently Inducing Features of Conditional Random Fields|
MONTE: An automated Monte Carlo based approach to nuclear magnetic resonance assignment of proteins|
and Se'an Slattery| Learning to extract symbolic knowledge from the World Wide Web. 
Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State,|
A basic introduction to the two flavors of naive Bayes document classification|
Cora Research Paper Search|
Overcoming Incomplete Perception with Util Distinction Memory|
Collective segmentation and labeling of distant entities in information extraction|
Using machine learning techniques to build domain-specific search engines|
Factorial conditional random fields|
Combining classifiers in text categorization|
Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State|
Conference on Machine Learning (ICML-|
Learning to use selective attention and short-term memory|
Instance-Based State Identification for Reinforcement Learning|
Training algorithms for linear text classifiers|
Distributional clustering for text classification|

Matrix momentum for practical natural gradient learning| Abstract.  An on-line learning rule, based on the introduction of a matrix momentum term, is presented, aimed at alleviating the computational costs of standard natural gradient learning.  The new rule, natural gradient matrix momentum, is analysed in the case of two-layer feed-forward neural network learning via methods of statistical physics.  It appears to provide a practical algorithm that performs as well as standard natural gradient descent in both the transient and asymptotic regimes but with a hugely reduced complexity. 
Optimisation of on--line principal component analysis| Abstract Various techniques, used to optimise on-line principal component analysis, are investigated by methods of statistical mechanics.  These include local and global optimisation of node-dependent learning-rates which are shown to be very efficient in speeding up the learning process.  They are investigated further for gaining insight into the learning rates' timedependence, which is then employed for devising simple practical methods to improve training performance.  Simulations demonstrate the benefit gained from using the new methods. 
Magnetization Enumerator for LDPC Codes a Statistical Physics Approach| Abstract We propose a method to determine the critical noise level for decoding Gallager type low density parity check error correcting codes.  The method is based on the magnetization enumerator (M), rather than on the weight enumerator (W) presented recently in the information theory literature.  The interpretation of our method is appealingly simple, and the relation between the different decoding schemes such as typical pairs decoding, MAP, and finite temperature decoding (MPM) becomes clear.  Our results are more optimistic than those derived via the methods of information theory and are in excellent agreement with recent results from another statistical physics approach. 
Finite size effects in on-line learning of multi-layer neural networks| Abstract We complement recent advances in thermodynamic limit analyses of mean on-line gradient descent learning dynamics in multi-layer networks by calculating fluctuations possessed by finite dimensional systems.  Fluctuations from the mean dynamics are largest at the onset of specialisation as student hidden unit weight vectors begin to imitate specific teacher vectors, increasing with the degree of symmetry of the initial conditions.  In light of this, we include a term to stimulate asymmetry in the learning process, which typically also leads to a significant decrease in training time.  An attractive feature of neural networks is their ability to learn a parametrised rule from a set of input/output training examples, by which the parameters of the network are adapted to minimise an error measuring the misfit of the network mapping on the training examples.  Different approaches to the learning process are typically evaluated by the expected error that the network will make on a randomly presented input example.  In on-line learning, statistical mechanics plays a strong role in calculating this generalisation error (see [1, 2, 4] and refs.  within) through self-averaging in the thermodynamic limit, for which an understanding of finite size effects would benefit further advances.  Connections to alternative finite dimensional methods (see [3] and refs.  within) will be pointed to in the course of our analysis.  In on-line learning, the weights parametrising the student network are successively updated according to the error incurred on a single example from a stream of input/output examples, f; ()g, generated by a teacher network (\Delta).  We assume that the teacher network the student attempts to learn is a soft committee machine[1, 4] of N inputs, and M hidden units, this being a one hidden layer network with weights connecting each hidden to output unit set to +1, and with each hidden unit n connected to all input units by B n (n = 1::M).  Explicitly, for the N dimensional training input vector , the output of the teacher is given by, i = M X n=1 g(B n \Delta ); (1) where g(x) is the activation function of the hidden units, and we take g(x) = erf(x= p 2).  The teacher generates a stream of training examples (; i ), with input components drawn from a normal distribution of zero mean, unit variance.  The student network that attempts to learn the teacher, by fitting the training examples, is also a soft committee machine, but with K hidden units.  For input , the student output is, oe(J; ) = K X i=1 g(J i \Delta ); (2) where the student weights J = fJ i g(i = 1::K) are sequentially modified to reduce the error that the student makes on an input , ffl(J; ) = 1 2 (oe(J; ) \Gamma i ) 2 = 1 2 / K X i=1 g(x i ) \Gamma M X n=1 g(y n ) ! 2 ; (3) where the activations are defined x i = J i \Delta , and y n = B n \Delta .  Gradient descent on the error(3) results in an update of the student weight vectors, J +1 = J \Gamma j N ffi i ; (4) where, ffi i = g 0 (x i ) 2 4 M X n=1 g(y n ) \Gamma K X j=1 g(x j ) 3 5 ; (5) and g 0 is the derivative of the activation function g.  The typical performance of the student on a randomly selected input example is given by the generalisation error, ffl g = hffl(J; )i, where h::i represents an average over the gaussian input distribution.  One finds that ffl g depends only on the order-parameters, R in = J i \Delta Bn , Q ij = J i \Delta J j , and Tnm = Bn \Delta Bm (i; j = 1::K; n; m = 1::M)[4], for which, using (4), we derive (stochastic) update equations, R +1 in\Gamma R in = j N ffi i y n ; (6) Q +1 ik\Gamma Q ik = j N \Gamma ffi i x j + ffi k x i \Delta + j 2 N 2 ffi i ffi k \Delta : (7) We average over the input distribution to obtain deterministic equations for the mean values of the order parameters, which are self-averaging in the thermodynamic limit, N!1.  The order-parameter approach contrasts with approaches which analyze the dynamics of the individual weight components, based upon approximate Fokker-Plank equations (see [3] and refs.  within).  The advantage of the order-parameter approach is that the system is modelled exactly in the thermodynamic limit, with only a small number of equations.  In this work we present a more realistic treatment by calculating the dynamic fluctuations induced
Learning from queries for maximum information gain in imperfectly learnable problems| Abstract In supervised learning, learning from queries rather than from random examples can improve generalization performance significantly.  We study the performance of query learning for problems where the student cannot learn the teacher perfectly, which occur frequently in practice.  As a prototypical scenario of this kind, we consider a linear perceptron student learning a binary perceptron teacher.  Two kinds of queries for maximum information gain, i. e. , minimum entropy, are investigated: Minimum student space entropy (MSSE) queries, which are appropriate if the teacher space is unknown, and minimum teacher space entropy (MTSE) queries, which can be used if the teacher space is assumed to be known, but a student of a simpler form has deliberately been chosen.  We find that for MSSE queries, the structure of the student space determines the efficacy of query learning, whereas MTSE queries lead to a higher generalization error than random examples, due to a lack of feedback about the progress of the student in the way queries are selected. 
Message passing for task redistribution on sparse graphs| Abstract The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics.  An efficient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results. 
LETTER TO THE EDITOR Globally optimal on-line learning rules for multi-layer neural networks| Abstract.  We present a method for determining the globally optimal on-line learning rule for a soft committee machine under a statistical mechanics framework.  This rule maximizes the total reduction in generalization error over the whole learning process.  A simple example demonstrates that the locally optimal rule, which maximizes the rate of decrease in generalization error, may perform poorly in comparison. 
The Dynamics of Matrix Momentum| Abstract We analyse the matrix momentum algorithm, which provides an efficient approximation to on-line Newton's method, by extending a recent statistical mechanics framework to include second order algorithms.  We study the efficacy of this method when the Hessian is available and also consider a practical implementation which uses a single example estimate of the Hessian.  The method is shown to provide excellent asymptotic performance, although the single example implementation is sensitive to the choice of training parameters.  We conjecture that matrix momentum could provide efficient matrix inversion for other second order algorithms. 
Weight vs| Magnetization Enumerator for Gallager Codes.  Abstract.  We propose a method to determine the critical noise level for decoding Gallager type low density parity check error correcting codes.  The method is based on the magnetization enumerator (M), rather than on the weight enumerator (W) presented recently in the information theory literature.  The interpretation of our method is appealingly simple, and the relation between the different decoding schemes such as typical pairs decoding, MAP, and finite temperature decoding (MPM) becomes clear.  Our results are more optimistic than those derived via the methods of information theory and are in excellent agreement with recent results from another statistical physics approach. 
On-line learning in soft committee machines,|
A statistical-mechanics analysis of coded CDMA with regular LDPC codes,|
cond-mat/0010073|
cond-mat/0010173|
Belief propagation vs| TAP for decoding corrupted messages. 
General gaussian priors for improved generalization|
Finite size effects and optimal test set size in linear perceptrons|
Typical Performance of Gallager-type Error-Correcting Codes: Phys|
Error-correcting Code on a Cactus - a Solvable Model:|
Tighter Decoding Reliability Bound for Gallager's ErrorCorrecting Code:|
On-Line learning in oft committee machines|
Exact solution for on-line learning in multilayer neural networks|
Typical performance of Gallager-type error-correcting codes|
Learning with Noise and Regularizers in Multilayer Neural Networks|
Efficient training of recurrent neural network with time delays|
Statistical Physics of Low Density Parity Check Error Correcting Codes|
Critical noise levels for LDPC decoding|
Training a Neural Network with Ternary Weights Using the CHIR Algorithm|
], Test Error fluctuations in finite linear perceptrons,|
The TAP approach to intensive and extensive connectivity systems,"|
Natural gradient descent for on-line learning' (|
Dynamics of On-Line Gradient Descent Learning for Multilayer Neural Networks|
Incorporating curvature information into on-line learning'|
Matrix momentum for practical natural gradient learning|
Statistical mechanics of error-correcting codes|
The Belief in TAP|
On-Line Learning in Neural Networks",|
Dynamics of Supervised Learning with Restricted Training Sets|
Learning by choice of internal representation: an energy minimization approach|
Training recurrent neural networks via trajectory modification|

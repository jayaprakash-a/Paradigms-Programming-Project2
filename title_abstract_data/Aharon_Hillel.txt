Spike Sorting: Bayesian Clustering of Non-Stationary Data| Abstract Spike sorting involves clustering spike trains recorded by a microelectrode according to the source neuron.  It is a complicated problem, which requires a lot of human labor, partly due to the non-stationary nature of the data.  We propose an automated technique for the clustering of non-stationary Gaussian sources in a Bayesian framework.  At a first search stage, data is divided into short time frames and candidate descriptions of the data as a mixture of Gaussians are computed for each frame.  At a second stage transition probabilities between candidate mixtures are computed, and a globally optimal clustering is found as the MAP solution of the resulting probabilistic model.  Transition probabilities are computed using local stationarity assumptions and are based on a Gaussian version of the Jensen-Shannon divergence.  The method was applied to several recordings.  The performance appeared almost indistinguishable from humans in a wide range of scenarios, including movement, merges, and splits of clusters. 
Computing Gaussian Mixture Models with EM using Side-Information| Abstract Estimation of Gaussian mixture models is an efficient and popular technique for clustering and density estimation.  An EM procedure is widely used to estimate the model parameters.  In this paper we show how side information in the form of equivalence constraints can be incorporated into this procedure, leading to improved clustering results.  Equivalence constraints are prior knowledge concerning pairs of data points, indicating if the points arise from the same source (positive constraint) or from different sources (negative constraint).  Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others.  We present a closed form EM procedure for handling positive constraints, and a Generalized EM procedure using a Markov net for the incorporation of negative constraints.  Using publicly available data sets we demonstrate that such side information may lead to considerable improvement in clustering tasks, and that our algorithm is preferable to another suggested method using this type of side information. 
Boosting margin based distance functions for clustering| Abstract The performance of graph based clustering methods critically depends on the quality of the distance function used to compute similarities between pairs of neighboring nodes.  In this paper we learn distance functions by training binary classifiers with margins.  The classifiers are defined over the product space of pairs of points and are trained to distinguish whether two points come from the same class or not.  The signed margin is used as the distance value.  Our main contribution is a distance learning method (DistBoost), which combines boosting hypotheses over the product space with a weak learner based on partitioning the original feature space.  Each weak hypothesis is a Gaussian mixture model computed using a semi-supervised constrained EM algorithm, which is trained using both unlabeled and labeled data.  We also consider SVM and decision trees boosting as margin based classifiers in the product space.  We experimentally compare the margin based distance functions with other existing metric learning methods, and with existing techniques for the direct incorporation of constraints into various clustering algorithms.  Clustering performance is measured on some benchmark databases from the UCI repository, a sample from the MNIST database, and a data set of color images of animals.  In most cases the DistBoost algorithm significantly and robustly outperformed its competitors. 
Learning Distance Functions using Equivalence Relations| Abstract We address the problem of learning distance metrics using side-information in the form of groups of "similar" points.  We propose to use the RCA algorithm, which is a simple and efficient algorithm for learning a full ranked Mahalanobis metric (Shental et al. , 2002).  We first show that RCA obtains the solution to an interesting optimization problem, founded on an information theoretic basis.  If the Mahalanobis matrix is allowed to be singular, we show that Fisher's linear discriminant followed by RCA is the optimal dimensionality reduction algorithm under the same criterion.  We then show how this optimization problem is related to the criterion optimized by another recent algorithm for metric learning (Xing et al. , 2002), which uses the same kind of side information.  We empirically demonstrate that learning a distance metric using the RCA algorithm significantly improves clustering performance, similarly to the alternative algorithm.  Since the RCA algorithm is much more efficient and cost effective than the alternative, as it only uses closed form expressions of the data, it seems like a preferable choice for the learning of full rank Mahalanobis distances. 
LEIBNIZ CENTER FOR RESEARCH IN COMPUTER SCIENCE TECHNICAL REPORT 2003-43 Computing Gaussian Mixture Models with EM using Equivalence Constraints| Abstract Gaussian mixture models for density estimation are usually estimated in an unsupervised manner, using an Expectation Maximization (EM) procedure.  In this paper we show how equivalence constraints can be incorporated into this procedure, leading to improved model estimation and improved clustering results.  Equivalence constraints provide additional information on pairs of data points, indicating if the points arise from the same source (positive constraint) or from different sources (negative constraint).  Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others.  We present a closed form EM procedure for handling positive constraints, and a Generalized EM procedure using a Markov network for the incorporation of negative constraints.  Using publicly available data sets, we demonstrate that incorporating equivalence constraints leads to considerable improvement in clustering performance, and that our algorithm outperforms all available competitors. 
Learning via Equivalence Constraints, with applications to the Enhancement of Image and Video Retrieval| Abstract Making sense of large amounts of unlabeled data is hard, but this is what we are up against in
Learning Distance Functions with Product Space Boosting| Abstract A good distance function is an essential tool in applications which involve querying large databases, such as image retrieval and bioinformatics.  We describe a non-parametric algorithm for distance function learning which is based on the boosting of low grade weak learners in a product space.  The algorithm learns a function defined over pairs of points, using supervision in the form of equivalence constraints.  The weak learners are based on partitioning the original feature space, using a generic density estimation generative model (GMM) augmented by equivalence constraints on pairs of datapoints.  Using a number of databases from the UCI repository, we show significantly improved results over methods which learn the parametric Mahalanobis distance.  We also show initial results of image retrieval, using a large database of facial images (YaleB). 
Learning Distance Functions for Image Retrieval| Abstract Image retrieval critically relies on the distance function used to compare a query image to images in the database.  We suggest to learn such distance functions by training binary classifiers with margins, where the classifiers are defined over the product space of pairs of images.  The classifiers are trained to distinguish between pairs in which the images are from the same class and pairs which contain images from different classes.  The signed margin is used as a distance function.  We explore several variants of this idea, based on using SVM and Boosting algorithms as product space classifiers.  Our main contribution is a distance learning method which combines boosting hypotheses over the product space with a weak learner based on partitioning the original feature space.  The weak learner used is a Gaussian mixture model computed using a constrained EM algorithm, where the constraints are equivalence constraints on pairs of data points.  This approach allows us to incorporate unlabeled data into the training process.  Using some benchmark databases from the UCI repository, we show that our margin based methods significantly outperform existing metric learning methods, which are based on learning a Mahalanobis distance.  We then show comparative results of image retrieval in a distributed learning paradigm, using two databases: a large database of facial images (YaleB), and a database of natural images taken from a commercial CD.  In both cases our GMM based boosting method outperforms all other methods, and its generalization to unseen classes is superior. 
Enhancing Image and Video Retrieval: Learning via Equivalence Constraint| Abstract This paper is about learning using partial information in the form of equivalence constraints.  Equivalence constraints provide relational information about the labels of data points, rather than the labels themselves.  Our work is motivated by the observation that in many real life applications partial information about the data can be obtained with very little cost.  For example, in video indexing we may want to use the fact that a sequence of faces obtained from successive frames in roughly the same location is likely to contain the same unknown individual.  Learning using equivalence constraints is different from learning using labels and poses new technical challenges.  In this paper we present three novel methods for clustering and classification which use equivalence constraints.  We provide results of our methods on a distributed image querying system that works on a large facial image database, and on the clustering and retrieval of surveillance data.  Our results show that we can significantly improve the performance of image retrieval by taking advantage of such assumptions as temporal continuity in the data.  Significant improvement is also obtained by making the users of the system take the role of distributed teachers, which reduces the need for expensive labeling by paid human labor. 
Learning with Equivalence Constraints and the Relation to Multiclass Learning|
On the Stability of Coupled Chemical Oscillators,|
Weinshall Learning with Equivalence Constraints, and the relation to Multiclass|
Adaptive low complexity algorithm for image zooming at fractional scaling ratio,|

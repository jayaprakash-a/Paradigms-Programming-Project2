A Generalized Representer Theorem| Abstract Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples.  We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a support vector kernel.  The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space. 
Predicting Time Series with Support Vector Machines| Abstract.  Support Vector Machines are used for time series prediction and compared to radial basis function networks.  We make use of two different cost functions for Support Vectors: training with (i) an ffl insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models.  Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D).  In both cases Support Vector Machines show an excellent performance.  In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 37%. 
Prior Knowledge in Support Vector Kernels| Abstract We explore methods for
Minimal Kernel Classifiers| Abstract A finite concave minimization algorithm is proposed for constructing kernel classifiers that use a minimal number of data points both in generating and characterizing a classifier.  The algorithm is theoretically justified on the basis of linear programming perturbation theory and a leave-one-out error bound as well as eective computational results on seven real world datasets.  A nonlinear rectangular kernel is generated by systematically utilizing as few of the data as possible both in training and in characterizing a nonlinear separating surface.  This can result in substantial reduction in kernel data-dependence (over 94% in six of the seven public datasets tested on) and with test set correctness equal to that obtained by using a conventional support vector machine classifier that depends on many more data points.  This reduction in data dependence results in a much faster classifier that requires less storage.  To eliminate data points, the proposed approach makes use of a novel loss function, the "pound" function () # , which is a linear combination of the 1-norm and the step function that measures both the magnitude and the presence of any error. 
Kernel PCA and De-Noising in Feature Spaces| Abstract Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms.  But it can also be considered as a natural generalization of linear principal component analysis.  This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA.  This is a nontrivial task, as the results provided by kernel PCA live in some high dimensional feature space and need not have pre-images in input space.  This work presents ideas for finding approximate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data. 
Entropy Numbers of Linear Function Classes| Abstract This paper collects together a miscellany of results originally motivated by the analysis of the generalization performance of the "maximum-margin" algorithm due to Vapnik and others.  The key feature of the paper is its operator-theoretic viewpoint.  New bounds on covering numbers for classes related to Maximum Margin classes are derived directly without making use of a combinatorial dimension such as the VC-dimension.  Specific contents of the paper include: # a new and self-contained proof of Maurey's theorem and some generalizations with small explicit values of constants; # bounds on the covering numbers of maximum margin classes suitable for the analysis of their generalization performance; # the extension of such classes to those induced by balls in quasi-Banach spaces (such as ` p norms with 0 < p < 1).  # extension of results on the covering numbers of convex hulls of basis functions to p-convex hulls (0 < p # 1); # an appendix containing the tightest known bounds on the entropy numbers of the identity operator between ` n p1 and ` n p2 (0 < p 1 < p 2 # 1). 
Kernel Machines and Boolean Functions| Abstract We give results about the learnability and required complexity of logical formulae to solve classification problems.  These results are obtained by linking propositional logic with kernel machines.  In particular we show that decision trees and disjunctive normal forms (DNF) can be represented by the help of a special kernel, linking regularized risk to separation margin.  Subsequently we derive a number of lower bounds on the required complexity of logic formulae using properties of algorithms for generation of linear estimators, such as perceptron and maximal perceptron learning. 
EXTENDED ABSTRACT Engineering Support Vector Machine Kernels That Recognize Translation Initiation Sites| Abstract In order to extract protein sequences from nucleotide sequences, it is an important step to recognize points from which regions encoding proteins start, the so-called translation initiation sites (TIS).  This can be modeled as a classification problem.  We demonstrate the power of support vector machines (SVMs) for this task, and show how to successfully incorporate biological prior knowledge by engineering an appropriate kernel function. 
Large-Scale Multiclass Transduction| Abstract We present a method for performing transductive inference on very large datasets.  Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be performed sufficiently fast.  This holds, for instance, for certain graph and string kernels.  Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. 
-Arc: Ensemble Learning in the Presence of Outliers| Abstract AdaBoost and other ensemble methods have successfully been applied to a number of classification tasks, seemingly defying problems of overfitting.  AdaBoost performs gradient descent in an error function with respect to the margin, asymptotically concentrating on the patterns which are hardest to learn.  For very noisy problems, however, this can be disadvantageous.  Indeed, theoretical analysis has shown that the margin distribution, as opposed to just the minimal margin, plays a crucial role in understanding this phenomenon.  Loosely speaking, some outliers should be tolerated if this has the benefit of substantially increasing the margin on the remaining points.  We propose a new boosting algorithm which allows for the possibility of a pre-specified fraction of points to lie in the margin area or even on the wrong side of the decision boundary. 
Classification on Proximity Data with LP--Machines| Abstract We provide a new linear program to deal with classification of data in the case of data given in terms of pairwise proximities.  This allows to avoid the problems inherent in using feature spaces with indefinite metric in Support Vector Machines, since the notion of a margin is purely needed in input space where the classification actually occurs.  Moreover in our approach we can enforce sparsity in the proximity representation by sacrificing training error.  This turns out to be favorable for proximity data.  Similar to --SV methods, the only parameter needed in the algorithm is the (asymptotical) number of data points being classified with a margin.  Finally, the algorithm is successfully compared with --SV learning in proximity space and K--nearest-neighbors on real world data from Neuroscience and molecular biology. 
Query Learning with Large Margin Classifiers| Abstract The active selection of instances can significantly improve the generalisation performance of a learning machine.  Large margin classifiers such as support vector machines classify data using the most informative instances (the support vectors).  This makes them natural candidates for instance selection strategies.  In this paper we propose an algorithm for the training of support vector machines using instance selection.  We give a theoretical justification for the strategy and experimental results on real and artificial data demonstrating its effectiveness.  The technique is most efficient when the data set can be learnt using few support vectors. 
SVMs---a practical consequence of learning theory| Is there anything worthwhile to learn about the new SVM algorithm, or does it fall into the category of "yet-another-algorithm," in which case readers should stop here and save their time for something more useful? In this short overview, I will try to argue that studying support-vector learning is very useful in two respects.  First, it is quite satisfying from a theoretical point of view: SV learning is based on some beautifully simple ideas and provides a clear intuition of what learning from examples is about.  Second, it can lead to high performances in practical applications.  In the following sense can the SV algorithm be considered as lying at the intersection of learning theory and practice: for certain simple types of algorithms, statistical learning theory can identify rather precisely the factors that need to be taken into account to learn successfully.  Real-world applications, however, often mandate the use of more complex models and algorithms---such as neural networks---that are much harder to analyze theoretically.  The SV algorithm achieves both.  It constructs models that are complex enough: it contains a large class of neural nets, radial basis function (RBF) nets, and polynomial classifiers as special cases.  Yet it is simple enough to be analyzed mathematically, because it can be shown to correspond to a linear method in a high-dimensional feature space nonlinearly related to input space.  Moreover, even though we can think of it as a linear algorithm in a high-dimensional space, in practice, it does not involve any computations in that high-dimensional space.  By the use of kernels, all necessary computations are performed directly in input space.  This is the characteristic twist of SV methods---we are dealing with complex algorithms for nonlinear pattern recognition, 1 regression, 2 or feature extraction, 3 but for the sake of analysis and algorithmics, we can pretend that we are working with a simple linear algorithm.  I will explain the gist of SV methods by describing their roots in learning theory, the optimal hyperplane algorithm, the kernel trick, and SV function estimation.  For details and further references, see Vladimir Vapnik's authoritative treatment, 2 the collection my colleagues and I have put together, 4 and the SV Web page at
Fast Kernels for String and Tree Matching| Abstract In this paper we present a new algorithm suitable for matching discrete objects such as strings and trees in linear time, thus obviating dynamic programming with quadratic time complexity.  Furthermore, prediction cost in many cases can be reduced to linear cost in the length of the sequence to be classified, regardless of the number of support vectors.  This improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner. 
Frame, Reproducing Kernel, Regularization and Learning| Abstract This work deals with a method for building Reproducing Kernel Hilbert Space (RKHS) from a Hilbert space with frame elements having special properties.  Conditions on existence and a method of construction are given.  Then, these RKHS are used within the framework of regularization theory for function approximation.  Implications on semiparametric estimation are discussed and a multiscale scheme of regularization is also proposed.  Results on toy and real-world approximation problems illustrate the effectiveness of such methods. 
Exponential Families for Conditional Random Fields| Abstract In this paper we define conditional random fields in reproducing kernel Hilbert spaces and show connections to Gaussian Process classification.  More specifically, we prove decomposition results for undirected graphical models and we give constructions for kernels.  Finally we present efficient means of solving the optimization problem using reduced rank decompositions and we show how stationarity can be exploited efficiently in the optimization process. 
Shrinking the Tube: A New Support Vector Regression Algorithm| Abstract A new algorithm for Support Vector regression is described.  For a priori chosen , it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction of the data points lie outside.  Moreover, it is shown how to use parametric tube shapes with non-constant radius.  The algorithm is analysed theoretically and experimentally. 
Sample Based Generalization Bounds| Abstract It is known that the covering numbers of a function class on a double sample (length 2m, where m is the number of points in the sample) can be used to bound the generalization performance of a classifier by using a margin based analysis.  Traditionally this has been done using a ``Sauer-like" relationship involving a combinatorial dimension such as the fat-shattering dimension.  In this paper we show that one can utilize an analogous argument in terms of the observed covering numbers on a single m-sample (being the actual observed data points).  The significance of this is that for certain interesting classes of functions, such as support vector machines, one can readily estimate the empirical covering numbers quite well.  We show how to do so in terms of the eigenvalues of the Gram matrix created from the data.  These covering numbers can be much less than a priori bounds indicate in situations where the particular data received is ``easy".  The work can be considered an extension of previous results which provided generalization performance bounds in terms of the VC-dimension of the class of hypotheses restricted to the sample, with the considerable advantage that the covering numbers can be readily computed, and they often are small. 
Natural Regularization in SVMs| Abstract We provide a regularization-theoretic analysis of a class of SV kernels {called natural kernels{ based on generative models with density p(xj#), such as the Fisher kernel proposed in [5].  In particular, we show that the latter corresponds to a regularization operator (prior) penalizing the L 2 (p)-norm of the estimated function.  Moreover, we compute the corresponding eigendecompositions and show that they isotropically map the input space into feature space. 
Regularized Principal Manifolds| Abstract Many settings of unsupervised learning can be viewed as quantization problems - the minimization of the expected quantization error subject to some restrictions.  This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning.  This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding.  We explore this connection in two ways: (1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm.  In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators.  Experimental results demonstrate the feasibility of the approach. 
Invariances in Classification : an efficient SVM implementation| Abstract.  Often, in pattern recognition, complementary knowledge is available.  This could be useful to improve the performance of the recognition system.  Part of this knowledge regards invariances, in particular when treating images or voice data.  Many approaches have been proposed to incorporate invariances in pattern recognition systems.  Some of these approaches require a pre-processing phase, others integrate the invariances in the algorithms.  We present a unifying formulation of the problem of incorporating invariances into a pattern recognition classifier and we extend the SimpleSVM algorithm [Vishwanathan et al. , 2003] to handle invariances efficiently. 
Gaussian process classification for segmenting and annotating sequences| Abstract Multiclass classification refers to the problem of assigning labels to instances where labels belong to some finite set of elements.  Often, however, the instances to be labeled do not occur in isolation, but rather in observation sequences.  One is then interested in predicting the joint label configuration, i. e.  the sequence of labels, using models that take possible interdependencies between label variables into account.  This scenario subsumes problems of sequence segmentation and annotation.  In this paper, we investigate the use of Gaussian Process (GP) classification for label sequences. 
svlab -- A Kernel Methods Package| by the user in a very modular way.  Based on this infrastructure kernel-based methods can be easily be constructed and developed. 
Robust Ensemble Learning for Data Mining| Abstract.  We propose a new boosting algorithm which similarly to #Support-Vector Classification allows for the possibility of a pre-speci#ed fraction # of points to lie in the margin area or even on the wrong side of the decision boundary.  It gives a nicely interpretable way of controlling the trade-off between minimizing training error and capacity.  Furthermore, it can act as a filter for finding and selecting informative patterns from a database. 
Online Learning with Kernels| Abstract We consider online learning in a Reproducing Kernel Hilbert Space.  Our method is computationally efficient and leads to simple algorithms.  In particular we derive update equations for classification, regression, and novelty detection.  The inclusion of the #-trick allows us to give a robust parameterization.  Moreover, unlike in batch learning where the #-trick only applies to the "-insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber's robust loss. 
Multi-instance kernels|
Estimating the Support of a High-Dimensional Distribution|
New Support Vector Algorithms|
Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators|
Nonlinear component analysis as a kernel eigenvalue problem",|
Nonlinear Component Analysis as a Kernel Eigenvalue Problem|
Support Vector Method for Function Approximation, Regression Estimation and Signal Processing|
Using support vector machines for time series prediction|
A Tutorial on Support Vector Regression, NeuroCOLT|
Sparse Greedy Matrix Approximation for Machine Learning|
Using support vector machines for time series prediction|
Support Vector Regression Machines|
Natural regularization from generative models|
Learning with Kernels|
Regression estimation with support vector learning machines|
Support vector machine reference manual|
Learning with Kernels: Support Vector Machines, Regularization,|
#-Arc: ensemble learning in the presence of noise,|
Support vector regression with automatic accuracy control|
Generalization bounds for convex combinations of kernel functions|
On a Kernel-Based Method for Pattern Recognition, Regression, Approximation, and Operator Inversion|
SV estimation of a distribution's support|
General cost functions for support vector regression|
Large Margin Classification for Moving Targets|
Robust ensemble learning|
Support Vector Method for Novelty Detection|
Sparse kernel feature analysis|
Lernen mit Kernen,|
Input space vs| feature space in kernel-based methods. 
Kernel-dependent support vector error bounds|
Advances in kernel methods---support vector learning|
Prior Knowledge in Support Vector Kernels|
Predicting Time Series with Support Vector Machines|
Invariant Feature Extraction and Classification in Kernel Spaces|
Sparse Greedy Gaussian Process Regression|
Estimating the support of a high-dimensional distribution|
Monoclonal antibodies against gastric H11 K1 ATPase|
A Maximum Margin Miscellany|
Introduction to support vector learning|
Kernels and dynamical systems|
Fast kernels on strings and trees|
Kernels and Regularization on Graphs|
Fast approximation of support vector kernel expansions, and an interpretation of clustering as approximation in feature spaces|
Advances|
Linear support vector regression machines,|
Entropy numbers for convex combinations and MLPs|
Asymptotically optimal choice of ffl-loss for support vector machines|
Generalization Bounds via Eigenvalues of the Gram Matrix," NeuroCOLT,|
Learning with kernels,|
Kernel principal component analysis|
Learning with Kernels The MIT press,|
SimpleSVM|
Semiparametric Support Vector and Linear Programming Machines|
Cholesky factorization for rank-k modifications of diagonal matrices|
From Regularization Operators to Support Vector Kernels|
Interior point optimizer for SVM pattern recognition|
Entropy Numbers, Operators and Support Vector Kernels|
Face recognition from long-term observations|
Shrinking the Tube: A New Support Vector Regression Algorithm|
Scholkopf B, "A Tutorial on Support Vector Regression",|
Learning with kernel,"|
Norm-based regularization of boosting|
Quantization functionals and regularized principal manifolds|
Generalization Bounds and Learning Rates for Regularized Principal Manifolds,|
Regularised principal manifolds,|
Linear programs for automatic accuracy control in regression|
Kernal principal component analysis|
Exponential families and kernels|
Predicting time series with support vector machines|
Superkernels|
Kernel Principal Component Analysis|
Learning the kernel with hyperkernels,|
Machine Learning with Hyperkernels|
Kernel PCA for feature extraction and denoising in feature space|
New support vector algorithms, neurocolt2|
Muller|
pr_LOQO,|
Kernel method for percentile feature extraction|
pr LOQO optimizer, available at <www|
pr loqo optimizer,|
A generalized representer theorem|
A generalized representer theorem, In \Proceedings of the 14th Annual Conference on Computational Learning Theory",|
Regularization with Dot-Product Kernels|
Modified hausdordistance for object matching|
Vapnik V, "Predicting Time Series with Support Vector Machines",|
Hyperkernels|
B Scholkopf, and K R Muller|
Behaviour and convergence of the constrained covariance|
Measuring statistical dependence with Hilbert-Schmidt norms|
and Klaus-Robert Muller| "Support Vector Methods in Learning and Feature Extraction. 
Introduction to large margin classifiers|
Constructing Descriptive and Discriminative Nonlinear Features: Rayleigh Coefficients in Kernel Feature Spaces|

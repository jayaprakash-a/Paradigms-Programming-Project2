Point-based value iteration: An anytime algorithm for POMDPs| Abstract This paper introduces the Point-Based Value Iteration (PBVI) algorithm for POMDP planning.  PBVI approximates an exact value iteration solution by selecting a small set of representative belief points and then tracking the value and its derivative for those points only.  By using stochastic trajectories to choose belief points, and by maintaining only one value hyper-plane per point, PBVI successfully solves large problems: we present results on a robotic laser tag problem as well as three test domains from the literature. 
Visibility-Based Pursuit-Evasion with Limited Field of View| Abstract We study a form of the pursuit-evasion problem, in which one or more searchers must move through a given environment so as to guarantee detection of any and all evaders, which can move arbitrarily fast.  Our goal is to develop techniques for coordinating teams of robots to execute this task in
Approximate Solutions for Partially Observable Stochastic Games with Common Payoffs| Abstract Partially observable decentralized decision making in robot teams is fundamentally different from decision making in fully observable problems.  Team members cannot simply apply single-agent solution techniques in parallel.  Instead, we must turn to game theoretic frameworks to correctly model the problem.  While partially observable stochastic games (POSGs) provide a solution model for decentralized robot teams, this model quickly becomes intractable.  We propose an algorithm that approximates POSGs as a series of smaller, related Bayesian games, using heuristics such as ####### to provide the future discounted value of actions.  This algorithm trades off limited look-ahead in uncertainty for computational feasibility, and results in policies that are locally optimal with respect to the selected heuristic.  Empirical results are provided for both a simple problem for which the full POSG can also be constructed, as well as more complex, robot-inspired, problems. 
Auction Mechanism Design for Multi-Robot Coordination| Abstract The design of cooperative multi-robot systems is a highly active research area in robotics.  Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures.  We propose a new algorithm which joins together these two lines of research.  For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy.  We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm.  We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. 
Online Fitted Reinforcement Learning| Abstract My paper in the main portion of the conference deals with fitted value iteration or Qlearning for offline problems, i. e. , those where we have a model of the environment so that we can examine arbitrary transitions in arbitrary order.  The same techniques also allow us to do Q-learning for an online problem, i. e. , one where we have no model but must instead perform experiments inside the MDP to gather data.  I will describe how. 
Planning under Uncertainty for Reliable Health Care Robotics| Abstract.  We describe a mobile robot system, designed to assist residents of an retirement facility.  This system is being developed to respond to an aging population and a predicted shortage of nursing professionals.  In this paper, we discuss the task of finding and escorting people from place to place in the facility, a task containing uncertainty throughout the problem.  Planning algorithms that model uncertainty well such as Partially Observable Markov Decision Processes (POMDPs) do not scale tractably to real world problems such as the health care domain.  We demonstrate an algorithm for representing real world POMDP problems compactly, which allows us to find good policies in reasonable amounts of time.  We show that our algorithm is able to find moving people in close to optimal time, where the optimal policy starts with knowledge of the person's location. 
Planning for Markov Decision Processes with Sparse Stochasticity (Draft Version)| Abstract Planning algorithms designed for deterministic worlds, such as A* search, usually run much faster than algorithms designed for worlds with uncertain action outcomes, such as value iteration.  Real-world planning problems often exhibit uncertainty, which forces us to use the slower algorithms to solve them.  In particular, planning problems which involve sensing actions always have significant uncertainty: there is no point consulting a sensor if you know the outcome in advance.  However, many sensor planning problems exhibit sparse uncertainty: there are long sequences of deterministic actions which accomplish tasks like moving sensor platforms into place, interspersed with a small number of sensing actions which have uncertain outcomes.  In this paper we describe a new planning algorithm, called MCP (short for MDP Compression Planning), which combines A* search with value iteration.  We present experiments which show that MCP can run substantially faster than competing planners in domains with sparse uncertainty; these experiments are based on a simulation of a ground robot cooperating with a helicopter to fill in a partial map and move to a goal location.  In deterministic planning problems, optimal paths are acyclic: no state is visited more than once.  Because of this property, algorithms like A* search can guarantee that they visit each state in the state space no more than once.  By visiting the states in an appropriate order, it is possible to ensure that we know the exact value of all of a state's possible successors before we visit that state; so, the first time we visit a state we can compute its correct value.  By contrast, if actions have uncertain outcomes, optimal paths may contain cycles: some states will be visited two or more times with positive probability.  Because of these cycles, there is no way to order states so that we determine the values of a state's successors before we visit the state itself.  Instead, the only way to compute state values is to solve a set of simultaneous equations.  In problems with sparse stochasticity, only a small fraction of all states have uncertain outcomes.  It is these few states that cause all of the cycles: while a deterministic state s may participate in a cycle, the only way it can do so is if one of its successors has an action with a stochastic outcome (and only if this stochastic action can lead to a predecessor of s).  In such problems, we would like to build a smaller MDP which contains only states which are related to stochastic actions.  (We will call such an MDP a compressed MDP, and we will call its states distinguished states. ) We could then run fast algorithms like A* (a) Segbot (b) Robotic helicopter (d) Planning map (e) Execution simulation (c) 3D Map Figure 1: Robot-Helicopter Coordination search to plan paths between distinguished states, and reserve slower algorithms like value iteration for deciding how to deal with stochastic outcomes.  There are two problems with such a strategy.  First, there can be a large number of states which are related to stochastic actions, and so it may be impractical to enumerate all of them and make them all distinguished states.  (We would prefer instead to distinguish only states which are likely to be encountered while executing some policy which we are considering. ) Second, there can be a large number of ways to get from one distinguished state to another: edges in the compressed MDP correspond to sequences of actions in the original MDP.  If we knew the values of all of the distinguished states exactly, then we could use A* search to generate optimal paths between them, but since we don't we can't.  In this paper, we will describe an algorithm which incrementally builds a compressed MDP using a sequence of deterministic searches.  It adds states and edges to the compressed MDP only by encountering them along trajectories; so, it never adds irrelevant states or edges to the compressed MDP.  Trajectories are generated by deterministic search, and so undistinguished states are treated only with fast algorithms.  Bellman errors in the values for distinguished states show us where to try additional trajectories, and help us build the relevant parts of the compressed MDP as quickly as possible. 
Reinforcement Learning with Function Approximation Converges to a Region| Abstract Many algorithms for approximate reinforcement learning are not known to converge.  In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point.  This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region.  The algorithms are SARSA(0) and V(0); the latter algorithm was used in the well-known TD-Gammon program. 
Locating moving entities in indoor environments with teams of mobile robots| ABSTRACT This article presents an implemented multi-robot system for playing the popular game of laser tag.  The object of the game is to search for and tag opponents that can move freely about the environment.  The main contribution of this paper is a new particle filter algorithm for tracking the location of many opponents in the presence of pervasive occlusion.  We achieve efficient tracking principally through a clever factorization of our posterior into roles that can be dynamically added and merged.  When searching for opponents, the individual agents greedily maximize their information gain, using a negotiation technique for coordinating their search efforts.  Experimental results are provided, obtained with a physical robot system in large-scale indoor environments and through simulation. 
Exponential Family PCA for Belief Compression in POMDPs| Abstract Standard value function approaches to finding policies for Partially Observable Markov Decision Processes (POMDPs) are intractable for large models.  This intractability of these algorithms is due to a great extent to their generating an optimal policy over the entire belief space.  However, in real POMDP problems most belief states are highly unlikely, and there is a highly structured, low-dimensional manifold of plausible beliefs embedded in the high-dimensional belief space.  We introduce a new method for solving large-scale POMDPs by taking advantage of belief space sparsity.  We reduce the dimensionality of the belief space by exponential family Principal Components Analysis [1], which allows us to turn the sparse, highdimensional belief space into a compact, low-dimensional representation in terms of learned features of the belief state.  We then plan directly on the low-dimensional belief features.  By planning in a low dimensional space, we can find policies for POMDPs that are orders of magnitude larger than can be handled by conventional techniques.  We demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task. 
Policy-contingent abstraction for robust robot control| Abstract This paper presents a scalable control algorithm that enables a deployed mobile robot to make high-level control decisions under full consideration of its probabilistic belief.  We draw on insights from the rich literature of structured robot controllers and hierarchical MDPs to propose PolCA, a hierarchical probabilistic control algorithm which learns both subtask-specific state abstractions and policies.  The resulting controller has been successfully implemented onboard a mobile robotic assistant deployed in a nursing facility.  To the best of our knowledge, this work is a unique instance of applying POMDPs to highlevel robotic control problems. 
Distributed Planning in Hierarchical Factored MDPs| We present a principled and efficient planning algorithm for collaborative multiagent dynamical systems.  All computation, during both the planning and the execution phases, is distributed among the agents; each agent only needs to model and plan for a small part of the system.  Each of these local subsystems is small, but once they are combined they can represent an exponentially larger problem.  The subsystems are connected through a subsystem hierarchy.  Coordination and communication between the agents is not imposed, but derived directly from the structure of this hierarchy.  A globally consistent plan is achieved by a message passing algorithm, where messages correspond to natural local reward functions and are computed by local linear programs; another message passing algorithm allows us to execute the resulting policy.  When two portions of the hierarchy share the same structure, our algorithm can reuse plans and messages to speed up computation. 
Finding Approximate POMDP solutions Through Belief Compression| Abstract Standard value function approaches to finding policies for Partially Observable Markov Decision Processes (POMDPs) are generally considered to be intractable for large models.  The intractability of these algorithms is to a large extent a consequence of computing an exact, optimal policy over the entire belief space.  However, in real-world POMDP problems, computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes.  The beliefs experienced by the controller often lie near a structured, low-dimensional subspace embedded in the high-dimensional belief space.  Finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value function.  We introduce a new method for solving large-scale POMDPs by reducing the dimensionality of the belief space.  We use Exponential family Principal Components Analysis (Collins, Dasgupta, & Schapire, 2002) to represent sparse, high-dimensional belief spaces using small sets of learned features of the belief state.  We then plan only in terms of the low-dimensional belief features.  By planning in this low-dimensional space, we can find policies for POMDP models that are orders of magnitude larger than models that can be handled by conventional techniques.  We demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks. 
Applying Metric-Trees to Belief-Point POMDPs| Abstract Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning.  These approaches operate on sets of belief points by individually learning a value function for each point.  In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property.  This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points.  We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 
Planning in Cost-Paired Markov Decision Process Games| Abstract We describe applications and theoretical results for a new class of two-player planning games.  In these games, each player plans in a separate Markov Decision Process (MDP), but the costs associated with a policy in one of the MDPs depend on the policy selected by the other player.  These costpaired MDPs represent an interesting and computationally tractable subset of adversarial planning problems.  To solve them, we extend the Double Oracle Algorithm of [3]. 
Model Uncertainty in Classical Conditioning| Abstract We develop a framework based on Bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments.  Traditional accounts of conditioning fit parameters within a fixed generative model of reinforcer delivery; uncertainty over the model structure is not considered.  We apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition, two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes.  According to the theory, second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations; conditioned inhibition results when a more complex model is justified by additional experience. 
Better Motion Prediction for People-tracking| Abstract--- An important building block for intelligent mobile robots is the ability to track people moving around in the environment.  Algorithms for person-tracking often incorporate motion models, which can improve tracking accuracy by predicting how people will move.  More accurate motion models produce better tracking because they allow us to average together multiple predictions of the person's location rather than depending entirely on the most recent observation.  Many implemented systems, however, use simple conservative motion models such as Brownian motion (in which the person's direction of motion is independent on each time step).  We present an improved motion model based on the intuition that people tend to follow efficient trajectories through their environments rather than random paths.  Our motion model learns common destinations within the environment by clustering training examples of actual trajectories, then uses a path planner to predict how a person would move along routes from his or her present location to these destinations.  We have integrated this motion model into a particle-filter-based person-tracker, and we demonstrate experimentally that our new motion model performs significantly better than simpler models, especially in situations in which there are extended periods of occlusion during tracking. 
A Learning Algorithm for Localizing People Based on Wireless Signal Strength that Uses Labeled and Unlabeled Data| Abstract This paper summarizes a probabilistic approach for localizing people through the signal strengths of a wireless IEEE 802. 11b network.  Our approach uses data labeled by ground truth position to learn a probabilistic mapping from locations to wireless signals, represented by piecewise linear Gaussians.  It then uses sequences of wireless signal data (without position labels) to acquire motion models of individual people, which further improves the localization accuracy.  The approach has been implemented and evaluated in an office environment. 
ARA*: Anytime A* with Provable Bounds on Sub-Optimality| Abstract In real world planning problems, time for deliberation is often limited.  Anytime planners are well suited for these problems: they find a feasible solution quickly and then continually work on improving it until time runs out.  In this paper we propose an anytime heuristic search, ARA*, which tunes its performance bound based on available search time.  It starts by finding a suboptimal solution quickly using a loose bound, then tightens the bound progressively as time allows.  Given enough time it finds a provably optimal solution.  While improving its bound, ARA* reuses previous search efforts and, as a result, is significantly more efficient than other anytime search methods.  In addition to our theoretical analysis, we demonstrate the practical utility of ARA* with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover. 
Hierarchical Linear Models and Cell Data| Abstract Hierarchical linear models are a generalization of Bayesian linear
Regret Bounds for Prediction Problems| Abstract We present a unified framework for reasoning about worst-case regret bounds for learning algorithms.  This framework is based on the theory of duality of convex functions.  It brings together results from computational learning theory and Bayesian statistics, allowing us to derive new proofs of known theorems, new theorems about known algorithms, and new algorithms.  1 The inference problem We are interested in the following kind of inference problem: on each time step t = 1 : : : T we must choose a prediction vector w t from a set of allowable predictions W .  The interpretation of w t depends on the details of the problem, but for example w t might be our guess at the mean of a sequence of numbers or the coefficients of a linear regression.  Then the loss function l t (w) is revealed to us, and we are penalized l t (w t ). 
Particle Filters for Rover Fault Diagnosis| 1 Introduction This article presents a number of complementary algorithms for detecting faults on-board operating robots, where we define a fault as a deviation from expected behavior.  Experience has shown that even carefully designed and tested robots may encounter faults [3].  One of the reasons for this is that components degrade over time another is that the operators of the robot rarely have complete knowledge of the environment in which it operates and hence may not have accounted for certain situations.  In a number of application domains, such as planetary exploration, search and rescue, mine mapping, nuclear waste cleanup, and demining, robots operate in environments where human intervention is expensive, slow, unreliable, or impossible.  It is therefore essential for the robots to monitor their behavior so that faults may be addressed before they result in catastrophic failures.  This monitoring needs to be efficient since there is limited computational power available on robots.  Not only are robots venturing into areas inaccessible or dangerous for humans, but they are also increasingly becoming a part of day to day life.  It is also important for these robots to detect faults in a timely manner, since failure to do so may result in expensive consequences, both monetary and in terms of consumer trust that may be hard to regain.  If faults go undetected, autonomous robots in real-world environments may behave in an unpredictable or dangerous manner.  On the other hand, detecting and recovering from faults can considerably improve the performance of the robots [2] [81].  Fault Detection and Identification (FDI) for robots is a complex problem.  This is because the space of possible faults is very large, robot sensors, actuators, and environment models are uncertain, and there is limited computation time and power.  Probability theory provides a natural representation of the uncertainty in the rover domain, but an exact Bayesian solution to FDI is intractable.  Traditional methods address this intractability by approximating the problem using linear approximations of rover dynamics and/or by ignoring uncertainty.  Often these approximations are unrealistic, and either faults go undetected, or an unreasonable number of false positives are triggered.  We instead prefer to approximate the solution using Monte Carlo methods.  The rationale behind this is that the two stages of this problem, building models and online monitoring, have very different computational constraints.  Theoretically there is little restriction on the time and computation available for building models, since this may be done offline, but monitoring these models must be done in real time with the limited computation available on a robot.  We consider approximate inference using complex models a better trade-off than exact inference with simple models.  Classical Monte Carlo methods for dynamic systems, such as particle filters, are capable of tracking complex nonlinear systems with noisy measurements.  The problem is that estimates from a particle filter tend to have a high variance for small sample sets.  Using large sample sets is computationally expensive and defeats the purpose.  In this article, we present a number of complementary algorithms for improving the accuracy of FDI with a computationally tractable set of samples in a particle filter.  Experimental results in the rover domain show significant improvement in accuracy over the classical approach.  The algorithms described in this article enable detection of a wider range and larger number of faults during robot operation than has hitherto been possible.  Furthermore, these algorithms provide the probability of the robot being in each of the fault and operational states given the sensor data.  They can handle noisy sensors, non-linear, non-Gaussian models of behavior, and are computationally efficient.  Estimating faults in terms of a probability distribution over all fault states captures the uncertainty in fault identification that results from noisy and insufficient data.  In addition, it allows a lot of flexibility in the types of planners/controllers that may be used for controlling the robot and for recovering from faults.  For example, such distributions are compatible with classical conditional planners or Markov Decision Processes (MDPs), which may use the most likely state to determine which action to take, and also Partially Observable Markov Decision Processes (POMDPs) [67], which use the distribution over the entire state space.  2 Particle Filters for Monitoring Faults Our formulation of the fault detection problem requires estimating robot and environmental state, as it changes over time, from a sequence of sensor measurements that provide noisy, partial information about the state.  The Bayesian approach to dynamic state estimation addresses this problem.  Particle filters have been extensively used for Bayesian state estimation in nonlinear systems with noisy measurements [35][22][15].  They approximate the probability distribution with a set of samples or particles.  The algorithms presented in this article all use particle filters.  Particle filters have a number of characteristics that make them attractive for fault detection on robots: they are non-parametric (can represent arbitrary distributions), can handle hybrid state spaces, can handle noisy sensing and motion, and can easily be extended to an anytime approach where the number of particles (and hence the estimation accuracy) can be adjusted to match available computation.  2. 1 Fault Detection and Identification (FDI) for Robots A fault is defined as a deviation from the expected behavior of the system.  Fault detection is defined as the process of determining that a fault has occurred.  Fault identification is the process of determining exactly which exception or fault occurred [36].  Fault detection and identification are typically passive, i. e.  they do not alter control actions.  This paper concentrates on a class of faults that are relatively difficult to detect because they cannot be inferred from sensor values at a given instance, but require a sequence of time-varying sensor values.  In addition, the expected behavior of the robot may be different in different operating conditions.  For example, high power draw on flat ground may be cause for concern, but high power draw on a slope might be perfectly acceptable.  The faults addressed here include mechanical component failures, such as broken motors and gears; faults due to environmental interactions, such as a wheel stuck against a rock; and sensor failures, such as broken encoders.  2. 2 FDI as recursive state estimation State estimation is the process of determining the state of a system from a sequence of data.  FDI has a natural representation as a state estimation problem.  We represent the possible fault and operational modes of the systems as explicit states.  The sequence of measurements is then used to determine the state of the system.  There are two main classes of state estimation methods: batch estimation methods and recursive estimation methods.  Batch methods treat all the data with equal importance and find an optimal estimate of the state given the entire sequence.  However, full batch estimation is computationally expensive and gets slower as the robot accumulates increasing volumes of data.  It is therefore not suitable for FDI.  Recursive state estimation methods make a Markov assumption, i. e. , the past and future are conditionally independent given the current state.  These methods incorporate the data as it becomes available and replace the data with a statistic.  Estimates at subsequent timesteps use this statistic instead of the history of data for state estimation.  2. 3 Classical particle filter For FDI we concentrate on a discrete time, first order Markov formulation of the state estimation problem.  The state being estimated is hybrid, i. e.  it consists of both discrete and continuous components.  Let } d d { D K 1 # = represent K hidden discrete fault and operational states of the robot, D d t the discrete state of the robot at time t and } d , , d { d t 1 t # the discrete, first order Markov chain representing the evolution of the state over time.  In addition to the discrete states there are also continuous states that track the dynamic behavior of the robot.  Let x n t x denote the multivariate continuous state at time t.  The state of the robot is observed through a sequence of measurements, z n t t t z z z z = }, { 1 # .  Probabilistic models of the change in state over time (state transition model) and the relationship between the measurements and the state (measurement model) capture the inherent noise.  State transitions depend on prior state, observation, and (in some cases) the control action.  These models are assumed to be stationary, i. e. , the models do not change with time.  Bayesian filtering, represented by [Equation 1], provides a recursive estimate of the posterior (newly updated) probability distribution over the states.  Here, control is omitted in equations for brevity.  We consider the following factored representation, where the discrete state transition is conditionally independent of the continuous state transitions given the previous discrete state 1 : 1 t 1 t t t d 1 t t t t t t t 1 t t dx )
Stable Fitted Reinforcement Learning| Abstract We describe the reinforcement learning problem, motivate algorithms which seek an approximation to the Q function, and present new convergence results for two such algorithms. 
Efficient Monitoring for Planetary Rovers| Abstract Planetary rovers operate in environments where human intervention is expensive, slow, unreliable, or impossible.  It is therefore essential to monitor the behavior of these robots so that contingencies may be addressed before they result in catastrophic failures.  This monitoring needs to be efficient since there is limited computational power available on rovers.  We propose an efficient particle filter for monitoring faults that combines the Unscented Kalman Filter (UKF) [7
CHIMP/MPI user guide|
Reuse, portability and parallel libraries|
Approximate Solutions to Markov Decision Problems|
Generalized 2 linear 2 models|
Generalized 2 linear 2 models| In Advances in Neural Information Processing Systems 15,. 
Translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma|
Chattering in SARSA(#)|
Planning in the presence of cost functions controlled by an adversary|
Locating moving entities in dynamic indoor environments|
Nonparametric Statistical Methods for Experimental Evaluations of Speedup Learning|

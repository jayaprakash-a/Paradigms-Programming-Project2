Variational inference for Dirichlet process mixtures| Abstract Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled their applications to a variety of practical data analysis problems.  However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives.  One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad, 2001; Wainwright and Jordan, 2003).  Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias, 2000; Ghahramani and Beal, 2001; Blei et al. , 2003).  In this paper, we present a variational inference algorithm for DP mixtures.  We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem. 
Nonparametric empirical Bayes for the Dirichlet process mixture model| Abstract The Dirichlet process prior allows flexible nonparametric mixture modeling.  The number of mixture components is not specified in advance and can grow as new data come in.  However, the behavior of the model is sensitive to the choice of the parameters, including an infinite-dimensional distributional parameter G0 .  Most previous applications have either fixed G0 as a member of a parametric family or treated G0 in a Bayesian fashion, using parametric prior specifications.  In contrast, we have developed an adaptive nonparametric method for constructing smooth estimates of G0 .  We combine this method with a technique for estimating #, the other Dirichlet process parameter, that is inspired by an existing characterization of its maximum-likelihood estimator.  Together, these estimation procedures yield a flexible empirical Bayes treatment of Dirichlet process mixtures.  Such a treatment is useful in situations where smooth point estimates of G0 are of intrinsic interest, or where the structure of G0 cannot be conveniently modeled with the usual parametric prior families.  Analysis of simulated and real-world datasets illustrates the robustness of this approach. 
Shortest Paths in a Dynamic Uncertain Domain| Abstract This paper describes solutions to finding shortest paths in stochastic graphs with partially unknown topologies.  We consider graphs which are both static and dynamic.  We solve the static problem by reduction to a Markov decision process and solve the dynamic problem by reduction to a partially observable Markov decision process.  We show these solutions to be intractable and explore reinforcement learning as a method of approximation.  Finally, we present empirical results of a reinforcement learning approach in this framework.  Suppose we are trying to deliver an important package to a town in a cluster of small islands.  These islands have recently been struck by a terrible storm and we can't be sure of the status of each bridge.  Some of them are intact but many of them washed away or are otherwise unusable.  What can we do in such a situation? Suppose now that there are efforts to fix some of the bridges while other bridges continue to fall apart due to additional rains.  Now how can we plan to deliver the package? These planning problems are interesting and difficult.  We know something about the state of the world but need to observe it to be sure of that state.  Furthermore, we want to deliver the package to the town as soon as possible and avoid wasting valuable time and resources on improbable and long paths through the islands. 
Modeling annotated data| ABSTRACT We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image).  We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type.  We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval. 
Topic Segmentation with an Aspect Hidden Markov Model| ABSTRACT We present a novel probabilistic method for topic segmentation on unstructured text.  One previous approach to this problem utilizes the hidden Markov model (HMM) method for probabilistically modeling sequence data [6].  The HMM treats a document as mutually independent sets of words generated by a latent topic variable in a time series.  We extend this idea by embedding Hofmann's aspect model for text[5] into the segmenting HMM to form an aspect HMM (AHMM).  In doing so, we provide an intuitive topical dependency between words and a cohesive segmentation model.  We apply this method to segment unbroken streams of New York Times articles as well as noisy transcripts of radio programs on SpeechBot 1 , an online audio archive indexed by an automatic speech recognition engine.  We provide experimental comparisons which show that the AHMM outperforms the HMM for this task. 
Hierarchical Dirichlet Processes| Abstract We consider problems involving groups of data, where each observation within a group is a draw from a mixture model, and where it is desirable to share mixture components between groups.  We assume that the number of mixture components is unknown a priori and is to be inferred from the data.  In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group.  Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process.  Such a base measure being discrete, the child Dirichlet processes necessarily share atoms.  Thus, as desired, the mixture models in the different groups necessarily share mixture components.  We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the "Chinese restaurant franchise. " We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures, and describe applications to problems in information retrieval and text modelling. 
Variational methods for the Dirichlet process| Abstract Variational inference methods, including mean field methods and loopy belief propagation, have been widely used for approximate probabilistic inference in graphical models.  While often less accurate than MCMC, variational methods provide a fast deterministic approximation to marginal and conditional probabilities.  Such approximations can be particularly useful in high dimensional problems where sampling methods are too slow to be effective.  A limitation of current methods, however, is that they are restricted to parametric probabilistic models.  MCMC does not have such a limitation; indeed, MCMC samplers have been developed for the Dirichlet process (DP), a nonparametric distribution on distributions (Ferguson, 1973) that is the cornerstone of Bayesian nonparametric statistics (Escobar & West, 1995; Neal, 2000).  In this paper, we develop a meanfield variational approach to approximate inference for the Dirichlet process, where the approximate posterior is based on the truncated stick-breaking construction (Ishwaran & James, 2001).  We compare our approach to DP samplers for Gaussian DP mixture models. 
Learning with Scope, with Application to Information Extraction and Classification| Abstract In probabilistic approaches to text classi#cation and information extraction, one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data.  In many text data sets however, there are additional, scope-limited features whose predictive power is applicable only to a certain subset of the data.  For example, in information extraction from web pages, word formatting may be indicative of extraction category in different ways on different web pages.  The central diculty with using such features is capturing and exploiting the new regularities encountered in previously unseen data.  In this work, we propose a hierarchical probabilistic model that uses both local/scopelimited features (e. g. , formatting) and global features (e. g. , word content).  The local regularities are represented as an unobserved random parameter for each local data set, and these regularities are captured in the inference process.  This process is akin to automatically retuning our classifier to the local regularities on each newly encountered web page.  Exact inference is intractable, and we present approximations via point estimates and variational methods.  Empirical results on large collections of web data show this method significantly improving performance over traditional models of global features alone. 
Latent Dirichlet Allocation| Abstract We propose a generative model for text and other collections of discrete data, that generalizes or improves on several previous models, including naive Bayes/unigram, mixtures of naive Bayes [6], and Hofmann's pLSI/aspect model [3].  In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable.  Inference and learning are carried out eciently via variational algorithms.  We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 
Hierarchical Topic Models and the Nested Chinese Restaurant Process| Abstract We address the problem of learning topic hierarchies from data.  The model selection problem in this domain is daunting---which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process.  This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections.  We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation.  We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 
Integrating Topics and Syntax| Abstract Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words.  We present a generative model that uses both kinds of dependencies, and is capable of simultaneously finding syntactic classes and semantic topics despite having no knowledge of syntax or semantics beyond statistical dependency.  This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 
Latent Dirichlet models for application in information retrieval|
"Dirichlet Allocation Models,"|
\Variational methods for the Dirichlet process", ACM International Conference Proceeding Series:|
scaling in random networks|

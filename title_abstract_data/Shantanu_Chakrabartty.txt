MARGIN PROPAGATION AND FORWARD DECODING IN ANALOG VLSI| ABSTRACT We propose margin propagation as an alternative to probability propagation in forward decoding.  In contrast to sumproduct probability propagation, margin propagation only incurs addition and subtraction in the computation and thus leads to reduced complexity of implementation.  Simulations indicate that margin based forward decoding is more robust to input noise and parameter mismatch than sumproduct probability decoding, and offers superior decoding performance.  We also present an analog VLSI implementation of the margin propagation network independent of MOS device models, and provide experimental results from a prototype fabricated in a ### ### process. 
Sub-Microwatt Analog VLSI Support Vector Machine for Pattern Classification and Sequence Estimation| Abstract An analog system-on-chip for kernel-based pattern classification and sequence estimation is presented.  State transition probabilities conditioned on input data are generated by an integrated support vector machine.  Dot product based kernels and support vector coefficients are implemented in analog programmable floating gate translinear circuits, and probabilities are propagated and normalized using sub-threshold current-mode circuits.  A 14-input, 24-state, and 720-support vector forward decoding kernel machine is integrated on a 3mm3mm chip in 0. 5m CMOS technology.  Experiments with the processor trained for speaker verification and phoneme sequence estimation demonstrate real-time recognition accuracy at par with floating-point software, at sub-microwatt power. 
Forward-Decoding Kernel-Based Phone Sequence Recognition| Abstract Forward decoding kernel machines (FDKM) combine large-margin classifiers with hidden Markov models (HMM) for maximum a posteriori (MAP) adaptive sequence estimation.  State transitions in the sequence are conditioned on observed data using a kernel-based probability model trained with a recursive scheme that deals effectively with noisy and partially labeled data.  Training over very large datasets is accomplished using a sparse probabilistic support vector machine (SVM) model based on quadratic entropy, and an on-line stochastic steepest descent algorithm.  For speaker-independent continuous phone recognition, FDKM trained over 177,080 samples of the TIMIT database achieves 80. 6% recognition accuracy over the full test set, without use of a prior phonetic language model. 
GiniSupport Vector Machines for Segmental Minimum Bayes Risk Decoding of Continuous Speech| Abstract We describe the use of Support Vector Machines (SVMs) for
SPIKE SORTING WITH SUPPORT VECTOR MACHINES| ABSTRACT Spike sorting of neural data from single electrode recordings is a hard problem in machine learning that relies on significant input by human experts.  We approach the task of learning to detect and classify spike waveforms in additive noise using two stages of large margin kernel classification and probability regression.  Controlled numerical experiments using spike and noise data extracted from neural recordings indicate significant improvements in detection and classification accuracy over linear amplitude- and template-based spike sorting techniques. 
THREE-DECADE PROGRAMMABLE FULLY DIFFERENTIAL LINEAR OTA| ABSTRACT Acoustic and sonar analog signal processing applications require design of operational transconductance amplifiers (OTAs) that can be configured over wide frequency range in multiple bands and yet achieve low power consumption and low harmonic distortion.  A fully differential, linear OTA is presented with digitally programmable transconductance ranging over three decades of dynamic range.  Measurements from a prototype fabricated in a ### ### m CMOS process demonstrate a 0. 4 nA/V to 0. 8 # A/V transconductance range, 40 dB common-mode rejection ratio (CMRR), and -48 dB third-order harmonic distortion, at 12 # W power dissipation. 
Silicon Support Vector Machine with On-Line Learning| Training of support vector machines (SVMs) amounts to solving a quadratic programming problem over the training data.  We present a simple on-line SVM training algorithm of complexity approximately linear in the number of training vectors, and linear in the number of support vectors.  The algorithm implements an on-line variant of sequential minimum optimization (SMO) that avoids the need for adjusting select pairs of training coefficients by adjusting the bias term along with the coefficient of the currently presented training vector.  The coefficient assignment is a function of the margin returned by the SVM classifier prior to assignment, subject to inequality constraints.  The training scheme lends efficiently to dedicated SVM hardware for real-time pattern recognition, implemented using resources already provided for run-time operation.  Performance gains are illustrated using the Kerneltron, a massively parallel mixed-signal VLSI processor for kernel-based real-time video recognition. 
SEQUENCE ESTIMATION AND CHANNEL EQUALIZATION USING FORWARD DECODING KERNEL MACHINES| ABSTRACT A forward decoding approach to kernel machine learning is presented.  The method combines concepts from Markovian dynamics, large margin classifiers and reproducing kernels for robust sequence detection by learning inter-data dependencies.  A MAP (maximum a posteriori) sequence estimator is obtained by regressing transition probabilities between symbols as a function of received data.  The training procedure involves maximizing a lower bound of a regularized cross-entropy on the posterior probabilities, which simplifies into direct estimation of transition probabilities using kernel logistic regression.  Applied to channel equalization, forward decoding kernel machines outperform support vector machines and other techniques by about 5dB in SNR for given BER, within 1dB of theoretical limits. 
ROBUST SPEECH FEATURE EXTRACTION BY GROWTH TRANSFORMATION IN REPRODUCING KERNEL HILBERT SPACE| ABSTRACT A robust speech feature extraction procedure, by kernel regression nonlinear predictive coding, is presented.  Features maximally insensitive to additive noise are obtained by growth transformation of regression functions spanning a Reproducing Kernel Hilbert Space (RKHS).  Experiments on TI-DIGIT demonstrate consistent robustness of the new features to noise of varying statistics, yielding significant improvements in digit recognition accuracy over identical models trained using Mel-scale cepstral features and evaluated at noise levels between 0 and 30 dB SNR. 
Forward Decoding Kernel Machines: A Hybrid HMM/SVM Approach to Sequence Recognition|
Hybrid Support vector Machine/Hidden Markov Model Approach for Continuous Speech recognition,|

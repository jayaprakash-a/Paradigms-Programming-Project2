Using Feature Selection to Find Inputs that Work Better as Extra Outputs| Abstract In supervised learning there is usually a clear distinction between inputs and outputs --- inputs are what you measure, outputs are what you predict from those measurements.  The distinction between inputs and outputs is not this simple.  Previously, we demonstrated that on synthetic problems some input features are more useful when used as extra outputs than when used as inputs[6].  This paper shows the same effect on a real problem, and presents a means of determining what features can be used as extra outputs.  We show that the feature selection method devised by Koller and Sahami[11] can be used to select features to use as extra outputs, and that using some features as as extra outputs instead of as inputs yields better performance on the DNA splice-junction domain.  1 MOTIVATION The goal in supervised learning is to learn functions that map inputs to outputs with high predictive accuracy.  The common practice in backprop nets is to use all features that will be available for test cases as inputs, and use as outputs only features that need to be predicted.  On real problems, where there may be many redundant or irrelevant features, using all the available features as inputs is often suboptimal.  Many algorithms learn better given a carefully selected subset of the features to use inputs[3, 10, 11].  If feature selection is used to find the features to use as inputs, what should be done with the features not selected? Usually, features not selected for use as inputs are discarded.  But, there are other ways to benefit from features without using them as inputs.  One way to benefit from features not used as inputs is multitask learning.  Multitask learning (MTL) is an inductive transfer method where extra tasks are learned in parallel with the main task while using a shared representation.  Because the extra tasks share a hidden layer with the main task, internal representations learned for the extra tasks can be used by the main task outputs, often improving performance on the main task.  MTL in backprop nets is well
A Non-Parametric EM-Style Algorithm for Imputing Missing Values| Abstract We present an iterative non-parametric algorithm for imputing missing values.  The algorithm is similar to EM except that it uses non-parametric models such as k-nearest neighbor or kernel regression instead of the parametric models used with EM.  An interesting feature of the algorithm is that the E and M steps collapse into a single step because the data being filled in is the model -- updating the filled-in values updates the model at the same time.  The main advantages of this approach compared to parametric EM methods are that: 1) it is more efficient for moderate size data sets, and 2) it is less susceptible to errors that parametric methods make when the parametric models do not fit the data well.  The robustness to model failure makes the non-parametric method more accurate when models of the data are not known apriori and cannot be determined reliably.  We evaluate the method using a real medical data set that has many missing values.  1 Motivation The Port Pneumonia Database [2] contains 2287 pneumonia patients.  For our experiments we use 216 of the 1000+ features available in the raw database.  6. 3% of the values of the 216 attributes are missing.  More than 99% of the cases in the database are missing at least one of the 216 values, and one of the 216 attributes is missing in 61% of the cases.  Values in this database may be missing because they were not recorded, because they were not measured, or because they were assumed normal. 
Case-Based Explanation of Non--Case-Based Learning Methods| We show how to generate case-based explanations for non--case-based learning methods such as artificial neural nets or decision trees.  The method uses the trained model (e. g. , the neural net or the decision tree) as a distance metric to determine which cases in the training set are most similar to the case that needs to be explained.  This approach is well suited to medical domains, where it is important to understand predictions made by complex machine learning models, and where training and clinical practice makes users adept at case interpretation. 
Greedy Attribute Selection| Abstract Many real-world domains bless us with a wealth of attributes to use for learning.  This blessing is often a curse: most inductive methods generalize worse given too many attributes than if given a good subset of those attributes.  We examine this problem for two learning tasks taken from a calendar scheduling domain.  We show that ID3/C4. 5 generalizes poorly on these tasks if allowed to use all available attributes.  We examine five greedy hillclimbing procedures that search for attribute sets that generalize well with ID3/C4. 5.  Experiments suggest hillclimbing in attribute space can yield substantial improvements in generalization performance.  We present a caching scheme that makes attribute hillclimbing more practical computationally.  We also compare the results of hillclimbing in attribute space with FOCUS and RELIEF on the two tasks. 
FeatureBoost: A Meta-Learning Algorithm that Improves Model Robustness| Abstract Most machine learning algorithms are lazy: they extract from the training set the minimum information needed to predict its labels.  Unfortunately, this often leads to models that are not robust when features are removed or obscured in future test data.  For example, a backprop net trained to steer a car typically learns to recognize the edges of the road, but does not learn to recognize other features such as the stripes painted on the road which could be useful when road edges disappear in tunnels or are obscured by passing trucks.  The net learns the minimum necessary to steer on the training set.  In contrast, human driving is remarkably robust as features become obscured.  Motivated by this, we propose a framework for robust learning that biases induction to learn many different models from the same inputs.  We present a meta algorithm for robust learning called FeatureBoost, and demonstrate it on several problems using backprop nets, k-nearest neighbor, and decision trees.  1.  Motivation Consider a backprop net learning to steer a car.  In the ALVINN system (Pomerleau, 1993) the principal internal features learned by ALVINN nets detect the left and right edges of the road.  Typically, ALVINN nets do not learn internal features that detect other road phenomena that could be useful for steering such as road centerlines, roadway signs, trees, other traffic, people, etc.  This creates a problem when the left or right edges of the road are obstructed by passing vehicles, or are missing as on bridges and in tunnels.  Yet human steering is remarkably robust to the loss of these features.  Human drivers can fall back on a number of alternate features as different subsets of road features come in and out of view.  Backprop nets can learn to steer better if they learn to recognize other road features such as centerlines (Caruana, 1997).  How can we force backprop nets to learn to use a variety of road features when learning to steer? A related problem arises in health care (Cooper et al. , 1997).  Basic inputs such as age, gender, and blood pressure are available for most patients before they enter the hospital.  Other measurements such as RBC counts, oxygenation, and Albumin become available after patients are hospitalized.  As you would expect, models trained to predict patient risk from both the pre and in-hospital features usually outperform models trained to predict risk from only the prehospital inputs.  But these models perform poorly on patients not yet admitted to the hospital when one marginalizes over the missing in-hospital features.  Models that use only the pre-hospital inputs are more accurate for patients not yet admitted to the hospital than marginalized models trained on all the features.  How can we force learning to learn models that make better predictions when some input features (such as the in-hospital attributes) are missing for some test cases? If the edges of the road, or the in-hospital features are always available, models learned the usual way perform well.  In the ALVINN and health care problems above, the difficulty arises when features are missing or obscured in the test cases.  Boosting algorithms such as AdaBoost are one way to make learned models more robust to feature obscuration.  If the main features such as the edges of the road are obscured or missing from a few training cases, boosting places more emphasis on these cases because they are predicted poorly.  This emphasis forces the learning algorithm to use other features such as road centerlines for these cases.  Unfortunately, boosting learns about centerlines by strongly emphasizing the cases that are missing road edges, even though centerlines may be visible in all images.  Boosting could learn about other features better if it used all of the training data containing those features to learn about them.  How can we make boosting take full advantage of all the redundant information in the training set? This paper introduces a general framework for induction called robust learning, which is motivated by our desire to model situations where features may be corrupted or missing in ways not adequately represented in the training set.  Guided by the framework, we devise a meta-learning algorithm called FeatureBoost, that trains models to use different subsets of features.  Because the final prediction from FeatureBoost combines the predictions of models that depend on different (often overlapping) subsets of features, it is more robust to missing or obscured features.  We develop the paper as follows: 1 Present a general framework for robust learning.  2 Examine a specialization of this framework that suggests one way to improve robustness.  3 Develop a meta-learning algorithm (FeatureBoost) inspired by this model.  4 Test FeatureBoost on a variety of learning problems and machine learning algorithms. 
Using grocery sales data for the detection of bio-terrorist attacks| SUMMARY In this paper we explore the potential of using sales of grocery items data for early detection of epidemiological outbreaks and bio-terrorism attacks.  These data are of special importance, as they are illustrative of non-symptom specific data that are expected to arrive earlier than medical data commonly used for such purposes.  We explore the characteristics of such data and create a detection algorithm that detects irregular patterns of purchases that indicate an epidemiological outbreak.  We show that it is feasible to use non-specific syndrome data, such as over-the-counter medication sales, for early detection of bio-terrorism attacks.  Our conclusions are based on experiments with a theoretical simulation of a large anthrax outbreak.  The proposed detection system consists of several layers and
Ensemble selection from libraries of models| Abstract We present a method for constructing ensembles from libraries of thousands of models.  Model libraries are generated using different learning algorithms and parameter settings.  Forward stepwise selection is used to add to the ensemble the models that maximize its performance.  Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area.  Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection. 
Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria| Abstract.  Many criteria can be used to evaluate the performance of supervised learning.  Different criteria are appropriate in different settings, and it is not always clear which criteria to use.  A further complication is that learning methods that perform well on one criterion may not perform well on other criteria.  For example, SVMs and boosting are designed to optimize accuracy, whereas neural nets typically optimize squared error or cross entropy.  We conducted an empirical study using a variety of learning methods (
Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping| Abstract The conventional wisdom is that backprop nets with excess hidden units generalize poorly.  We show that nets with excess capacity generalize well when trained with backprop and early stopping.  Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model.  Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity.  2) Regardless of size, nets learn task subcomponents in similar sequence.  Big nets pass through stages similar to those learned by smaller nets.  Early stopping can stop training the large net when it generalizes comparably to a smaller net.  We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity. 
MIT Press| Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs.  Abstract In supervised learning there is usually a clear distinction between inputs and outputs --- inputs are what you will measure, outputs are what you will predict from those measurements.  This paper shows that the distinction between inputs and outputs is not this simple.  Some features are more useful as extra outputs than as inputs.  By using a feature as an output we get more than just the case values but can learn a mapping from the other inputs to that feature.  For many features this mapping may be more useful than the feature value itself.  We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead.  This result is surprising since a feature used as an output is not used during testing. 
Experience with a Learning Personal Assistant| Abstract Personal software assistants that help users with tasks like finding information, scheduling calendars, or managing work-flow will require significant customization to each individual user.  For example, an assistant that helps schedule a particular user's calendar will have to know that user's scheduling preferences.  This paper explores the potential of machine learning methods to automatically create and maintain such customized knowledge for personal software assistants.  We describe the design of one particular learning assistant: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience.  Results are summarized from approximately five user-years of experience, during which CAP has learned an evolving set of several thousand rules that characterize the scheduling preferences of its users.  Based on this experience, we suggest that machine learning methods may play an important role in future personal software assistants. 
An Empirical Evaluation of Supervised Learning for ROC Area| Abstract.  We present an empirical comparison of the AUC performance of seven supervised learning methods:
Bridging the lexical chasm: statistical approaches to answer-finding| Abstract This paper investigates whether a machine can automatically learn the task of finding, within a large collection of candidate responses, the answers to questions.  The learning process consists of inspecting a collection of answered questions and characterizing the relation between question and answer with a statistical model.  For the purpose of learning this relation, we propose two sources of data: Usenet FAQ documents and customer service call-center dialogues from a large retail company.  We will show that the task of "answer-finding" differs from both document retrieval and traditional questionanswering, presenting challenges different from those found in these problems.  The central aim of this work is to discover, through theoretical and empirical investigation, those statistical techniques best suited to the answer-finding problem. 
How Useful Is Relevance?| Abstract Eliminating irrelevant attributes prior to induction boosts the performance of many learning algorithms.  Relevance, however, is no guarantee of usefulness to a particular learner.  We test two methods of finding relevant attributes, FOCUS and RELIEF, to see how the attributes they select perform with ID3/C4. 5 on two learning problems from a calendar scheduling domain.  A more direct attribute selection procedure, hillclimbing in attribute space, finds superior attribute sets. 
(Not) Bounding the True Error| Abstract We present a new approach to bounding the true error rate of a continuous valued classifier based upon PAC-Bayes bounds.  The method first constructs a distribution over classifiers by determining how sensitive each parameter in the model is to noise.  The true error rate of the stochastic classifier found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound.  In this paper we demonstrate the method on artificial neural networks with results of a 2 3 order of magnitude improvement vs.  the best deterministic neural net bounds. 
Semi-supervised Clustering with User Feedback| Abstract We present a new approach to clustering based on the observation that \it is easier to criticize than to construct. " Our approach of semi-supervised clustering allows a user to iteratively provide feedback to a clustering algorithm.  The feedback is incorporated in the form of constraints which the clustering algorithm attempts to satisfy on future iterations.  These constraints allow the user to guide the clusterer towards clusterings of the data that the user finds more useful.  We demonstrate semi-supervised clustering with a system that learns to cluster news stories from a Reuters data set. 
Learning Many Related Tasks at the Same Time with Backpropagation|
Multitask Learning|
A Study of Control Parameters Affecting Online Performance of Genetic Algorithms for Function Optimization|
Removing the Genetics from the Standard Genetic Algorithm|
Eshelman L and Das R 1989 A study of control parameters affecting online performance of genetic algorithms for function optimization|
Introduction to IND Version 2|1 and Recursive Partitioning. 
Biases in the Crossover Landscape|
Representation and Hidden Bias: Gray vs| Binary Coding for Genetic Algorithms. 
Using the Future to Sort Out the Present: Rankprop and Multitask Learning for Medical Risk Evaluation|
Using genetic search to exploit the emergent behavior of neural networks|
Learning to learn: knowledge consolidation and transfer in inductive systems|
Introduction to ind and recursive partitioning|
Intelligent agent design issues: Internal agent state and incomplete perception|
Semi-supervised clustering with user feedback| Unpublished manuscript. 
Bias in the crossover landscape| Pages 10--19 of: Genetic algorithms and their applications:. 
An evaluation of machine-learning methods for predicting pneumonia mortality|
Experiences with a learning personal assistant|
Meta clustering|
Meta clustering|
The Automatic Training of Rule Bases that Use Numerical Uncertainty Representations|
Representation and Hidden Bias II: Eliminating Defining Length Bias in Genetic Search via Shuffle Crossover|
Biases in the Crossover Landscape", in:|
Searching for optimal FIR multiplierless digital filters with simulated annealing|
Emissions from the combustion of peat: An experimental study,|
High Precision Information Extraction|
An empirical comparison of supervised learning algorithms using difference performance metrics|
Algorithms and Applications for Multitask Learning|
Estimating the Number of Local Minima in Big, Nasty Search Spaces|
Integrating supervised and unsupervised learning,|
Benefitting from the Variables that Variable Selection Discards|
Statistical machine learning for large-scale optimization,|
Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs|
"Applications and Algorithms for Multitask Learning,"|
Multitask Learning: A Knowledge-Based Source of Inductive Bias|
Featureboost: A meta-learning algorithm that improves model robustness|
`Do universities that are more market orientated perform better',|
Excellence-market orientation link: Some consequences for service firms',|
Market orientation and business performance: Some European evidence',|
Multitask connectionist learning|
Using Multiple Representations to Improve Inductive Bias: Gray and Binary Coding for Genetic Algorithms|
Early statistical detection of anthrax outbreaks by tracking over-the-counter medication sales|
Multitask learning: A knowledge-based of source inductive bias|
Rem vin th genetic fro th standard geneti algorithm I Prieditis A an Russel S| editors, P ding of th Twelft Internationa Confe en o Machin arnin (ML-95). 

Learning vehicular dynamics, with application to modeling| Abstract We consider the problem of modeling a helicopter's dynamics based on stateaction trajectories collected from it.  The contributions of this paper are twofold.  First, we consider the linear models learned by CIFER(the industry standard in helicopter identification), and show that their parameterization makes certain properties of dynamical systems, such as inertia, fundamentally difficult to capture.  We propose an alternative (acceleration based) parameterization that does not suffer from this deficiency, and that can be learned as efficiently from data.  Second, even though a Markov decision process model of helicopter's dynamics would explicitly model only the one-step transitions, we are often interested in a model's predictive performance over longer timescales.  In this paper, we present an efficient algorithm for (approximately) minimizing the prediction error over long time scales.  We present empirical results on two different helicopters.  Although this work was motivated by the problem of modeling helicopters, the ideas presented here are general, and can be applied to modeling large classes of vehicular dynamics. 
Discriminative Probabilistic Models for Relational Data| Abstract In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent.  For example, in hypertext classification, the labels of linked pages are highly correlated.  A standard approach is to classify each entity independently, ignoring the correlations between them.  Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities.  In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach.  First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models.  Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy.  We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities.  We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies. 
Apprenticeship learning via inverse reinforcement learning| Abstract We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform.  This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off.  We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert.  Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function.  We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function. 
Learning first-order Markov models for control| Abstract First-order Markov models have been successfully applied to many problems, for example in modeling sequential data using Markov chains, and modeling control problems using the Markov decision processes (MDP) formalism.  If a first-order Markov model's parameters are estimated from data, the standard maximum likelihood estimator considers only the first-order (single-step) transitions.  But for many problems, the firstorder conditional independence assumptions are not satisfied, and as a result the higher order transition probabilities may be poorly approximated.  Motivated by the problem of learning an MDP's parameters for control, we propose an algorithm for learning a first-order Markov model that explicitly takes into account higher order interactions during training.  Our algorithm uses an optimization criterion different from maximum likelihood, and allows us to learn models that capture longer range effects, but without giving up the benefits of using first-order Markov models.  Our experimental results also show the new algorithm outperforming conventional maximum likelihood estimation in a number of control problems where the MDP's parameters are estimated from data. 
Link Prediction in Relational Data| Abstract Many real-world domains are
Disabling ideology in health and welfare: The case of occupational therapy|
Label and Link Prediction in Relational Data|

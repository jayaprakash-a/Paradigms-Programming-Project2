A probabilistic approach to semantic representation| Abstract Semantic networks produced from human data have statistical properties that cannot be easily captured by spatial representations.  We explore a
Prediction and Semantic Association| Abstract We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context.  We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language. 
Prediction and Change Detection| Abstract We measure the ability of human observers to predict the next datum in a sequence that is generated by a simple statistical process undergoing change at random points in time.  Accurate performance in this task requires the identification of changepoints.  We assess individual differences between observers both empirically, and using two kinds of models: a Bayesian approach for change detection and a family of cognitively plausible fast and frugal models.  Some individuals detect too many changes and hence perform sub-optimally due to excess variability.  Other individuals do not detect enough changes, and perform sub-optimally because they fail to notice short-term temporal trends. 
The Author-Topic Model for Authors and Documents| Abstract We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information.  Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words.  A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors.  We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts.  Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions.  We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics.  We show topics recovered by the authortopic model, and demonstrate applications to computing similarity between authors and entropy of author output. 
The Large-Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth| Abstract We present statistical analyses of the large-scale structure of three types of semantic networks: word associations, WordNet, and Roget's thesaurus.  We show that they have a small-world structure, characterized by sparse connectivity, short average path-lengths between words, and strong local clustering.  In addition, the distributions of the number of connections follow power laws that indicate a scale-free pattern of connectivity, with most nodes having relatively few connections joined together through a small number of hubs with many connections.  These regularities have also been found in certain other complex natural networks, such as the world wide web, but they are not consistent with many conventional models of semantic organization, based on inheritance hierarchies, arbitrarily structured networks, or high-dimensional vector spaces.  We propose that these structures reflect the mechanisms by which semantic networks grow.  We describe a simple model for semantic growth, in which each new word or concept is connected to an existing network by differentiating the connectivity pattern of an existing node.  This model generates appropriate small-world statistics and power-law connectivity distributions, and also suggests one possible mechanistic basis for the effects of learning history variables (age-ofacquisition, usage frequency) on behavioral performance in semantic processing tasks. 
Probabilistic author-topic models for information discovery| ABSTRACT We propose a new unsupervised learning technique for extracting information from large text collections.  We model documents as if they were generated by a two-stage stochastic process.  Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic.  The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture.  The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm.  We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics.  We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors.  An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer. 
Inferring causal networks from observations and interventions| Abstract Information about the structure of a causal system can come in the form of observational data--random samples of the system's autonomous behavior---or interventional data---samples conditioned on the particular values of one or more variables that have been experimentally manipulated.  Here we study people's ability to infer causal structure from both observation and intervention, and to choose informative interventions on the basis of observational data.  In three causal inference tasks, participants were to some degree capable of distinguishing between competing causal hypotheses on the basis of purely observational data.  Performance improved substantially when participants were allowed to observe the effects of interventions that they performed on the systems.  We develop computational models of how people infer causal structure from data and how they plan intervention experiments, based on the representational framework of causal graphical models and the inferential principles of optimal Bayesian decision-making and maximizing expected information gain.  These analyses suggest that people can make rational causal inferences, subject to psychologically reasonable representational assumptions and computationally reasonable processing constraints. 
Integrating Topics and Syntax| Abstract Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words.  We present a generative model that uses both kinds of dependencies, and is capable of simultaneously finding syntactic classes and semantic topics despite having no knowledge of syntax or semantics beyond statistical dependency.  This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 
Finding scientific topics|
The sensitization and differentiation of dimensions during category learning|
A model for recognition memory: Rem: Retrieving effectively from memory|
Small worlds in semantic networks|
Interactions between perceptual and conceptual learning|
Word association spaces for predicting semantic similarity effects in episodic memory|
Conceptual interrelatedness and Caricatures|
Categorical perception of novel dimensions|
Multidimensional scaling|
Multideimnsional scaling,|
Modeling semantic and orthographic similarity effects on memory for individual words|
Predicting similarity ratings to faces using physical descriptions|
in press) Interactions between perceptual and conceptual learning|
in progress)| Semantic spaces based on free association that predict memory performance. 
The author-topic model for authors and documents|
Finding scientific topis|
Assessment of elbow joint kinematics in passive motion by electromagnetic motion tracking|

Non-Gaussian Component Analysis: a Semiparametric Framework for Linear Dimension Reduction| Abstract We propose a new linear method for dimension reduction to identify nonGaussian components in high dimensional data.  Our method, NGCA (Non-Gaussian Component Analysis), uses a very general semiparametric framework.  In contrast to existing projection methods we define what is uninteresting (Gaussian): by projecting out uninterestingness we can estimate the relevant non-Gaussian subspace.  We show that the estimation error of finding the non-Gaussian components tends to zero at a parametric rate.  Once NGCA components are identified and extracted, various tasks can be applied in the data analysis process, say, data visualization, clustering, denoising or classification.  A numerical study demonstrates the usefulness of our method. 
Pattern Recognition from One Example by Chopping| Abstract We investigate the learning of the appearance of an object from a single image of it.  Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination.  This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object.  We propose a generic scheme called chopping to address this task.  It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object.  Those splits are extended to the complete image space with a simple learning algorithm.  Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity.  Experiments with the COIL-100 database and with a database of 150 degraded L A T E X symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity. 
On the Convergence of Eigenspaces in Kernel Principal Component Analysis| Abstract This paper presents a non-asymptotic statistical analysis of KernelPCA with a focus different from the one proposed in previous work on this topic ([2], [9]).  Here instead of considering the reconstruction error of KPCA we are interested in approximation error bounds for the eigenspaces themselves.  We prove an upper bound depending on the spacing between eigenvalues but not on the dimensionality of the eigenspace.  As a consequence this allows to infer stability results for these estimated spaces. 
Kernel Projection Machine: a New Tool for Pattern Recognition| Abstract This paper investigates the effect of Kernel Principal Component Analysis (KPCA) within the classification framework, essentially the regularization properties of this dimensionality reduction method.  KPCA has been previously used as a pre-processing step before applying an SVM but we point out that this method is somewhat redundant from a regularization point of view and we propose a new algorithm called Kernel Projection Machine to avoid this redundancy, based on an analogy with the statistical framework of regression for a Gaussian white noise model.  Preliminary experimental results show that this algorithm reaches the same performances as an SVM. 
A new algorithm for Bayesian MCMC CART sampling, preprint,|
The "progressive mixture" estimator for regression trees,|
M#ethodes de m#elange et d'agr#egation en reconnaissance de formes| Application aux arbres de d#ecision. 
Hierarchical testing designs for pattern recognition|
BCI competition 2003 -- data set IIa: Spatial patterns of self-controlled brain rhythm modulations,|
Oracle Bounds and Exact Algorithm for Dyadic Classification Trees|
Multiple Randomized Classifiers:|
Statistical performance of support vector machines|
Determination of food sources for benthic invertebrates in a salt marsh (|
Sequential testing designs for pattern recognition,"|
Statistical properties of kernel principal component analysis|

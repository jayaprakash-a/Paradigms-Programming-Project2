Detecting, Localizing and Recovering Kinematics of Textured Animals| Abstract We develop and demonstrate an object recognition system capable of accurately detecting, localizing, and recovering the kinematic configuration of textured animals in real images.  We build a deformation model of shape automatically from videos of animals and an appearance model of texture from a labeled collection of animal images, and combine the two models automatically.  We develop a simple texture descriptor that outperforms the state of the art.  We test our animal models on two datasets; images taken by professional photographers from the Corel collection, and assorted images from the web returned by Google.  We demonstrate quite good performance on both datasets.  Comparing our results with simple baselines, we show that for the Google set, we can recognize objects from a collection demonstrably hard for object recognition. 
Object Recognition as Machine Translation -- Part 2: Exploiting Image Database Clustering Models| Abstract.  We treat object recognition as a process of attaching words to images and image regions.  To accomplish this we exploit clustering methods which learn the joint statistics of words and image regions.  We show how these models can then be used to attach words to images outside the training set.  This "auto-annotation" process has applications such as image indexing, as well as being related to object recognition.  Predicted words can be compared to actual words associated with images in a held out set, and we introduce several performance measures based on this observation.  These measures are then used to make principled comparisons of model variants, and proposed enhancements.  Word prediction is most simply done as a function of the entire image.  However, for recognition we need to learn the correspondence between words and specific image regions.  Here we first show that the existing models can be used for this purpose, and then we propose modifications to improve performance based on this goal.  Finally, we propose word prediction performance as a segmentation measure and report the results for two segmentation approaches. 
Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary| Abstract.  We describe a model of object recognition as machine translation.  In this model, recognition is a process of annotating image regions with words.  Firstly, images are segmented into regions, which are classi#ed into region types using a variety of features.  A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM.  This process is analogous with learning a lexicon from an aligned bitext.  For the implementation we describe, these words are nouns taken from a large vocabulary.  On a large test set, the method can predict numerous words with high accuracy.  Simple methods identify words that cannot be predicted well.  Weshowhowto cluster words that individually are dicult to predict into clusters that can be predicted well | for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept.  The method is trained on a substantial collection of images.  Extensive experimental results illustrate the strengths and weaknesses of the approach. 
Is Machine Colour Constancy Good Enough?| Abstract.  This paper presents a negative result: current machine colour constancy algorithms are not good enough for colour-based object recognition.  This result has surprised us since we have previously used the better of these algorithms successfully to correct the colour balance of images for display.  Colour balancing has been the typical application of colour constancy, rarely has it been actually put to use in a computer vision system, so our goal was to show how well the various methods would do on an obvious machine colour vision task, namely, object recognition.  Although all the colour constancy methods we tested proved insufficient for the task, we consider this an important finding in itself.  In addition we present results showing the correlation between colour constancy performance and object recognition performance, and as one might expect, the better the colour constancy the better the recognition rate. 
Experiments in Sensor Sharpening for Color Constancy| Abstract Sensor sharpening has been proposed as a method for improving color constancy algorithms but it has not been tested in the context of real color constancy algorithms.  In this paper we test sensor sharpening as a method for improving color constancy algorithms in the case of three different cameras, the human cone sensitivity estimates, and the XYZ response curves.  We find that when the sensors are already relatively sharp, sensor sharpening does not offer much improvement and can have a detrimental effect.  However, when the sensors are less sharp, sharpening can have a substantive positive effect.  The degree of improvement is heavily dependent on the particular color constancy algorithm.  Thus we conclude that using sensor sharpening for improving color constancy can offer a significant benefit, but its use needs to be evaluated with respect to both the sensors and the algorithm. 
A Statistical Model for General Contextual Object Recognition| Abstract.  We consider object recognition as the process of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects.  Given a set of images and their associated text (e. g.  keywords, captions, descriptions), the objective is to segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions.  Previous models are limited by the scope of the representation.  In particular, they fail to exploit spatial context in the images and words.  We develop a more expressive model that takes this into account.  We formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens.  By learning both word-to-region associations and object relations, the proposed model augments scene segmentations due to smoothing implicit in spatial consistency.  Context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the EM algorithm for estimating the model parameters and densities of the unknown alignment variables.  Instead, we develop an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step.  The experiments indicate that our approximate inference and learning algorithm converges to good local solutions.  Experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition.  Most significantly, spatial context combined with a nonlinear discrete object representation allows our models to cope well with over-segmented scenes. 
Analysis and Improvement of Multi-Scale Retinex| Abstract The main thrust of this paper is to modify the multi-scale retinex (MSR) approach to image enhancement so that the processing is more justified from a theoretical standpoint.  This leads to a new algorithm with fewer arbitrary parameters that is more flexible, maintains color fidelity, and still preserves the contrast-enhancement benefits of the original MSR method.  To accomplish this we identify the explicit and implicit processing goals of MSR.  By decoupling the MSR operations from one another, we build an algorithm composed of independent steps that separates out the issues of gamma adjustment, color balance, dynamic range compression, and color enhancement, which are all jumbled together in the original MSR method.  We then extend MSR with color constancy and chromaticity-preserving contrast enhancement. 
Color Constancy with Specular and Non-Specular Surfaces| Abstract There is a growing trend in machine color constancy research to use only image chromaticity information, ignoring the magnitude of the image pixels.  This is natural because the main purpose is often to estimate only the chromaticity of the illuminant.  However, the magnitudes of the image pixels also carry information about the chromaticity of the illuminant.  One such source of information is through image specularities.  As is well known in the computational color constancy field, specularities from inhomogeneous materials (such as plastics and painted surfaces) can be used for color constancy.  This assumes that the image contains specularities, that they can be identified, and that they do not saturate the camera sensors.  These provisos make it important that color constancy algorithms which make use of specularities also perform well when the they are absent.  A further problem with using specularities is that the key assumption, namely that the specular component is the color of the illuminant, does not hold in the case of colored metals.  In this paper we investigate a number of color constancy algorithms in the context of specular and nonspecular reflection.  We then propose extensions to several variants of Forsyths CRULE algorithm [1-4] which make use of specularities if they exist, but do not rely on their presence.  In addition, our approach is easily extended to include colored metals, and is the first color constancy algorithm to deal with such surfaces.  Finally, our method provides an estimate of the overall brightness, which chromaticity-based methods cannot do, and other RGB based algorithms do poorly when specularities are present. 
Recognition as Translating Images into Text| ABSTRACT We present an overview of a new paradigm for tackling long standing computer vision problems.  Specifically our approach is to build statistical models which translate from a visual representations (images) to semantic ones (associated text).  As providing optimal text for training is difficult at best, we propose working with whatever associated text is available in large quantities.  Examples include large image collections with keywords, museum image collections with descriptive text, news photos, and images on the web.  In this paper we discuss how the translation approach can give a handle on difficult questions such as: What counts as an object? Which objects are easy to recognize and which are hard? Which objects are indistinguishable using our features? How to integrate low level vision processes such as feature based segmentation, with high level processes such as grouping.  We also summarize some of the models proposed for translating from visual information to text, and some of the methods used to evaluate their performance. 
Colour Constancy for Scenes with Varying Illumination| Abstract.  We present an algorithm which uses information from both surface reflectance and illumination variation to solve for colour constancy.  Most colour constancy algorithms assume that the illumination across a scene is constant, but this is very often not valid for real images.  The method presented in this work identifies and removes the illumination variation, and in addition uses the variation to constrain the solution.  The constraint is applied conjunctively to constraints found from surface reflectances.  Thus the algorithm can provide good colour constancy when there is sufficient variation in surface reflectances, or sufficient illumination variation, or a combination of both.  We present the results of running the algorithm on several real scenes, and the results are very encouraging. 
Estimating the scene illumination chromaticity by using a neural network| A neural network can learn color constancy, defined here as the ability to estimate the chromaticity of a scene's overall illumination.  We describe a multilayer neural network that is able to recover the illumination chromaticity given only an image of the scene.  The network is previously trained by being presented with a set of images of scenes and the chromaticities of the corresponding scene illuminants.  Experiments with real images show that the network performs better than previous color constancy methods.  In particular, the performance is better for images with a relatively small number of distinct colors.  The method has application to machine vision problems such as object recognition, where illumination-independent color descriptors are required, and in digital photography, where uncontrolled scene illumination can create an unwanted color cast in a photograph.  2002 Optical Society of America OCIS codes:
Word Sense Disambiguation with Pictures| Abstract We introduce using images for word sense disambiguation, either alone, or in conjunction with traditional text based methods.  The approach is based on a recently developed method for automatically annotating images by using a statistical model for the joint probability for image regions and words.  The model itself is learned from a data base of images with associated text.  To use the model for word sense disambiguation, we constrain the predicted words to be possible senses for the word under consideration.  When word prediction is constrained to a narrow set of choices (such as possible senses), it can be quite reliable.  We report on experiments using the resulting sense probabilities as is, as well as augmenting a state of the art text based word sense disambiguation algorithm.  In order to evaluate our approach, we developed a new corpus, ImCor, which consists of a substantive portion of the Corel image data set associated with disambiguated text drawn from the SemCor corpus.  Our experiments using this corpus suggest that visual information can be very useful in disambiguating word senses.  It also illustrates that associated non-textual information such as image data can help ground language meaning. 
Improvements to Gamut Mapping Colour Constancy Algorithms| Abstract.  In his paper we introduce two improvements to the threedimensional gamut mapping approach to computational colour constancy.  This approach consist of two separate parts.  First the possible solutions are constrained.  This part is dependent on the diagonal model of illumination change, which in turn, is a function of the camera sensors.  In this work we propose a robust method for relaxing this reliance on the diagonal model.  The second part of the gamut mapping paradigm is to choose a solution from the feasible set.  Currently there are two general approaches for doing so.  We propose a hybrid method which embodies the benefits of both, and generally performs better than either.  We provide results using both generated data and a carefully calibrated set of 321 images.  In the case of the modification for diagonal model failure, we provide synthetic results using two cameras with a distinctly different degree of support for the diagonal model.  Here we verify that the new method does indeed reduce error due to the diagonal model.  We also verify that the new method for choosing the solution offers significant improvement, both in the case of synthetic data and with real images. 
White Point Estimation for Uncalibrated Images| Abstract Color images often must be color balanced to remove unwanted color casts.  We extend previous work on using a neural network for illumination, or white-point, estimation from the case of calibrated images to that of uncalibrated images of unknown origin.  The results show that the chromaticity of the ambient illumination can be estimated with an average CIE Lab error of 5DE.  Comparisons are made to the grayworld and white patch methods. 
Investigations into Multi-Scale Retinex| Abstract The main thrust of this paper is to investigate the multi-scale retinex (MSR) approach to image enhancement to explain the effect of the processing from a theoretical standpoint.  This leads to a new algorithm with fewer arbitrary parameters that is more flexible, maintains colour fidelity, and still preserves the contrast-enhancement benefits of the original MSR method.  To accomplish this we identify the explicit and implicit processing goals of MSR.  By decoupling the MSR operations from one another, we build an algorithm composed of independent steps that separates out the issues of gamma adjustment, colour balance, dynamic range compression, and colour enhancement, which are all jumbled together in the original MSR method.  We then extend MSR with colour constancy and chromaticity-preserving contrast enhancement. 
Degree: Master of Science Title of thesis: Computational Color Constancy: Taking Theory into Practice tice| Abstract The light recorded by a camera is a function of the scene illumination, the reflective characteristics of the objects in the scene, and the camera sensors.  The goal of color constancy is to separate the effect of the illumination from that of the reflectances.  In this work, this takes the form of mapping images taken under an unknown light into images which are estimates of how the scene would appear under a fixed, known light.  The research into color constancy has yielded a number of disparate theoretical results, but testing on image data is rare.  The thrust of this work is to move towards a comprehensive algorithm which is applicable to image data.  Necessary preparatory steps include measuring the illumination and reflectances expected in real scenes, and determining the camera response function.  Next, a number of color constancy algorithms are implemented, with emphasis on the gamut mapping approach introduced by D.  Forsyth and recently extended by G.  Finlayson.  These algorithms all assume that the color of the illumination does not vary across the scene.  The results of these algorithms running on a variety of images, as well as on generated data, are iv presented.  In addition, the possibility of using sensor sharpening to improve algorithm performance is investigated.  The final part of this work deals with images of scenes where the illumination is not necessarily constant.  A recent promising result from Finlayson, Funt, and Barnard demonstrates that if the illumination variation can be identified, it can be used as a powerful constraint.  However, in its current form this algorithm requires human input and is limited to using a single such constraint.  In this thesis the algorithm is first modified so that it provides conjunctive constraints with the other gamut mapping constraints and utilizes all available constraints due to illumination variation.  Then a method to determine the variation in illumination from a properly segmented image is introduced.  Finally the comprehensive algorithm is tested on simple images segmented with region growing.  The results are very encouraging. 
Learning Color Constancy| Abstract We decided to test a surprisingly simple hypothesis; namely, that the relationship between an image of a scene and the chromaticity of scene illumination could be learned by a neural network.  The thought was that if this relationship could be extracted by a neural network, then the trained network would be able to determine a scene's illumination from its image, which would then allow correction of the image colors to those relative to a standard illuminant, thereby providing color constancy.  Using a database of surface reflectances and illuminants, along with the spectral sensitivity functions of our camera, we generated thousands of images of randomly selected illuminants lighting `scenes' of 1 to 60 randomly selected reflectances.  During the learning phase the network is provided the image data along with the chromaticity of its illuminant.  After training, the network outputs (very quickly) the chromaticity of the illumination given only the image data.  We obtained surprisingly good estimates of the ambient illumination lighting from the network even when applied to scenes in our lab that were completely unrelated to the training data.  Descriptive Summary Existing color constancy algorithms [1], [2] , [3], [4], [9], [6], [8] generally employ assumptions about either the surface reflectances that will occur in a scene or about the possible spectral power distributions of scene illuminants.  Given the assumptions and 3-band image data (either CIE XYZ specification or camera RGB) these algorithms calculate the chromaticity of the unknown scene illumination.  If the assumptions are satisfied --which generally they are not -- the estimate of the illumination will be correct and can then be used to adjust the image data so that the image would be the same as if had been taken under some standard, known illuminant.  To the extent that the adjusted colors are as they would have been under the standard illuminant, the system can be said to exhibit `color constancy'.  In contrast, the neural network we have developed has no built-in constraints.  It is an adaptive model that makes no explicit assumptions about the input data.  All rules are implicitly learned from the training set, which contains a large number of (artificially generated) scenes.  The experimental results (see below) show that the neural network outperforms the grey-world and white-patch algorithms, especially in the case of scenes containing a small number (1 to 5) of distinct RGB measurements (Since 'color' is a perceptual quality, in what follows we'll avoid using it and instead simply use RGB to mean the response of the camera at a given pixel).  Good performance with only a small number of distinct RGB's means that the network is particularly well suited for processing small, local image regions.  This is important because generally a scene contains more than one source of light, so the assumption that the scene illumination is constant will, at best, hold true only locally within an image. 
Is Colour Constancy Good Enough?,|
Computational colour constancy: taking theory into practice,"|
Learning the Semantics of Words and Pictures|
Clustering Art|
Joint MAP Registration and High-Resolution Image Estimation Using a Sequence of Undersampled Images,"|
Modeling the statistics of image features and associated text|
Color Constancy under Varying Illumination|
Sensor sharpening for computational color constancy|
Adaptive Illuminant Estimation Using Neural Networks,|
The Effects of Segmentation and Feature Choice in a Translation Model of Object Recognition|
Color constancy for scenes with varying illumination|
Practical colour constancy,|
Is color constancy good enough?"|
Camera characterization for color research,|
Modeling color constancy with neural networks|
Neural Network Color Constancy and Specular Reflecting Surfaces,|
Luminance-based multi-scale Retinex,|
Learning Colour Constancy,|
Investigations into MultiScale retinex (|
A data set for colour research|
Exploiting image semantics for picture libraries|
High-resolution image reconstruction from a sequence of rotated and translated frames and its application to an infrared imaging system|
Shadow Identification using Colour Ratios|
Color Constancy with Fluorescent Surfaces|
High resolution image reconstruction from digital video with global and non-global scene motion,|
Colour by Correlation in a Three-Dimensional Colour Space|
"Words Sense Disambiguation with Pictures"| In: Workshop on Learning Word Meaning from Non-Linguistic Data (2003) Held in Conjunction with The Human Language Technology Conference.  27. 
Exploiting text and image feature co-occurrence statistics in large datasets|
ImCor: A linking of SemCor sense disambiguated text to corel image data|
NCATS Scale|
Computer Vision Meets Digital Libraries|
White point estimation for uncalibrated images (color constancy)|
Bayesian latent semantic analysis of multimedia databases|

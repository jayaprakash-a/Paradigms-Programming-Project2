Learning Eigenfunctions Links Spectral Embedding and Kernel PCA| Abstract In this paper, we show a direct relation between spectral embedding methods and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of an operator defined from a kernel and the unknown data generating density.  Whereas spectral embedding methods only provided coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nystrom formula) for Multi-Dimensional Scaling, spectral clustering, Laplacian eigenmaps, Locally Linear Embedding (LLE) and Isomap.  The analysis provides, for all such spectral embedding methods, the definition of a loss function, whose empirical average is minimized by the traditional algorithms.  The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn.  Experiments with LLE, Isomap, spectral clustering and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding. 
Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering| {bengioy,vincentp,paiemeje,delallea,lerouxni,ouimema} @iro. umontreal. ca Abstract Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors.  This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering.  This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel.  Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data. 
Convex Neural Networks| Abstract Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks.  We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem.  This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors. 
Non-Local Manifold Parzen Windows| Abstract In order to escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x.  With this objective, we present a non-local non-parametric density estimator.  It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold.  It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models. 
Kernel Matching Pursuit| Abstract Matching Pursuit algorithms learn a function that is a weighted sum of basis functions, by sequentially appending functions to an initially empty basis, to approximate a target function in the leastsquares sense.  We show how matching pursuit can be extended to use non-squared error loss functions, and how it can be used to build kernel-based solutions to machine learning problems, while keeping control of the sparsity of the solution.  We present a version of the algorithm that makes an optimal choice of both the next basis and the weights of all the previously chosen bases.  Finally, links to boosting algorithms and RBF training procedures, as well as an extensive experimental comparison with SVMs for classification are given, showing comparable results with typically much sparser models. 
Manifold Parzen Windows| Abstract The similarity between objects is a fundamental element of many learning algorithms.  Most non-parametric methods take this similarity to be fixed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies.  We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigenvectors of regularized local covariance matrices.  Experiments in density estimation show significant improvements with respect to Parzen density estimators.  The density estimators can also be used within Bayes classifiers, yielding classification rates similar to SVMs and much superior to the Parzen classifier. 
A Neural Probabilistic Language Model| Abstract A goal of statistical language modeling is to learn the joint probability function of sequences of words.  This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed representation for each word (i. e.  a similarity between words) along with (2) the probability function for word sequences, expressed with these representations.  Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence.  We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model. 
Spectral Clustering and Kernel PCA are Learning Eigenfunctions| Abstract In this paper, we show a direct equivalence between spectral clustering and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of a kernel, when the functions are from a function space whose scalar product is defined with respect to a density model.  This defines a natural mapping for new data points, for methods that only provided an embedding, such as spectral clustering and Laplacian eigenmaps.  The analysis hinges on a notion of generalization for embedding algorithms based on the estimation of underlying eigenfunctions, and suggests ways to improve this generalization by smoothing the data empirical distribution. 
A Neural Probabilistic Language Model| Abstract A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language.  This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.  Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set.  We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences.  The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations.  Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence.  Training such large
Computational method to predict mitochondrially imported proteins and their targeting sequences|
Analyse de donnees cinetiques de l'expression du genome: une approche par modelisation|
A new tool to study genetic expression using 2--D electrophoresis data: the functional map concept|
Two-dimensional electrophoresis computerized processing,|
Color Documents on the Web with DJVU|

TOWARDS ROBUST AND ADAPTIVE SPEECH RECOGNITION MODELS| Abstract.  In this paper, we discuss a family of new Automatic Speech Recognition (ASR) approaches, which somewhat deviate from the usual ASR approaches but which have recently been shown to be more robust to nonstationary noise, without requiring specific adaptation or "multi-style" training.  More specifically, we will motivate and briefly describe new approaches based on multi-stream and subband ASR.  These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) streams representing the speech signal are processed by different (independent) "experts", each expert focusing on a different characteristic of the signal, and that the different stream likelihoods (or posteriors) are combined at some (temporal) stage to yield a global recognition output.  As a further extension to multi-stream ASR, we will finally introduce a new approach, referred to as HMM2, where the HMM emission probabilities are estimated via state specific feature based HMMs responsible for merging the stream information and modeling their possible correlation. 
Offline Cursive Word Recognition using Continuous Density Hidden Markov Models Trained with PCA or ICA Features| Abstract This work presents an Offline Cursive Word Recognition System dealing with single writer samples.  The system is based on a continuous density Hiddden Markov Model trained using either the raw data, or data transformed using Principal Component Analysis or Independent Component Analysis.  Both techniques significantly improved the recognition rate of the system.  Preprocessing, normalization and feature extraction are described as well as the training technique adopted.  Several experiments were performed using a publicly available database.  The accuracy obtained is the highest presented in the literature over the same data. 
MULTI-MODAL AUDIO-VISUAL EVENT RECOGNITION FOR FOOTBALL ANALYSIS| Abstract.  The recognition of events within multi-modal data is a challenging problem.  In this paper we focus on the recognition of events by using both audio and video data.  We investigate the use of data fusion techniques in order to recognise these sequences within the framework of Hidden Markov Models (HMM) used to model audio and video data sequences.  Specifically we look at the recognition of play and break sequences in football and the segmentation of football games based on these two events.  Recognising relatively simple semantic events such as this is an important step towards full automatic indexing of such video material.  These experiments were done using approximately 3 hours of data from two games of the Euro96 competition.  We propose that modelling the audio and video streams separately for each sequence and fusing the decisions from each stream should yield an accurate and robust method of segmenting multi-modal data. 
F-RATIO CLIENT-DEPENDENT NORMALISATION FOR BIOMETRIC AUTHENTICATION TASKS| ABSTRACT This study investigates a new client-dependent normalisation to improve biometric authentication systems.  There exists many client-dependent score normalisation techniques applied to speaker authentication, such as Z-Norm, D-Norm and T-Norm.  Such normalisation is intended to adjust the variation across different client models.  We propose "F-ratio" normalisation, or F-Norm, applied to face and speaker authentication systems.  This normalisation requires only that as few as two client-dependent accesses are available (the more the better).  Different from previous normalisation techniques, F-Norm considers the client and impostor distributions simultaneously.  We show that Fratio is a natural choice because it is directly associated to Equal Error Rate.  It has the effect of centering the client and impostor distributions such that a global threshold can be easily found.  Another difference is that F-Norm actually "interpolates" between client-independent and client-dependent information by introducing a mixture parameter.  This parameter can be optimised to maximise the class dispersion (the degree of separability between client and impostor distributions) while the aforementioned normalisation techniques cannot.  The results of 13 unimodal experiments carried out on the XM2VTS multimodal database show that such normalisation is advantageous over Z-Norm, client-dependent threshold normalisation or no normalisation. 
Towards Predicting Optimal Subsets of Base-Experts in Biometric Authentication Tasks| data streams is a very promising approach, both in experiments and to some extend in various real-life applications.  However, combining too many systems (base-experts) will also increase both hardware and computation costs.  One way to selecting a subset of optimal base-experts out of N is to carry out the experiments explicitly.  There are 2 N- 1 possible combinations.  In this paper, we propose an analytical solution to this task when weighted sum fusion mechanism is used.  The proposed approach is at least valid in the domain of person authentication.  It has a complexity that is additive between the number of examples and the number of possible combinations while the conventional approach, using brute-force experimenting, is multiplicative between these two terms.  Hence, our approach will scale better with large fusion problems.  Experiments on the BANCA multi-modal database verified our approach.  While we will consider here fusion in the context of identity verification via biometrics, or simply biometric authentication, it can also have an important impact in meetings because this a priori information can assist in retrieving highlights in meeting analysis as in "who said what".  Furthermore, automatic meeting analysis also requires many systems working together and involves possibly many audio-visual media streams.  Development in fusion of identity verification will provide insights into how fusion in meetings can be done.  The ability to predict fusion performance is another important step towards understanding the fusion problem. 
An Investigation of Spectral Subband Centroids for Speaker Authentication| Abstract.  Most conventional features used in speaker authentication are based on estimation of spectral envelopes in one way or another, in the form of cepstrums, e. 
On the Prediction of Solar Activity Using Different Neural Network Models| Abstract Accurate prediction of ionospheric parameters is crucial for telecommunication companies.  These parameters strongly rely on solar activity.  In this paper, we analyze the use of neural networks for sunspots time series prediction.  Three types of models are tested and experimental results are reported for a particular sunspots time series: the IR5 index. 
Piecing Together the Emotion Jigsaw| Abstract.  People are emotional, and machines are not.  That constrains their communication, and defines a key challenge for the information sciences.  Different groups have addressed it from different angles, trying to develop methods of detecting emotion, agents that convey emotion, systems that predict behaviour in emotional circumstances, and so on.  Progress has been limited.  The new network of excellence HUMAINE explores the idea that progress depends on addressing the problem as a whole, not in isolated fragments. 
Scalability Analysis of Audio-Visual Person Identity Verification| Abstract.  In this work, we present a multimodal identity verification system based on the fusion of the face image and the text independent speech data of a person.  The system conciliates the monomodal face and speaker verification algorithms by fusing their respective scores.  In order to assess the authentication system at dierent scales, the performance is evaluated at various sizes of the face and speech user template.  The user template size is a key parameter when the storage space is limited like in a smart card.  Our experimental results show that the multimodal fusion allows to reduce significantly the user template size while keeping a satisfactory level of performance.  Experiments are performed on the newly recorded multimodal database BANCA. 
Confidence measures for multimodal identity verification| Abstract Multimodal fusion for identity verification has already shown great improvement compared to unimodal algorithms.  In this paper, we propose to integrate con#dence measures during the fusion process.  We present a comparison of three different methods to generate such confidence information from unimodal identity verification systems.  These methods can be used either to enhance the performance of a multimodal fusion algorithm or to obtain a confidence level on the decisions taken by the system.  All the algorithms are compared on the same benchmark database, namely XM2VTS, containing both speech and face information.  Results show that some confidence measures did improve statistically significantly the performance, while other measures produced reliable confidence levels over the fusion decisions. 
Robust Features for Frontal Face Authentication in Difficult Image Conditions| Abstract.  In this report we extend the recently proposed DCT-mod2 feature extraction technique (which utilizes polynomial coefficients derived from 2D DCT coefficients obtained from horizontally & vertically neighbouring blocks) via the use of various windows and diagonally neighbouring blocks.  We also evaluate enhanced PCA, where traditional PCA feature extraction is combined with DCT-mod2.  Results using test images corrupted by a linear and a non-linear illumination change, white Gaussian noise and compression artefacts, show that use of diagonally neighbouring blocks and windowing is detrimental to robustness against illumination changes while being useful for increasing robustness against white noise and compression artefacts.  We also show that the enhanced PCA technique retains all the positive aspects of traditional PCA (that is robustness against white noise and compression artefacts) while also being robust to illumination direction changes; moreover, enhanced PCA outperforms PCA with histogram equalisation pre-processing. 
Client Dependent GMM-SVM Models for Speaker Verification| Support Vector Machines to postprocess scores obtained by the GMMs.  A cross-validation method is also used in the baseline system to increase the number of client scores in the training phase, which enhances the results of the SVM models.  Experiments carried out on the XM2VTS and PolyVar databases confirm the interest of this hybrid approach. 
Noise-Robust Multi-Stream Fusion for Text-Independent Speaker Authentication| Abstract.  Multi-stream approaches have proven to be very successful in speech recognition tasks and to a certain extent in speaker authentication tasks.  In this study we propose a noiserobust multi-stream text-independent speaker authentication system.  This system has two steps: first train the stream experts under clean conditions and then train the combination mechanism to merge the scores of the stream experts under both clean and noisy conditions.  The idea here is to take advantage of the rather predictable reliability and diversity of streams under different conditions.  Hence, noise-robustness is mainly due to the combination mechanism.  This two-step approach offers several practical advantages: the stream experts can be trained in parallel (e. g. , by using several machines); heterogeneous types of features can be used and the resultant system can be robust to different noise types (wide bands or narrow bands) as compared to sub-streams.  An important finding is that a trade-off is often necessary between the overall good performance under all conditions (clean and noisy) and good performance under clean conditions.  To reconcile this trade-off, we propose to give more emphasis or prior to clean conditions, thus, resulting in a combination mechanism that does not deteriorate under clean conditions (as compared to the best stream) yet is robust to noisy conditions. 
Use of Genetic Programming for the Search of a New Learning Rule for Neural Networks| Abstract--- In previous work ([1, 2, 3]) we explained how to use standard optimization methods such as simulated annealing,
An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition| Abstract This paper presents a novel Hidden Markov Model architecture to model the joint probability of pairs of asynchronous sequences describing the same event. 
Estimation of Conditional Distributions using Gaussian Mixture Models| March 2002 submitted for publication Abstract.  This paper proposes the use of Gaussian Mixture Models to estimate conditional probability density functions.  A conditional Gaussian Mixture Model has been compared to the geostatistical method of Sequential Gaussian Simulations.  The data set used is a part of the digital elevation model of Switzerland. 
Stochastic Learning of Strategic Equilibria for Auctions| Abstract This paper presents a new application of stochastic adaptive learning algorithms to the computation of strategic equilibria in auctions.  The proposed approach addresses the problems of tracking a moving target and balancing exploration (of action space) versus exploitation (of better modeled regions of action space).  Neural networks are used to represent a stochastic decision model for each bidder.  Experiments confirm the correctness and usefulness of the approach. 
Automatic Analysis of Multimodal Group Actions in Meetings| Abstract.  This paper investigates the recognition of group actions in meetings.  A statistical framework is proposed in which group actions result from the interactions of the individual participants.  The group actions are modelled using dierent HMM-based approaches, where the observations are provided by a set of audio-visual features monitoring the actions of individuals.  Experiments demonstrate the importance of taking interactions into account in modelling the group actions.  It is also shown that the visual modality contains useful information, even for predominantly audio-based events, motivating a multimodal approach to meeting analysis. 
Theoretical Basis and Software Specifications|
A GENTLE HESSIAN FOR EFFICIENT GRADIENT DESCENT| ABSTRACT Several second-order optimization methods for gradient descent algorithms have been proposed over the years, but they usually need to compute the inverse of the Hessian of the cost function (or an approximation of this inverse) during training.  In most cases, this leads to an O(n 2 ) cost in time and space per iteration, where n is the number of parameters, which is prohibitive for large n.  We propose instead a study of the Hessian before training.  Based on a second order analysis, we show that a block-diagonal Hessian yields an easier optimization problem than a full Hessian.  We also show that the condition of block-diagonality in common machine learning models can be achieved by simply selecting an appropriate training criterion.  Finally, we propose a version of the SVM criterion applied to MLPs, which verifies the aspects highlighted in this second order analysis, but also yields very good generalization performance in practice, taking advantage of the margin effect.  Several empirical comparisons on two benchmark datasets are given to illustrate this approach. 
FACE VERIFICATION USING SYNTHESIZED NON-FRONTAL MODELS| Abstract.  In this report we address the problem of non-frontal face verification when only a frontal training image is available (e. g.  a passport photograph) by augmenting a client's frontal face model with artificially synthesized models for non-frontal views.  In the framework of a Gaussian Mixture Model (GMM) based classifier, two techniques are proposed for the synthesis: UBMdiff and LinReg.  Both techniques rely on a priori information and learn how face models for the frontal view are related to face models at a non-frontal view.  The synthesis and augmentation approach is evaluated by applying it to two face verification systems: Principal Component Analysis (PCA) based and DCTmod2 [31] based; the two systems are a representation of holistic and non-holistic approaches, respectively.  Results from experiments on the FERET database suggest that in almost all cases, frontal model augmentation has beneficial effects for both systems; they also suggest that the LinReg technique (which is based on multivariate regression of classifier parameters) is more suited to the PCA based system and that the UBMdiff technique (which is based on differences between two general face models) is more suited to the DCTmod2 based system.  The results also support the view that the standard DCTmod2/GMM system (trained on frontal faces) is less affected by out-of-plane rotations than the corresponding PCA/GMM system; moreover, the DCTmod2/GMM system using augmented models is, in almost all cases, more robust than the corresponding PCA/GMM system. 
The BANCA Database and Experimental Protocol for Speaker Verification| Abstract.  Identity verification has become a very important research topic recently, particularly using methods based on the face or the voice of the individuals.  In the context of the BANCA european project, a novel multi-modal database was recently recorded, spanning 5 european languages, 2 modalities (face and voice), 2 microphones, 2 cameras and almost 300 individuals.  As we believe that this database offers many advantages for this research community, this paper essentially presents the database and its associated experimental protocol, as well as a baseline state-of-the-art system using the voice data for a text-independent speaker verification task. 
STATISTICAL TRANSFORMATION TECHNIQUES FOR FACE VERIFICATION USING FACES ROTATED IN DEPTH| Abstract.  In the framework of a Bayesian classifier based on mixtures of gaussians, we address the problem of non-frontal face verification (when only a single (frontal) training image is available) by extending each frontal face model with artificially synthesized models for non-frontal views.  The synthesis methods are based on several implementations of Maximum Likelihood Linear Regression (MLLR), as well as standard multi-variate linear regression (LinReg).  All synthesis techniques rely on prior information and learn how face models for the frontal view are related to face models for non-frontal views.  The synthesis and extension approach is evaluated by applying it to two face verification systems: PCA based (holistic features) and DCTmod2 based (local features).  Experiments on the FERET database suggest that for the PCA based system, the LinReg based technique is more suited than the MLLR based techniques; for the DCTmod2 based system, the results show that synthesis via a new MLLR implementation obtains better performance than synthesis based on traditional MLLR.  The results further suggest that extending frontal models considerably reduces errors.  It is also shown that the DCTmod2 based system is less affected by out-of-plane rotations than the PCA based system; this can be attributed to the local feature representation of the face, and, due to the classifier based on mixtures of gaussians, the lack of constraints on spatial relations between face parts, allowing for movement of facial areas. 
Face Verification Using Adapted Generative Models| Abstract It has been shown previously that systems based on local features and relatively complex generative models, namely 1D Hidden Markov Models (HMMs) and pseudo-2D HMMs, are suitable for face recognition (here we mean both identification and verification).  Recently a simpler generative model, namely the Gaussian Mixture Model (GMM), was also shown to perform well.  In this paper we first propose to increase the performance of the GMM approach (without sacrificing its simplicity) through the use of local features with embedded positional information; we show that the performance obtained is comparable to 1D HMMs.  Secondly, we evaluate different training techniques for both GMM and HMM based systems.  We show that the traditionally used Maximum Likelihood (ML) training approach has problems estimating robust model parameters when there is only a few training images available; we propose to tackle this problem through the use of Maximum a Posteriori (MAP) training, where the lack of data problem can be effectively circumvented; we show that models estimated with MAP are significantly more robust and are able to generalize to adverse conditions present in the BANCA database. 
Shared Context Probabilistic Transducers| Abstract Recently, a model for supervised learning of probabilistic transducers represented by suffix trees was introduced.  However, this algorithm tends to build very large trees, requiring very large amounts of computer memory.  In this paper, we propose a new, more compact, transducer model in which one shares the parameters of distributions associated to contexts yielding similar conditional output distributions.  We illustrate the advantages of the proposed algorithm with comparative experiments on inducing a noun phrase recognizer. 
A STATISTICAL SIGNIFICANCE TEST FOR PERSON AUTHENTICATION| ABSTRACT Assessing whether two models are statistically significantly different from each other is a very important step in research, although it has unfortunately not received enough attention in the field of person authentication.  Several performance measures are often used to compare models, such as half total error rates (HTERs) and equal error rates (EERs), but most being aggregates of two measures (such as the false acceptance rate and the false rejection rate), simple statistical tests cannot be used as is.  We show in this paper how to adapt one of these tests in order to compute a confidence interval around one HTER measure or to assess the statistical significantness of the difference between two HTER measures.  We also compare our technique with other solutions that are sometimes used in the literature and show why they yield often too optimistic results (resulting in false statements about statistical significantness). 
Generalization of a Parametric Learning Rule \Lambda| Abstract In previous work (
Online Policy Adaptation for Ensemble Classifiers| Abstract.  Ensemble algorithms can improve the performance of a given learning algorithm through the combination of multiple base classifiers into an ensemble.  In this paper, the idea of using an adaptive policy for training and combining the base classifiers is put forward.  The eectiveness of this approach for online learning is demonstrated by experimental results on several UCI benchmark databases. 
Writer adaptation techniques in HMM based Off-Line Cursive Script Recognition| Abstract.  This work presents the application of HMM adaptation techniques to the problem of Off-Line Cursive Script Recognition.  Instead of training a new model for each writer, one first creates a unique model with a mixed database and then adapts it for each different writer using his own small dataset.  Experiments on a publicly available benchmark database show that an adapted system has an accuracy higher than 80% even when less than 30 word samples are used during adaptation, while a system trained using the data of the single writer only needs at least 200 words in order to achieve the same performance as the adapted models. 
Spectral Subband Centroids as Complementary Features for Speaker Authentication| Perceptual Linear Prediction (RASTA-PLP).  In this study, Spectral Subband Centroids (SSCs) are examined.  These features are the centroid frequency in each subband.  They have properties similar to formant frequencies but are limited to a given subband.  Empirical experiments carried out on the NIST2001 database using SSCs, MFCCs, LFCCs and their combinations by concatenation suggest that SSCs are somewhat more robust compared to conventional MFCC and LFCC features as well as being partially complementary. 
Why Do Multi-Stream, Multi-Band and Multi-Modal Approaches Work on Biometric User Authentication Tasks?| Abstract.  Multi-band, multi-stream and multi-modal approaches have proven to be very successful both in experiments and in real-life applications, among which speech recognition and biometric authentication are of particular interest here.  However, there is a lack of a theoretical study to justify why and how they work, when one combines the streams at the feature or classifier score levels.  In this paper, we attempt to cast a light onto the latter subject.  Our findings suggest that combining several experts using the mean operator, Multi-Layer-Perceptrons and Support Vector Machines always perform better than the average performance of the underlying experts.  Furthermore, in practice, most combined experts using the methods mentioned above perform better than the best underlying expert. 
On the Optimization of a Synaptic Learning Rule| Abstract This paper presents a new approach to neural modeling based on the idea of using an automated method to optimize the parameters of a synaptic learning rule.  The synaptic modification rule is considered as a parametric function.  This function has local inputs and is the same in many neurons.  We can use standard optimization methods to select appropriate parameters for a given type of task.  We also present a theoretical analysis permitting to study the generalization property of such parametric learning rules.  By generalization, we mean the possibility for the learning rule to learn to solve new tasks.  Experiments were performed on three types of problems: a biologically inspired circuit (for conditioning in Aplysia), Boolean functions (linearly separable as well as non linearly separable) and classification tasks.  The neural network architecture as well as the form and initial parameter values of the synaptic learning function can be designed using a priori knowledge. 
HMM2- EXTRACTION OF FORMANT STRUCTURES AND THEIR USE FOR ROBUST ASR HMM2- EXTRACTION OF FORMANT STRUCTURES AND THEIR USE FOR ROBUST ASR| Abstract: As recently introduced in [1], an HMM2 can be considered as a particular case of an HMM mixture in which the HMM emission probabilities (usually estimated through Gaussian mixtures or an artificial neural network) are modeled by state-dependent, feature-based HMM (referred to as frequency HMM).  A general EM training algorithm for such a structure has been developed [2].  Although there are numerous motivations for using such a structure, and many possible ways to exploit it, this paper will mainly focus on one particular instantiation of HMM2 in which the frequency HMM will be used to extract formant structure information, which will then be used as additional acoustic features in a standard Automatic Speech Recognition (ASR) system.  While the fact that this architecture is able to automatically extract meaningful formant information is interesting by itself, empirical results also show the robustness of these features to noise, and their potential to enhance state-of-the-art noise-robust HMMbased ASR. 
A Multi-sample Multi-source Model for Biometric Authentication| Abstract.  In this study, two techniques that can improve the authentication process are examined: (i) multiple samples and (ii) multiple biometric sources.  We propose the fusion of multiple samples obtained from multiple biometric sources at the score level.  By using the average operator, both the theoretical and empirical results show that integrating as many samples and as many biometric sources as possible can improve the overall reliability of the system.  This strategy is called multi-sample multi-source approach.  This strategy was tested on a real-life database using neural networks trained in one-versus-all configuration and evaluated on a validation set. 
models for speech and speaker recognition| Abstract.  Generative probability models such as Hidden Markov Models are usually used for modeling sequences of data because of their ability to handle variable size sequences and missing information.  On the other hand, because of their discriminative properties, discriminative models like Support Vector Machines (SVMs) usually yield better performance in classification problem and can construct flexible decision boundaries.  An ideal classifier should have all the power of these two complementary approaches.  A series of recent papers has suggested some techniques for mixing generative models and discriminative models.  In one of them a fixed size vector (the Fisher score) containing sufficient statistics of a sequence is computed for a previously trained HMM and can then be used as input to a discriminative model for classification.  The purpose of this project is thus to study, experiment, enhance and adapt these new approaches of integrating discriminative models such as SVM into generative models for sequence processing problems, such as speaker and speech recognition. 
Improving Kernel Classifiers for Object Categorization Problems| Abstract This paper presents an approach for improving the performance of kernel classifiers applied to object categorization problems.  The approach is based on the use of distributions centered around each training points, which are exploited for inter-class invariant image representation with local invariant features.  Furthermore, we propose an extensive use of unlabeled images for improving the SVMbased classifier. 
An EM Algorithm for Asynchronous Input/Output Hidden Markov Models| Abstract--- In learning tasks in which input sequences are mapped to output sequences, it is often the case that the input and output sequences are not synchronous.  For example, in speech recognition, acoustic sequences are longer than phoneme sequences.  Input/Output Hidden Markov Models have already been proposed to represent the distribution of an output sequence given an input sequence of the same length.  We extend here this model to the case of asynchronous sequences, and show an Expectation-Maximization algorithm for training such models. 
Links between perceptrons, MLPs and SVMs| Abstract We propose to study links between three important classification algorithms: Perceptrons, Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs).  We first study ways to control the capacity of Perceptrons (mainly regularization parameters and early stopping), using the margin idea introduced with SVMs.  After showing that under simple conditions a Perceptron is equivalent to an SVM, we show it can be computationally expensive in time to train an SVM (and thus a Perceptron) with stochastic gradient descent, mainly because of the margin maximization term in the cost function.  We then show that if we remove this margin maximization term, the learning rate or the use of early stopping can still control the margin.  These ideas are extended afterward to the case of MLPs.  Moreover, under some assumptions it also appears that MLPs are a kind of mixture of SVMs, maximizing the margin in the hidden layer space.  Finally, we present a very simple MLP based on the previous findings, which yields better performances in generalization and speed than the other models. 
Scaling Large Learning Problems with Hard Parallel Mixtures| A challenge for statistical learning is to deal with large data sets, e. g.  in data mining.  The training time of ordinary Support Vector Machines is at least quadratic, which raises a serious research challenge if we want to deal with data sets of millions of examples.  We propose a "hard parallelizable mixture" methodology which yields significantly reduced training time through modularization and parallelization: the training data is iteratively partitioned by a "gater" model in such a way that it becomes easy to learn an "expert" model separately in each region of the partition.  A probabilistic extension and the use of a set of generative models allows representing the gater so that all pieces of the model are locally trained.  For SVMs, time complexity appears empirically to locally grow linearly with the number of examples, while generalization performance can be enhanced.  For the probabilistic version of the algorithm, the iterative algorithm provably goes down in a cost function that is an upper bound on the negative log-likelihood. 
Some Remarks on our paper: Support Vector Machines for Large-Scale Regression Problems| Abstract The following remarks have also been added in the appendix of the report,
TOWARDS USING HIERARCHICAL POSTERIORS FOR FLEXIBLE AUTOMATIC SPEECH RECOGNITION SYSTEMS| ABSTRACT Local state (or phone) posterior probabilities are often investigated as local classifiers (e. g. , hybrid HMM/ANN systems) or as transformed acoustic features (e. g. , "TANDEM") towards improved speech recognition systems.  In this paper, we present initial results towards boosting these approaches by improving the local state, phone, or word posterior estimates, using all possible acoustic information (as available in the whole utterance), as well as possible prior information (such as topological constraints).  Furthermore, this approach results in a family of new HMM based systems, where only (local and global) posterior probabilities are used, while also providing a new, principled, approach towards a hierarchical use/integration of these posteriors, from the frame level up to the sentence level.  Initial results on several speech (as well as other multimodal) tasks resulted in significant improvements.  In this paper, we present recognition results on Numbers'95 and on a reduced vocabulary version (1000 words) of the DARPA Conversational Telephone Speech-to-text (CTS) task. 
HMM and IOHMM Modeling of EEG Rhythms for Asynchronous BCI Systems|
Surprising Outcome While Benchmarking Statistical Tests| Abstract Although non-parametric tests have already been proposed for that purpose, statistical significance tests for non-standard measures (different from the classification error) are less often used in the literature.  This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions.  More precisely, using a very large dataset to estimate the whole "population", we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size.  A surprising conclusion is that paired tests such as McNemar badly fail when comparing models which are similar (such as SVMs with different kernels).  On the other hand, non-parametric tests were relatively robust to class unbalance and the closeness of the models. 
HMM2- A NOVEL APPROACH TO HMM EMISSION PROBABILITY ESTIMATION HMM2- A NOVEL APPROACH TO HMM EMISSION PROBABILITY ESTIMATION| Abstract: In this paper, we discuss and investigate a new method to estimate local emission probabilities in the framework of hidden Markov models (HMM).  Each feature vector is considered to be a sequence and is supposed to be modeled by yet another HMM.  Therefore, we call this approach `HMM2'.  There is a variety of possible topologies of such HMM2 systems, e. g.  incorporating trellis or ergodic HMM structures.  Preliminary HMM2 speech recognition experiments on cepstral and spectral features yielded worse results than state-of-the-art systems.  However, we believe that HMM2 systems have a lot of potential advantages and are therefore worth investigating further. 
Evaluation of formant-like features for automatic speech recognition| Abstract This study investigates possibilities to find a low-dimensional, formant-related physical representation of speech signals, which is suitable for automatic speech recognition.  This aim is motivated by the fact that formants are known to be discriminant features for speech recognition.  Combinations of automatically extracted formant-like features and state-of-the-art, noise-robust features have previously been shown to be more robust in adverse conditions than state-of-the-art features alone.  However, it is not clear how these automatically extracted formant-like features behave in comparison with true formants.  The purpose of this paper is to investigate two methods to automatically extract formant-like features, i. e.  robust formants and HMM2 features, and to compare these features to hand-labeled formants as well as to mel-frequency cepstral coecients in terms of their performance on a vowel classification task.  The speech data and hand-labeled formants that were used in this study are a subset of the American English vowels database presented in [Hillenbrand et al. , J.  Acoust.  Soc.  Am.  97, 3099-3111 (1995)].  Classification performance was measured on the original, clean data as well as in (simulated) adverse conditions.  In combination with standard automatic speech recognition methods, the classification performance of the robust formant and HMM2 features compare very well to the performance of the hand-labeled formants. 
The Expected Performance Curve: a New Assessment Measure for Person Authentication| Abstract ROC and DET curves are often used in the field of person authentication to assess the quality of a model or even to compare several models.  We argue in this paper that this measure can be misleading as it compares performance measures that cannot be reached simultaneously by all systems.  We propose instead new curves, called Expected Performance Curves (EPC).  These curves enable the comparison between several systems according to a criterion, decided by the application, which is used to set thresholds according to a separate validation set.  A free sofware is available to compute these curves.  A real case study is used throughout the paper to illustrate it.  Finally, note that while this study was done on an authentication problem, it also applies to most 2-class classification tasks. 
MODELING INDIVIDUAL AND GROUP ACTIONS IN MEETINGS: A TWO-LAYER HMM FRAMEWORK| 1 IDIAP, Martigny, Switzerland IDIAP Research Report 04-09
STATISTICAL TRANSFORMATIONS OF FRONTAL MODELS FOR NON-FRONTAL FACE VERIFICATION| ABSTRACT In the framework of a face verification system using local features and a Gaussian Mixture Model based classifier, we address the problem of non-frontal face verification (when only a single (frontal) training image is available) by extending each client's frontal face model with artificially synthesized models for non-frontal views.  Furthermore, we propose the Maximum Likelihood Shift (MLS) synthesis technique and compare its performance against a Maximum Likelihood Linear Regression (MLLR) based technique (originally developed for adapting speech recognition systems) and the recently proposed "difference between two Universal Background Models" (UBMdiff) technique.  All techniques rely on prior information and learn how a generic face model for the frontal view is related to generic models at non-frontal views.  Experiments on the FERET database suggest that that the proposed MLS technique is more suitable than MLLR (due to a lower number of free parameters) and UBMdiff (due to lack of heuristics).  The results further suggest that extending frontal models considerably reduces errors. 
Multimodal Speech Processing Using Asynchronous Hidden Markov Models| Abstract This paper advocates that for some multimodal tasks involving more than one stream of data representing the same sequence of events, it might sometimes be a good idea to be able to desynchronize the streams in order to maximize their joint likelihood.  We thus present a novel Hidden Markov Model architecture to model the joint probability of pairs of asynchronous sequences describing the same sequence of events.  An Expectation-Maximization algorithm to train the model is presented, as well as a Viterbi decoding algorithm, which can be used to obtain the optimal state sequence as well as the alignment between the two sequences.  The model was tested on two audio-visual speech processing tasks, namely speech recognition and text-dependent speaker verification, both using the M2VTS database.  Robust performances under various noise conditions were obtained in both cases. 
Offline Recognition of Unconstrained Handwritten Texts Using HMMs and Statistical Language Models| Abstract.  This paper presents a system for the offline recognition of large vocabulary unconstrained handwritten texts.  The only assumption made about the data is that it is written in English.  This allows the application of Statistical Language Models in order to improve the performance of our system.  Several experiments have been performed using both single and multiple writer data.  Lexica of variable size (from 10,000 to 50,000 words) have been used.  The use of language models is shown to improve the accuracy of the system (when the lexicon contains 50,000 words, error rate is reduced by #50% for single writer data and by #25% for multiple writer data).  Our approach is described in detail and compared with other methods presented in the literature to deal with the same problem.  An experimental setup to correctly deal with unconstrained text recognition is proposed. 
Martigny - Valais - Suisse TOWARDS ROBUST AND ADAPTIVE SPEECH RECOGNITION MODELS|
How Do Correlation and Variance of Base-Experts Affect Fusion in Biometric Authentication Tasks?| Abstract--- Combining multiple information sources such as subbands, streams (with different features) and multi modal data has been shown to be a very promising trend, both in experiments and to some extents in real-life biometric authentication applications.  Despite considerable efforts in fusions, there is a lack of understanding on the roles and effects of correlation and variance (of both the client and impostor scores of base-classifiers/experts).  Often, scores are assumed to be independent.  In this paper, we explicitly consider this factor using a theoretical model, called Variance Reduction-Equal Error Rate (VR-EER) analysis.  Assuming that client and impostor scores are approximately Gaussian distributed, we showed that Equal Error Rate (EER) can be modeled as a function of F-ratio, which itself is a function of 1) correlation, 2) variance of base-experts and 3) difference of client and impostor means.  To achieve lower EER, smaller correlation and average variance of base-experts, and larger mean difference are desirable.  Furthermore, analysing any of these factors independently, e. g.  focusing on correlation alone, could be miss-leading.  Experimental results on the BANCA multimodal database confirm our findings using VR-EER analysis.  We analysed four commonly encountered scenarios in biometric authentication which include fusing correlated/uncorrelated baseexperts of similar/different performances.  The analysis explains and shows that fusing systems of different performances is not always beneficial.  One of the most important findings is that positive correlation "hurts" fusion while negative correlation (greater "diversity", which measures the spread of prediction score with respect to the fused score), improves fusion.  However, by linking the concept of ambiguity decomposition to classification problem, it is found that diversity is not sufficient to be an evaluation criterion (to compare several fusion systems), unless measures are taken to normalise the (class-dependent) variance.  Moreover, by linking the concept of bias-variance-covariance decomposition to classification using EER, it is found that if the inherent mismatch (between training and test sessions) can be learned from the data, such mismatch can be incorporated into the fusion system as a part of training parameters. 
Towards Computer Understanding of Human Interactions| We also comment on current developments and the future challenges in automatic meeting analysis. 
WHY DO MULTI-STREAM, MULTI-BAND AND MULTI-MODAL APPROACHES WORK ON BIOMETRIC USER AUTHENTICATION TASKS?| ABSTRACT Multi-band, multi-stream and multi-modal approaches have proven to be very successful both in experiments and in real-life applications, among which speech recognition and biometric authentication are of particular interest here.  However, there is a lack of a theoretical study to justify why and how they work, when one combines the streams at the feature or classifier score levels.  In this paper, we attempt to cast a light onto the latter subject.  While there exists literature discussing this aspect, a study on the relationship between correlation, variance reduction and Equal Error Rate (often used in biometric authentication) has not been treated theoretically as done here, using the mean operator.  Our findings suggest that combining several experts using the mean operator, MultiLayer-Perceptrons and Support Vector Machines always perform better than the average performance of the underlying experts.  Furthermore, in practice, most combined experts using the methods mentioned above perform better than the best underlying expert. 
Increasing Speech Recognition Noise Robustness with HMM2| Abstract: The purpose of this paper is to investigate the behavior of HMM2 models for the recognition of noisy speech.  It has previously been shown that HMM2 is able to model dynamically important structural information inherent in the speech signal, often corresponding to formant positions/tracks.  As formant regions are known to be robust in adverse conditions, HMM2 seems particularly promising for improving speech recognition robustness.  Here, we review different variants of the HMM2 approach with respect to their application to noise-robust automatic speech recognition.  It is shown that HMM2 has the potential to tackle the problem of mismatch between training and testing conditions, and that a multi-stream combination of (already noise-robust) cepstral features and formant-like features (extracted by HMM2) improves the noise robustness of a state-of-the-art automatic speech recognition system. 
JOINT DECODING FOR PHONEME-GRAPHEME CONTINUOUS SPEECH RECOGNITION| ABSTRACT Standard ASR systems typically use phoneme as the subword units.  Preliminary studies have shown that the performance of the ASR system could be improved by using grapheme as additional subword units.  In this paper, we investigate such a system where the word models are defined in terms of two different subword units, i. e. , phoneme and grapheme.  During training, models for both the subword units are trained, and then during recognition either both or just one subword unit is used.  We have studied this system for a continuous speech recognition task in American English language.  Our studies show that grapheme information used along with phoneme information improves the performance of ASR. 
Towards Predicting Optimal Fusion Candidates: A Case Study on Biometric Authentication Tasks| Abstract.  Combining multiple information sources, typically from several data streams is a very promising approach, both in experiments and to some extend in various real-life applications.  However, combining too many systems (baseexperts) will also increase both hardware and computation costs.  One way to selecting a subset of optimal base-experts out of N is to carry out the experiments explicitly.  There are 2 N- 1 possible combinations.  In this paper, we propose an analytical solution to this task when weighted sum fusion mechanism is used.  The proposed approach is at least valid in the domain of person authentication.  It has a complexity that is additive between the number of examples and the number of possible combinations while the conventional approach, using brute-force experimenting, is multiplicative between these two terms.  Hence, our approach will scale better with large fusion problems.  Experiments on the BANCA multi-modal database verified our approach.  While we will consider here fusion in the context of identity verification via biometrics, or simply biometric authentication, it can also have an important impact in meetings because this a priori information can assist in retrieving highlights in meeting analysis as in "who said what".  Furthermore, automatic meeting analysis also requires many systems working together and involves possibly many audio-visual media streams.  Development in fusion of identity verification will provide insights into how fusion in meetings can be done.  The ability to predict fusion performance is another important step towards understanding the fusion problem. 
NOISE-ROBUST MULTI-STREAM FUSION FOR TEXT-INDEPENDENT SPEAKER AUTHENTICATION| ABSTRACT The the use of several acoustic feature types, also called the multi-stream approach, has proven to be very successful in speech recognition tasks and to a certain extent in speaker authentication tasks.  In this study we propose a noise-robust multi-stream text-independent speaker authentication system.  This system is trained in two steps: first train the stream experts under clean conditions and then train the combination mechanism to merge the scores of the stream experts under both clean and noisy conditions.  The idea here is to take advantage of the rather predictable reliability and diversity of streams under different conditions.  Hence, noise-robustness is mainly due to the combination mechanism.  This two-step approach offers several practical advantages: the stream experts can be trained in parallel (e. g. , by using several machines); heterogeneous types of features can be used and the resultant system can be robust to different noise types (wide bands or narrow bands) as compared to sub-streams.  An important finding is that a trade-off is often necessary between the overall good performance under all conditions (clean and noisy) and good performance under clean conditions.  To reconcile this trade-off, we propose to give more emphasis or prior to clean conditions, thus, resulting in a combination mechanism that does not deteriorate under clean conditions (as compared to the best stream) yet is robust to noisy conditions. 
The BANCA Database and Evaluation Protocol| Abstract.  In this paper we describe the acquisition and content of a new large, realistic and challenging multi-modal database intended for training and testing multi-modal verification systems.  The BANCA database was captured in four European languages in two modalities (face and voice).  For recording, both high and low quality microphones and cameras were used.  The subjects were recorded in three different scenarios, controlled, degraded and adverse over a period of three months.  In total 208 people were captured, half men and half women.  In this paper we also describe a protocol for evaluating verification algorithms on the database.  The database will be made available to the research community
BOOSTING WORD ERROR RATES| ABSTRACT We apply boosting techniques to the problem of word error rate minimisation in speech recognition.  This is achieved through a new definition of sample error for boosting and a training procedure for hidden Markov models.  For this purpose we define a sample error for sentence examples related to the word error rate.  Furthermore, for each sentence example we define a probability distribution in time that represents our belief that an error has been made at that particular frame.  This is used to weigh the frames of each sentence in the boosting framework.  We present preliminary results on the well-known Numbers 95 database that indicate the importance of this temporal probability distribution. 
Learning a synaptic learning rule| Abstract This paper presents an original approach to neural modeling based on the idea of
Multimodal Group Action Clustering in Meetings| ABSTRACT We address the problem of clustering multimodal group actions in meetings using a two-layer HMM framework.  Meetings are structured as sequences of group actions.  Our approach aims at creating one cluster for each group action, where the number of group actions and the action boundaries are unknown a priori.  In our framework, the first layer models typical actions of individuals in meetings using supervised HMM learning and low-level audio-visual features.  A number of options that explicitly model certain aspects of the data (e. g. , asynchrony) were considered.  The second layer models the group actions using unsupervised HMM learning.  The two layers are linked by a set of probabilitybased features produced by the individual action layer as input to the group action layer.  The methodology was assessed on a set of multimodal turn-taking group actions, using a public five-hour meeting corpus.  The results show that the use of multiple modalities and the layered framework are advantageous, compared to various baseline methods. 
Taking on the Curse of Dimensionality in Joint Distributions Using Neural Networks| Abstract|The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially.  In this paper we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow at most as the square of the number of variables, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions.  The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units.  The connectivity of the neural network can be pruned by using dependency tests between the variables (thus reducing significantly the number of parameters).  Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks, and show that significant improvements can be obtained by pruning the network. 
SVMTorch, an Algorithm for Large-Scale Regression Problems| Abstract.  Recently, many researchers have proposed decomposition algorithms for SVM regression problems (see for instance [11, 3, 6, 10]).  In a previous paper [1], we also proposed such an algorithm, named SVMTorch.  In this paper, we show that while there is actually no convergence proof for any other decomposition algorithm for SVM regression problems to our knowledge, such a proof does exist for SVMTorch for the particular case where no shrinking is used and the size of the working set is equal to 2, which is the size that gave the fastest results on most experiments we have done.  This convergence proof is in fact mainly based on the convergence proof given by Keerthi and Gilbert [4] for their SVM classification algorithm. 
New Approaches Towards Robust, Adaptive Speech Recognition (invited paper)| Abstract.  In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches.  More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR.  These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent) \experts", each expert focusing on a different characteristic of the signal, and that the different stream likelihoods (or posteriors) are combined at some (temporal) stage to yield a global recognition output.  As a further extension to multi-stream ASR, we will finally introduce a new approach, referred to as HMM2, where the HMM emission probabilities are estimated via state specific feature based HMMs responsible for merging the stream information and modeling their possible correlation. 
Improving Face Verification Using Skin Color Information| Abstract The performance of face verification systems has steadily improved over the last few years, mainly focusing on models rather than on feature processing.  State-of-the-art methods often use the gray-scale face image as input.  In this paper, we propose to use an additional feature to the face image: the skin color.  The new feature set is tested on a benchmark database, namely XM2VTS, using a simple discriminant artificial neural network.  Results show that the skin color information improves the performance. 
CONDITIONAL GAUSSIAN MIXTURE MODELS FOR ENVIRONMENTAL RISK MAPPING| Abstract.  This paper proposes the use of Gaussian Mixture Models to estimate conditional probability density functions in an environmental risk mapping context.  A conditional Gaussian Mixture Model has been compared to the geostatistical method of Sequential Gaussian Simulations and shows good performances in reconstructing local PDF.  The data sets used for this comparison are parts of the digital elevation model of Switzerland. 
BOOSTING HMMS WITH AN APPLICATION TO SPEECH RECOGNITION| ABSTRACT Boosting is a general method for training an ensemble of classifiers with a view to improving performance relative to that of a single classifier.  While the original AdaBoost algorithm has been defined for classification tasks, the current work examines its applicability to sequence learning problems, focusing on speech recognition.  We apply boosting at the phoneme model level and recombine expert decisions using multi-stream techniques. 
Tangent Vector Kernels for Invariant Image Classification with SVMs| Abstract This paper presents an application of the general sample-to-object approach to the problem of invariant image classification.  The approach results in defining new SVM kernels based on tangent vectors that take into account prior information on known invariances.  Real data of face images are used for experiments.  The presented approach integrates virtual sample and tangent distance methods.  We observe a significant increase in performance with respect to standard approaches.  The experiments also illustrate (as expected) that prior knowledge becomes more important as the amount of training data decreases. 
The Vehicle Routing Problem with Time Windows -- Part II: Genetic Search|
SVMTorch: Support Vector Machines for Large-Scale Regression Problems|
A Parallel Mixture of SVMs for Very Large Scale Problems|
Modelling human interaction in meetings,|
HMM2 - A novel approach to HMM emission probability estimation|
A comparative study of adaptation methods for speaker verification,|
The Expected Performance Curve|
Comparison of client model adaptation schemes|
Evaluation of biometric technology on XM2VTS,|
Learning the decision function for speaker verification,|
"A new margin-based criterion for efficient gradient descent,|
Face Verification Competition on the XM2VTS Database|
A pragmatic view of the application of|
Non-Linear Variance Reduction Techniques in Biometric Authentication,|
An Investigation of Spectral Subband Centroids For Speaker Authentication|
Multimodal Authentication Using Asynchronous HMMs|
Hybrid generative-discriminative models for speech and speaker recognition,|
Database and Evaluation Protocol|
An EM algorithm for asynchronous input/output hidden Markov models,|
Aspects th'eoriques de l'optimisation d'une rgle d'apprentissage,|
A genetic approach to the vehicle routing problem with time windows|
Experimental Protocol on the BANCA database|
Improving face authentication using virtual samples|
Augmenting Frontal Face Models for Non-Frontal Verification",|
A new speech recognition baseline system for numbers 95 version 1|3 based on torch. 
\Offline recognition of large vocabulary vursive handwritten text,"|
Variance Reduction Techniques in Biometric Authentication,"|
The BANCA Database and Evaluation Protocol|
HMM2 - extraction of formant structures and their use for robust ASR|
A multi-sample multi-source model for biometric authentication,|
Special uses and abuses of the Fiat-Shamir passport protocol|
Database, Protocol and Tools for Evaluating Score-Level Fusion Algorithms in Biometric Authentication,"|
"On Automatic Annoation of Meeting Databases," ICIP,|
An alternative to silence removal for text-independent speaker verification,"|
Use of Modular Architectures for Time Series Prediction|
A Connectionist System for Medium-Term Horizon Time Series Prediction|
Offline Recognition of Large Vocabulary Cursive Handwritten Text|
Phoneme-Grapheme based automatic speech recognition system,|
Modeling human interactions in meetings|
Speech and face based biometric authentication|
Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks|
The Handbook of Brain Theory and Neural Networks, Sencond Edition, ch| Hidden Markov models and other finite state automata for sequence processing. 
Speech recognition using advanced HMM2 features|
Automatic Analysis of Multimodal Group Actions in Meetin s,|
Training asynchronous input/output hidden markov models|
Detecting Group Interest-Level in Meetings,"|
SVMTorch web page,|
utomatic nalysis of Multimodal Group ctions in Meetings,in|
On the search for new learning rules for ANNs|
"Automatic speech recognition using dynamic Bayesian networks with both acoustic and articulatory variables," IDIAP,|
(eds|) Machine Learning for Multimodal Interaction, First International Workshop, MLMI. 
"On automatic annotation of meeting databases,|
Face Authentication Competition on the BANCA Database|

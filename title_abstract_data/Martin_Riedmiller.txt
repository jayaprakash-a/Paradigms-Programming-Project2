Reinforcement Learning on a Omnidirectional Mobile Robot| Abstract--- With this paper we describe a well suited, scalable problem for reinforcement learning approaches in the field of mobile robots.  We show a suitable representation of the problem for a reinforcement approach and present our results with a model based standard algorithm.  Two different approximators for the value function are used, a grid based approximator and a neural network based approximator. 
Learning to Control at Multiple Time Scales| Abstract.  In reinforcement learning the interaction between the agent and the environment generally takes place on a fixed time scale, which means that the control interval is set to a fixed time step.  In order to determine a suitable fixed time scale one has to trade off accuracy in control against learning complexity.  In this paper we present an alternative approach that enables the agent to learn a control policy by using multiple time scales simultaneously.  Instead of preselecting a fixed time scale, there are several time scales available during learning and the agent can select the appropriate time scale depending on the system state.  The different time scales are multiples of a finest time scale which is denoted as the primitive time scale.  Actions on a coarser time scale consist of several identical actions on the primitive time scale and are called multistep actions (MSAs).  The special structure of these actions is eciently exploited in our recent MSA-Q-learning algorithm.  We use the MSAs to learn a control policy for a thermostat control problem.  Our algorithm yields a fast and highly accurate control policy; in contrast, the standard Q-learning algorithms without MSAs fails to learn any useful control policy for this problem. 
Distributed Value Functions| Abstract Many interesting problems, such as power grids, network switches, and traffic flow, that are candidates for solving with reinforcement learning (RL), also have properties that make distributed solutions desirable.  We propose an algorithm for distributed reinforcement learning based on distributing the representation of the value function across nodes.  Each node in the system only has the ability to sense state locally, choose actions locally, and receive reward locally (the goal of the system is to maximize the sum of the rewards over all nodes and over all time).  However each node is allowed to give its neighbors the current estimate of its value function for the states it passes through.  We present a value function learning rule, using that information, that allows each node to learn a value function that is an estimate of a weighted sum of future rewards for all the nodes in the network.  With this representation, each node can choose actions to improve the performance of the overall system.  We demonstrate our algorithm on the distributed control of a simulated power grid.  We compare it against other methods including: use of a global reward signal, nodes that act locally with no communication, and nodes that share rewards (but not value function) information with each other.  Our results show that the distributed value function algorithm outperforms the others, and we conclude with an analysis of what problems are best suited for distributed value functions and the new research directions opened up by this work. 
Karlruhe Brainstormers - Design Principles Karlsruhe-Brainstormers| Abstract.  The following paper describes the design principles of the Karlruhe Brainstormers team for the RoboCup Simulator League.  The basic motivation behind our approach is to broadly apply Machine Learning techniques.  In particular, our longterm goal is to apply Reinforcement Learning techniques to autonomously learn team playing capabilities.  This longterm goal determined the structure of the decision module, which has to choose between several available 'high-level' moves based on evaluation functions.  We plan to reach the final autonomously learning agent in several stages.  The current version uses a hybrid decision module with both rule-based and learning components. 
Concepts and facilities of a neural reinforcement learning control architecture for technical process control| Abstract| The paper presents the concepts of a neural control architecture that is able to learn high quality control behaviour in technical process control from scratch.  As the input to the learning system, only the control target must be specified.  In the first part of the article, the underlying theoretical principles of dynamic programming methods are explained and their adaptation to the context of technical process control is described.  The second part discusses the basic capabilities of the learning system on a typical benchmark problem, where a special focus lies on the quality of the acquired control law.  The application to a highly nonlinear chemical reactor and to an instable multi output system shows the ability of the proposed neural control architecture to learn even dicult control strategies from scratch. 
Advanced Supervised Learning in Multi-layer Perceptrons From Backpropagation to Adaptive Learning Algorithms| Abstract--- Since the presentation of the backpropagation algorithm [1] a vast variety of improvements of the technique for training the weights in a feed-forward neural network have been proposed.  The following article introduces the concept of supervised learning in multi-layer perceptrons based on the technique of gradient descent.  Some problems and drawbacks of the original backpropagation learning procedure are discussed, eventually leading to the development of more sophisticated techniques.  This article concentrates on adaptive learning strategies.  Some of the most popular learning algorithms are described and discussed according to their classification in terms of global and local adaptation strategies.  The behavior of several learning procedures on some popular benchmark problems is reported, thereby illuminating convergence, robustness, and scaling properties of the respective algorithms. 
Preprint from: RoboCup 2004 Symposium Papers and Team Description Papers CD ROM edited by| Abstract.  A mobile robot named KURT3D was developed at the Fraunhofer Institute for Autonomous Intelligent Systems during the last three years.  The key innovation of this system lies in the capability for autonomous or operatorassisted 6D SLAM (simultaneous localization and mapping) and 3D map generation of natural scenes.  Hence, KURT3D already meets the basic requirement regarding urban search and rescue.  For the rescue robot league competition, it is additionally configured with dedicated state-of-the-art equipment.  The robot and the operator station are rather compact and easy to set up.  The operator uses a joystick as a remote control for the robot and can watch a live video of the scene where the robot drives.  Data are transmitted via wireless LAN.  A 3D laser scanner, which is mounted on an outdoor variant of KURT3D, is used as the main sensor for map generation as well as for navigation and localization.  The whole system has been used with a proven record of success for different tasks of map building, so that we are confident of managing the rescue robot league competition, too. 
A direct adaptive method for faster Backpropagation learning: the RPROP algorithm In:|
Fast Network Pruning and Feature Extraction by using the Unit-OBS Algorithm|
An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems|
A Neural Reinforcement Learning Approach to Learn Local Dispatching Policies in Production Scheduling|
RPROP: A Fast Adaptive Learning Algorithm|
Reinforcement Learning for Cooperating and Communicating Reactive Agents in Electrical Power Grids|
"A Fast Adaptive Learning Algorithm|
SNNS Stuttgart Network Simulator| User Manual, Version 4. 2. 
Learning to control dynamic systems|
Untersuchungen zu Konvergenz und Generalisierungsfahigkeit uberwachter Lernverfahren im SNNS|
Schnelle adaptive Lernverfahren fur mehrschichtige Feed--Forward Netzwerke --Vergleich und Weiterentwicklung--|
Advanced supervised learning in multilayer perceptrons - from backpropagation to adaptive learning techniques|
Description and Implementation Details|
Daily prediction of the foreign exchange rate between the us dollar and the german mark using neural networks|
Autonomously learning neural controllers|
Supervised Learning in Multilayer Perceptrons -- from Backpropagation to Adaptive Learning Techniques|
The robocup special interest group on multiagent learning,|
New soccer server initiative|
RoboCup-2003: New Scientific and Technical Advances|
High quality thermostat control by reinforcement learning - a case study|
Speeding-up Reinforcement Learning with Multi-step Actions|
A Neural Approach for the Control of Piezoelectric Micromanipulation Robots, in:|
Aspects of learning neural control|
Reinforcement learning without an explicit terminal state|
Application of sequential reinforcement learning to control dynamic systems|
Generating continuous control signals for reinforcement controllers using dynamic output elements|
RPROP - A fast adaptative learning algorithm"|
Using machine learning techniques in complex multi-agent domains|
SNNS User Manual, Report 3/93,|
Learning Situation Dependent Success Rates of Actions in a RoboCup Scenario|
Using Neural Reinforcement Controllers in Robotics,|

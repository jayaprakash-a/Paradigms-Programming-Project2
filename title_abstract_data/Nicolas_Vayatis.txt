On the Bayes-risk consistency of regularized boosting methods| Abstract The probability of error of classification methods based on convex combinations of simple base classifiers by \boosting" algorithms is investigated.  The main result of the paper is that certain regularized boosting algorithms provide Bayes-risk consistent classifiers under the only assumption that the Bayes classifier may be approximated by a convex combination of the base classifiers.  Non-asymptotic distribution-free bounds are also developed which offer interesting new insight into how boosting works and help explain its success in practical classification problems. 
The Role of Critical Sets in Vapnik-Chervonenkis Theory| Abstract In the present paper, we present the theoretical basis, as well as an empirical validation, of a protocol designed to obtain effective VC dimension estimations in the case of a simple pattern recognition issue.  We first formulate particular (distributiondependent) VC bounds in which a special attention has been given to the exact exponential rate of convergence.  We show indeed that the most significant contribution in such bounds is due to the "worst" elements of the model class (designated as the critical sets).  We then explain how these results can lead to a rigorous framework for computer simulations involving speed-up techniques for rare event simulation (importance sampling) as well as parameter estimation (linear regression).  We thus obtain accurate empirical estimates of the complexity measure and of the multiplicative constant in VC bounds.  In particular, we develop the idea of a local complexity characterization associated to every critical set. 
Distribution-Dependent Vapnik-Chervonenkis Bounds| Abstract.  Vapnik-Chervonenkis (VC) bounds play an important role in statistical learning theory as they are the fundamental result which explains the generalization ability of learning machines.  There have been consequent mathematical works on the improvement of VC rates of convergence of empirical means to their expectations over the years.  The result obtained by Talagrand in 1994 seems to provide more or less the final word to this issue as far as universal bounds are concerned.  Though for fixed distributions, this bound can be practically outperformed.  We show indeed that it is possible to replace the 2ffl 2 under the exponential of the deviation term by the corresponding Cram'er transform as shown by large deviations theorems.  Then, we formulate rigorous distributionsensitive VC bounds and we also explain why these theoretical results on such bounds can lead to practical estimates of the effective VC dimension of learning structures. 
Large deviations bounds for empirical processes| Abstract - Vapnik-Chervonenkis bounds on speeds of convergence of empirical means to their expectations have been continuously improved over the years.  The result obtained by M.  Talagrand in 1994 [11] seems to provide the final word as far as universal bounds are concerned.  However, for fixed families of underlying probability distributions, the exponential rate in the deviation term can be fairly improved by the
Generalization Error Bounds for Aggregation by Mirror Descent with Averaging| Abstract We consider the problem of constructing an aggregated estimator from a finite class of base functions which approximately minimizes a convex risk functional under the ` 1 constraint.  For this purpose, we propose a stochastic procedure, the mirror descent, which performs gradient descent in the dual space.  The generated estimates are additionally averaged in a recursive fashion with specific weights.  Mirror descent algorithms have been developed in different contexts and they are known to be particularly efficient in high dimensional problems.  Moreover their implementation is adapted to the online setting.  The main result of the paper is the upper bound on the convergence rate for the generalization error. 
How to Estimate the Vapnik-Chervonenkis Dimension of Support Vector Machines Through Simulations ?| Abstract Vapnik-Chervonenkis (VC) dimension appears as one of the central concepts in Statistical Learning Theory (Vapnik 1995, 1998).  Though it led to some important mathematical results and contributed to the development of a new class of very efficient algorithms, the Support Vector Machines, there are few attempts (Vapnik et al. , 1994, Vapnik 1995) to turn VC dimension into a practical notion that could be measured or observed in particular learning problems.  In the presentpaper,we review some techniques for computing VC dimension through simulations and, after pointing out some theoretical observations, we propose a new set-up for numerical experiments. 
Ranking and scoring using empirical risk minimization| General sucient conditions are proposed in several special cases that guarantee fast rates of convergence. 
Learning Complexity and Pattern Recognition,|
Thse de doctorat de l'Ecole Polytechnique,|
On the bayes-risk consistency of boosting methods|
Inegalites de Vapnik-Chervonenkis et mesures de complexite|
On the Bayes-risk consistency of regularized boosting methods (with discussion),|
Ranking and scoring using empirical risk|

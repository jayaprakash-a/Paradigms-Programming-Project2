(Submitted version { do not cite)| Abstract This paper presents modifications to a standard probabilistic context-free grammar that enable a predictive parser to avoid garden pathing without resorting to any ad-hoc heuristic repair.  The resulting parser is shown to apply eciently to both newspaper text and telephone conversations with complete coverage and excellent accuracy.  The distribution over trees is peaked enough to allow the parser to find parses eciently,even with the much larger search space.  Empirical results are provided for both Wall St.  Journal and Switchboard test corpora. 
Supervised and unsupervised PCFG adaptation to novel domains| Abstract This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation.  The MAP framework is general enough to include some previous model adaptation approaches, such as corpus mixing in Gildea (2001), for example.  Other approaches falling within this framework are more effective.  In contrast to the results in Gildea (2001), we show F-measure parsing accuracy gains of as much as 2. 5% for high accuracy lexicalized parsing through the use of out-of-domain treebanks, with the largest gains when the amount of indomain data is small.  MAP adaptation can also be based on either supervised or unsupervised adaptation data.  Even when no in-domain treebank is available, unsupervised techniques provide a substantial accuracy gain over unadapted grammars, as much as nearly 5% F-measure improvement. 
Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm| Abstract This paper describes discriminative language modeling for a large vocabulary speech recognition task.  We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs).  The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer.  The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data.  However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0. 5% reduction in word error rate, for a total 1. 8% absolute reduction from the baseline of 39. 2%. 
UNSUPERVISED LANGUAGE MODEL ADAPTATION| ABSTRACT This paper investigates unsupervised language model adaptation, from ASR transcripts.  N-gram counts from these transcripts can be used either to adapt an existing n-gram model or to build an n-gram model from scratch.  Various experimental results are reported on a particular domain adaptation task, namely building a customer care application starting from a general voicemail transcription system.  The experiments investigate the effectiveness of various adaptation strategies, including iterative adaptation and self-adaptation on the test data.  They show an error rate reduction of 3. 9% over the unadapted baseline performance, from 28% to 24. 1%, using 17 hours of unsupervised adaptation material.  This is 51% of the 7. 7% adaptation gain obtained by supervised adaptation.  Self-adaptation on the test data resulted in a 1. 3% improvement over the baseline. 
Language model adaptation with MAP estimation and the perceptron algorithm| Abstract In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm.  Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches.  When combined, the resulting system provides a 0. 7 percent absolute reduction in word error rate over MAP estimation alone.  In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early pass word lattices, since the improved error rate improves acoustic model adaptation. 
Generalized Algorithms for Constructing Statistical Language Models| Abstract Recent text and speech processing applications such as speech mining raise new and more general problems related to the construction of language models.  We present and describe in detail several new and efficient algorithms to address these more general problems and report experimental results demonstrating their usefulness.  We give an algorithm for computing efficiently the expected counts of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton; describe a new technique for creating exact representations of # -gram language models by weighted automata whose size is practical for offline use even for a vocabulary size of about 500,000 words and an # -gram order ##### ; and present a simple and more general technique for constructing class-based language models that allows each class to represent an arbitrary weighted automaton.  An efficient implementation of our algorithms and techniques has been incorporated in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities.  1 Motivation Statistical language models are crucial components of many modern natural language processing systems such as speech recognition, information extraction, machine translation, or document classification.  In all cases, a language model is used in combination with other information sources to rank alternative hypotheses by assigning them some probabilities.  There are classical techniques for constructing language models such as #gram models with various smoothing techniques (see Chen and Goodman (1998) and the references therein for a survey and comparison of these techniques).  In some recent text and speech processing applications, several new and more general problems arise that are related to the construction of language models.  We present new and efficient algorithms to address these more general problems.  Counting.  Classical language models are constructed by deriving statistics from large input texts.  In speech mining applications or for adaptation purposes, one often needs to construct a language model based on the output of a speech recognition system.  But, the output of a recognition system is not just text.  Indeed, the word error rate of conversational speech recognition systems is still too high in many tasks to rely only on the one-best output of the recognizer.  Thus, the word lattice output by speech recognition systems is used instead because it contains the correct transcription in most cases.  A word lattice is a weighted finite automaton (WFA) output by the recognizer for a particular utterance.  It contains typically a very large set of alternative transcription sentences for that utterance with the corresponding weights or probabilities.  A necessary step for constructing a language model based on a word lattice is to derive the statistics for any given sequence from the lattices or WFAs output by the recognizer.  This cannot be done by simply enumerating each path of the lattice and counting the number of occurrences of the sequence considered in each path since the number of paths of even a small automaton may be more than four billion.  We present a simple and efficient algorithm for computing the expected count of any given sequence in a WFA and report experimental results demonstrating its efficiency.  Representation of language models by WFAs.  Classical sical # -gram language models admit a natural representation by WFAs in which each state encodes a left context of width less than # .  However, the size of that representation makes it impractical for offline optimizations such as those used in large-vocabulary speech recognition or general information extraction systems.  Most offline representations of these models are based instead on an approximation to limit their size.  We describe a new technique for creating an exact representation of # -gram language models by WFAs whose size is practical for offline use even in tasks with a vocabulary size of about 500,000 words and for ##### .  Class-based models.  In many applications, it is natural and convenient to construct class-based language models, that is models based on classes of words (Brown et al. , 1992).  Such models are also often more robust since they may include words that belong to a class but that were not found in the corpus.  Classical class-based models are based on simple classes such as a list of words.  But new clustering algorithms allow one to create more general and more complex classes that may be regular languages.  Very large and complex classes can also be defined using regular expressions.  We present a simple and more general approach to class-based language models based on general weighted context-dependent rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996).  Our approach allows us to deal efficiently with more complex classes such as weighted regular languages.  We have fully implemented the algorithms just mentioned and incorporated them in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities (Allauzen et al. , 2003).  In the following, we will present in detail these algorithms and briefly describe the corresponding GRM utilities.  2 Preliminaries Definition 1 A system ############# ### ### is a semiring (Kuich and Salomaa, 1986) if: ######### ### is a commutative monoid with identity element # ; ######### ### is a monoid with identity element # ; # distributes over # ; and # is an annihilator for # : for all ####### #!# #"# #####$# # .  Thus, a semiring is a ring that may lack negation.  Two semirings often used in speech processing are: the log semiring %&#'##(*),+. -0/1####2 3 45##6###-0# #1# (Mohri, 2002) which is isomorphic to the familiar real or probability semiring ##(879##6##5:;# #<#=### via a }@?1A morphism with, for all #B# C##D(E)F+. -0/ : #;# 2 3G4 CH##ID}@?1AJ#LKNMPOQ#RIS#P#Q6,KTMPOQ#UI#CT#U# and the convention that: KTMPOQ#UI#-## # # and ID}@?1A####1#V#W- , and the tropical semiring XY#Y#Z([7\) +. -0/1#G]!^@_`##6###-##G#1# which can be derived from the log semiring using the Viterbi approximation.  Definition 2 A weighted finite-state transducer a over a semiring # is an 8-tuple ab#c#ed###f###g!#Gh##GiH# jk# lQ#UmP# where: d is the finite input alphabet of the transducer; f is the finite output alphabet; g is a finite set of states; h0nog the set of initial states; ipnqg the set of final states; jrn0gs:D#td#)u+#v=/w#x:y##fz)u+wvN/. #{:k#\:Vg a finite set of transitions; l#|<h#}~# the initial weight function; and m#|#i}# the final weight function mapping i to # .  A Weighted automaton '##ed###g!#Gh##GiH# jk# lQ#UmP# is defined in a similar way by simply omitting the output labels.  We denote by #####n#d# the set of strings accepted by an automaton and similarly by ##Z# the strings described by a regular expression .  Given a transition F##j , we denote by # = its input label, x = its origin or previous state and # = its destination state or next state, " N its weight, P = its output label (transducer case).  Given a state iD#`g , we denote by jV i# the set of transitions leaving i .  A path ^#'1~x=NU.  is an element of jk with consecutive transitions: # L#~ S#x wZ , #P#=NN=# .  We extend # and to paths by setting: #V## and F#. ~# .  A cycle is a path whose origin and destination states coincide: # ;##.  We denote by #Zi# i. # the set of paths from i to iand by #Zi#U#Gi1# and #Zi#U#U#Gi5# the set of paths from i to i#with input label &#&d and output label (transducer case).  These definitions can be extended to subsets $# kng , by: ####GQ# -##)8#5xtUL5`#Zi#U#Gi. -# .  The labeling functions (and similarly ) and the weight function can also be extended to paths by defining the label of a path as the concatenation of the labels of its constituent transitions, and the weight of a path as the # -product of the weights of its constituent transitions: # S## ~ #NNU# , " ### ~ #=N##0" .  We also extend to any finite set of paths by setting: " ;9#" .  The output weight associated by to each input string y##d# is: S t#ZB#[# °5¼{½@¾=² 5#lQ##x## <#m#L# # S t#ZB# is defined to be # when #Zh##UQ# i9#k#'.  Similarly, the output weight associated by a transducer a to a pair of input-output string #L#U<# is: a; #LQ#G<## °5¼8½@¾=² wQl#x L##," P#,mB#Z# L# aS #L#U<### # when #Zh##U#U#Gi9###.  A successful path in a weighted automaton or transducer is a path from an initial state to a final state.  is unambiguous if for any string ##yd# there is at most one successful path labeled with .  Thus, an unambiguous transducer defines a function.  For any transducer a , denote by !1#Za## the automaton obtained by projecting a on its output, that is by omitting its input labels.  Note that the second operation of the tropical semiring and the log semiring as well as their identity elements are identical.  Thus the weight of a path in an automaton over the tropical semiring does not change if is viewed as a weighted automaton over the log semiring or viceversa.  3 Counting This section describes a counting algorithm based on general weighted automata algorithms.  Let # ##gk#Gh##GiH##d## . #G##l##Gm# be an arbitrary weighted automaton over the probability semiring and let be a regular expression defined over the alphabet d .  We are interested in counting the occurrences of the sequences #####L# in while taking into account the weight of the paths where they appear.  3. 1 Definition When is deterministic and pushed, or stochastic, it can be viewed as a probability distribution over all strings
A General Weighted Grammar Library| Abstract.  We present a general weighted grammar software library, the GRM Library, that can be used in a variety of applications in text, speech, and biosequence processing.  The underlying algorithms were designed to support a wide variety of semirings and the representation and use of very large grammars and automata of several hundred million rules or transitions.  We describe several algorithms and utilities of this library and point out in each case their application to several text and speech processing tasks. 
Discriminative Syntactic Language Modeling for Speech Recognition| Abstract We describe a method for discriminative training of a language model that makes use of syntactic features.  We follow a reranking approach, where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second "reranking" model is then used to choose an utterance from these 1000-best lists.  The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.  We describe experiments on the Switchboard speech recognition task.  The syntactic features provide an additional 0. 3% reduction in test--set error rate beyond the model of (Roark et al. , 2004a; Roark et al. , 2004b) (significant at p < 0. 001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1. 2% over the baseline Switchboard system. 
Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction| Abstract Generating semantic lexicons semiautomatically could be a great time saver, relative to creating them by hand.  In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars.  Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area.  Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand.  Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an "enhancer" of existing broad-coverage resources. 
Incremental Parsing with the Perceptron Algorithm| Abstract This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm.  A beam-search algorithm is used during both training and decoding phases of the method.  The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.  We demonstrate that training a perceptron model to combine with the generative model during search provides a 2. 1 percent F-measure improvement over the generative model alone, to 88. 8 percent. 
Storing automatically generated treebanks in lattices of derivations| Abstract Building treebanks { syntactically annotated corpora { is an expensive and time-consuming enterprise.  One alternative to manual annotation is automatic annotation, i. e.  parsing, of new text.  For reasons of accuracy, as well as the demands of certain training methods, it can be important to have a set of weighted alternatives, rather than just one.  In this paper, we propose a format for storing multiple weighted trees.  The basic idea is to represent the set of candidate trees as a tree of derivation choices.  A tree of derivations can be represented as a deterministic weighted finite-state automaton, and existing minimization techniques can be used to exploit shared structure among already split derivations to produce a lattice of trees.  We present empirical trials on an automatically generated treebank for approximately two million words of newspaper text.  We examine in detail the amount of shared structure that such an approach can exploit. 
A GENERALIZED CONSTRUCTION OF INTEGRATED SPEECH RECOGNITION TRANSDUCERS| ABSTRACT We showed in previous work that weighted finite-state transducers provide a common representation for many components of a speech recognition system and described general algorithms for combining these representations to build a single optimized and compact transducer integrating all these components, directly mapping from HMM states to words.  This approach works well for certain well-controlled input transducers, but presents some problems related to the efficiency of composition and the applicability of determinization and weight-pushing with more general transducers.  We generalize our prior construction of the integrated speech recognition transducer to work with an arbitrary number of component transducers and, to a large extent, release the constraints imposed to the type of input transducers by providing more general solutions to these problems.  This generalization allowed us to deal with cases where our prior optimization did not apply.  Our experiments in the AT&T HMIHY 0300 task and an AT&T VoiceTone task show the efficiency of our generalized optimization technique.  We report a 1. 6 recognition speed-up in the HMIHY 0300 task, 1. 8 speed-up in a VoiceTone task using a word-based language model, and 1. 7 using a class-based model.  1.  MOTIVATION In previous work, we showed that weighted finite-state transducers provide a common and natural representation for many components of a speech recognition system, e. g. , HMMs, contextdependency, pronunciation dictionaries, and language models [8].  We also described general algorithms for combining these representations flexibly and efficiently and showed that they can be used to build a single optimized transducer that integrates these components, directly mapping from HMM states to words [9, 10].  In this method, weighted transducer composition is used to combine the component transducers, while determinization, minimization and weight-pushing optimize the result in time and space.  The resultant transducer has a standardized representation, unique up to state renumbering.  This approach works well for certain well-controlled input transducers, but presents some problems with more general transducers.  These problems are related to composition, determinization, and weight-pushing.  Composition can use unacceptable amounts of time and space when there are significant delays in matching due to #-transitions.  In simple cases, this can be avoided by a careful construction of the component transducers.  In practice, inexperienced users are often not fully aware of this.  For example, they may place the output labels in the lexicon transducer L at the ends of words rather than at the beginning, which can significantly slow down composition.  For more complex transducers, trying to manually place the input and output labels for the best effect becomes difficult.  Not all transducers are determinizable.  The lexicon transducer, for example, is not determinizable if it contains homophones.  In our prior construction, we added disambiguation symbols at word ends as needed to solve this problem.  For more general transducer inputs, this is insufficient.  A related problem is that # This author's new address is:
IMPROVED NAME RECOGNITION WITH META-DATA DEPENDENT NAME NETWORKS| ABSTRACT A transcription system that requires accurate general name transcription is faced with the problem of covering the large number of names it may encounter.  Without any prior knowledge, this requires a large increase in the size and complexity of the system due to the expansion of the lexicon.  Furthermore, this increase will adversely affect the system performance due to the increased confusability.  Here we propose a method that uses meta-data, available at runtime to ensure better name coverage without significantly increasing the system complexity.  We tested this approach on a voicemail transcription task and assumed meta-data to be available in the form of a caller ID string (as it would show up on a caller ID enabled phone) and the name of the mailbox owner.  Networks representing possible spoken realization of those names are generated at runtime and included in network of the decoder.  The decoder network is built at training time using a class-dependent language model, with caller and mailbox name instances modeled as class tokens.  The class tokens are replaced at test time with the name networks built from the meta-data.  The proposed algorithm showed a reduction in the error rate of name tokens of 22. 1%. 
Robust Probabilistic Predictive Syntactic Processing|
Robust Probabilistic Predictive Syntactic Processing: Motivations, Models and Applications|
Broad coverage predictive parsing| Presented at the 12th Annual CUNY Conference on Human Sentence. 
Probabilistic top-down parsing and language modeling|
Robust garden path parsing|
Compact non-left-recursive grammars using the selective left-corner transform and factoring|
Corrective language modeling for large vocabulary ASR with the perceptron algorithm|
The AT&T RT-02 STT system|
Efficient probabilistic top-down and left-corner parsing|
Generalized algorithms for constructing language models|
"MAP stochastic grammar adaptation,"|
Markov Parsing: Lattice Rescoring with a Statistical Parser|
Noun-phrase co-occurerence statistics for semi-automaticsemantic lexicon construction|
Noun phrase cooccurrence statistics for semi-automatic lexicon construction|
Measuring efficiency in high-accuracy, broad-coverage statistical parsing|
Incremental parsing with the perceptron algorithm|
Discriminative n-gram language modeling|
The AT&T 1xRT CTS system|

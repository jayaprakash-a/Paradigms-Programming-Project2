Vectorizing for a SIMdD DSP architecture| grained parallelism was recently extended to support SIMD operations on disjoint vector elements.  In this paper we demonstrate how SIMdD (SIMD on disjoint data) supports effective vectorization of digital signal processing (DSP) benchmarks, by facilitating data reorganization and reuse.  In particular we show that this model can be adopted by a compiler to achieve near-optimal performance for important classes of kernels. 
All We Believe Fails in Impossible Worlds A possible-world semantics for a "knowing at most" operator| Abstract We extend the familiar possible-world semantics of modal logic by considering the 'impossible' worlds of a Kripke structure.  We obtain a simple semantics for Levesque's ''All I know" logic.  We provide a natural proof theory and prove the expected soundness and completeness theorems.  From a mathematical point of view we offer a natural generalization of modal logic that significantly strengthens its expressive power.  Considered in the context of Knowledge Representation, such a logic is a standard monotonic logic that allows formal treatment of default reasoning.  \Lambda e-mail 'shai@techsel. bitnet', phone (972-4)294266. 
Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli| Abstract We present a generalized reverse correlation technique that can be used to estimate the spatio-temporal receptive fields (STRFs) of sensory neurons from their responses to arbitrary stimuli such as auditory vocalizations or natural visual scenes.  The general solution for STRF estimation requires normalization of the stimulus--response cross-correlation by the stimulus autocorrelation matrix.  When the second-order stimulus statistics are stationary, normalization involves only the diagonal elements of the Fourier-transformed auto-correlation matrix (the power spectrum).  In the non-stationary case normalization requires the entire auto-correlation matrix.  We present modelling studies that demonstrate the feasibility and accuracy of this method as well as neurophysiological data comparing STRFs estimated using natural versus synthetic stimulus ensembles.  For both auditory and visual neurons, STRFs obtained with these different stimuli are similar, but exhibit systematic differences that may be functionally significant.  This method should be useful for determining what aspects of natural signals are represented by sensory neurons and may reveal novel response properties of these neurons. 
On Learning in the Limit and Non-Uniform (epsilon, delta)-Learning| Abstract We compare the two most common types of success criteria for learning processes.  The (ffl; ffi ) criterion employed in the PAC model and its many variants and extensions, and the identification in the limit criterion that is used in inductive inference models.  By applying common techniques from the theory of Probability, and a stochastic variant of Rissanen's MDL principle, we demonstrate close connections between these two types of learnability notions.  We also show that, once computability issues are set aside, stochastic identification in the limit is intimately related to the countability of a concept class. 
Online Learning versus Offline Learning| Abstract.  We present an off-line variant of the mistake-bound model of learning.  Just like in the well studied on-line model, a learner in the offline model has to learn an unknown concept from a sequence of elements of the instance space on which he makes "guess and test" trials.  In both models, the aim of the learner is to make as few mistakes as possible.  The difference between the models is that, while in the on-line model only the set of possible elements is known, in the off-line model the sequence of elements (i. e. , the identity of the elements as well as the order in which they are to be presented) is known to the learner in advance.  We give a combinatorial characterization of the number of mistakes in the off-line model.  We apply this characterization to solve several natural questions that arise for the new model.  First, we compare the mistake bounds of an off-line learner to those of a learner learning the same concept classes in the on-line scenario.  We show that the number of mistakes in the on-line learning is at most a log n factor more than the off-line learning, where n is the length of the sequence.  In addition, we show that if there is an off-line algorithm that does not make more than a constant number of mistakes for each sequence then there is an online algorithm that also does not make more than a constant number of mistakes.  The second issue we address is the effect of the ordering of the elements on the number of mistakes of an off-line learner.  It turns out that there are sequences on which an off-line learner can guarantee at most one mistake, yet a permutation of the same sequence forces him to err on many elements.  We prove, however, that the gap, between the off-line mistake bounds on permutations of the same sequence of n-many elements, cannot be larger than a multiplicative factor of log n,
Limitations of Learning via Embeddings in Euclidean Half-Spaces| Abstract This paper considers the embeddability of general concept classes in Euclidean half spaces.  By embedding in half spaces we refer to a mapping from some concept class to half spaces so that the labeling given to points in the instance space is retained.  The existence of an embedding for some class may be used to learn it using an algorithm for the class it is embedded into.  The Support Vector Machines paradigm employs this idea for the construction of a general learning system.  We show that an overwhelming majority of the family of finite concept classes of constant VC dimension d cannot be embedded in low-dimensional half spaces.  (In fact, we show that the Euclidean dimension must be almost as high as the size of the instance space. ) We strengthen this result even further by showing that an overwhelming majority of the family of finite concept classes of constant VC dimension d cannot be embedded in half spaces (of arbitrarily high Euclidean dimension) with a large margin.  (In fact, the margin cannot be substantially larger than the margin achieved by the trivial embedding. ) Furtehrmore, this bounds are robust in the sense that allowing each image half space to err on a small fraction of the instances does not imply a significant weakening of these dimension and margin bounds. 
On the Existence of Propositional Proof Systems and Oracle-relativized Propositional Logic| Abstract We investigate sufficient conditions for the existence of an optimal propositional proof system.  We concentrate on conditions of the form CoNF = NF .  We introduce a purely combinatorial property of complexity classes - the notions of slim vs.  fat classes.  These notions partition the collection of all previously studied time-complexity classes into two complementary sets.  We show that for every slim class an appropriate collapse entails the existence of an optimal propositional proof system, while, for every fat class there exists an oracle relative to which such an entailment fails.  As the classes P (polynomial functions), E (2 O(n) functions) and EE (2 O(2 n ) functions) are slim, this result includes all the previously known sufficiency conditions for the existence of optimal propositional proof systems.  On the other hand, the classes NEXP, QP (the class of quasipolynomial functions) and EEE (2 O(2 2 n ) functions), as well as any other natural time-complexity class which is not covered by our sufficiency result, are fat classes.  We introduce a notion of a propositional proof system relative to an oracle, As the proofs of all the known sufficiency conditions for the existence of optimal propositional proof systems carry over to the corresponding oracle-relativized notions, this result shows that no extension of our sufficiency condition to non-slim classes can be obtained by the type of reasoning used so far in proofs on these issues. 
On the Theory of Average Case Complexity| Abstract This paper takes the next step in developing the theory of average case complexity initiated by Leonid A Levin.  Previous works [Levin 84, Gurevich 87, Venkatesan and Levin 88] have focused on the existence of complete problems.  We widen the scope to other basic questions in computational complexity.  Our results include: ffl the equivalence of search and decision problems in the context of average case complexity; ffl an initial analysis of the structure of distributional-NP (i. e.  NP problems coupled with ``simple distributions") under reductions which preserve average polynomial-time; ffl a proof that if all of distributional-NP is in average polynomial-time then non-deterministic exponential-time equals deterministic exponential time (i. e. , a collapse in the worst case hierarchy)
A theoretical framework for learning from a pool of disparate data sources| ABSTRACT Many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task.  For example, aiming towards the design of an automated diagnostic tool for some disease, one may wish to integrate data gathered in many dierent hospitals.  A major obstacle to such endeavors is that dierent data sources may vary considerably in the way they choose to represent related data.  In practice, the problem is usually solved by a manual construction of semantic mappings and translations between the dierent sources.  Recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations.  In this work we propose a theoretical framework for making classification predictions from a collection of dierent data sources, without creating explicit translations between them.  Our framework allows a precise mathematical analysis of the complexity of such tasks, and it provides a tool for the development and comparison of dierent learning algorithms.  Our main objective, at this stage, is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework. 
On the Diculty of Approximately Maximizing Agreements| Abstract We address the computational complexity of learning in the agnostic framework.  For a variety of common concept classes we prove that, unless P=NP, there is no polynomial time approximation scheme for finding a member in the class that approximately maximizes the agreement with a given training sample.  In particular our results apply to the classes of monomials, axis-aligned hyper-rectangles, closed balls and monotone monomials.  For each of these classes we prove the NP-hardness of approximating maximal agreement to within some fixed constant (independent of the sample size and of the dimensionality of the sample space).  For the class of half-spaces, we prove that, for any # } 0, it is NP-hard to approximately maximize agreements to within a factor of (418=415 #), improving on the best previously known constant for this problem, and using a simpler proof.  An interesting feature of our proofs is that, for each of the classes we discuss, we find patterns of training examples that, while being hard for approximating agreement within that concept class, allow ecient agreement maximization within other concept classes.  These results bring up a new aspect of the model selection problem { they imply that the choice of hypothesis class for agnostic learning from among those considered in this paper can drastically effect the computational complexity of the learning process. 
On the difficulty of approximately maximizing agreements| Abstract We address the computational complexity of learning in the agnostic framework.  For a variety of common concept classes we prove that, unless P=NP, there is no polynomial time approximation scheme for finding a member in the class that approximately maximizes the agreement with a given training sample.  In particular our results apply to the classes of monomials, axis-aligned hyperrectangles, closed balls and monotone monomials.  For each of these classes we prove the NPhardness of approximating maximal agreement to within some fixed constant (independent of the sample size and of the dimensionality of the sample space).  For the class of half-spaces, we prove that, for any ffl ? 0, it is NP-hard to approximately maximize agreements to within a factor of (418=415\Gamma ffl), improving on the best previously known constant for this problem, and using a simpler proof.  An interesting feature of our proofs is that, for each of the classes we discuss, we find patterns of training examples that, while being hard for approximating agreement within that concept class, allow efficient agreement maximization within other concept classes.  These results bring up a new aspect of the model selection problem -- they imply that the choice of hypothesis class for agnostic learning from among those considered in this paper can drastically effect the computational complexity of the learning process. 
Agnostic Boosting| Abstract.  We extend the boosting paradigm to the realistic setting of agnostic learning, that is, to a setting where the training sample is generated by an arbitrary (unknown) probability distribution over examples and labels.  We define a fi-weak agnostic learner with respect to a hypothesis class F as follows: given a distribution P it outputs some hypothesis h 2 F whose error is at most erP (F ) + fi, where erP (F ) is the minimal error of an hypothesis from F under the distribution P (note that for some distributions the bound may exceed a half).  We show a boosting algorithm that using the weak agnostic learner computes a hypothesis whose error is at most maxfc1 (fi)er(F ) c 2 (fi) ; fflg, in time polynomial in 1=ffl.  While this generalization guarantee is significantly weaker than the one resulting from the known PAC boosting algorithms, one should note that the assumption required for fi-weak agnostic learner is much weaker.  In fact, an important virtue of the notion of weak agnostic learning is that in many cases such learning is achieved by efficient algorithms. 
Localized Boosting| Abstract We introduce and analyze LocBoost, a new boosting algorithm, which leads to the incremental construction of a mixture of experts type architecture.  We provide upper bounds on the expected loss of such models in terms of the smoothness properties of the gating functions appearing in the mixture of experts model.  Furthermore, an incremental algorithm is proposed for the construction of the classifier, based on a maximum-likelihood approach and the EM algorithm.  Preliminary numerical results appear to be promising. 
Efficient Learning of Linear Perceptrons| Abstract We introduce an efficient agnostic learning algorithm for the class of half-spaces in ! n .  We make no assumptions whatsoever on the example-generating distribution.  Our performance guarantee is that, given any j ? 0, our algorithm runs in time polynomial in the sample size and dimension, and outputs a hypothesis half-space that classifies correctly at least the number of points classified correctly with margin j by any other half-space.  While our algorithm's running time is not polynomial in 1=j, we prove that unless P=NP no such `fully polynomial' approximation scheme exists. 
Algorithms for Learning by Distances| Abstract We consider the information complexity of learning in metric spaces.  We discuss two models of such learning processes.  The first one is the Learning By Distances (LBD) model of Ben-David et al [BIK].  In this model a concept is a point in a metric space, at each step of the learning process the student offers a hypothesis and receives from the teacher an approximation of its distance to the target.  We also present a new Relative Distances (RD) model.  In this model, at each step, the student presents two points and receives a bit indicating which of them is closer to the target.  We investigate the learning complexity in both models.  We provide general lower and upper bounds on the complexity of learning concept classes in these models.  We then analyze the complexity of several natural concept classes in two metric spaces; the space of boolean formulas with the metric induced by the number of satisfying assignments and spaces defined on graphs with the metric induced by the length of the shortest path between pairs of nodes. 
Detecting Change in Data Streams| Abstract Detecting changes in a data stream is an important area of research with many applications.  In this paper, we present novel methods for the detection and estimation of change.  In contrast to previously proposed tools, our techniques provide proven guarantees on the statistical significance of detected change.  In addition to providing reliable detection of change, our method allows for a meaningful description and quantification of those changes.  Our techniques are nonparametric and so they require no prior assumptions on the nature of the distribution that generates the data, except for assuming that points in the stream are generated independently.  Additionally, these techniques work for both continuous and discrete data.  In an experimental study we demonstrate the usefulness of our techniques. 
The Computational Complexity of Densest Region Detection| Abstract We investigate the computational complexity of the task of detecting dense regions of an unknown distribution from un-labeled samples of this distribution.  We introduce a formal learning model for this task that uses a hypothesis class as its `anti-overfitting' mechanism.  The learning task in our model can be reduced to a combinatorial optimization problem.  We can show that for some constants, depending on the hypothesis class, these problems are NP-hard to approximate to within these constant factors.  We go on and introduce a new criterion for the success of approximate optimization geometric problems.  The new criterion requires that the algorithm competes with hypotheses only on the points that are separated by some margin from their boundaries.  Quite surprisingly, we discover that for each of the two hypothesis classes that we investigate, there is a `critical value' of the margin parameter .  For any value below the critical value the problems are NP-hard to approximate, while, once this value is exceeded, the problems become poly-time solvable. 
Learning with Restricted Focus of Attention| Abstract We consider learning tasks in which the learner faces restrictions on the amount of information he can extract from each example he encounters.  We introduce a formal framework for the analysis of such scenarios.  We call it RFA (Restricted Focus of Attention) learning.  While being a natural refinement of the PAC learning model, some of the fundamental PAC-learning results and techniques fail in the RFA paradigm; learnability in the RFA model is no longer characterized by the VC dimension, and many PAC learning algorithms are not applicable in the RFA setting.  Hence, the RFA formulation reflects the need for new techniques and tools to cope with some fundamental constraints of realistic learning problems.  In this work we also present some paradigms and algorithms that may serve as a first step towards answering this need.  Two main types of restrictions are considered here -- in the stronger one, called k-RFA, only k of the n attributes of each example are revealed to the learner, while in the weakest one, called k-wRFA, the restriction is made on the size of each observation (k bits), and no restriction is made on how the observations are extracted from the examples.  For the stronger k-RFA restriction we develop a general technique for composing efficient k-RFA algorithms, and apply it to deduce, for instance, the efficient k-RFA learnability of k-DNF formulas, and the efficient 1-RFA learnability of axis-aligned rectangles in the Euclidean space R n .  We also prove the k-RFA learnability of richer classes of Boolean functions (such as k-decision lists) with respect to a given distribution, and the efficient (n\Gamma 1)-RFA learnability (for fixed n), under product distributions, of classes of subsets of R n which are defined by mild surfaces.  For the weaker k-wRFA restriction, we show that for k = O(log n), efficient k-wRFA learning is robust against classification noise.  As a straightforward application, we construct a new simple noise-tolerant algorithm for the class of k-Decision Lists by constructing an intuitive k-wRFA algorithm for this task. 
Hardness results for neural network approximation problems| We consider the problem of efficiently learning in two-layer neural networks.  We investigate the computational complexity of agnostically learning with simple families of neural networks as the hypothesis classes.  We show that it is NP-hard to find a linear threshold network of a fixed size that approximatelyminimizes the proportion of misclassified examples in a training set, even if there is a network that correctly classifies all of the training examples.  In particular, for a training set that is correctly classified by some two-layer linear threshold network with k hidden units, it is NP-hard to find such a network that makes mistakes on a proportion smaller than c=k 2 of the examples, for some constant c.  We prove a similar result for the problem of approximately minimizing the quadratic loss of a two-layer network with a sigmoid output unit. 
Learning Distributions by Their Density Levels: A Paradigm for Learning without a Teacher| Abstract We propose a mathematical model for learning the high-density areas of an unknown distribution from (unlabeled) random points drawn according to this distribution.  While this type of a learning task has not been previously addressed in the Computational Learnability literature, we believe that this it a rather basic problem that appears in many practical learning scenarios.  From a statistical theory standpoint, our model may be viewed as a restricted instance of the fundamental issue of inferring information about a probability distribution from the random samples it generates.  From a computational learning angle, what we propose is a new framework of un-supervised concept learning.  The examples provided to the learner in our model are not labeled (and are not necessarily all positive or all negative).  The only information about their membership is indirectly disclosed to the student through the sampling distribution.  We investigate the basic features of the proposed model and provide lower and upper bounds on the sample complexity of such learning tasks.  Our main result is that the learnability of a class of distributions in this setting is equivalent to the finiteness of the VC-dimension of the class of the high-density areas of these distributions.  One direction of the proof involves a reduction of the density-level-learnability to p-concepts learnability, while the sufficiency condition is proved through the introduction of a generic learning algorithm. 
a modal logic for subjective default reasoning| Abstract In this paper we introduce DML: Default Modal Logic.  DML is a logic endowed with a two-place modal connective that has the intended meaning of "If ff, then normally fi".  On top of providing a well-defined tool for analyzing common default reasoning, DML allows nesting of the default operator.  We present a semantic framework in which many of the known default proof systems can be naturally characterized, and prove soundness and completeness theorems for several such proof systems.  Our semantics is a "neighbourhood modal semantics", and it allows for subjective defaults, that is, defaults may vary among different worlds within the same model.  The semantics has an appealing intuitive interpretation and may be viewed as a set-theoretic generalization of the probabilistic interpretations of default reasoning.  We show that our semantics is most general in the sense that any modal semantics that is sound for some basic axioms for default reasoning is a special case of our semantics.  Such a generality result may serve to provide a semantical analysis of the relative strength of different proof systems and to show the nonexistence of semantics with certain properties. 
NONPARAMETRIC CHANGE DETECTION IN 2D RANDOM SENSOR FIELD| ABSTRACT The problem of detecting changes from data collected from a largescale randomly deployed two dimensional sensor field is considered.  Under a nonparametric change detection framework, we propose detection algorithms using two measures of change.  Theoretical performance guarantee is derived from the Vapnik-Chervonenkis theory.  By exploiting the structures of the search domain, we design a suboptimal recursive algorithm to detect the area of largest change which, for M sample points, runs in time O(M 2 log M) (compared to an O(M 4 ) required for a straightforward exhaustive search).  The lost of performance diminishes as M increases. 
Non-Parametric Approach to Change Detection and Estimation in Large Scale Sensor Networks| Abstract --- We consider a non-parametric, spatial sample-based scheme for the detection and estimation of changes of a random field by collecting packets from randomly distributed sensors.  We assume that each sensor has a fixed probability of successfully sending to a mobile access point a packet containing its local state---either "excited" or "baseline" and its location.  The task we are concerned with here is as follows: Given two sets of packets collected over two nonoverlapping time-windows, construct a test to determine if the distribution generating the sensor's states has changed between these two time windows.  Furthermore, if a change of distribution has occurred, we wish to estimate the distribution of the change. 
Behavioral/Systems/Cognitive Natural Stimulus Statistics Alter the Receptive Field Structure of V1 Neurons| Studies of the primary visual cortex (V1) have produced models that account for neuronal responses to synthetic stimuli such as sinusoidal gratings.  Little is known about how these models generalize to activity during natural vision.  We recorded neural responses in area V1 of awake macaques to a stimulus with natural spatiotemporal statistics and to a dynamic grating sequence stimulus.  We fit nonlinear receptive field models using each of these data sets and compared how well they predicted time-varying responses to a novel natural visual stimulus.  On average, the model fit using the natural stimulus predicted natural visual responses more than twice as accurately as the model fit to the synthetic stimulus.  The natural vision model produced better predictions in #75% of the neurons studied.  This large difference in predictive power suggests that natural spatiotemporal stimulus statistics activate nonlinear response properties in a different manner than the grating stimulus.  To characterize this modulation, we compared the temporal and spatial response properties of the model fits.  During natural stimulation, temporal responses often showed a stronger late inhibitory component, indicating an effect of nonlinear temporal summation during natural vision.  In addition, spatial tuning underwent complex shifts, primarily in the inhibitory, rather than excitatory, elements of the response profile.  These differences in late and spatially tuned inhibition accounted fully for the difference in predictive power between the two models.  Both the spatial and temporal statistics of the natural stimulus contributed to the modulatory effects. 
On the Power of Randomization in On-Line Algorithms|
Learning Changing Concepts by Exploiting the Structure of Change|
RuleBase: an Industry-Oriented Formal Verifiation Tool|
Detecting changes in data streams|
G#abar Tardos, and Avi Wigderson,|
Localization vs| Identification of Semi-Algebraic Sets. 
Spatial filter selection for EEG-based communication,|
Exploiting Task Relatedness for Multiple Task Learning|
Minorations de formes lin'eaires de logarithmes elliptiques|,. 
A New Measure for the Study of On-Line Algorithms|
Can Finite Samples Detect Singularities of Real-Valued Functions?|
A Six Degree-of-Freedom, Hydraulic, One Person Motion Simulator|
Some questions of distribution in the theory of rank correlation|
Detecting c ange in data streams|
Steady state and transient electr othermal simulation of power devices and circuits based on fully physical therma l model,|
Minoration de la hauteur de Neron-Tate sur les varietes abeliennes de|
Characterizations of learnability for classes of f0; : : : ; ng-valued functions|
Learning by Distances|
Learnability with Restricted Focus of Attention guarantees Noise-Tolerance|
Minorations de hauteurs sur les vari#et#es ab#eliennes|
On the existence of optimal proof systems and oracle-relativized propositional logic|
A force-controlled pneumatic actuator,|
On the power of randomization in on-line algorithms| Algorithmica, special issue on on-line algorithms,. 
Minorations des hauteurs normalis'ees des sousvari'et'es ab'eliennes,|
A Note On Vc-Dimension And Measure Of Sets Of Reals|
A Note on Non-complete Problems in NPImage|
"999, Toehold strategies, toehold laws and rival bidders,|
A Composition Theorem for Learning Algorithms with Applications to Geometric Concept Classes|
Three Events that Defines an REA Methodology for Systems Analysis, Design and Implementation|
The role of an astrocyte surface molecule in neuronal migration in the developing rat cerebellum|
Beyond Morality and Ethics---Executive Objective Function, the R-rating Puzzle and the Production of Violent Movies,"|
Information, Blockbusters, and Stars: A Study of the Film Industry,"|
On the hardness of unsupervised learning|
Minoration des hauteurs normalises des sous-varits de varits abeliennes 2,|
Toehold strategies, takeover laws and rival bidders|
On the hardness of learning with neural networks|
The computational complexity of densest regios detection|
The Global Time Assumption and Semantics for Concurrent Systems|
Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli|
Optimal Financial Contracts for a Start-Up with Unlimited Operating Discretion,"|
On the relevance of debt maturity structure|
A Parametrization Scheme for Classifying Models of Learnability|
\Voluntary Provision of Public Goods: the Multiple Unit Case,"|
Scale-sensitive Dimensions, Uniform Convergence, and Learnability," pages 292--301|
The two-cardinals transfer property and resurrection of supercompactness|
Axonal elongation into peripheral nervous system bridges after CNS injury in adult rats|
On the Theory of Average Case|
Three ^events~ that define an REA approach to systems analysis, design, and implementation|
Rulebase: an industry-oriented formal verification tool|
`Analyse d'une mthode d'indexation automatique base sur une analyse syntaxique de texte',|
On Learning in the Limit and Non-Uniform (#, #)Learning|
Characterizations of Learnability for Classes of {0, |. . , n}-Valued Functions. 
Reliability and Electricity Pricing|
Applying VC-dimension Analysis To Object Recognition|
Vc-dimension analysis of object recognition tasks|
On Shelah's compactness of cardinals,|
Non-embedability in Euclidean Half Spaces|
Managing your total IT cost of ownership|
Ceruloplasmin regulates iron levels in the CNS and prevents free radical injury",|
A Composition Theorem for Learning Algorithms with Application to Geometric Concepts Classes|
Managing your IT total cost of ownership,|
-- Minoration de la hauteur normalisee des hypersurfaces| Acta Arith. 
-- Minorations des hauteurs normalisees des sous-varietes des tores|
-- On the height of subvarieties of group varieties|
-- Petits points, points rationnels|
-- Le problme de Lehmer en dimension superieure|
On Ultrafilters and NP|
Transpiration of a 64-year old maritime pine stand in Portugal,|
A sions|

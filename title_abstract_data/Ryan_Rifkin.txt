Musical query-by-description as a multiclass learning problem| Abstract---We present the query-by-description (QBD) component of "Kandem," a time-aware music retrieval system.  The QBD system we describe learns a relation between descriptive text concerning a musical artist and their actual acoustic output, making such queries as "Play me something loud with an electronic beat" possible by merely analyzing the audio content of a database.  We show a novel machine learning technique based on Regularized Least-Squares Classification (RLSC) that can quickly and efficiently learn the non-linear relation between descriptive language and audio features by treating the problem as a large number of possible output classes linked to the same set of input features.  We show how the RLSC training can easily eliminate irrelevant labels. 
Bayesian Estimation of Transcript Levels Using a General Model of Array Measurement Noise| ABSTRACT Gene arrays demonstrate a promising ability to characterize expression levels across the entire genome but suffer from signi cant levels of measurement noise.  We present a rigorous new approach to estimate transcript levels and ratios from one or more gene array experiments, given a model of measurement noise and available prior information.  The Bayesian estimation of array measurements (BEAM) technique provides a principled method to identify changes in expression level, combine repeated measurements, or deal with negative expression level measurements.  BEAM is more exible than existing techniques, because it does not assume a speci c functional form for noise and prior models.  Instead, it relies on computational techniques that apply to a broad range of models.  We use Affymetrix yeast chip data to illustrate the process of developing accurate noise and prior models from existing experimental data.  The resulting noise model includes novel features such as heavytailed additive noise and a gene-speci c bias term.  We also verify that the resulting noise and prior models t data from an Affymetrix human chip set. 
A Note on Support Vector Machine Degeneracy| Abstract When training Support Vector Machines (SVMs) over non-separable data sets, one sets the threshold b using any dual cost coefficient that is strictly between the bounds of 0 and C.  We show that there exist SVM training problems with dual optimal solutions with all coefficients at bounds, but that all such problems are degenerate in the sense that the "optimal separating hyperplane" is given by w = 0, and the resulting (degenerate) SVM will classify all future points identically (to the class that supplies more training data).  We also derive necessary and sufficient conditions on the input data for this to occur.  Finally, we show that an SVM training problem can always be made degenerate by the addition of a single data point belonging to a certain unbounded polyhedron, which we characterize in terms of its extreme points and rays. 
From Regression to Classification in Support Vector Machines| Abstract We study the relation between support vector machines (SVMs) for regression (SVMR) and SVM for classification (SVMC).  We show that for a given SVMC solution there exists a SVMR solution which is equivalent for a certain choice of the parameters.  In particular our result is that for # sufficiently close to one, the optimal hyperplane and threshold for the SVMC problem with regularization parameter C c are equal to 1 1- # times the optimal hyperplane and threshold for SVMR with regularization parameter C r = (1- #)C c .  A direct consequence of this result is that SVMC can be seen as a special case of SVMR. 
SPEAKER RECOGNITION USING LOCAL MODELS| ABSTRACT Many of the problems arising in speech processing are characterized by extremely large training and testing sets, constraining the kinds of models and algorithms that lead to tractable implementations.  In particular, we would like the amount of processing associated with each test frame to be sublinear (i. e. , logarithmic) in the number of training points.  In this paper, we consider smoothed kernel regression models at each test frame, using only those training frames that are close to the desired test frame.  The problem is made tractable via the use of approximate nearest neighbors techniques.  The resulting system is conceptually simple, easy to implement, and fast, with performance comparable to more sophisticated methods.  Preliminary results on a NIST speaker recognition task are presented, demonstrating the feasibility of the method. 
A bayesian approach to transcript estimation from gene array data: the BEAM technique| ABSTRACT We present a new statistically optimal approach to estimate transcript levels and ratios from one or more gene array experiments.  The Bayesian Estimation of Array Measurements (BEAM) technique uses a model of measurement noise and prior information to estimate biological expression levels.  It provides a principled method to deal with negative expression level measurements, combine multiple measurements, and identify changes in expression level.  BEAM is more flexible than existing techniques, because it does not assume a specific functional form for noise and prior models.  Rather, it uses a more accurate noise model developed from experimental data, a process we illustrate here using Aymetrix yeast chips. 
Molecular classification of multiple tumor types| ABSTRACT Using gene expression data to classify tumor types is a very promising tool in cancer diagnosis.  Previous works show several pairs of tumor types can be successfully distinguished by their gene expression patterns (Golub et al.  (1999), Ben-Dor et al.  (2000), Alizadeh et al.  (2000)).  However, the simultaneous classification across a heterogeneous set of tumor types has not been well studied yet.  We obtained 190 samples from 14 tumor classes and generated a combined expression dataset containing 16063 genes for each of those samples.  We performed multi-class classification by combining the outputs of binary classifiers.  Three binary classifiers (k-nearest neighbors, weighted voting, and support vector machines) were applied in conjunction with three combination scenarios (one-vs-all, all-pairs, hierarchical partitioning).  We achieved the best cross validation error rate of 18. 75% and the best test error rate of 21. 74% by using the one-vs-all support vector machine algorithm.  The results demonstrate the feasibility of performing clinically useful classification from samples of multiple tumor types. 
Measured Effects of a Narrowband Interference Suppressor on GPS Receivers| ABSTRACT Narrowband and partial-band interference can severely degrade the performance of GPS receivers.  The use of digital signal processing in a receiver has been shown to provide protection against this type of interference in communications systems and has been proposed for application in future GPS systems.  This paper discusses the measured effects of narrowband interference suppression on GPS receiver performance.  Emphasis is placed on its effects on pseudorange measurements, since navigation accuracy is ultimately the measure of interest to the user.  Performance metrics presented include: the GPS receiver input carrier-to-noise ratio, receiver crosscorrelation function, and pseudorange errors.  The results demonstrate that substantial interference suppression can be attained with modest navigation accuracy degradation by incorporating narrowband interference suppression technology into GPS receivers. 
Chapter 1| Abstract In this chapter we characterize the role of b, which is the constant in the standard form of
A Stochastic Integer Program with Dual Network Structure and its Application to the Ground Holding Problem| Abstract In this paper, we analyze a generalization of a classic network flow model.  The generalization involves the replacement of deterministic demand with stochastic demand.  While this generalization destroys the original network structure, we show that the matrix underlying the stochastic model is dual network.  Thus, the integer program associated with the stochastic model can be solved eciently using network flow or linear programming techniques.  We also develop an application of this model to the ground holding problem in air trac management.  The use of this model for the ground holding problem improves upon prior models by allowing for easy integration into the newly developed ground delay program procedures based on the Collaborative Decision Making paradigm. 
Statistical Learning: Stability is Sufficient for Generalization and Necessary and Sufficient for Consistency of Empirical Risk Minimization| Abstract Solutions of learning problems by Empirical Risk Minimization (ERM) -and almost-ERM when the minimizer does not exist -- need to be consistent, so that they may be predictive.  They also need to be well-posed in the sense of being stable, so that they might be used robustly.  We propose a statistical form of leave-one-out stability, called CVEEE loo stability.  Our main new results are two.  We prove that for bounded loss classes CVEEE loo stability is (a) sufficient for generalization, that is convergence in probability of the empirical error to the expected error, for any algorithm satisfying it and, (b) necessary and sufficient for generalization and consistency of ERM.  Thus CVEEE loo stability is a weak form of stability that represents a sufficient condition for generalization for general learning algorithms while subsuming the classical conditions for consistency of ERM.  We discuss alternative forms of stability.  In particular, we conclude that for ERM a certain form of well-posedness is equivalent to consistency. 
Bagging Regularizes| Abstract Intuitively, we expect that averaging --- or bagging --- different regressors with low correlation should smooth their behavior and be somewhat similar to regularization.  In this note we make this intuition precise.  Using an almost classical definition of stability, we prove that a certain form of averaging provides generalization bounds with a rate of convergence of the same order as Tikhonov regularization --- similar to fashionable RKHSbased learningalgorithms. 
Estimating Dataset Size Requirements for Classifying DNA Microarray Data| ABSTRACT A statistical methodology for estimating dataset size requirements for classifying microarray data using learning curves is introduced.  The goal is to use existing classi cation results to estimate dataset size requirements for future classi cation experiments and to evaluate the gain in accuracy and signi cance of classi ers built with additional data.  The method is based on tting inverse power-law models to construct empirical learning curves.  It also includes a permutation test procedure to assess the statistical signi cance of classi cation performance for a given dataset size.  This procedure is applied to several molecular classication problems representing a broad spectrum of levels of complexity. 
USING THE FISHER KERNEL METHOD FOR WEB AUDIO CLASSIFICATION| ABSTRACT As the multimedia contentoftheWeb increases techniques to automatically classify this content become more important.  In this paper we present a system to classify audio files collected from the Web.  The system classifies any audio file as belonging to one of three categories: `speech', `music' and `other'.  To classify the audio files, we use the technique of Fisher kernels.  The technique as proposed by Jaakkola assumes a probabilistic generative model for the data, in our case a Gaussian mixture model.  Then a discriminative classifier uses the GMM as an intermediate step to produce appropriate feature vectors.  Support Vector Machines are our choice of discriminative classifier.  We present classification results on a collection of more than 173 hours of Web audio randomly collected.  Webelieve our results represent one of the first realistic studies of audio classification performance on "found" data.  Our final system yielded a classification rate of 81. 8%. 
Improving Multiclass Text Classification with the Support Vector Machine| Abstract We compare Naive Bayes and Support Vector Machines on the task of multiclass text classification.  Using a variety of approaches to combine the underlying binary classifiers, we find that SVMs substantially outperform Naive Bayes.  We present full multiclass results on two well-known text data sets, including the lowest error to date on both data sets.  We develop a new indicator of binary performance to show that the SVM's lower multiclass error is a result of its improved binary performance.  Furthermore, we demonstrate and explore the surprising result that one-vs-all classification performs favorably compared to other approaches even though it has no error-correcting properties. 
Multiclass cancer diagnosis using tumor gene expression signatures|
Regression and classi#cation with regularization, preprint,|
Svmfu|
Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning|
An analytical method for multiclass molecular cancer classification|
b|
Stability, generalization, and uniform convergence|
Vol| 190 of NATO Science Series III: Computer and Systems Sciences,. 
Regression and classification with regularization|
Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning|
In defence of one-Vs-all classification|
A Fresh Look at Historical Approaches to Machine Learning|
vs-all classification|
An Empirical Comparison of SNoW and SVMs for Face Detection",|
Institute for Brain and Neural Systems,|
SvmFu 3,|
Svmfu: A fast, flexible support vector machine classification algorithm|

Primitives, Perceptual Organization and Object Recognition| Abstract: We argue that any computational theory of object recognition should explain how to decide what is to be recognized and how to recognize objects at the level of abstract categories.  We show that several current theories perform poorly by these criteria and propose that object representation must be in terms of the spatial relationships between stylized primitives, while the fundamental process of recognition is to assemble these primitives from image evidence, a version of perceptual organization.  We illustrate our approach with programs aimed at finding people and horses in pictures, and discuss the role of learning in the corresponding recognition process.  ``I am not asking what it appears to be doing.  I asked if it was smoking. " ``Appearances are not held to be a clue to the truth," said his cousin.  "But we seem to have no other. 
Interactive motion generation from examples| Abstract There are many applications that demand large quantities of natural looking motion.  It is difficult to synthesize motion that looks natural, particularly when it is people who must move.  In this paper, we present a framework that generates human motions by cutting and pasting motion capture data.  Selecting a collection of clips that yields an acceptable motion is a combinatorial problem that we manage as a randomized search of a hierarchy of graphs.  This approach can generate motion sequences that satisfy a variety of constraints automatically.  The motions are smooth and human-looking.  They are generated in real time so that we can author complex motions interactively.  The algorithm generates multiple motions that satisfy a given set of constraints, allowing a variety of choices for the animator.  It can easily synthesize multiple motions that interact with each other using constraints.  This framework allows the extensive re-use of motion capture data for new purposes. 
Automatic Annotation of Everyday Movements| Abstract This paper describes a system that can annotate a video sequence with: a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view.  The system does not require a fixed background, and is automatic.  The system works by tracking people in 2D, lifting the tracks to 3D and then classifying the lifted tracks by comparison with a set of manually annotated human motions.  The tracker clusters potential body segments to build an appearance model of each actor and then identifies the best match to each model in each frame.  The lifting process uses a scaled orthographic camera model combined with a camera motion model to identify the best matching 3D motion example.  Finally, this example is used to identify the activity of the body.  Activities are classified by matching to a collection of motion capture data that has been annotated by hand, using a class structure that describes everyday motions and allows motion annotations to be composed --- one may jump while running, for example.  Descriptions computed from video of real motions show that the method is accurate. 
Detecting, Localizing and Recovering Kinematics of Textured Animals| Abstract We develop and demonstrate an object recognition system capable of accurately detecting, localizing, and recovering the kinematic configuration of textured animals in real images.  We build a deformation model of shape automatically from videos of animals and an appearance model of texture from a labeled collection of animal images, and combine the two models automatically.  We develop a simple texture descriptor that outperforms the state of the art.  We test our animal models on two datasets; images taken by professional photographers from the Corel collection, and assorted images from the web returned by Google.  We demonstrate quite good performance on both datasets.  Comparing our results with simple baselines, we show that for the Google set, we can recognize objects from a collection demonstrably hard for object recognition. 
Controllable and Scalable Simulation for Animation| Simulation is an important means of generating animations.  For example, we might use simulation to animate a virtual city in order to train emergency response personnel.  Such an application requires the responsiveness and realism that simulation offers, and it also requires the ability to stage specific events in a very large animated environment.  Traditional simulation technology fails on the latter count: it is difficult to direct a given simulation toward a desired outcome, and existing simulations rarely scale well to large virtual worlds.  This thesis addresses controllable and scalable simulation for the purposes of computer animation.  We describe a technique for directing the outcome of simulations by formulating the problem as one of probabilistic sampling.  A Markov chain Monte Carlo (MCMC) algorithm is used to perform the sampling, which allows the generation of multiple animations from a desired distribution.  Furthermore, if the distribution assigns probabilities according to the plausibility of an animation, then we can be certain that most of the sampled animations will appear reasonable to a viewer.  A range of examples are presented from the domain of collision intensive rigid-body simulation, the majority of which could not be produced using previous technology.  We also describe a new rigid-body simulation algorithm that was developed for this work.  Scalable simulation is achieved through simulation culling, a method for focusing the computational effort on visible parts of a simulation.  Aspects of the simulation that are not in view are not explicitly computed, thus saving large amounts of work.  Approximations and random models are used to ensure that objects that leave the view re-enter when necessary in a plausible state, even though their full motion was not computed while out of view.  A virtual fairground and a large virtual city are presented as case studies.  These examples raise a number of open problems, which we discuss in some detail.  This thesis treats control and scale as largely independent problems, yet we show that both may be viewed as sampling problems.  We conclude with a look at how direction and culling might be integrated to enable large-scale virtual environments for realistic training and entertainment. 
Mixtures of Trees for Object Recognition| Abstract Efficient detection of objects in images is complicated by variations of object appearance due to intra-class object differences, articulation, lighting, occlusions, and aspect variations.  To reduce the search required for detection, we employ the bottom-up approach where we find candidate image features and associate some of them with parts of the object model.  We represent objects as collections of local features, and would like to allow any of them to be absent, with only a small subset sufficient for detection; furthermore, our model should allow efficient correspondence search.  We propose a model, Mixture of Trees, that achieves these goals.  With a mixture of trees, we can model the individual appearances of the features, relationships among them, and the aspect, and handle occlusions.  Independences captured in the model make efficient inference possible.  In our earlier work, we have shown that mixtures of trees can be used to model objects with a natural tree structure, in the context of human tracking.  Now we show that a natural tree structure is not required, and use a mixture of trees for both frontal and view-invariant face detection.  We also show that by modeling faces as collections of features we can establish an intrinsic coordinate frame for a face, and estimate the out-of-plane rotation of a face. 
Finding Naked People| Abstract.  This paper demonstrates a content-based retrieval strategy that can tell whether there are naked people present in an image.  No manual intervention is required.  The approach combines color and texture properties to obtain an effective mask for skin regions.  The skin mask is shown to be effective for a wide range of shades and colors of skin.  These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure.  This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on geometric properties such as the structure of individual parts, and the relationships between parts, and constraints on color and texture.  The system is demonstrated to have 60% precision and 52% recall on a test set of 138 uncontrolled images of naked people, mostly obtained from the internet, and 1401 assorted control images, drawn from a wide collection of sources. 
Finding People by Sampling| Abstract We show how to use a sampling method to find sparsely clad people in static images.  People are modeled as an assembly of nine cylindrical segments.  Segments are found using an EM algorithm, and then assembled into hypotheses incrementally, using a learned likelihood model.  Each assembly step passes on a set of samples of its likelihood to the next; this yields effective pruning of the space of hypotheses.  The collection of available nine-segment hypotheses is then represented by a set of equivalence classes, which yield an efficient pruning process.  The posterior for the number of people is obtained from the class representatives.  People are counted quite accurately in images of real scenes using an MAP estimate.  We show the method allows top-down as well as bottom up reasoning.  While the method can be overwhelmed by very large numbers of segments, we show that this problem can be avoided by quite simple pruning steps. 
Object Recognition as Machine Translation -- Part 2: Exploiting Image Database Clustering Models| Abstract.  We treat object recognition as a process of attaching words to images and image regions.  To accomplish this we exploit clustering methods which learn the joint statistics of words and image regions.  We show how these models can then be used to attach words to images outside the training set.  This "auto-annotation" process has applications such as image indexing, as well as being related to object recognition.  Predicted words can be compared to actual words associated with images in a held out set, and we introduce several performance measures based on this observation.  These measures are then used to make principled comparisons of model variants, and proposed enhancements.  Word prediction is most simply done as a function of the entire image.  However, for recognition we need to learn the correspondence between words and specific image regions.  Here we first show that the existing models can be used for this purpose, and then we propose modifications to improve performance based on this goal.  Finally, we propose word prediction performance as a segmentation measure and report the results for two segmentation approaches. 
Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary| Abstract.  We describe a model of object recognition as machine translation.  In this model, recognition is a process of annotating image regions with words.  Firstly, images are segmented into regions, which are classi#ed into region types using a variety of features.  A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM.  This process is analogous with learning a lexicon from an aligned bitext.  For the implementation we describe, these words are nouns taken from a large vocabulary.  On a large test set, the method can predict numerous words with high accuracy.  Simple methods identify words that cannot be predicted well.  Weshowhowto cluster words that individually are dicult to predict into clusters that can be predicted well | for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept.  The method is trained on a substantial collection of images.  Extensive experimental results illustrate the strengths and weaknesses of the approach. 
Words and Pictures in the News| Abstract We discuss the properties of a collection of news photos and captions, collected from the Associated Press and Reuters.  Captions have a vocabulary dominated by proper names.  We have implemented various text clustering algorithms to organize these items by topic, as well as an iconic matcher that identifies articles that share a picture.  We have found that the special structure of captions allows us to extract some names of people actually portrayed in the image quite reliably, using a simple syntactic analysis.  We have been able to build a directory of face images of individuals from this collection. 
Benchmarks for storage and retrieval in multimedia databases| ABSTRACT There is a substantial body of research on computer methods for managing collections of images and videos.  There is little evidence that this research has had important impact on an any community yet.  I use an invitation to speak on a topic on which I am not expert to air some opinions about evaluating image retrieval research.  In my opinion, there is little to be gained in measuring current solutions with reference collections, because these solutions differ so widely from user needs that the exercise becomes empty.  The user studies literature is not well enough read by the image retrieval community.  As a result, we tend to study somewhat artificial problems.  A study of the user needs literature suggests that we will need to solve deep problems to produce useful solutions to image retrieval problems, but that there may be a need for a number of technologies that can be built in practice.  I believe we should concentrate on these issues, rather than on measuring the performance of current systems. 
Efficient Unsupervised Learning for Localization and Detection in Object Categories| Abstract We describe a novel method for learning templates for recognition and localization of objects drawn from categories.  A generative model represents the configuration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features.  The complexity of the model in the number of features is low, meaning our model is much more efficient to train than comparative methods.  Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features.  This results in both accuracy and localization improvements.  Our model has been carefully tested on standard datasets; we compare with a number of recent template models.  In particular, we demonstrate state-of-the-art results for detection and localization. 
Identifying nude pictures| Abstract This paper demonstrates an automatic system for telling whether there are naked people present in an image.  The approach combines color and texture properties to obtain a mask for skin regions, which is shown to be effective for a wide range of shades and colors of skin.  These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure.  This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on color and texture and constraints on geometric properties such as the structure of individual parts and the relationships between parts.  The system demonstrates excellent performance on a test set of 565 uncontrolled images of naked people, mostly obtained from the internet, and 4289 assorted control images, drawn from a wide collection of sources. 
Enriching a Motion Collection by Transplanting Limbs| Abstract This paper describes a method that can significantly increase the size of a collection of motion observations by cutting limbs from one motion sequence and attaching them to another.  Not all such transplants are successful, because correlations across the body are a significant feature of human motion.  The method uses randomized search based around a set of rules to generate transplants that are (a) likely to be successful and (b) likely to enrich the existing motion collection.  The resulting frames are annotated by a classifier to tell whether they look like human motion or not.  We evaluate the method by obtaining motion demands from an application, synthesizing motions to meet those demands, and then scoring the synthesized motions.  Motions synthesized using transplants are generally somewhat better than those synthesized without using transplants, because transplanting generates many frames quite close to the original frames, so that it is easier for the motion synthesis process to find a good path in the motion graph.  Furthermore, we show classifier errors tend to have relatively little impact in practice.  Finally, we show that transplanted motion data can be used to synthesize motions of a group coordinated in space and time without producing motions that share frames. 
Recognition as Translating Images into Text| ABSTRACT We present an overview of a new paradigm for tackling long standing computer vision problems.  Specifically our approach is to build statistical models which translate from a visual representations (images) to semantic ones (associated text).  As providing optimal text for training is difficult at best, we propose working with whatever associated text is available in large quantities.  Examples include large image collections with keywords, museum image collections with descriptive text, news photos, and images on the web.  In this paper we discuss how the translation approach can give a handle on difficult questions such as: What counts as an object? Which objects are easy to recognize and which are hard? Which objects are indistinguishable using our features? How to integrate low level vision processes such as feature based segmentation, with high level processes such as grouping.  We also summarize some of the models proposed for translating from visual information to text, and some of the methods used to evaluate their performance. 
Strike a Pose: Tracking People by Finding Stylized Poses| Abstract We develop an algorithm for finding and kinematically tracking multiple people in long sequences.  Our basic assumption is that people tend to take on certain canonical poses, even when performing unusual activities like throwing a baseball or figure skating.  We build a person detector that quite accurately detects and localizes limbs of people in lateral walking poses.  We use the estimated limbs from a detection to build a discriminative appearance model; we assume the features that discriminate a figure in one frame will discriminate the figure in other frames.  We then use the models as limb detectors in a pictorial structure framework, detecting figures in unrestricted poses in both previous and successive frames.  We have run our tracker on hundreds of thousands of frames, and present and apply a methodology for evaluating tracking on such a large scale.  We test our tracker on real sequences including a feature-length film, an hour of footage from a public park, and various sports sequences.  We find that we can quite accurately automatically find and track multiple people interacting with each other while performing fast and unusual motions. 
Searching for Character Models| Abstract We introduce a method to automatically improve character models for a handwritten script without the use of transcriptions and using a minimum of document specific training data.  We show that we can use searches for the words in a dictionary to identify portions of the document whose transcriptions are unambiguous.  Using templates extracted from those regions, we retrain our character prediction model to drastically improve our search retrieval performance for words in the document. 
Recovering Shape and Irradiance Maps from Rich Dense Texton Fields| Abstract We describe a method that recovers an estimate of surface shape and of the irradiance eld for a textured surface.  The method assumes the surface is viewed in scaled orthography, and we demonstrate the appropriateness of this assumption.  Our method uses interest points to obtain the locations of putative texton instances, clusters the textons into types, and then uses an autocalibration method to recover the frontal appearance of each texton model.  This yields (a) a dense set of normal estimates, each up to a two-fold ambiguity (b) a dense set of irradiance estimates and (c) whether each instance is, in fact, an instance of the relevant texton.  Because we are able to obtain a very large number of instances of a large number of different textons, this information is obtained at sites very closely spaced in the image.  As a result, we need only a simple smoothness constraint to reconstruct a surface model, using EM to resolve the normal ambiguity.  We show results on images of real scenes, comparing our reconstructions with those obtained using other methods and demonstrating the accuracy of both the recovered shape and the irradiance estimate. 
Shape from Texture without Boundaries| Abstract.  We describe a shape from texture method that constructs a maximum a posteriori estimate of surface coefficients using only the deformation of individual texture elements.  Our method does not need to use either the boundary of the observed surface or any assumption about the overall distribution of elements.  The method assumes that texture elements are of a limited number of types of fixed shape.  We show that, with this assumption and assuming generic view and texture, each texture element yields the surface gradient unique up to a two-fold ambiguity.  Furthermore, texture elements that are not from one of the types can be identified and ignored.  An EM-like procedure yields a surface reconstruction from the data.  The method is defined for othographic views --- an extension to perspective views appears to be complex, but possible.  Examples of reconstructions for synthetic images of surfaces are provided, and compared with ground truth.  We also provide examples of reconstructions for images of real scenes.  We show that our method for recovering local texture imaging transformations can be used to retexture objects in images of real scenes. 
Recognizing Algebraic Surfaces from their Outlines| Abstract.  The outline in a single picture of a generic algebraic surface of degree three or greater completely determines the projective geometry of the surface.  The result holds for a generic perspective view of ageneric algebraic surface, where the camera calibration parameters and the focal point are unknown.  Known camera calibration appears not to reduce the projectiveambiguity. The result is constructive. 
MORSE: A 3D Object Recognition System Based on Geometric Invariants| Abstract MORSE is an object recognition system, based on geometric invariants of 3D structures taken from a single 2D intensity view.  The system exploits the geometric constraints inherent in object classes such as polyhedra, rotational symmetry, bi-lateral symmetry and extruded surfaces.  Invariants have been used in the past to index many of these classes, but MORSE is designed to treat multi-class recognition in a unform system architecture.  The class constraints are also used to drive image feature extraction and grouping.  1 Invariant Representation The computer recognition of objects has attracted considerable research effort over the last 25 years.  It is now widely accepted that object recognition, in the setting of real world scenes and based on a single perspective view, is a difficult problem and cannot be achieved without the use of object models to guide the processing of image data and to confirm object hypotheses.  It is also accepted that the most reliable information which is available in a scene is derived from a geometric description \Lambda The research reported here is funded in part by DARPA Contract #MDA972-91-C-0053 of the object based on its projection in the form of 2D geometric image features, as opposed to, for example, its intensity shading.  Thus, object recognition systems draw on a library of geometric models, which usually contain information about the shape and appearance of a set of known objects, to determine which, if any, of those objects appear in a given image or image sequence.  Recognition is considered successful if the geometric configuration in an image can be explained as a perspective projection of a geometric model of the object.  A major constraint underlying the work presented here is that recognition is based on one uncalibrated view of a scene.  Our motivation is that this restriction applies in many of the current and future applications for object recognition, such as aerial surveillance, image database query processing, image-hypertext editing, and scene construction for virtual reality.  Even if more images are available, for example in the case of video processing, camera calibration will not generally be known initially.  Any grouping, recognition hypothesis, or object recovered up to some ambiguity from a single image, can be propagated to advantage to subsequent views.  This paper describes a number of examples of 3D object classes that can be recognized from a single image.  These classes are defined geometrically, as opposed to function[16] or other ontological categories.  For example a surface of revolution is a geometric object class, as opposed to a type of vase, which would be functional.  Rather than identifying a particular model directly, recognition is class based, proceeding by first classifying based on image curves, and subsequently identifying a particular object.  We present here a prototype class based recognition architecture, which integrates these ideas.  Class informs each level of the architecture from image grouping through to organization of the model base and finally the constraints imposed by object classes on the 3D scene configuration.  2 The MORSE System These ideas about 3D invariant class representation and recognition from a single view form the basis for a new object recognition system called MORSE 1 2. 1 The Architecture Control Representation is organized into a number of layers as illustrated in figure 1.  These stages of representation are not very different from other recognition architectures, however the main principles of class and global consistency provide a framework for control and geometric data query management.  Segmentation and grouping The key to successful recognition is efficient and robust feature segmentation and grouping.  There are four levels of image feature representation and grouping: Level I: Pixel-level features are defined with respect to an image coordinate system and reflect the quantized nature of pixel coordinates.  Typically, features will be produced using an edge operator with subpixel accuracy, and the resulting edgels linked into list.  This level is topological - linked curves and vertices are represented.  Level II: Geometric features curves from level I are described in terms of geometric primitives, where appropriate.  For example: algebraic curves, smooth curves, concavities.  Level III: Generic Grouped features This level of grouping is applied to all features produced at level II.  The output is a number of groupings and databases which are used by the class-based groupers described below.  Generic grouping includes: near incidence (jumping small gaps, completing corners and junctions); collinearity; marking bitangent and other distinguished points; affine or projective equivalence of curve segments (e. g.  concavities).  Pairing up concavities uses the distinguished points provided by bitangents and associated cast tangents.  Level IV: Class based grouping Each class has an associated "class based grouper" or agent that interrogates the level III groupings and databases, and attempts to form groups appropriate for its class.  If 1 The acronym is Multiple Object Recognition System by Scene Entailment and is named after an Oxford detective character.  successful, class based invariants are extracted from the grouped features and directly index the relevant class library.  An example of the class specific grouping mechanism is described in section 3.  Indexing and Hypothesis Combination Indexing is handled by a series of hash-tables, one per class, that take the invariants of a system of generalized features and associate with them models in the modelbase.  For complex objects, there may be many feature groups that index to the object, leading to a situation where a single instance could cause many verifications.  The number of potential hypotheses can be reduced by forming joint hypotheses (cliques), based on either topological or invariant geometric relations between feature groups that have indexed to the same model instance.  The Modelbase This has three components: ffl Model class properties, which implements the class constraints to enable grouping, correspondence and indexing.  The invariant properties of the classes to be implemented in MORSE are summarized in table 1.  ffl Object property model base, which contains information on object properties that follow from its identity as an object, such as orientation with respect to other objects; - it is at this level that scene-level knowledge is stored.  ffl Retrieval mechanisms, which will normally consist of hash-tables, used to associate feature groups with objects.  The Scene An additional source of constraints and parameters is the 3D scene, which can also be viewed as a database which reflects the current configuration of the world and cameras.  It provides a representation of all information currently available about the common Euclidean frame in which objects reside.  Verification Verification proceeds at many levels of the formation of a hypothesis.  Many model-based vision systems only apply verification at the final object instance hypothesis level.  In MORSE, intermediate stages of representation and grouping are also verified with respect to information at lower levels of representation.  For example, a set of polyhedral face hypotheses can be refined and verified by applying a "snake" defined by polyhedral incidence and projection constraints.  This idea can be extended even to modeling and verifying local image intensity surface events, such as corners.  2. 2 Model Acquisition In MORSE models are directly acquired from multiple views 2 .  The fact that such models can serve as sufficient representations for recognition is a major advantage of the invariant approach.  We expect that only a small number of views will be required for most objects and that these views will be defined by the extraction of a sufficient set of stable features over a wide range of viewpoints.  2 For some classes, such as a rotationally symmetric object, a single view will suffice. 
Planar Object Recognition using Projective Shape Representation| Abstract We describe a model based recognition system, called LEWIS, for the identification of planar objects based on a projectively invariant representation of shape.  The advantages of this shape description include simple model acquisition (direct from images), no need for camera calibration or object pose computation, and the use of index functions.  We describe the feature construction and recognition algorithms in detail and provide an analysis of the combinatorial advantages of using index functions.  Index functions are used to select models from a model base and are constructed from projective invariants based on algebraic curves and a canonical projective coordinate frame.  Examples are given of object recognition from images of real scenes, with extensive object libraries.  Successful recognition is demonstrated despite partial occlusion by unmodelled objects, and realistic lighting conditions. 
Sampling plausible solutions to multi-body constraint problems| Abstract Traditional collision intensive multi-body simulations are difficult to control due to extremesensitivity to initial conditions or model parameters.  Furthermore, there may be multiple ways to achieve any one goal, and it may be difficult to codify a user's preferences before they have seen the available solutions.  In this paper we extend simulation models to include plausible sources of uncertainty, and then use a Markov chain Monte Carlo algorithm to sample multiple animations that satisfy constraints.  A user can choose the animation they prefer, or applications can take direct advantage of the multiple solutions.  Our technique is applicable when a probability can be attached to each animation,with "good" animations having high probability, and for such cases we provide a definition of physical plausibility for animations.  We demonstrate our approach with examples of multi-body rigid-body simulations that satisfy constraints of various kinds, for each case presenting animations that are true to a physical model, are significantly different from each other, and yet still satisfy the constraints. 
Automatic detection of human nudes| Abstract.  This paper demonstrates an automatic system for telling whether there are human nudes present in an image.  The system marks skin-like pixels using combined color and texture properties.  These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure.  If the grouper finds a su/ciently complex structure, the system decides a human is present.  The approach is shown to be e0ective for a wide range of shades and colors of skin and human configurations.  This approach o0ers an alternate view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on color and texture and constraints on geometric properties such as the structure of individual parts and the relationships between parts.  The system demonstrates excellent performance on a test set of 565 uncontrolled images of human nudes, mostly obtained from the internet, and 4289 assorted control images, drawn from a wide variety of sources. 
The Joy of Sampling| Abstract.  A standard method for handling Bayesian models is to use Markov chain Monte Carlo methods to draw samples from the posterior.  We demonstrate this method on two core problems in computer vision---structure from motion and colour constancy.  These examples illustrate a samplers producing useful representations for very large problems.  We demonstrate that the sampled representations are trustworthy, using consistency checks in the experimental design.  The sampling solution to structure from motion is strictly better than the factorisation approach, because: it reports uncertainty on structure and position measurements in a direct way; it can identify tracking errors; and its estimates of covariance in marginal point position are reliable.  Our colour constancy solution is strictly better than competing approaches, because: it reports uncertainty on surface colour and illuminant measurements in a direct way; it incorporates all available constraints on surface reflectance and on illumination in a direct way; and it integrates a spatial model of reflectance and illumination distribution with a rendering model in a natural way.  One advantage of a sampled representation is that it can be resampled to take into account other information.  We demonstrate the effect of knowing that, in our colour constancy example, a surface viewed in two different images is in fact the same object.  We conclude with a general discussion of the strengths and weaknesses of the sampling paradigm as a tool for computer vision. 
Shape from Texture and Integrability| Abstract We describe a shape from texture method that constructs a maximum a posteriori estimate of surface coefficients using both the deformation of individual texture elements --as in local methods --- and the overall distribution of elements --- as in global methods.  The method described applies to a much larger family of textures than any previous method, local or global.  We demonstrate an analogy with shape from shading, and use this to produce a numerical method.  Examples of reconstructions for synthetic images of surfaces are provided, and compared with ground truth.  The method is defined for orthographic views, but can be generalised to perspective views simply. 
Noise in Bilinear Problems| Abstract: Despite the wide application of bilinear problems to problems both in computer vision and in other fields, their behaviour under the effects of noise is still poorly understood.  In this paper, we show analytically that marginal distributions on the solution components of a bilinear problem can be bimodal, even with Gaussian measurement error.  We demonstrate and compare three different methods of estimating the covariance of a solution.  We show that the Hessian at the mode substantially underestimates covariance.  Many problems in computer vision can be posed as bilinear problems: i. e.  one must find a solution to a set of equations of the form c k = # ij g ijk a i b j for c k a set of known terms (henceforth measurements), and g ijk a set of known interaction terms.  Typically, a i and b j are constrained in some way to allow a unique solution.  The most familiar example is Tomasi and Kanade's formulation of orthographic structure-from-motion [9]; shapefrom-shading and other vision problems can be framed this way too (see [5] for a review.  Other naturally bilinear problems include: inverse kinematics for parallel manipulators [6]; and molecular conformation [1]).  The effect of noise in the measurements is not well understood.  Figure 1 shows a scatter plot of of point positions reconstructed from an orthographic image sequence with Gaussian noise.  Not only are the distributions quite obviously not Gaussian, some even appear bimodal.  There is no reason to expect that they should be Gaussian. Aswe shall see, noise can lead to bimodal marginal posteriors on a i , meaning that straightforward covariance estimates are extremely unreliable.  This paper compares three methods of estimating covariance for marginals on a i and b j in bilinear problems.  In section 1, we analyse some simple examples which illustrate the problem.  We then examine three possible ways of estimating a covariance, and show that two which appear in the literature can be rather misleading in their estimates of covariance.  We focus on the orthographic structure-frommotion problem because it is most familiar, but the conclusions that we draw are equally applicable to any bilinear problem.  1 Analytical examples Even quite simple examples display considerable complexity, but have the advantage that analysis is possible.  1. 1 A One-Dimensional Example Consider a 2 2 measurement matrix D, which is assumed to be close to a rank-1 matrix, differing only by a Gaussian noise matrix W, with w ij # N(0,# 2 ).  We can write this as: # 1 u # # xy # = D + W We have constrained the first component of the column vector to be 1 to remove the scaling ambiguity that would otherwise be present.  Since the noise is iid Gaussian, we can easily write the posterior pdf: P (u, x, y|D)
Towards Auto-Documentary: Tracking the Evolution of News Stories| ABSTRACT News videos constitute an important source of information for tracking and documenting important events.  In these videos, news stories are often accompanied by short video shots that tend to be repeated during the course of the event.  Automatic detection of such repetitions is essential for creating auto-documentaries, for alleviating the limitation of traditional textual topic detection methods.  In this paper, we propose novel methods for detecting and tracking the evolution of news over time.  The proposed method exploits both visual cues and textual information to summarize evolving news stories.  Experiments are carried on the TREC-VID data set consisting of 120 hours of news videos from two different channels. 
Proxy Simulations For Efficient Dynamics| Abstract Proxy simulations reduce the cost of simulation in large virtual worlds, such as those used in training simulations or computer games.  A proxy takes the place of an accurate simulation for objects that are out of view, while the accurate model continues to manage visible objects.  A proxy must ensure that objects enter the view at reasonable times throughout the simulation and in states that reflect their time spent out of view.  The quality of a proxy simulation is measured by how well it maintains reasonable behaviors, where the definition of reasonable depends on the environment and its application.  We present two examples of proxy simulations based on discrete event models: one for city traffic simulation and another for multi-agent path planning and motion.  For these examples, we demonstrate dynamics computation speedups of over two orders of magnitude as the environments grow in size and complexity. 
Exploiting Image Semantics for Picture Libraries| ABSTRACT We consider the application of a system for learning the semantics of image collections to digital libraries.  We discuss our approach to browsing and search, and investigate the integration both in more detail. 
Radiance Caching and Local Geometry Correction| Abstract We present a final gather algorithm which splits the irradiance integral into two components.  One component captures the incident radiance on surfaces due to distant surfaces.  This incident radiance is represented as a spatially varying field of spherical harmonic coefficients.  Since distant surfaces do not cause rapid changes in incident radiance, this field is smooth and slowly varying and can be computed quickly and represented efficiently.  On the other hand, nearby surfaces may create drastic changes in irradiance, because their position on the visible hemisphere change quickly.  We correct the irradiance we obtain from spherical harmonics using an explicit representation of nearby geometry.  By assuming nearby geometry is always visible, we can efficiently restore the high frequency detail missing from the irradiance.  Current techniques need to sample the nearby surfaces densely to approximate this rapid change of irradiance.  This creates unnecessary visibility tests (or raytraces) that slow down the final gather.  We demonstrate that by assuming nearby surfaces are always visible, we obtain very fast final gather results whose quality compares well with standard techniques but is computed much faster.  We also demonstrate the feasibility of using nearby surfaces on scenes without global illumination to restore the high frequency shading detail due to geometric detail. 
Names and Faces| Abstract We show that a large and realistic face dataset can be built from news photographs and their associated captions.  Our dataset consists of 44,773 face images, obtained by applying a face finder to approximately half a million captioned news images.  This dataset is more realistic than usual face recognition datasets, because it contains faces captured "in the wild" in a variety of configurations with respect to the camera, taking a variety of expressions, and under illumination of widely varying color.  Faces are extracted from the images and names from the associated caption.  Our system uses a clustering procedure to find the correspondence between faces and associated names in news picture-caption pairs.  The context in which a name appears in a caption provides powerful cues as to whether it is depicted in the associated image.  By incorporating simple natural language techniques, we are able to improve our name assignment significantly.  Once the procedure is complete, we have an accurately labeled set of faces, an appearance model for each individual depicted, and a natural language model that can produce accurate results on captions in isolation. 
Shape Representations from Shading Primitives| Abstract Diffuse interreflections mean that surface shading and shape are related in ways that are difficult to untangle; in particular, distant and invisible surfaces may affect the shading field that one sees.  The effects of distant surfaces are confined to relatively low spatial frequencies in the shading field, meaning that we can expect signatures, called shading primitives, corresponding to shape properties.  We demonstrate how these primitives can be used to support the construction of useful shape representations.  Approaches to this include testing hypotheses of geometric primitives for consistency with the shading field, and looking for shading events that are distinctive of some shape event.  We show that these approaches can be composed, leading to an attractive process of representation that is intrinsically bottom up.  This representation can be extracted from images of real scenes, and that the representation is diagnostic.  1 Background Changes in surface brightness are a powerful cue to the shape of a surface; the study of extracting shape information from image shading starts with [12] and is comprehensively summed up in Brooks' book [13].  The approach views shading as a local effect; surface brightness is modelled as a product of a visibility term and a non-negative function of the Gauss map, leading to a partial differential equation---the image irradiance equation---whichexpresses the relationship between surface geometry and image brightness.  Shape from shading theories that view shading as a local effect are now widely agreed to be unsatisfactory, for three reasons: the local shading model omits the effects of diffuse interreflections, a source of substantial effects in the brightness of surfaces; the underlying shape representation, a dense depth map, contains excess detail for most recognition applications; and the necessary assumptions are unrealistically restrictive.  New models of shape from shading can be obtained by changing either the type of shape representation sought in the shading field [9], or the model of shading [18, 19, 16]. 
Motion synthesis from annotations| Abstract This paper describes a framework that allows a user to synthesize human motion while retaining control of its qualitative properties.  The user paints a timeline with annotations --- like walk, runor jump--- from a vocabulary which is freely chosen by the user.  The system then assembles frames from a motion database so that the final motion performs the specified actions at specified times.  The motion can also be forced to pass through particular configurations at particular times, and to go to a particular position and orientation.  Annotations can be painted positively (for example, must run), negatively (for example, may not run backwards) or as a don't-care.  The system uses a novel search method, based around dynamic programming at several scales, to obtain a solution efficiently so that authoring is interactive.  Our results demonstrate that the method can generate smooth, natural-looking motion.  The annotation vocabulary can be chosen to fit the application, and allows specification of composite motions (runand jumpsimultaneously, for example).  The process requires a collection of motion data that has been annotated with the chosen vocabulary.  This paper also describes an effective tool, based around repeated use of support vector machines, that allows a user to annotate a large collection of motions quickly and easily so that they may be used with the synthesis algorithm. 
The Bigger Hammer Approach: Using Massively Parallel Computation to Address Low Base Rate Problems| Abstract This paper discusses the problem of modeling phenomena with a low base rate of occurrence using terrorism as an example.  The paper suggests that Data Farming---developing data sets through utilization of massive computation---may be a useful way to deal with this challenge.  An abstract model is described and analyzed as a simple case study.  Preliminary results are examined to provide support for the assertion that Data Farming has utility when modeling low base rate phenomena.  These results were surprising, which provided support for our position that without some understanding of the whole of the possibility space associated with a system; one cannot judge truly what a low base rate event is and what conditions lead to changes in the likelihood of said events.  The paper also discusses how the requirement for holistic understanding combined with current computational limits necessitate carefully designed experiments using distilled models that deal only with the essence of the question at hand.  Two pressing research issues are suggested at the paper's conclusion. 
Who's in the Picture?| Abstract The context in which a name appears in a caption provides powerful cues as to who is depicted in the associated image.  We obtain 44,773 face images, using a face detector, from approximately half a million captioned news images and automatically link names, obtained using a named entity recognizer, with these faces.  A simple clustering method can produce fair results.  We improve these results significantly by combining the clustering process with a model of the probability that an individual is depicted given its context.  Once the labeling procedure is over, we have an accurately labeled set of faces, an appearance model for each individual depicted, and a natural language model that can produce accurate results on captions in isolation. 
Class-Based Grouping in Perspective Images| Abstract A major issue for object recognition systems is the organization of object structure and separation of individual objects within an image of a complex scene.  This paper demonstrates that general geometric object classes can be defined which provide effective constraints for grouping image features into coherent object boundaries.  These classes also support the computation of 3D invariant descriptions including symmetry axes, canonical coordinate frames and projective signatures.  The key idea is that a 3D geometric class defines relations which must hold between points on the image outline (the perspective projection of the object's surface).  The resulting image constraints enable both identification and grouping of image features belonging to objects of that class.  The classes include surfaces of revolution, canal surfaces (pipes) and polyhedra.  Recognition proceeds by first recognising an object as belonging to one of the classes (for example a surface of revolution) and subsequently classifying the object (for example as a particular vase).  This differs from conventional object recognition systems where recognition is generally targetted at particular objects.  The constraints and grouping methods are viewpoint invariant, and proceed with no information on object pose.  We demonstrate the effectiveness of this class-based grouping on real, cluttered scenes using grouping algorithms developed for canal-surfaces, rotationally symmetric surfaces and polyhedra. 
Making Latin Manuscripts Searchable using gHMMs| Abstract We describe a method that can make a scanned, handwritten mediaeval latin manuscript accessible to full text search.  A generalized HMM is tted, using transcribed latin to obtain a transition model and one example each of 22 letters to obtain an emission model.  We show results for unigram, bigram and trigram models.  Our method transcribes 25 pages of a manuscript of Terence with fair accuracy (75% of letters correctly transcribed).  Search results are very strong; we use examples of variant spellings to demonstrate that the search respects the ink of the document.  Furthermore, our model produces fair searches on a document from which we obtained no training data.  1.  Intoduction There are many large corpora of handwritten scanned documents, and their number is growing rapidly.  Collections range from the complete works of Mark Twain to thousands of pages of zoological notes spanning two centuries.  Large scale analyses of such corpora is currently very difcult, because handwriting recognition works poorly.  Recently, Rath and Manmatha have demonstrated that one can use small bodies of aligned material as supervised data to train a word spotting mechanism [6].  The result can make scanned handwritten documents searchable.  Current techniques assume a closed vocabulary one can search only for words in the training set and search for instances of whole words.  This approach is particularly unattractive for an inected language, because individual words can take so many forms that one is unlikely to see all in the training set.  Furthermore, one would like the method used to require very little aligned training data, so that it is possible to process documents written by different scribes with little overhead.  Mediaeval Latin manuscripts are a natural rst corpus for studying this problem, because there are many scanned manuscripts and because the handwriting is relatively regular.  We expect the primary user need to be search over a large body of documents to allow comparisons between documents rather than transcription of a particular document (which is usually relatively easy to do by hand).  Desirable features for a system are: First, that it use little or no aligned training data (an ideal, which we believe may be attainable, is an unsupervised learning system).  Second, that one can search the document for an arbitrary string (rather than, say, only complete words that appear in the training data).  This would allow a user to determine whether a document contains curious or distinctive spellings, for example (gure 7).  We show that, using a statistical model based on a generalized HMM, we can search a medieval manuscript with considerable accuracy, using only one instance each of each letter in the manuscript to train the method (22 instances in total; Latin has no j, k, w, or z).  Furthermore, our method allows fairly accurate transcription of the manuscript.  We train our system on 22 glyphs taken from a a 12th century latin manuscript of Terences Comedies (obtained from a repository of over 80 scanned medieval works maintained by Oxford University [1]).  We show that our search is accurate using a considerable portion of this manuscript aligned by hand; we then show that fair search results are available on a different manuscript (MS.  Auct.  D.  2.  16, Latin Gospels with beast-headed evangelist portraits made at Landvennec, Brittany, late 9th or early 10th century, from [1]) without change of letter templates.  1. 1.  Previous Work Handwriting recognition is a traditional problem, too well studied to review in detail here (see [5]).  Typically, online handwriting recognition (where strokes can be recorded) works better than ofine handwriting recognition.  Handwritten digits can now be recognized with high accuracy [2, 4].  Handwritten amounts can be read with fair accuracy, which is signicantly improved if one segments the amount into digits at the same time as one recognizes it [3, 4].  Recently several authors have proposed new techniques for search and translation in this unrestricted setting.  Manmatha et al [6] introduce the technique of ^word spotting,~ which segments text into word images, recties the word images, and then uses an aligned training set to learn correspondences between rectied word images and strings.  The method is not suitable for a heavily inected language, because words take so many forms.  In an inected language, the natural unit to match to is a subset of a word, rather than a whole word, implying that one should segment the text into blocks which may be smaller than words while recognizing.  Vinciarelli et al [7] introduce a method for line by line recognition based around an HMM and quite similar to techniques used in the speech recognition community.  Their method uses a window that slides along the text to obtain features; this has the difculty that the same window is in some places too small (and so uninformative) and in others too big (and so spans more than one letter, and is confusing).  Their method requires a substantial body of aligned training data, which makes it impractical for our applications.  2.  The Model Our models for both search and transcription are based on the generalized HMM and differ only in their choice of transition model.  In an HMM, each hidden node c t emits a single evidence node x t .  In a generalized HMM, we allow each c t to emit a series of xs whose length is itself a random variable.  In our model, the hidden nodes correspond to letters and each x t is a single column of pixels.  Allowing letters to emit sets of columns lets us accomodate letter templates of variable width.  In particular, this means that we can unify segmenting ink into letters and recognizing blocks of ink; gure 3 shows an example of how useful this is.  2. 1.  Generating a line of text Our hidden state consists of a character label c, width w and vertical position y.  The statespace of c contains the characters `a-`z, a space ` , and a special end state #.  Let T c be the template associated with character c, T ch , T cw be respectively the height and width of that template, and m be the height of the image. 
A novel algorithm for color constancy,|
Towards a cost effective ilu preconditioner with high level fill|
Projectively Invariant Representations Using Implicit Algebraic Curves|
Learning the Semantics of Words and Pictures|
Reflections on Shading|
Mutual illumination|
Clustering Art|
Transformational invariance - a primer|
Words and Pictures in the News", HLT-NAACL03 Workshop on Learning Word Meaning from Non-Linguistic Data,|
Probabilistic Methods for Finding People|
A Novel Approach to Color Constancy,|
Finding and Tracking People from the Bottom Up|
Invariant Descriptors for 3D Object Recognition and Pose|
A Novel Algorithm for Colour Constancy|
Bayesian Structure from Motion|
Applications of invariance in computer vision|
Modeling the statistics of image features and associated text|
Knowledge acquisition for expert systems: Some pitfalls and suggestions|
Body plans|
How Does CONDENSATION Behave with a Finite Number of Samples?|
Recognising rotationally symmetric surfaces from their outlines|
Efficient recognition of rotationally symmetric surfaces and straight homogeneous generalized cylinders|
Query by image and video content: The QBIC system|
Probabilistic models of verbal and body gestures|
Canonical Frames for Planar Object Recognition|
Computer Vision - A Modern Approach,|
Computer Vision Tools for Finding Images and Video Sequences|
Efficient radiosity in dynamic environments|
Involving patients in health care: explanation in the clinical setting|
Searching for digital pictures|
Interactive ray tracing with the visibility complex|
View-Dependent Culling of Dynamic Systems in Virtual Environments|
Human Tracking with Mixtures of Trees|
The Effects of Segmentation and Feature Choice in a Translation Model of Object Recognition|
\Invariant description of object representation and pose,|
Recognizing general curved objects efficiently|
Finding Pictures of Objects in Large Collections of Images|
Extracting Projective Structure from Single Perspective Views of 3D Point Sets|
Finding objects in image databases by grouping|
Cloud liquid water climatology from Special Sensor Microwave/Imager|
Sampling, Resampling and Colour Constancy|
Shading Primitives: Finding Folds and Shallow Grooves|
Efficient model library access by projectively invariant indexing functions,|
and Nic Pillow| 3D Object Recognition using Invariants. 
Fast recognition using algebraic invariants|
Benchmarks for Storage and Retrieval in Multimedia Databases| In Proceedings of SPIE International Society for Optical Engineering Vol. 
The left image is the surface of a synthetic dented cylinder| The dent shape is Gaussian.  The right image is a cross section of the dent fitted with a separable Gaussian. 
"Searching for Digital Picutres"|
An experimental evaluation of projective invariants|
Using medical informatics for explanation in a clinical setting|
report, 20 pp|,. 
Synthesizing constrained motions from examples|
Exploiting image semantics for picture libraries|
Human tracking with mixture of trees|
Improved integration for cloth simulation|
Relative motion and pose from arbitrary plane curves|
`Efficient Recognition of Rotationally Symmetric Surfaces and Straight Homogeneous Generalized Cylinders',|
Recognizing an algebraic surface from its outline|
Dynamics modeling and culling|
Jigsaw puzzle solver using shape and color,|
Discrete and Computational Geometry,|
Shape from shading in the light of mutual illumination|
Eds), Report of the NSF/ARPA workshop on 3D object representation for computer vision|
MORSE: Multiple Object Recognition by Scene Entailment|
Extracting projective information from single views of 3D point sets|
Knowledge-based cephalometric analysis: A comparison with clinicians using interactive methods|
Relative motion and pose from invariants",|
"Words Sense Disambiguation with Pictures"| In: Workshop on Learning Word Meaning from Non-Linguistic Data (2003) Held in Conjunction with The Human Language Technology Conference.  27. 
Primitives, perceptual organisation and object recognition|
Efficient multi-agent path planning|
Using global consistency to recognise Euclidean objects with an uncalibrated camera,|
Using projective invariants for constant time library indexing on model based vision|
The effects of prototype-based biases on leadership appraisals: A test of the leadership categorization theory|
Body plans| Computer Vision and Pattern Recognition (CVPR) '. 
Exploiting text and image feature co-occurrence statistics in large datasets|
Recognizing an algebraic surface by its outline|
Representations of 3D Objects that Incorporate Surface Markings|
Recognizing algebraic surfaces from aspects|
3D Object Recognition Using Invariance|
Finding pictures of objects in large collections of images,|
The appearance of clothing|
Eds|) Applications of Invariance in Computer Vision II. 
s x 0 2|
A novel approach to colour constancy|
"Finding Objects by Grouping,|
Planar Object Recognition using Projective Shape Representation| International Journal of. 
Finding Objects by Grouping Primitives|
Using ethnography to investigate life scientists' information needs|
Panel discussion|
Bregler Finding Naked People,|
`Recognizing rotationally symmetric surfaces from their outlines'|
Measurement in social psychological research,"|

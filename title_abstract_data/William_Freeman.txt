An Experimental Analysis of Cryptographic Overhead in Performance-Critical Systems| ABSTRACT This paper studies the performance implications of using cryptographic controls in performance-critical systems.  Full cryptographic controls beyond basic authentication are considered and experimentally validated in the concept of network file systems.  This paper demonstrates that processor speeds have recently become fast enough to support cryptographic controls in many performance-critical systems.  Integrity and authentication using keyed-hash and RSA as well as confidentiality using RC5 are tested.  This analysis demonstrates that full cryptographic controls are feasible in a distributed network file system, by showing the performance overhead for including signature, hash and encryption algorithms on various embedded and workstation computers.  The results from these experiments are used to predict the performance impact using three proposed network disk security schemes. 
Bayesian Decision Theory, the Maximum Local Mass Estimate, and Color Constancy| Abstract Vision algorithms are often developed in a Bayesian framework.  Two estimators are commonly used: maximum a posteriori (MAP), and minimum mean squared error (MMSE).  We argue that neither is appropriate for perception problems.  The MAP estimator makes insufficient use of structure in the posterior probability.  The squared error penalty of the MMSE estimator does not reflect typical penalties.  We describe a new estimator, which we call maximum local mass (MLM) [10, 26, 65], which integrates the local probability density.  The MLM method is sensitive to local structure of the posterior probability, which MAP is not.  The new method uses an optimality criterion that is appropriate for perception tasks: it finds the most probable approximately correct answer.  For the case of low observation noise, we provide an efficient approximation.  We apply this new estimator to color constancy.  An unknown illuminant falls on surfaces of unknown colors.  We seek to estimate both the illuminant spectrum and the surface spectra from photosensor responses which depend on the product of these unknown spectra.  In simulations, we show that the MLM method performs better than the MAP estimator, and better than two standard color constancy algorithms.  The MLM method may prove useful in other vision problems as well.  1 The General Problem The task of perception is to infer properties of the world from sensor measurements.  For visual perception, such properties can be object shapes, colors, reflectances, or velocities.  A common approach to this problem is Bayesian analysis, which combines the visual data with prior probabilities to find the posterior probability of properties of the world, which we will call scene parameters.  Typically, the goal is to choose a best estimate of the scene parameters.  One selects an optimality criterion and uses the posterior probability distribution to find the optimal scene parameter estimate.  Two decision rules are almost universally used: maximum a posteriori (MAP) and minimum mean squared error (MMSE).  We believe that neither is the best choice for many problems in computational vision.  The MAP rule is to choose the scene parameter values with the highest posterior probability density.  This estimator is closely related to maximum likelihood methods, and many computational vision algorithms employ it. 
Learning Joint Statistical Models for Audio-Visual Fusion and Segregation| Abstract People can understand complex auditory and visual information, often using one to disambiguate the other.  Automated analysis, even at a lowlevel, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates.  Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships.  We learn the joint distribution of the visual and auditory signals using a non-parametric approach.  First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation.  We then model the complicated stochastic relationships between the signals using a nonparametric density estimator.  These learned densities allow processing across signal modalities.  We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video. 
Contextual models for object detection using boosted random fields| Abstract We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which use boosting to learn the graph structure and local evidence of a conditional random field (CRF).  The graph structure is learned by assembling graph fragments in an additive model.  The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference.  We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade.  We apply our system to detect stuff and things in office and street scenes. 
Computer Vision for Interactive Computer Graphics| Abstract Computers looking through a camera at people is a potentially powerful technique to facilitate human-computer interaction.  The computer can interpret the user's movements, gestures, and glances.  Fundamental visual algorithms include tracking, shape recognition, and motion analysis.  For interactive graphics applications, these algorithms need to be robust, fast, and run on inexpensive hardware.  Fortunately, the interactive applications also make the vision problems easier: they constrain the possible visual interpretations and provide helpful visual feedback to the user.  Thus, some fast and simple vision algorithms can fit well with interactive graphics applications.  We describe several vision algorithms for interactive graphics, and present various vision controlled graphics applications whichwehave built which use them: vision-based computer games, a hand signal recognition system, and a television set controlled by hand gestures.  Some of these applications can employ a special artificial retina chip for image detection or pre-processing. 
Comparison of Graph Cuts with Belief Propagation for Stereo, using Identical MRF Parameters| Abstract Recent stereo algorithms have achieved impressive results by modelling the disparity image as a Markov Random Field (MRF).  An important component of an MRF-based approach is the inference algorithm used to find the most likely setting of each node in the MRF.  Algorithms have been proposed which use Graph Cuts or Belief Propagation for inference.  These stereo algorithms differ in both the inference algorithm used and the formulation of the MRF.  It is unknown whether to attribute the responsibility for differences in performance to the MRF or the inference algorithm.  We address this through controlled experiments by comparing the Belief Propagation algorithm and the Graph Cuts algorithm on the same MRF's, which have been created for calculating stereo disparities.  We find that the labellings produced by the two algorithms are comparable.  The solutions produced by Graph Cuts have a lower energy than those produced with Belief Propagation, but this does not necessarily lead to increased performance relative to the ground-truth. 
Example-Based Super-Resolution| Abstract Image-based models for computer graphics lack resolution independence: they
Learning Hierarchical Models of Scenes, Objects, and Parts| Abstract We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes.  The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator.  Each object category then has its own distribution over these parts, which are shared between objects.  We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters.  Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available.  We also extend this hierarchical framework to scenes containing multiple objects. 
Adv| Neural Information Processing Systems 11,.  Abstract We seek the scene interpretation that best explains image data.  For example, wemaywant to infer the projected velocities (scene) which best explain two consecutive image frames (image).  From synthetic data, we model the relationship between image and scene patches, and between a scene patch and neighboring scene patches.  Given a new image, we propagate likelihoods in a Markov network (ignoring the effect of loops) to infer the underlying scene.  This yields an efficient method to form low-level scene interpretations.  We demonstrate the technique for motion analysis and estimating high resolution images from low-resolution ones. 
Exploiting the Sparse Derivative Prior for Super-Resolution and Image Demosaicing| Abstract When a band-pass filter is applied to a natural image, the distribution of the output has a consistent, distinctive form across many different images, with the distribution sharply peaked at zero and relatively heavy-tailed.  This prior has been exploited for several image processing tasks.  We show how this prior on the appearance of natural images can also be used to estimate full-resolution images from incomplete data.  The unobserved image pixels are modeled with a factor graph.  The constraints in the factor graph are based on the characteristic distribution of image derivatives.  We also introduce an efficient representation for finding candidate values in the image being estimated, avoiding combinatorial explosion.  The usefulness of this approach is demonstrated by applying it to two applications: extracting a high-resolution image from a low-resolution version and estimating a full-color image from an image with one color sample per pixel. 
THE STEERABLE PYRAMID: A FLEXIBLE ARCHITECTURE FOR MULTI-SCALE DERIVATIVE COMPUTATION| ABSTRACT We describe an architecture for efficient and accurate linear decomposition of an image into scale and orientation subbands.  The basis functions of this decomposition are directional derivative operators of any desired order.  We describe the construction and implementation of the transform.  1 Differential algorithms are used in a wide variety of image processing problems.  For example, gradient measurements are used as a first stage of many edge detection, depth-from-stereo, and optical flow algorithms.  Higher-order derivatives have also been found useful in these applications.  Extraction of these derivative quantities may be viewed as a decomposition of a signal via terms of a local Taylor series expansions [1].  Another widespread tool in signal and image processing is multi-scale decomposition.  Apart from the advantages of decomposing signals into information at different scales, the typical recursive form of these algorithms leads to large improvements in computational efficiency.  Many authors have combined multi-scale decompositions with differential measurements (eg. , [2, 3]).  In these cases, a multi-scale pyramid is constructed, and then differential operators (typically, differences of neighboring pixels) are applied to the subbands of the pyramid.  Since both the pyramid decomposition and the derivative operation are linear and shift-invariant, we may combine them into a single operation.  The advantages of doing so are that the resulting derivatives may be more accurate (see [4]).  In this paper, we propose a simple, efficient decomposition architecture for combining these two operations.  The decomposition is the latest incarnation of 1 Source code and filter kernels for implementation of the steerable pyramid are available via anonymous ftp from ftp. 
Context-based vision system for place and object recognition| 2003 massachusetts institute of technology, cambridge, ma 02139 usa --- www. ai. mit. edu massachusetts institute of technology --- artificial intelligence laboratory @ MIT Abstract While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are.  In this paper we present a context-based vision system for place and object recognition.  The goal is to identify familiar locations (e. g. , office 610, conference room 941, Main Street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e. g. , table, chair, car, computer).  We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and how such contextual information introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types.  The algorithm has been integrated into a mobile system that provides real-time feedback to the user. 
The Design and Use of Steerable Filters| Abstract Oriented filters are useful in many early vision and image processing tasks.  One often needs to apply the same filter, rotated to different angles under adaptive control, or wishes to calculate the filter response at various orientations.  We present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively "steer" a filter to any orientation, and to determine analytically the filter output as a function of orientation.  Steerable filters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation.  We show how to design and steer the filters, and present examples of their use in several tasks: the analysis of orientation and phase, angularly adaptive filtering, edge detection, and shape-from-shading.  One can also build a self-similar steerable pyramid representation which may be used to implement a steerable "wavelet" decomposition.  The same concepts can be generalized to the design of 3-D steerable filters, which should be useful in the analysis of image sequences and volumetric data. 
Efficient Graphical Models for Processing Images| Abstract Graphical models are powerful tools for processing images.  However, the large dimensionality of even local image data poses a difficulty: representing the range of possible graphical model node variables with discrete states leads to an overwhelmingly large number of states for the model, often making both exact and approximate inference computationally intractable.  We propose a representation that allows a small number of discrete states to represent the large number of possible image values at each pixel or local image patch.  Each node in the graph represents the best regression function, chosen from a set of candidate functions, for estimating the unobserved image pixels from the observed samples.  This permits a small number of discrete states to summarize the range of possible image values at each point in the image.  Belief propagation is then used to find the best regressor to use at each point.  To demonstrate the usefulness of this technique, we apply it to two problems: super-resolution and color demosaicing.  In both cases, we find our method compares well against other techniques for these problems. 
Nonparametric Belief Propagation| Abstract In many applications of graphical models arising in computer vision, the hidden variables of interest are most naturally specified by continuous, non-Gaussian distributions.  There exist inference algorithms for discrete approximations to these continuous distributions, but for the highdimensional variables typically of interest, discrete inference becomes infeasible.  Stochastic methods such as particle filters provide an appealing alternative.  However, existing techniques fail to exploit the rich structure of the graphical models describing many vision problems.  Drawing on ideas from regularized particle filters and belief propagation (BP), this paper develops a nonparametric belief propagation (NBP) algorithm applicable to general graphs.  Each NBP iteration uses an efficient sampling procedure to update kernel-based approximations to the true, continuous likelihoods.  The algorithm can accomodate an extremely broad class of potential functions, including nonparametric representations.  Thus, NBP extends particle filtering methods to the more general vision problems that graphical models can describe.  We apply the NBP algorithm to infer component interrelationships in a partsbased face model, allowing location and reconstruction of occluded features. 
Learning bilinear models for two-factor problems in vision| Abstract In many vision problems, wewant to infer two (or more) hidden factors which interact to produce our observations.  We may want to disentangle illuminant and object colors in color constancy; rendering conditions from surface shape in shape-from-shading; face identity and head pose in face recognition; or font and letter class in character recognition.  We refer to these two factors generically as \style" and \content".  Bilinear models offer a powerful framework for extracting the two-factor structure of a set of observations, and are familiar in computational vision from several well-known lines of research.  This paper shows how bilinear models can be used to learn the style-content structure of a pattern analysis or synthesis problem, which can then be generalized to solve related tasks using di#erentstyles and/or content.  We focus on three kinds of tasks: extrapolating the style of data to unseen content classes, classifying data with known content under a novel style, and translating two sets of data, generated in di#erentstyles and with distinct content, into each other's styles.  We show examples from color constancy, face pose estimation, shape-from-shading, typography and speech. 
Learning Local Evidence for Shading and Reflectance| Abstract A fundamental, unsolved vision problem is to distinguish image intensityvariations caused by surface normal variations from those caused by reflectance
Image quilting for texture synthesis and transfer| Abstract We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images.  We call this process image quilting.  First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures.  Second, we extend the algorithm to perform texture transfer -- rendering an object with a texture taken from a different object.  More generally, we demonstrate how an image can be re-rendered in the style of a different image.  The method works directly on the images and does not require 3D information. 
Shiftable multiscale transforms| Abstract Orthogonal wavelet transforms have recently become a popular representation for multiscale signal and image analysis.  One of the major drawbacks of these representations is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal.  Wavelet transforms are also unstable with respect to dilations of the input signal, and in two dimensions, rotations of the input signal.  We formalize these problems by defining a type of translation invariance that we call "shiftability".  In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem.  Shiftability may also be considered in the context of other domains, particularly orientation and scale.  We explore "jointly shiftable" transforms that are simultaneously shiftable in more than one domain.  Two examples of jointly shiftable transforms are designed and implemented: a one-dimensional transform that is jointly shiftable in position and scale, and a two-dimensional transform that is jointly shiftable in position and orientation.  We demonstrate the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement. 
Sharing Features: Efficient Boosting Procedures for Multiclass Object Detection| Abstract We consider the problem of detecting a large number of different object classes in cluttered scenes.  Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data.  We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes.  The detectors for each class are trained jointly, rather than independently.  For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes.  In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges. 
Describing Visual Scenes using Transformed Dirichlet Processes| Abstract Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes.  In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image.  Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data.  For visual scenes, mixture components describe the spatial structure of visual features in an object--centered coordinate frame, while transformations model the object positions in a particular image.  Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler.  Applied to a dataset of partially labeled street scenes, we show that the TDP's inclusion of spatial structure improves detection performance, flexibly exploiting partially labeled training images. 
Design galleries: a general approach to setting parameters for computer graphics and animation| Abstract Image rendering maps scene parameters to output pixel values; animation maps motion-control parameters to trajectory values.  Because these mapping functions are usually multidimensional, nonlinear, and discontinuous, finding input parameters that yield desirable output values is often a painful process of manual tweaking.  Interactive evolution and inverse design are two general methodologies for computer-assisted parameter setting in which the computer plays a prominent role.  In this paper we present another such methodology.  Design Gallery TM (DG) interfaces present the user with the broadest selection, automatically generated and organized, of perceptually different graphics or animations that can be produced byvarying a given input-parameter vector.  The principal technical challenges posed by the DG approach are dispersion, finding a set of input-parameter vectors that optimally disperses the resulting output-value vectors, and arrangement, organizing the resulting graphics for easy and intuitive browsing by the user.  We describe the use of DG interfaces for several parameter-setting problems: light selection and placement for image rendering, both standard and image-based; opacity and color transfer-function specification for volume rendering; and motion control for particle-system and articulatedfigure animation. 
Visual Hand Tracking Using Nonparametric Belief Propagation| Abstract--- This paper develops probabilistic methods for visual tracking of a three-dimensional geometric hand model from monocular image sequences.  We consider a redundant representation in which each model component is described by its position and orientation in the world coordinate frame.  A prior model is then defined which enforces the kinematic constraints implied by the model's joints.  We show that this prior has a local structure, and is in fact a pairwise Markov random field.  Furthermore, our redundant representation allows color and edge-based likelihood measures, such as the Chamfer distance, to be similarly decomposed in cases where there is no self--occlusion.  Given this graphical model of hand kinematics, we may track the hand's motion using the recently proposed nonparametric belief propagation (NBP) algorithm.  Like particle filters, NBP approximates the posterior distribution over hand configurations as a collection of samples.  However, NBP uses the graphical structure to greatly reduce the dimensionality of these distributions, providing improved robustness.  Several methods are used to improve NBP's computational efficiency, including a novel KD-tree based method for fast Chamfer distance evaluation.  We provide simulations showing that NBP may be used to refine inaccurate model initializations, as well as track hand motion through extended image sequences. 
(Published on Web)| Learning low-level vision.  Abstract We show a learning-based method for low-level vision problems.  We set-up a Markov network of patches of the image and the underlying scene.  A factorization approximation allows us to easily learn the parameters of the Markov network from synthetic examples of image/scene pairs, and to efficiently propagate image information.  Monte Carlo simulations justify this approximation.  We apply this to the \super-resolution" problem (estimating high frequency details from a low-resolution image), showing good results.  For the motion estimation problem, we show resolution of the aperture problem and filling-in arising from application of the same probabilistic machinery. 
Recovering Intrinsic Images from a Single Image| 2002 massachusetts institute of technology, cambridge, ma 02139 usa --- www. ai. mit. edu massachusetts institute of technology --
Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation| Abstract We describe a three--dimensional geometric hand model suitable for visual tracking applications.  The kinematic constraints implied by the model's joints have a probabilistic structure which is well described by a graphical model.  Inference in this model is complicated by the hand's many degrees of freedom, as well as multimodal likelihoods caused by ambiguous image measurements.  We use nonparametric belief propagation (NBP) to develop a tracking algorithm which exploits the graph's structure to control complexity, while avoiding costly discretization.  While kinematic constraints naturally have a local structure, self-occlusions created by the imaging process lead to complex interpendencies in color and edge--based likelihood functions.  However, we show that local structure may be recovered by introducing binary hidden variables describing the occlusion state of each pixel.  We augment the NBP algorithm to infer these occlusion variables in a distributed fashion, and then analytically marginalize over them to produce hand position estimates which properly account for occlusion events.  We provide simulations showing that NBP may be used to refine inaccurate model initializations, as well as track hand motion through extended image sequences. 
Learning Motion Analysis| Abstract We seek a learning-based algorithm that applies to various low-level vision problems.  For a given problem, we want to find the scene interpretation that best explains image data.  Specializing to the optical flow problem, we may want to infer the projected velocities (scene) which best explain two consecutive image frames (image).  We use synthetic data to generate examples of pairs images with their corresponding scene interpretation, the true projected velocities (optical flow).  From these data, we learn candidate scene explanations for local image regions, and derive a compatibility function between neighboring scene regions.  Given new image data, we propagate beliefs in a Markov network to infer the underlying optical flow.  This yields an ecient method to infer low-level scene interpretations.  We first present the results of this method for a toy world of irregularly shaped blobs.  Then we extend the technique to function on more realistic images, showing reasonable results. 
Learning Low-Level Vision| Abstract We show a learning-based method for low-level vision problems{estimating scenes from images.  We generate a synthetic world of scenes and their corresponding rendered images.  We model that world with a Markov network, learning the network parameters from the examples.  Bayesian belief propagation allows us to efficiently find a local maximum of the posterior probability for the scene, given the image.  We call this approach VISTA{Vision by Image/Scene TrAining.  We apply VISTA to the \super-resolution" problem (estimating high frequency details from a low-resolution image), showing good results.  For the motion estimation problem, we show #gure/ground discrimination, solution of the aperture problem, and filling-in arising from application of the same probabilistic machinery. 
Bethe free energy, Kikuchi approximations and belief propagation algorithms| Abstract Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes.  However, there
Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes| Abstract Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not.  We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities.  We present a conditional random field for jointly solving the tasks of object detection and scene classification. 
Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology| Abstract Graphical models, suchasBayesian networks and Markov Random Fields represent statistical dependencies of variables by a graph.  Local \belief propagation" rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities in singly connected graphical models.  Recently, a number of researchers have empirically demonstrated good performance of \loopy belief propagation"{using these same rules on graphs with loops.  Perhaps the most dramatic instance is the near Shannon-limit performance of \Turbo codes", whose decoding algorithm is equivalenttoloopy belief propagation.  Except for the case of graphs with a single loop, there has been little theoretical understanding of the performance of loopy propagation.  Here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly Gaussian random variables.  We give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation.  We give sufficient conditions for convergence and show that when belief propagation converges it gives the correct posterior means for all graph topologies, not just networks with a single loop.  The related \max-product" belief propagation algorithm finds the maximum posterior probability estimate for singly connected networks.  We show that, even for non-Gaussian probability distributions, the convergence points of the maxproduct algorithm in loopy networks are at least local maxima of the posterior probability.  These results motivate using the powerful belief propagation algorithm in a broader class of networks, and help clarify the empirical performance results. 
Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms| Abstract Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs.  The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles.  We show that BP fixed points correspond to the stationary points of the Bethe approximation to the free energy for a factor graph.  We explain how to obtain regionbased free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms.  We emphasize the conditions a free energy approximation must satisfy in order to be a "valid" approximation.  We describe the relationship between four different methods that can be used to generate valid approximations: the "Bethe method," the "junction graph method," the "cluster variation method," and the "region graph method. " The region graph method is the most general of these methods, and it subsumes all the other methods.  Region graphs also provide the natural graphical setting for GBP algorithms.  We explain how to obtain three different versions of GBP algorithms and show that their fixed points will always correspond to stationary points of the region graph approximation to the free energy.  We also show that the region graph approximation is exact when the region graph has no cycles. 
2nd Annual IEEE International Conference on Image Processing| ABSTRACT We describe an architecture for efficient and accurate linear decomposition of an image into scale and orientation subbands.  The basis functions of this decomposition are directional derivative operators of any desired order.  We describe the construction and implementation of the transform.  1 Differential algorithms are used in a wide variety of image processing problems.  For example, gradient measurements are used as a first stage of many edge detection, depth-from-stereo, and optical flow algorithms.  Higher-order derivatives have also been found useful in these applications.  Extraction of these derivative quantities may be viewed as a decomposition of a signal via terms of a local Taylor series expansions [1].  Another widespread tool in signal and image processing is multi-scale decomposition.  Apart from the advantages of decomposing signals into information at different scales, the typical recursive form of these algorithms leads to large improvements in computational efficiency.  Many authors have combined multi-scale decompositions with differential measurements (eg. , [2, 3]).  In these cases, a multi-scale pyramid is constructed, and then differential operators (typically, differences of neighboring pixels) are applied to the subbands of the pyramid.  Since both the pyramid decomposition and the derivative operation are linear and shift-invariant, we may combine them into a single operation.  The advantages of doing so are that the resulting derivatives may be more accurate (see [4]).  In this paper, we propose a simple, efficient decomposition architecture for combining these two operations.  The decomposition is the latest incarnation of 1 Source code and filter kernels for implementation of the steerable pyramid are available via anonymous ftp from ftp. 
A Conversation about the Bethe Free Energy and Sum-Product| Abstract This discussion document records an email conversation in preparation for the Trieste meeting.  Background The result that `belief propagation fixed-points are zero gradient points of the Bethe free energy' (Yedidia, 2000; Yedidia et al. , 2000c)
Discovering object categories in image collections|
A Factorization Approach to Grouping| Abstract The foreground group in a scene may be `discovered' and computed as a factorized approximation to the pairwise affinity of the elements in the scene.  A pointwise approximation of the pairwise affinity information may in fact be interpreted as a `saliency' index, and the foreground of the scene may be obtained by thresholding it.  An algorithm called `affinity factorization' is thus obtained whichmay be used for grouping.  The affinity factorization algorithm is demonstrated on displays com- posed of points, of lines and of brightness values.  Its relationship to the Shi-Malik normalized cuts algorithms is explored both analytically and experimentally.  The affinity factorization algorithm is shown to be com- putationally efficient (O(n) floating-point operations for a scene com- posed of n elements) and to perform well on displays where the back- ground is unstructured.  Generalizations to solve more complex problems are also discussed. 
Teaching applied computing without programming: a case-based introductory course for general education| Abstract We introduce general-education students to key ideas in applied computing through case studies from computer
An example-based approach to style translation for line drawings| Abstract We present an example-based system for translating line drawings into different styles.  The system is given a training set of many different lines, each drawn by an artist in various styles, which is used to translate new lines made by a user into a particular desired style with a K-nearest neighbor algorithm.  This algorithm fits each input line as a linear combination of the several training lines in the same style which are most similar to it.  The fit line can then be rendered in different styles because the training set contains versions of each training line in eachstyle.  By describing input lines as linear combinations of training set lines, this procedure is expressive enough to fit a broad range of input drawings.  By restricting these linear combinations to contain only the most similar training set lines, this procedure is constrained enough to preserve the distinctive stylistic features of translated lines.  We represent input lines by splines with nonuniformly spaced control points, which emphasizes these stylistic features.  Our example-based approachhasanumber of advantages over conventional parameteric approaches to translating style.  It can handle styles which are difficult to describe parametrically, and its repertoire can be easily extended by the user at any time.  Moreover, given appropriate representations, it can be generalized to modify the style of other kinds of graphics objects, such as the font of a letter or the movementstyle of an animated character. 
Exploiting Spatial and Spectral Image Regularities for Color Constancy| Abstract We study the problem of color constancy--infering from an image the spectrum of the illumination and the reflectance spectra of all the depicted surfaces.  This estimation problem is underdetermined: many surface and illumination spectra can be described as a linear combination of 3 basis functions, giving 3 unknowns per pixel, plus 3 parameters for a global illumination.  A trichromatic visual system makes fewer measurements than there are unknowns.  We address this problem by writing the reflectance spectra of small groups of pixels as a linear combination of spatio-spectral basis functions.  These aggregated surface reflectance spectra require fewer parameters to describe than the sum of the spectral parameters for the individual surface pixels, giving us more measurements than unknown parmaeters.  We explore this problem in a Bayesian context, showing when the problem is over or underdetermined based on analyzing the local curvature characteristics of the log-likelihood function.  We show that using the spatio-spectral basis functions gives improved reflectance and illumination spectral estimates when applied to real image data. 
Discovering objects and their location in images| Abstract We seek to discover the object categories depicted in a set of unlabelled images.  We achieve this using a model developed in the statistical text literature: probabilistic Latent Semantic Analysis (pLSA).  In text analysis this is used to discover topics in a corpus using the bag-of-words document representation.  Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics.  The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors.  The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision.  Performance of this unsupervised method is compared to the supervised approach of Fergus et al.  [8] on a set of unseen images containing only one object per image.  We also extend the bag-of-words vocabulary to include `doublets' which encode spatially local co-occurring regions.  It is demonstrated that this extended vocabulary gives a cleaner image segmentation.  Finally, the classification and segmentation methods are applied to a set of images containing multiple objects per image.  These results demonstrate that we can successfully build object class models from an unsupervised analysis of images. 
Learning style translation for the lines of a drawing| MIT We present an example-based method for translating line drawings into different styles.  We fit each line as a linear combination of similar lines in a training set, and interpolate between the corresponding training examples in the output style.  The synthesized lines preserve the desired stylistic features of the output style. 
Shape Recipes: Scene Representations that Refer to the Image| Abstract The goal of low-level vision is to estimate an underlying scene, given an observed image.  Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store.  We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene configurations.  Shape recipes are an example: these are the regression coefficients that predict the bandpassed shape from image data.  We describe the benefits of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation. 
On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs| Problems involving probabilistic belief propagation arise in a wide variety of applications, including error correcting codes, speech recognition and image understanding.  Typically, a probability distribution is assumed over a set of variables and the task is to infer the values of the unobserved variables given the observed ones.  The assumed probability distribution is described using a graphical model [13] --- the qualitative aspects of the distribution are specified bya graph structure.  The graph may either be directed as in a Bayesian network [17], [11] or undirected as in a Markov Random Field [17], [9].  Here we focus on the problem of finding an assignment for the unobserved variables that is most probable given the observed ones.  In general, this problem is NP hard [18] but if the graph is singly connected (i. e.  there is only one path between anytwo given nodes) then there exist efficient local message{passing schemes to perform this task.  Pearl [17] derived suchascheme for singly connected Bayesian networks.  The algorithm, which he called \belief revision", is identical to his algorithm for finding posterior marginals over nodes except that the summation operator is replaced with a maximization.  Aji et al.  [2] have shown that both of Pearl's algorithms can be seen as special cases of generalized distributive laws over particular semirings.  In particular, Pearl's algorithm for finding maximum a posteriori (MAP) assignments can be seen as a generalized distributivelawover the max-product semiring.  We will henceforth refer to it as the \max-product" algorithm.  Pearl showed that for singly connected networks, the max-product algorithm is guaranteed to converge and that the assignment based on the messages at convergence is guaranteed to give the optimal assignment{values corresponding to the MAP solution.  Several groups have recently reported excellent experimental results by running the max-product algorithm on graphs with loops [22], [6], [3], [19], [6], [10].  Benedetto et al.  used the max-product algorithm to decode \turbo" codes and obtained excellent results that were slightly inferior to the original turbo decoding algorithm (which is equivalent to the sum-product algorithm).  Weiss [19] compared the performance of sum-product and max-product on a \toy" turbo code problem while distinguishing between converged and unconverged cases.  He found that if one considers only the convergent cases, the performance of max-product decoding is significantly better than sum-product decoding.  However, the max-product algorithm converges less often so its overall performance (including both convergent and nonconvergent cases) is inferior.  Progress in the analysis of the max-product algorithm has been made for two special topologies: single loop graphs, and \cycle codes".  For graphs with a single loop [22], [19], [20], [5], [2], it can be shown that the algorithm converges to a stable fixed point or a periodic oscillation.  If it converges to a stable fixed-point, then the assignment based on the fixed-point messages is the optimal assignment.  For graphs that correspond to cycle codes (low density paritycheck codes in which each bit is checked by exactly twocheck nodes), Wiberg [22] gave sufficient conditions for max-product to converge to the transmitted codeword and Horn [10] gave sufficient conditions for convergence to the MAP assignment.  In this paper we analyze the max-product algorithm in graphs of arbitrary topology.  We show that at a #xed-pointof the algorithm, the assignment is a \neighborhood maximum" of the posterior probability: the posterior probabilityof the max-product assignment is guaranteed to be greater than all other assignments in a particular large region around that assignment.  These results motivate using this powerful algorithm in a broader class of networks. 
Separating Style and Content with Bilinear Models| Abstract PERCEPTUAL systems routinely separate \content" from \style", classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions.  Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive.  Existing factor models are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms.  Here we show how perceptual systems may learn to solve these crucial tasks using surprisingly simple bilinear models.  We report promising results in three realistic perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants. 
MITSUBISHI ELECTRIC RESEARCH LABORATORIES CAMBRIDGE RESEARCH CENTER Orientation Histograms for Hand Gesture Recognition| Abstract We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation.  We use the orientation histogram as a feature vector for gesture class#cation and interpolation.  This method is simple and fast to compute, and offers some robustness to scene illumination changes.  We have implemented a real-time version, which can distinguish a small vocabulary of about 10 different hand gestures.  All the computation occurs on a workstation; special hardware is used only to digitize the image.  A user can operate a computer graphic crane under hand gesture control, or play a game.  We discuss limitations of this method.  For moving or \dynamic gestures", the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition. 
Properties and Applications of Shape Recipes| In low-level vision, the representation of scene properties such as shape, albedo, etc. , are very high dimensional as they have to describe complicated structures.  The approach proposed here is to let the image itself bear as much of the representational burden as possible.  In many situations, scene and image are closely related and it is possible to find a functional relationship between them.  The scene information can be represented in reference to the image where the functional specifies how to translate the image into the associated scene.  We illustrate the use of this representation for encoding shape information.  We show how this representation has appealing properties such as locality and slow variation across space and scale.  These properties provide a way of improving shape estimates coming from other sources of information like stereo. 
Generalized Belief Propagation| Abstract Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes.  However, there
Bayesian Reconstruction of 3D Human Motion from Single-Camera Video|
Separating style and content with biliear models|
Computer vision for computer games|
The steerable pyramid: A flexible architecture for multi-scale derivative computation,"|
Bayesian method for recovering surface and illuminant properties from photosensor responses|
Median filter for reconstructing missing color samples|
Separating Style and Content|
Understanding belief propagation and its generalizations|
Steerable Filters and Local Analysis of Image Structure|
Understanding belief propagation and its generalization|
Learning to Estimate Scenes from Images|
The generic viewpoint assumption in a framework for visual perception|
Exploiting the generic view assumption to estimate scene parameters|
Example-based head tracking|
Bayesian estimation of 3-d human motion from an image sequence|
Bethe free energies, Kikuchi approximations, and belief propagation algorithms|
Shape-Time Photography|
Motion without movement|
Separating style and context with bilinear models|
Shape-from-shading Analysis with Shadelets and Bumplets|
Demonstration of an Interactive Multimedia Environment|
Bayesian Model of Surface Perception|
Deep radio occultations and "evolute flashes"; their characteristics and utility for planetary studies,|
An Animated On-Line Community with Artificial Agents|
A flexible architecture for multi-scale derivative computation|
Wavelet image transforms with continuous parameterization| Vision and Modeling Technical Report 161, The Media Lab,. 
Steerable filters for early vision, image analysis, and wavelet decomposition|
Steerable filters|
Generalised belief propagation|
Bayean color constancy,|
"Bethe free energy, Kikuchi approximations, and belief propagation algorithms,"|
Exploiting the generic viewpoint assumption|
Characterizing belief propagation and its generalizations|
The generic viewpoint assumption in a Bayesian framework|
computer vision or interactive computer graphics|
The design and use of steereable filters,|
The Design and Use of Steerable Filters|InTrans. 
sky, "Visual Hand Tracking Using Nonparametric Belief Propagation,|
Bayesian colour constancy|
Shiftable multiiscale transforms|

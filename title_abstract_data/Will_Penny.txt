Dynamic Linear Models, Recursive Least Squares and Steepest-Descent Learning| Abstract In this short research note we highlight the differences between Dynamic Linear Models, Recursive Least Squares and Steepest-Descent Learning.  All models are viewed as special cases of linear dynamical systems and their learning rules as special cases of the Kalman filter.  Steepest-Descent Learning is a fixed learning rate algorithm for modelling stationary data.  Recursive Least Squares is an adaptive learning rate algorithm for modelling stationary data.  Dynamic Linear Models are adaptive learning rate algorithms for modelling stationary and non-stationary data.  They have state-noise and observation noise parameters which can be updated on-line so as to maximise the evidence of the observations. 
Mixtures of General Linear Models for Functional Neuroimaging| Abstract|We set out a new general framework for making inferences from neuroimaging data, which includes a standard approach to neuroimaging analysis, Statistical Parametric Mapping (SPM), as a special case.  The model offers numerous conceptual and statistical advantages that derive from analysing data at the `cluster-level' rather than the `voxel-level' and from explicit modelling of the shape and position of clusters of activation.  This provides a natural and principled way to pool data from nearby voxels for parameter and variance-component estimation.  The model can also be viewed as performing a spatio-temporal cluster analysis.  The parameters of the model are estimated using an Expectation Maximisation (EM) algorithm. 
EEG-Based Communication: A Pattern Recognition Approach",|
The general Linear Model; Contrasts and classical inference|
Application of the Gibbs sampler to fMRI data|

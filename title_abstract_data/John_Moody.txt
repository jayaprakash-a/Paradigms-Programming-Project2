LEARNING RATE SCHEDULES FOR FASTER STOCHASTIC GRADIENT SEARCH| Abstract.  Stochastic gradient descent is a general algorithm that includes LMS, on-line backpropagation, and adaptive k-means clustering as special cases.  The standard choices of the learning rate j (both adaptive and fixed functions of time) often perform quite poorly.  In contrast, our recently proposed class of "search then converge" (STC) learning rate schedules (Darken and Moody, 1990b, 1991) display the theoretically optimal asymptotic convergence rate and a superior ability to escape from poor local minima However, the user is responsible for setting a key parameter.  We propose here a new methodology for creating the first automatically adapting learning rates that achieve the optimal rate of convergence. 
Network Dynamics and Field Evolution: The Growth of Interorganizational Collaboration in the Life Sciences| Abstract We develop and test four alternative logics of attachment - - accumulative advantage, homophily, follow-the-trend, and multiconnectivity - - to account for both the structure and dynamics of interorganizational collaboration in the field of biotechnology.  The commercial field of the life sciences is typified by wide dispersion in the sources of basic knowledge and rapid development of the underlying science, fostering collaboration among a broad range of institutionally diverse actors.  We map the network dynamics of the field over the period 1988-99.  Using multiple novel methods, including analysis of network degree distributions, network visualizations, and multi-probability models to estimate dyadic attachments, we demonstrate how a preference for diversity shapes network evolution.  Commercialization strategies pursued by early commercial entrants are supplanted by collaborative activities influenced more by universities, research institutes, venture capital, and small firms.  As organizations increase both the number of activities on which they collaborate and the diversity of organizations with whom they are linked, cohesive sub-networks form that are characterized by multiple, independent pathways.  These structural components, in turn, condition the choices and opportunities available to members of a field, thereby reinforcing an attachment logic based on connection to partners that are diversely and differently linked.  The dual analysis of network and institutional evolution provides an explanation for the decentralized structure of this science-based field. 
Field Testing the Tongues Speech-to-Speech Machine Translation System| Abstract The Tongues portable, rapid-development, speech-to-speech machine translation system was developed specifically to allow a realistic field-test of a deployable prototype.  In this paper we will describe the system, its field-testing using regular US Army officers and naive Croatians, and the evaluation of these tests.  The evaluation includes analysis of answers to a questionnaire, analysis of system transcript logs, and the authors' qualitative observations.  The overall result of the test was that while the system did successfully aid translation, it requires further development before it would be ready for regular field use. 
The Effective Number of Parameters: An Analysis of Generalization and Regularization in Nonlinear Learning Systems| Abstract We present an analysis of how the generalization performance (expected test set error) relates to the expected training set error for nonlinear learning systems, such as multilayer perceptrons and radial basis functions.  The principal result is the following relationship (computed to second order) between the expected test set and training set errors: hE test ( )i 0 hE train ( )i + 2oe 2 eff p eff ( ) n : (1) Here, n is the size of the training sample , oe 2 eff is the effective noise variance in the response variable(s), is a regularization or weight decay parameter, and p eff ( ) is the effective number of parameters in the nonlinear model.  The expectations h i of training set and test set errors are taken over possible training sets and training and test sets 0 respectively.  The effective number of parameters p eff ( ) usually differs from the true number of model parameters p for nonlinear or regularized models; this theoretical conclusion is supported by Monte Carlo experiments.  In addition to the surprising result that p eff ( ) 6= p, we propose an estimate of (1) called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's FPE and AIC, Mallows CP , and Barron's PSE to the nonlinear setting.  1 1 GPE and p eff ( ) were previously introduced in Moody (1991).  1 Background and Motivation Many of the nonlinear learning systems of current interest for adaptive control, adaptive signal processing, and time series prediction, are supervised learning systems of the regression type.  Understanding the relationship between generalization performance and training error and being able to estimate the generalization performance of such systems is of crucial importance.  We will take the prediction risk (expected test set error) as our measure of generalization performance.  2 Learning from Examples Consider a set of n real-valued input/output data pairs (n) = fi = (x i ; y i ); i = 1; : : : ; ng drawn from a stationary density \Xi().  The observations can be viewed as being generated according to the signal plus noise model 2 y i = (x i ) + ffl i (2) where y i is the observed response (dependent variable), x i are the independent variables sampled with input probability density \Omega(x), ffl i is independent, identicallydistributed (iid) noise sampled with density \Phi(ffl) having mean 0 and variance oe 2 , 3 and (x) is the conditional mean, an unknown function.  From the signal plus noise perspective, the density \Xi() = \Xi(x; y) can be represented as the product of two components, the conditional density \Psi(yjx) and the input density \Omega(x): \Xi(x; y) = \Psi(yjx) \Omega(x) j \Phi(y \Gamma (x)) \Omega(x) : (3) The learning problem is then to find an estimate b (x) of the conditional mean (x) on the basis of the training set (n).  In many real world problems, few a priori assumptions can be made about the functional form of (x).  Since a parametric function class is usually not known, one must resort to a nonparametric regression approach, whereby one constructs an estimate b (x) = f(x) for (x) from a large class of functions F known to have good approximation properties (for example, F could be all possible radial basis function networks and multilayer perceptrons).  The class of approximation functions is usually the union of a countable set of subclasses (specific network architectures) 4 A ae F for which the elements of each subclass f(w; x) 2 A are continuously parametrized by a set of p = p(A) weights w = fw ff ; ff = 1; : : : ; pg.  The task of finding the estimate f(x) thus consists of two problems: choosing the best architecture b A and choosing the best set of weights b w given the architecture.  Note that in 2 The assumption of additive noise ffl which is independent of x is a standard assumption and is not overly restrictive.  Many other conceivable signal/noise models can be transformed into this form.  For example, the multiplicative model y = (x)(1 + ffl) becomes y 0 = 0 (x) + ffl 0 for the transformed variable y 0 = log(y).  3 Note that we have made only a minimal assumption about the noise ffl, that it is has finite variance oe 2 independent of x.  Specifically, we do not need to make the assumption that the noise density \Phi(ffl) is of known form (e. g.  gaussian) for the following development.  4 For example, a "fully connected two layer perceptron with five internal units". 
The Dependence Identification Neural Network Construction Algorithm| Abstract An algorithm for constructing and training multilayer neural networks, dependence identification, is presented in this paper.  Its distinctive features are that (i) it transforms the training problem into a set of quadratic optimization problems that are solved by a number of linear equations and (ii) it constructs an appropriate network to meet the training specifications.  The architecture and network weights produced by the algorithm can also be used as initial conditions for further on-line training by backpropagation or a similar iterative gradient descent training algorithm if necessary.  In addition to constructing an appropriate network based on training data, the dependence identification algorithm significantly speeds up learning in feedforward multilayer neural networks compared to standard backpropagation. 
Active Exploration in Dynamic Environments| Abstract Whenever an agent learns to control an unknown environment, two opposing principles have to be combined, namely: exploration (long-term optimization) and exploitation (short-term optimization).  Many real-valued connectionist approaches to learning control realize exploration by randomness in action selection.  This might be disadvantageous when costs are assigned to "negative experiences".  The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner.  This is achieved by a so-called competence map, which is trained to predict the controller's accuracy, and is used for guiding exploration.  Based on this, a bistable system enables smoothly switching attention between two behaviors -- exploration and exploitation -- depending on expected costs and knowledge gain.  The appropriateness of this method is demonstrated by a simple robot navigation task. 
TONGUES: RAPID DEVELOPMENT OF A SPEECH-TO-SPEECH TRANSLATION SYSTEM| ABSTRACT We carried out a one-year project to build a portable speech-tospeech translation system in a new language that could run on a small portable computer.  Croatian was chosen as the target language.  The resulting system was tested with real users on a trip to Croatia in the spring of 2001.  We describe its basic components, the methods we used to build them, initial evaluation results, and related significant observations.  This work was done in conjunction with the US Army Chaplain School; chaplains are often the only personnel in a position to communicate with local people over non-military issues such as medical supplies, refugees, etc.  This paper thus reports on a realistic instance of rapidly deploying and field-testing a speech-to-speech translator using current technology. 
RAPID DEVLOPEMENT OF SPEECH-TO-SPEECH TRANSLATION SYSTEMS| ABSTRACT This paper describes building of the basic components, particularly speech recognition and synthesis, of a speech-tospeech translation system.  This work is described within the framework of the "Tongues: small footprint speech-tospeech translation device" developed at CMU and Lockheed Martin for use by US Army Chaplains. 
EXPLORATORY DATA ANALYSIS BY THE SELF-ORGANIZING MAP: STRUCTURES OF WELFARE AND POVERTY IN THE WORLD| The self-organizing map (SOM) is a method that represents statistical data sets in an ordered fashion, as a natural groundwork on which the distributions of the individual indicators in the set can be displayed and analyzed.  As a case study that instructs how to use the SOM to compare states of economic systems, the standard of living of different countries is analyzed using the SOM.  Based on a great number (39) of welfare indicators the SOM illustrates rather refined relationships between the countries two-dimensionally.  This method is directly applicable to the financial grading of companies, too. 
Neural Network Models for the Blood Glucose Metabolism of a Diabetic| Abstract We study the application of neural networks to modeling the blood glucose metabolism of a diabetic.  In particular we consider recurrent neural networks and time series convolution neural networks which we compare to linear models and to nonlinear compartment models.  We include a linear error model to take into account the uncertainty in the system and for handling missing blood glucose observations.  Our results indicate that best performance can be achieved by the combination of the recurrent neural network and the linear error model. 
Appears in| Abstract We present an analysis of how the generalization performance (expected test set error) relates to the expected training set error for nonlinear learning systems, such as multilayer perceptrons and radial basis functions.  The principal result is the following relationship (computed to second order) between the expected test set and training set errors: hE test ( )i 0 hE train ( )i + 2oe 2 eff p eff ( ) n : (1) Here, n is the size of the training sample , oe 2 eff is the effective noise variance in the response variable(s), is a regularization or weight decay parameter, and p eff ( ) is the effective number of parameters in the nonlinear model.  The expectations h i of training set and test set errors are taken over possible training sets and training and test sets 0 respectively.  The effective number of parameters p eff ( ) usually differs from the true number of model parameters p for nonlinear or regularized models; this theoretical conclusion is supported by Monte Carlo experiments.  In addition to the surprising result that p eff ( ) 6= p, we propose an estimate of (1) called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's FPE and AIC, Mallows CP , and Barron's PSE to the nonlinear setting.  1 1 GPE and peff ( ) were previously introduced in Moody (1991).  1 Background and Motivation Many of the nonlinear learning systems of current interest for adaptive control, adaptive signal processing, and time series prediction, are supervised learning systems of the regression type.  Understanding the relationship between generalization performance and training error and being able to estimate the generalization performance of such systems is of crucial importance.  We will take the prediction risk (expected test set error) as our measure of generalization performance.  2 Learning from Examples Consider a set of n real-valued input/output data pairs (n) = fi = (x i ; y i ); i = 1; : : : ; ng drawn from a stationary density \Xi().  The observations can be viewed as being generated according to the signal plus noise model 2 y i = (x i ) + ffl i (2) where y i is the observed response (dependent variable), x i are the independent variables sampled with input probability density \Omega(x), ffl i is independent, identicallydistributed (iid) noise sampled with density \Phi(ffl) having mean 0 and variance oe 2 , 3 and (x) is the conditional mean, an unknown function.  From the signal plus noise perspective, the density \Xi() = \Xi(x; y) can be represented as the product of two components, the conditional density \Psi(yjx) and the input density \Omega(x): \Xi(x; y) = \Psi(yjx) \Omega(x) j \Phi(y \Gamma (x)) \Omega(x) : (3) The learning problem is then to find an estimate b (x) of the conditional mean (x) on the basis of the training set (n).  In many real world problems, few a priori assumptions can be made about the functional form of (x).  Since a parametric function class is usually not known, one must resort to a nonparametric regression approach, whereby one constructs an estimate b (x) = f(x) for (x) from a large class of functions F known to have good approximation properties (for example, F could be all possible radial basis function networks and multilayer perceptrons).  The class of approximation functions is usually the union of a countable set of subclasses (specific network architectures) 4 A ae F for which the elements of each subclass f(w; x) 2 A are continuously parametrized by a set of p = p(A) weights w = fw ff ; ff = 1; : : : ; pg.  The task of finding the estimate f(x) thus consists of two problems: choosing the best architecture b A and choosing the best set of weights b w given the architecture.  Note that in 2 The assumption of additive noise ffl which is independent of x is a standard assumption and is not overly restrictive.  Many other conceivable signal/noise models can be transformed into this form.  For example, the multiplicative model y = (x)(1 + ffl) becomes y 0 = 0 (x) + ffl 0 for the transformed variable y 0 = log(y).  3 Note that we have made only a minimal assumption about the noise ffl, that it is has finite variance oe 2 independent of x.  Specifically, we do not need to make the assumption that the noise density \Phi(ffl) is of known form (e. g.  gaussian) for the following development.  4 For example, a "fully connected two layer perceptron with five internal units". 
Towards Faster Stochastic Gradient Search| Abstract Stochastic gradient descent is a general algorithm which includes LMS, on-line backpropagation, and adaptive k-means clustering as special cases.  The standard choices of the learning rate j (both adaptive and fixed functions of time) often perform quite poorly.  In contrast, our recently proposed class of "search then converge" learning rate schedules (Darken and Moody, 1990) display the theoretically optimal asymptotic convergence rate and a superior ability to escape from poor local minima.  However, the user is responsible for setting a key parameter.  We propose here a new methodology for creating the first completely automatic adaptive learning rates which achieve the optimal rate of convergence. 
Optimization of trading systems and portfolios,|
Fast learning in units of locally-tuned processing units|
Feedback control of Petri nets based on place invariants",|
Data Visualization and Feature Selection: New Algorithms for Nongaussian Data|
The Burau representation of the braid group B n is unfaithful for large n|
Selecting neural network architectures via the prediction risk: application to corporate bond rating prediction|
Principled Architecture Selection for Neural Networks: Application to Corporate Bond Rating Prediction|
Generalization, weight decay, and architecture selection for nonlinear learning systems|
Fast Learning in Multi-Resolution Hierarchies|
Learning with localized receptive fields|
When does Bootstrap Work: Asymptotic Results and|
Prediction risk and architecture selection for neural networks|
Constructing Heterogeneous Committees Using Input Feature Grouping: Application to Economic Forecasting|
Architecture selection strategies for neural networks: Application to corporate bond rating prediction|
iLearning with localized receptive elds,|
Feature selection based on joint mutual information|
What is the `true price'?---State space models for high frequency financial data,|
Improved Estimates for the Rescaled Range and Hurst Exponents,|
Fast Pruning Using Principal Components|
Neural Modeling of Physiological Processes,|
Supervisory control of Petri nets with uncontrollable/unobservable transitions|
Petri Net Supervisors for Discrete Event Systems|
Supervisory control using computationally efficient linear techniques: A tutorial introduction|
Learning to trade via direct reinforcement|
Peer influence groups: identifying dense clusters in large networks,|
A trivial but fast reinforcement controller|
Predicting the U|S.  index of industrial production. 
A Method for Deadlock Prevention in Discrete Event Systems Using Petri Nets|
The faithfulness question for the Burau representation,|
Weight Space Probability Densities in Stochastic Learning: I| Dynamics and Equilibria. 
Fast adaptive k-means clustering: some empirical results|
"The Dependence Identification Neural Network Construction Algorithm|
Note on Learning Rate Schedules for Stochastic Optimization|
The Burau representation of the braid group Bn is unfaithful for large n ,|
Internal representations for associative memory|
Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks|
A neural network visualization and sensitivity analysis toolkit,|
"A smoothing regularizer for feedforward and recurrent neural networks,|
Supervisory control of discrete event systems using Petri nets,|
Performance functions and reinforcement learning for trading systems and portfolios,|
Automated synthesis of deadlock prevention supervisors using Petri nets",|
Note on generlization, regularization and architecture selection in non linear learning systems|
Minimizing downside risk via stochastic dynamic programming|
Petri net supervisors for DES with uncontrollable and unobservable transitions|
Networks with Learned Unit Response Functions|
What is the itrue pricej? State space models for high frequency FX data|
Reinforcement Learning for Trading Systems and Portfolios|
Automated synthesis of deadlock prevention supervisors using Petri Nets", Technical Report of the ISIS Group, isis-2000-003,|
Internal representaions for associative memory|
Multi-effect Decompositions for Financial Data Modeling|
Probability densities and equilibria in stochastic learning,|
Learning with localised receptive fields,|
Atmospheric deposition of nutrients to the North Atlantic Basin,|
Input variable selection for neural networks: Application to predicting the U|S.  business cycle,. 
Smoothing Regularizers for Projective Basis Function Networks|
Fast learning in mult-resolution hierarchies|
Internal representation for associative memory|
Neural Networks in Financial Engineering",|
Note on Development of Modularity in Simple Cortical Models|
"A New Method for Constructing and Training Multilayer Neural Networks,|

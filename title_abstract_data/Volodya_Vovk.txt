The Existence of Predictive Complexity and the Legendre Transformation| Abstract Predictive complexity is a generalisation of Kolmogorov complexity.  In this paper we point out some properties of predictive complexity connected with the Legendre (--Young--Fenchel) transformation.  Our main result is that mixability is necessary for the existence of conditional predictive complexity (it is known to be sufficient under very mild assumptions).  We formulate a differential criterion of mixability and show that it reduces to a very simple form if we employ the Legendre transformation.  The Legendre transformation also turns out to have a probabilistic meaning which allows us to prove that a variant of predictive complexity specifies a unique (up to a parametrisation) mixable game. 
Machine-Learning Applications of Algorithmic Randomness| Abstract Most machine learning algorithms share the following drawback: they only output bare predictions but not the confidence in those predictions.  In the 1960s algorithmic information theory supplied universal measures of confidence but these are, unfortunately, non-computable.  In this paper we combine the ideas of algorithmic information theory with the theory of Support Vector machines to obtain practicable approximations to universal measures of confidence.  We show that in some standard problems of pattern recognition our approximations work well. 
Competitive on-line statistics| Abstract A radically new approach to statistical modelling, which combines mathematical techniques of Bayesian statistics with the philosophy of the theory of competitive on-line algorithms, has arisen over the last decade in computer science (to a large degree, under the influence of Dawid's prequential statistics).  In this approach, which we call "competitive on-line statistics", it is not assumed that data are generated by some stochastic mechanism; the bounds derived for the performance of competitive on-line statistical procedures are guaranteed to hold (and not just hold with high probability or on the average).  This paper reviews some results in this area; the new material in it includes the proofs for the performance of the Aggregating Algorithm in the problem of linear regression with square loss. 
Complexity Approximation Principle| Abstract We propose a new inductive principle, which we call Complexity Approximation Principle.  This principle is a natural generalization of Rissanen's MDL principle and Wallace's MML principle and is based on the notion of predictive complexity, a recent generalization of Kolmogorov complexity.  Like the MDL principle, Complexity Approximation Principle can be regarded as an explication of Occam's razor.  The subject of this note is another inductive principle, which can be regarded as a direct generalization of
PRICING EUROPEAN OPTIONS WITHOUT PROBABILITY| Abstract It is well known that in the case where the stock price S t is governed by the equation dS t =S t = dt + oedW t , any European option satisfying weak regularity conditions has a fair price (the Black---Scholes formula and its generalizations).  We consider the case where no probabilistic assumptions are made about S t ; instead, we assume that the derivative security D which pays a dividend of (dS t =S t ) 2 (the squared relative increase in the price of S t ) each instant dt is traded in the market.  We prove that the ``regular" European options have fair prices provided that both S t and D t (the price process of D) are continuous and the fractal dimensions of the graphs of S t and D t satisfy certain inequalities.  Intuitively our assumptions are much weaker than the usual assumption dS t =S t = dt + oedW t . 
Universal Portfolio Selection| Abstract A typical problem in portfolio selection in stock markets is that it is not clear which of the many available strategies should be used.  We apply a general algorithm of prediction with expert advice (the Aggregating Algorithm) to two different idealizations of the stock market.  One is the well-known game introduced by Cover in connection with his "universal portfolio" algorithm; the other is a more realistic modification of Cover's game introduced in this paper, where market's participants are allowed to take "short positions", so that the algorithm may be applied to currency and futures markets.  Besides applying the Aggregating Algorithm to a countable (or finite) family of arbitrary investment strategies, we also apply it, in the case of Cover's game, to the uncountable family of "constant rebalanced portfolios" considered by Cover.  We generalize Cover's worst-case bounds for his "universal portfolio" algorithm (which can be regarded as a special case of the Aggregating Algorithm corresponding to learning rate 1) to the case of learning rates not exceeding 1.  Finally, we discuss a general approach to designing investment strategies in which, instead of making statistical or other assumptions about the market, natural assumptions of computability are made about possible investment strategies; this approach leads to natural extensions of the notion of Kolmogorov complexity. 
Support Vector Regression with ANOVA Decomposition Kernels| Abstract Support Vector Machines using ANOVA Decomposition Kernels (SVAD) [Vapng] are a way of imposing a structure on multi-dimensional kernels which are generated as the tensor product of one-dimensional kernels.  This gives more accurate control over the capacity of the learning machine (VCdimension).  SVAD uses ideas from ANOVA decomposition methods and extends them to generate kernels which directly implement these ideas.  SVAD is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel.  The Boston housing data set from UCI has been tested on Bagging [Bre94] and Support Vector methods before [DBK + 97] and these results are compared to the SVAD method. 
Transductive Confidence Machines for Pattern Recognition| Abstract We propose a new algorithm for pattern recognition that outputs some measures of "reliability" for every prediction made, in contrast to the current algorithms that output "bare" predictions only.  Our algorithm is based on the well-known nearest neighbours algorithm and uses a similar rule to infer predictions. 
Pattern Recognition and Density Estimation under the General i|i. d.  Assumption.  Abstract Statistical learning theory considers three main problems, pattern recognition, regression and density estimation.  When analyzing learning algorithms, the usual assumption made is that the data is generated by an i. i. d.  source; sometimes, however, extra assumptions are also made.  This paper studies solvability of these three main problems (mainly concentrating on pattern recognition and density estimation) in the "highdimensional" case, where the patterns in the training and test sets are never repeated.  We show that, assuming an i. i. d.  data source but without any further assumptions, the problems of pattern recognition and regression can often be solved (and there are practically useful algorithms to solve them).  On the other hand, the problem of density estimation, as we formalize it, cannot be solved under the general i. i. d.  assumption, and additional assumptions are required. 
Derandomizing Stochastic Prediction Strategies| Abstract In this paper we continue study of the games of prediction with expert advice with uncountably many experts.  A convenient interpretation of such games is to construe the pool of experts as one "stochastic predictor", who chooses one of the experts in the pool at random according to the prior distribution on the experts and then replicates the (deterministic) predictions of the chosen expert.  We notice that if the stochastic predictor's total loss is at most L with probability at least p then the learner's loss can be bounded by cL + a ln 1 p for the usual constants c and a.  This interpretation is used to revamp known results and obtain new results on tracking the best expert.  It is also applied to merging overconfident experts and to fitting polynomials to data. 
The typicalness framework: a comparison with the Bayesian approach| Abstract When correct priors are known, Bayesian algorithms give optimal decisions, and accurate confidence values for predictions can be obtained.  If the prior is incorrect however, these confidence values have no theoretical base -- even though the algorithms' predictive performance may be good.  There also exist many successful learning algorithms which only depend on the iid assumption.  Often however they produce no confidence values for their predictions.  Bayesian frameworks are often applied to these algorithms in order to obtain such values, however they can rely on unjustified priors.  In this paper we outline the typicalness framework which can be used in conjunction with many other machine learning algorithms.  The framework provides confidence information based only on the standard iid assumption and so is much more robust to different underlying data distributions.  We show how the framework can be applied to existing algorithms.  We also present experimental results which show that the typicalness approach performs close to Bayes when the prior is known to be correct.  Unlike Bayes however, the method still gives accurate confidence values even when different data distributions are considered. 
Ridge Regression Learning Algorithm in Dual Variables| Abstract In this paper we study a dual version of the Ridge Regression procedure.  It allows us to perform non-linear regression by constructing a linear regression function in a high dimensional feature space.  The feature space representation can result in a large increase in the number of parameters used by the algorithm.  In order to combat this "curse of dimensionality", the algorithm allows the use of kernel functions, as used in Support Vector methods.  We also discuss a powerful family of kernel functions which is constructed using the ANOVA decomposition method from the kernel corresponding to splines with an infinite number of nodes.  This paper introduces a regression estimation algorithm which is a combination of these two elements: the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinitenode splines.  Experimental results are then presented (based on the Boston Housing data set) which indicate the performance of this algorithm relative to other algorithms. 
A Criterion for the Existence of Predictive Complexity for Binary Games| Abstract It is well known that there exists a universal (i. e. , optimal to within an additive constant if allowed to work infinitely long) algorithm for lossless data compression (Kolmogorov, Levin).  The game of lossless compression is an example of an on-line prediction game; for some other on-line prediction games (such as the simple prediction game) a universal algorithm is known not to exist.  In this paper we give an analytic characterization of those binary on-line prediction games for which a universal prediction algorithm exists. 
Probability Theory for the Brier Game| Abstract The usual theory of prediction with expert advice does not differentiate between good and bad "experts": its typical results only assert that it is possible to efficiently merge not too extensive pools of experts, no matter how good or how bad they are.  On the other hand, it is natural to expect that good experts' predictions will in some way agree with the actual outcomes (e. g. , they will be accurate on the average).  In this paper we show that, in the case of the Brier prediction game (also known as the square-loss game), the predictions of a good (in some weak and natural sense) expert must satisfy the law of large numbers (both strong and weak) and the law of the iterated logarithm; we also show that two good experts' predictions must be in asymptotic agreement.  To help the reader's intuition, we give a Kolmogorov-complexity interpretation of our results.  Finally, we briefly discuss possible extensions of our results to more general games; the limit theorems for sequences of events in conventional probability theory correspond to the log-loss game. 
Black--Scholes formula without stochastic assumptions| Abstract The usual stochastic assumptions of the Black--Scholes formula are replaced by assumptions about fractal dimensions describing the price paths of the underlying security and the square (the European option which pays the squared price of the underlying at the expiry).  In particular, it is sufficient to require that the price path of the underlying security should have a fractal dimension of at most 1 1 2 (which is the fractal dimension of typical stochastic processes, such as the price path of the underlying in the Black--Scholes model) and that the "remaining volatility path" implied by the square should have a fractal dimension of less than 1 1 2 (ie, it should be "substochastic").  For simplicity we assume zero interest rate. 
Competitive On-line Linear Regression| Abstract We apply a general algorithm for merging prediction strategies (the Aggregating Algorithm) to the problem of linear regression with the square loss; our main assumption is that the response variable is bounded.  It turns out that for this particular problem the Aggregating Algorithm resembles, but is slightly different from, the well-known ridge estimation procedure.  From general results about the Aggregating Algorithm we deduce a guaranteed bound on the difference between our algorithm's performance and the best, in some sense, linear regression function's performance.  We show that the AA attains the optimal constant in our bound, whereas the constant attained by the ridge regression procedure in general can be 4 times worse. 
Game-theoretic versions of Kolmogorov's strong law of large numbers| Abstract We prove two variants of Kolmogorov's strong law of large numbers in a completely worst-case framework, eschewing any probabilistic assumptions.  The first variant is an assertion about a game involving the Bookmaker predicting the values of unprobabilized random variables; in an intuitive sense it is much stronger than the usual strong law of large numbers for martingales.  The second variant is an assertion about a security market.  The final version of this paper was prepared for the seminar on the foundations of probability (Aalborg University, June 1995) organized by Steffen Lauritzen.  Predictive strong law of large numbers 1 1 Predictive strong law of large numbers In this section we consider the following perfect-information game between 3 players, the Bookmaker, the Statistician, and the Nature.  The game proceeds in trials.  At each trial i, i = 1; 2; : : :, the Bookmaker tries to predict the real number X i the Nature is to produce at the end of the trial.  His prediction consists of two numbers, E i and D i 0; roughly, E i is the Bookmaker's expectation of X i , and D i is his expectation of the accuracy L i := (X i \Gamma E i ) 2 (measured by the Brier scoring rule, see Dawid [1]) of the prediction E i .  Along the lines of Chapter 3 of de Finetti [2] we give an operative interpretation to the numbers E i and D i as follows.  Before X i is disclosed, the Bookmaker lets the Statistician buy any amount, positive or negative, of X i -tickets for $E i each and L i -tickets for $D i each.  An X i -ticket (resp.  L i -ticket) is a contract which obliges the Bookmaker to pay the Statistician $X i (resp.  $(X i \Gamma E i ) 2 ) after X i is disclosed.  We will use the notation e i and d i for the number of X i -tickets and L i -tickets, respectively, bought by the Statistician at trial i.  We describe the unfolding of the game, including the evolution of the Statistician's capital K i , as follows: FOR i = 1; 2; : : :: Bookmaker selects E i 2 IR and D i 0 Statistician selects e i ; d i 2 IR K i := K i\Gamma 1 \Gamma e i E i \Gamma d i D i (1) Nature selects X i 2 IR K i := K i + e i X i + d i (X i \Gamma E i ) 2 .  (2) Initially, the Statistician's capital is K 0 := 1.  We call the pair (e i ; d i ) the portfolio held by the Statistician at trial i.  Equation (1) shows how the Statistician's capital decreases when he buys the new portfolio and (2) describes the proceeds from holding the portfolio.  We can summarize (1) and (2) as follows: K i := K i\Gamma 1 + e i (X i \Gamma E i ) + d i i (X i \Gamma E i ) 2 \Gamma D i j : (3) An important difference between our framework and de Finetti's is that de Finetti assumes that the Bookmaker can directly price any ticket of interest, and we try to minimize the quantity of tickets priced by the Bookmaker.  In particular, for discussing Kolmogorov's strong law of large numbers it suffices to assume that the Bookmaker can price only the X i -tickets and L i -tickets.  To complete the description of our game, which will be denoted by G, it remains to specify the rule for determining who won the game given its path E 1 D 1 e 1 d 1 X 1 E 2 D 2 e 2 d 2 X 2 : : : : (4) We say that this path satisfies SLLN if lim n!1 1 n n X i=1 (X i \Gamma E i ) = 0: (5)
Density estimation using Support Vector Machines|
Losses, complexities and the Legendre transformation|
Prediction algorithms and confidence measures based on algorithmic randomness theory|
Computationally Efficient Transductive Machines|
Transductive confidence machine for pattern recognition|
Qualified Prediction for Large Data Sets in the Case of Pattern Recognition|
Probability theory for the Brier game| Accepted for publication in Theoretical Computer Science.  Preliminary version. 
Aggregating strategies| In M Fulk and J Case, editors,. 
Predicting nearly as well as the best pruning of a decision tree through dynamic programming scheme|
Statistical applications of algorithmic randomness|
Support vector density estimation,|
Density estimation using sv machines|
Ridge regression learning algorithm in dual variables|
On-line competitive linear regression|
Learning by transduction| In Cooper and Morla, eds,. 
Complexity estimation principle,|
Transductive confidence machines for pattern recognition|
Comparing the bayes and typicalness frameworks|
Comparing the Bayes and Typicalness Frameworks|
Comparing the Bayesian and typicalness frameworks|
Inductive Confidence Machine for pattern recognition|
Ridge Regression Confidence Machine|
A game of prediction with expert advice, accepted for publication|
Ridge regression in dual variables|
Advanced in Kernel methods: Support Vector Learning| Chap.  Support vector regression with ANOVA decomposition kernels. 
Ridge Regresson Confidence Machine|
A Game of Prediction with Experts Advice|
Competitive on-line linear regression, TR,|

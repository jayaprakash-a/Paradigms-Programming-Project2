Discriminative Speaker Adaptation with Conditional Maximum Likelihood Linear Regression| Abstract We present a simplified derivation of the extended Baum-Welch procedure, which shows that it can be used for Maximum Mutual Information (MMI) of a large class of continuous emission density hidden Markov models (HMMs).  We use the extended Baum-Welch procedure for discriminative estimation of MLLR-type speaker adaptation transformations.  The resulting adaptation procedure, termed Conditional Maximum Likelihood Linear Regression (CMLLR), is used successfully for supervised and unsupervised adaptation tasks on the Switchboard corpus, yielding an improvement over MLLR.  The interaction of unsupervised CMLLR with segmental minimum Bayes risk lattice voting procedures is also explored, showing that the two procedures are complimentary. 
Convergence of DLLR Rapid Speaker Adaptation Algorithms| Abstract Discounted Likelihood Linear Regression (DLLR) is a speaker adaptation technique for cases where there is insufficient data for MLLR adaptation.  Here, we provide an alternative derivation of DLLR by using a censored EM formulation which postulates additional adaptation data which is hidden.  This derivation shows that DLLR, if allowed to converge, provides maximum likelihood solutions.  Thus the robustness of DLLR to small amounts of data is obtained by slowing down the convergence of the algorithm and by allowing termination of the algorithm before overtraining occurs.  We then show that discounting the observed adaptation data by postulating additional hidden data can also be extended to MAP estimation of MLLR-type adaptation transformations. 
Modeling Systematic Variations in Pronunciation via a Language-Dependent Hidden Speaking Mode| ABSTRACT This paper describes the research efforts of the "Hidden Speaking Mode" group participating in the 1996 summer workshop on speech recognition.  The goal of this project is to model pronunciation variations that occur in conversational speech in general and, more specifically, to investigate the use of a hidden speaking mode to represent systematic variations that are correlated with the word sequence (e. g.  predictable from syntactic structure).  This paper describes the theoretical formulation of hidden mode modeling, as well as some results in error analysis, language modeling and pronunciation modeling. 
UNSUPERVISED SEMANTIC INTENT DISCOVERY FROM CALL LOG ACOUSTICS| ABSTRACT Unforeseen user intents can account for a significant portion of unsuccessful calls in an automatic voice response system.  Discovering these unforeseen semantic intents usually requires expensive manual transcriptions.  We propose a method to cluster the acoustics from logged calls by their estimated semantic intents.  This is achieved through training a mixture of language models in an unsupervised manner.  Each cluster is presented to the application developer with a suggested language model to cover the semantic intent of the data in that cluster.  The application developer validates the cluster and its suggested language model, and then updates the application.  A quantative evaluation on a corporate voice-dialer application shows that updating the application in this manner yields a relative 13. 4% reduction in semantic error rate. 
Convergence Theorems for Generalized Alternating Minimization Procedures| Abstract The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models.  In cases where the algorithms strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood.  In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework.  We study EM variants in which the E-Step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-Steps cannot be realized.  Since these variants are not EM procedures, the standard (G)EM convergence results do not apply to them.  We present an information geometric framework for describing such algorithms and analysing their convergence properties.  We apply this framework to analyse the convergence properties of incremental EM and variational EM.  For the former, we discuss conditions under these algorithms converge in likelihood.  For the latter, we characterize exactly the degree to which the E-Step approximation prevents convergence to local maxima in likelihood. 
DISCOUNTED LIKELIHOOD LINEAR REGRESSION FOR RAPID ADAPTATION| ABSTRACT Rapid adaptation schemes that employ the EM algorithm may suffer from overtraining problems when used with small amounts of adaptation data.  An algorithm to alleviate this problem is derived within the information geometric framework of Csiszar and Tusnady, and is used to improve MLLR adaptation on NAB and Switchboard adaptation tasks.  It is shown how this algorithm approximately optimizes a discounted likelihood criterion. 
Convergence of EM variants,|
Comments on "Efficient training algorithms for HMMs using incremental estimation",|
Information geometry and EM variants|
Word-based acoustic confidence measures for large-vocabulary speech recognition|
Robust estimation for rapid speaker adaptation using discounted likelihood techniques|
Maximum mutual information estimation of acoustic hmm emission densities,|
The JHU March 2001 Hub-5 conversational speech transcription system,"|
Convergence of EM variants,|
Adapting acoustin models to new domains and conditions using untranscribed data,"|
"Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode," in Research Report #24, Large Vocabulary Continuous Speech Recognition Workshop Technical Report Series|
Model systematic variations in pronunciation via a language-dependent hidden speaking mode|

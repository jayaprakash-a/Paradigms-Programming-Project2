Using a neural net to instantiate a deformable model| Abstract Deformable models are an attractive approach to recognizing nonrigid objects whichhave considerable within class variability. However, there are severe search problems associated with fitting the models to data.  We show that by using neural networks to provide better starting points, the search time can be significantly reduced.  The method is demonstrated on a character recognition task.  In previous work wehave developed an approach to handwritten character recognition based on the use of deformable models (Hinton, Williams and Revow, 1992a; Revow, Williams and Hinton, 1993).  Wehave obtained good performance with this method, but a major problem is that the search procedure for fitting each model to an image is very computationally intensive, because there is no efficient algorithm (like dynamic programming) for this task.  In this paper we demonstrate that it is possible to \compile down" some of the knowledge gained while fitting models to data to obtain better starting points that significantly reduce the search time.  1 DEFORMABLE MODELS FOR DIGIT RECOGNITION The basic idea in using deformable models for digit recognition is that each digit has a model, and a test image is classified by finding the model which is most likely to have generated it.  The quality of the matchbetween model and test image depends on the deformation of the model, the amount of ink that is attributed to noise and the distance of the remaining ink from the deformed model. 
What kind of a graphical model is the brain?| Abstract If neurons are treated as latent variables, our visual systems are non-linear, densely-connected graphical models containing billions of variables and thousands of billions of parameters.  Current algorithms would have difficulty learning a graphical model of this scale.  Starting with an algorithm that has difficulty learning more than a few thousand parameters, I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware.  The latest member of this series can learn deep, multi-layer belief nets quite rapidly.  It turns a generic network with three hidden layers and 1. 7 million connections into a very good generative model of handwritten digits.  After learning, the model gives classification performance that is comparable to the best discriminative methods. 
The Evolution of Learning: An Experiment in Genetic Connectionism| This paper explores how an evolutionary process can produce systems that learn.  A general framework for the evolution of learning is outlined, and is applied to the task of evolving mechanisms suitable for supervised learning in single-layer neural networks.  Dynamic properties of a network's information-processing capacity are encoded genetically, and these properties are subjected to selective pressure based on their success in producing adaptive behavior in diverse environments.  As a result of selection and genetic recombination, various successful learning mechanisms evolve, including the well-known delta rule.  The effect of environmental diversity on the evolution of learning is investigated, and the role of different kinds of emergent phenomena in genetic and
Stochastic Neighbor Embedding| Abstract We describe a probabilistic approach to the task of embedding highdimensional objects into a low-dimensional space in a way that preserves neighbor identities.  A Gaussian is centered on each object in the highdimensional space and the densities under this Gaussian are used to define a probability distribution over all the potential neighbors of the object.  The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed in the low-dimensional space.  The natural cost function is a sum of Kullback-Leibler divergences, one per object.  This leads to a simple gradient for adjusting the positions of objects in the low-dimensional space.  Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each high-dimensional object by a mixture of widely separated low-dimensional points.  This allows the embeddings of ambiguous objects, like the document count vector for the word "bank", to be close to the embeddings of both "river" and "finance" without forcing outdoor concepts to be located close to corporate ones. 
Using Pairs of Data-Points to Define Splits for Decision Trees| Abstract Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a computationally expensive search in the continuous space of completely unrestricted hyperplanes.  We show that the limitations of the former can be overcome without resorting to the latter.  For every pair of training data-points, there is one hyperplane that is orthogonal to the line joining the data-points and bisects this line.  In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed axis-aligned splits, particularly when the training sets are data limited. 
1| Multilayer networks of linear-Gaussian units. 
Scaling in a Hierarchical Unsupervised Network| Increasing the image size leads to faster and more reliable learning; (2) Increasing the depth of the network from one to two hidden layers leads to better representations at the first hidden layer, and (3) Once one part of the network has discovered how to represent disparity, it "supervises" other parts of the network, greatly speeding up their learning. 
A NEW VIEW OF ICA| ABSTRACT We present a new way of interpreting ICA as a probability density model and a new way of fitting this model to data.  The advantage of our approach is that it suggests simple, novel extensions to overcomplete, undercomplete and multilayer non-linear versions of ICA.  1.  ICA AS A CAUSAL GENERATIVE MODEL Factor analysis is based on a causal generative model in which an observation vector is generated in three stages.  First, the activities of the factors (also known as latent or hidden variables) are chosen independently from one dimensional Gaussian priors.  Next, these hidden activities are multiplied by a matrix of weights (the "factor loading" matrix) to produce a noise-free observation vector.  Finally, independent Gaussian "sensor noise" is added to each component of the noise-free observation vector.  Given an observation vector and a factor loading matrix, it is tractable to compute the posterior distribution of the hidden activities because this distribution is a Gaussian, though it generally has off-diagonal terms in the covariance matrix so it is not as simple as the prior distribution over hidden activities.  ICA can also be viewed as a causal generative model [1, 2] that differs from factor analysis in two ways.  First, the priors over the hidden activities remain independent but they are non-Gaussian.  By itself, this modification would make it intractable to compute the posterior distribution over hidden activities.  Tractability is restored by eliminating sensor noise and by using the same number of factors as input dimensions.  This ensures that the posterior distribution over hidden activities collapses to a point.  Interpreting ICA as a type of causal generative model suggests a number of ways in which it might be generalized, for instance to deal with more hidden units than input dimensions.  Most of these generalizations retain marginal independence of the hidden activities and add sensor noise, but fail to preserve the property that the posterior distribution collapses to a point.  As # Funded by the Wellcome Trust and the Gatsby Charitable Foundation.  a result inference is intractable and crude approximations are needed to model the posterior distribution, e. g. , a MAP estimate in [3], a Laplace approximation in [4, 5] or more sophisticated variational approximations in [6]. 
Relative Density Nets: A New Way to Combine Backpropagation with HMM's| Abstract Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians.  This leads us to consider substituting other density models.  We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's.  Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 
Instantiating deformable models with a neural net| Abstract Deformable models are an attractive approach to recognizing objects which have considerable within-class variability such as handwritten characters.  However, there are severe search problems associated with fitting the models to data which could be reduced if a better starting point for the search were available.  We show that by training a neural network to predict how a deformable model should be instantiated from an input image, such improved starting points can be obtained.  This method has been implemented for a system that recognizes handwritten digits using deformable models, and the results show that the search time can be significantly reduced without compromising recognition performance. 
Products of Experts| Abstract It is possible to combine multiple probabilistic models of the same data by multiplying the probabilities together and then renormalizing.  This is a very ecient way to model high-dimensional data which simultaneously satisfies many different lowdimensional constraints.  Each individual expert model can focus on giving high probability to data vectors that satisfy just one of the constraints.  Data vectors that satisfy this one constraint but violate other constraints will be ruled out by their low probability under the other expert models.  Training a product of models appears dicult because, in addition to maximizing the probabilities that the individual models assign to the observed data, it is necessary to make the models disagree on unobserved regions of the data space: It is fine for one model to assign a high probability to an unobserved region as long as some other model assigns it a very low probability.  Fortunately, if the individual models are tractable there is a fairly ecient way to train a product of models.  This training algorithm suggests a biologically plausible way of learning neural population codes. 
Hierarchical Non-linear Factor Analysis and Topographic Maps| Abstract We first describe a hierarchical, generative model that can be viewed as a non-linear generalization of factor analysis and can be implemented in a neural network.  The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections.  These connections can be learned using simple rules that require only locally available information.  We then show how to incorporate non-adaptive lateral connections into the generative model.  The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localized disparity detectors in the first hidden layer form a topographic map. 
The EM Algorithm for Mixtures of Factor Analyzers| Abstract Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing different local factor models in different regions of the input space.  This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians.  We present an exact Expectation--Maximization algorithm for fitting the parameters of this mixture of factor analyzers. 
Optimal Smoothing in Visual Motion Perception| When a ash is aligned with a moving object, subjects perceive the ash to lag behind the moving object.  Two different models have been proposed to explain this " ash-lag" effect.  In the motion extrapolation model, the visual system extrapolates the location of the moving object to counteract neural propagation delays , whereas in the latency difference model, it is hypothesized that moving objects are processed and perceived more quickly than ashed objects.  However, recent psychophysical experiments suggest that neither of these interpretations is feasible (Eagleman & Sejnowski, 2000a , 2000b , 2000c), hypothesizing instead that the visual system uses data from the future of an event before committing to an interpretation .  We formalize this idea in terms of the statistical framework of optimal smoothing and show that a model based on smoothing accounts for the shape of psychometric curves from a ash-lag experiment involving random reversals of motion direction.  The smoothing model demonstrates how the visual system may enhance perceptual accuracy by relying not only on data from the past but also on data collected from the immediate future of an event. 
A New Learning Algorithm for Mean Field Boltzmann Machines| Abstract.  We present a new learning algorithm for Mean Field Boltzmann Machines based on the contrastive divergence optimization criterion.  In addition to minimizing the divergence between the data distribution and the equilibrium distribution, we maximize the divergence between one-step reconstructions of the data and the equilibrium distribution.  This eliminates the need to estimate equilibrium statistics, so we do not need to approximate the multimodal probability distribution of the free network with the unimodal mean field distribution.  We test the learning algorithm on the classification of digits. 
Learning Causally Linked Markov Random Fields| Abstract We describe a learning procedure for a generative model that contains a hidden Markov Random Field (MRF) which has directed connections to the observable variables.  The learning procedure uses a variational approximation for the posterior distribution over the hidden variables.  Despite the intractable partition function of the MRF, the weights on the directed connections and the variational approximation itself can be learned by maximizing a lower bound on the log probability of the observed data.  The parameters of the MRF are learned by using the mean field version of contrastive divergence [1].  We show that this hybrid model simultaneously learns parts of objects and their inter-relationships from intensity images.  We discuss the extension to multiple MRF's linked into in a chain graph by directed connections. 
Energy-Based Models for Sparse Overcomplete Representations| Abstract We present a new way of extending independent components analysis (ICA) to overcomplete representations.  In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs.  This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs.  By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution.  Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002).  When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data.  In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces. 
A simple algorithm that discovers efficient perceptual codes| Abstract We describe the "wake-sleep" algorithm that allows a multilayer, unsupervised, neural network to build a hierarchy of representations of sensory input.  The network has bottom-up "recognition" connections that are used to convert sensory input into underlying representations.  Unlike most artificial neural networks, it also has top-down "generative" connections that can be used to reconstruct the sensory input from the representations.  In the "wake" phase of the learning algorithm, the network is driven by the bottom-up recognition connections and the top-down generative connections are trained to be better at reconstructing the sensory input from the representation chosen by the recognition process.  In the "sleep" phase, the network is driven top-down by the generative connections to produce a fantasized representation and a fantasized sensory input.  The recognition connections are then trained to be better at recovering the fantasized representation from the fantasized sensory input.  In both phases, the synaptic learning rule is simple and local.  The combined effect of the two phases is to create representations of the sensory input that are efficient in the following sense: On average, it takes more bits to describe each sensory input vector directly than to first describe the representation of the sensory input chosen by the recognition process and then describe the difference between the sensory input and its reconstruction from the chosen representation. 
Varieties of Helmholtz Machine| Abstract The Helmholtz machine is a new unsupervised learning architecture that uses topdown and bottom up connections to build probability density models of input and inverses to those models.  The wake-sleep learning algorithm for Helmholtz machines involves just the purely local delta rule.  This paper suggests a number of different varieties of Helmholtz machines, each with its own strengths and weaknesses, and draws conclusions from the hypothesis that cortical information processing can be viewed in their terms. 
Using Fast Weights to Deblur Old Memories| Abstract Connectionist models usually have a single weight on each connection.  Some interesting new properties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero.  If a network learns a set of associations and then these associations are "blurred" by subsequent learning, all the original associations can be "deblurred" by rehearsing on just a few of them.  The rehearsal allows the fast weights to take on values that temporarily cancel out the changes in the slow weights caused by the subsequent learning. 
Learning Hierarchical Structures with Linear Relational Embedding| Abstract We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts.  Its final goal is to be able to generalize, i. e.  infer new instances of these relations among the concepts.  On a task involving family relationships we show that LRE can generalize better than any previously published method.  We then show how LRE can be used effectively to find compact distributed representations for variable-sized recursive data structures, such as trees and lists.  1 Linear Relational Embedding Our aim is to take a large set of facts about a domain expressed as tuples of arbitrary symbols in a simple and rigid syntactic format and to be able to infer other "common-sense" facts without having any prior knowledge about the domain.  Let us imagine a situation in which we have a set of concepts and a set of relations among these concepts, and that our data consists of few instances of these relations that hold among the concepts.  We want to be able to infer other instances of these relations.  For example, if the concepts are the people in a certain family, the relations are kinship relations, and we are given the facts "Alberto has-father Pietro" and "Pietro has-brother Giovanni", we would like to be able to infer "Alberto has-uncle Giovanni".  Our approach is to learn appropriate distributed representations of the entities in the data, and then exploit the generalization properties of the distributed representations [2] to make the inferences.  In this paper we present a method, which we have called Linear Relational Embedding (LRE), which learns a distributed representation for the concepts by embedding them in a space where the relations between concepts are linear transformations of their distributed representations.  Let us consider the case in which all the relations are binary, i. e.  involve two concepts.  In this case our data consists of triplets (concept 1 ; relation; concept 2 ), and the problem we are trying to solve is to infer missing triplets when we are given only few of them.  Inferring a triplet is equivalent to being able to complete it, that is to come up with one of its elements, given the other two.  Here we shall always try to complete the third element of the triplets 1 .  LRE will then represent each concept in the data as a learned vector in a 1 Methods analogous to the ones presented here that can be used to complete any element of a triplet can be found in [4].  Euclidean space and each relationship between the two concepts as a learned matrix that maps the first concept into an approximation to the second concept.  Let us assume that our data consists of C such triplets containing N distinct concepts and M binary relations.  We shall call this set of triplets C; V = fv 1 ;:::;v N g will denote the set of n-dimensional vectors corresponding to the N concepts, and R = fR 1 ;:::;R M g the set of (n # n) matrices corresponding to the M relations.  Often we shall need to indicate the vectors and the matrix which correspond to the concepts and the relation in a certain triplet c. In this case we shall denote the vector corresponding to the first concept with a, the vector corresponding to the second concept with b and the matrix corresponding to the relation with R.  We shall therefore write the triplet c as (a c ;R c ; b c ) where a c ; b c 2VandR c 2R.  The operation that relates a pair (a c ;R c ) to a vector b c is the matrix-vector multiplication, R c # a c , which produces an approximation to b c .  If for every triplet (a c ;R c ; b c ) we think of R c # a c as a noisy version of one of the concept vectors, then one way to learn an embedding is to maximize the probability that it is a noisy version of the correct completion, b c . We imagine that a concept has an average location in the space, but that each "observation" of the concept is a noisy realization of this average location.  Assuming spherical Gaussian noise with a variance of 1=2 on each dimension, the discriminative goodness function that corresponds to the log probability of getting the right completion, summed over all training triplets is: D = C X c=1 1 k c log e 1 R c #a c b c jj 2 X v i 2V e32 R c #a c v i jj 2 (1) where k c is the number of triplets in C having the first two terms equal to the ones of c,but differing in the third term 2 .  Learning based on maximizing D with respect to all the vector and matrix components has given good results, and has proved successful in generalization as well [5].  However, when we learn an embedding by maximizing D, we are not making use of exactly the information that we have in the triplets.  For each triplet c, we are making the vector representing the correct completion b c more probable than any other concept vector given R c # a c , while the triplet states that R c # a c must be equal to b c .  The numerator of D does exactly this, but we also have the denominator, which is necessary in order to stay away from the trivial 0 solution 3 .  We noticed however that the denominator is critical at the beginning of the learning, but as the vectors and matrices differentiate we could gradually lift this burden, allowing P C c=1 kR c # a c b c k 2 to become the real goal of the learning.  To do this we modify the discriminative function to include a parameter #, which is annealed from 1 to 0 during learning 4 : G = C X c=1 1 k c log e3 R c #a c b c k 2 [ X v i 2V e3 R c #a c v i k 2 ] # (2) 2 We would like our system to assign equal probability to each of the correct completions.  The discrete probability distribution that we want to approximate is therefore: Px = 1 d P d i=1 (b i x) where is the discrete delta function and x ranges over the vectors in V .  Our system implements the discrete probability distribution: Qx = 1 Z exp( R # a xk 2 ) where Z is the normalization factor.  The 1=kc factor in eq. 1 ensures that we are minimizing the Kullback-Leibler divergence between P and Q.  3 The obvious approach to find an embedding would be to minimize the sum of squared distances between R c # a c and b c over all the triplets, with respect to all the vector and matrix components.  Unfortunately this minimization (almost) always causes all of the vectors and matrices to collapse to the trivial 0 solution.  4 For one-to-many relations we must not decrease the value of # all the way to 0, because this would cause some concept vectors to become coincident.  This is because the only way to make R c # a c equal to kc different vectors, is by collapsing them onto a unique vector. 
Making stochastic source coding efficient by recovering information| Abstract In this paper, we introduce a new algorithm called "bits-back coding" that makes stochastic source codes efficient.  For a given one-to-many source code, we show that this algorithm can actually be more efficient than the algorithm that always picks the shortest codeword.  Optimal efficiency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths.  After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution can be used for bits-back coding.  It turns out that a commonly used technique for determining parameters --- maximum likelihood estimation --- actually minimizes the optimal bitsback coding cost.  A tractable approximation to maximum likelihood estimation --incremental expectation maximization --- minimizes the bits-back coding cost as well.  We illustrate the performance of bits-back coding first on a toy problem and then using real data with a binary Bayesian network that produces 2 60 possible codewords for each symbol.  For both tasks, the rate for bits-back coding is nearly one half of that obtained by picking the shortest codeword for each symbol. 
Efficient Stochastic Source Coding and an Application to a Bayesian Network Source Model| In this paper, we introduce a new algorithm called "bits-back coding" that makes stochastic source codes efficient.  For a given one-to-many source code, we show that this algorithm can actually be more efficient than the algorithm that always picks the shortest codeword.  Optimal efficiency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths.  It turns out that a commonly used technique for determining parameters --- maximum likelihood estimation --- actually minimizes the bits-back coding cost when codewords are chosen according to the Boltzmann distribution.  A tractable approximation to maximum likelihood estimation --- the generalized expectation maximization algorithm --- minimizes the bits-back coding cost.  After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution can be used for bits-back coding.  We illustrate the performance of bits-back coding using using nonsynthetic data with a binary Bayesian network source model that produces 2 60 possible codewords for each input symbol.  The rate for bits-back coding is nearly one half of that obtained by picking the shortest codeword for each symbol. 
Using a neural net to instantiate a deformable model| Abstract Deformable models are an attractive approach to recognizing nonrigid objects which have considerable within class variability.  However, there are severe search problems associated with fitting the models to data.  We show that by using neural networks to provide better starting points, the search time can be significantly reduced.  The method is demonstrated on a character recognition task.  In previous work we have developed an approach to handwritten character recognition based on the use of deformable models (Hinton, Williams and Revow, 1992a; Revow, Williams and Hinton, 1993).  We have obtained good performance with this method, but a major problem is that the search procedure for fitting each model to an image is very computationally intensive, because there is no efficient algorithm (like dynamic programming) for this task.  In this paper we demonstrate that it is possible to "compile down" some of the knowledge gained while fitting models to data to obtain better starting points that significantly reduce the search time.  1 DEFORMABLE MODELS FOR DIGIT RECOGNITION The basic idea in using deformable models for digit recognition is that each digit has a model, and a test image is classified by finding the model which is most likely to have generated it.  The quality of the match between model and test image depends on the deformation of the model, the amount of ink that is attributed to noise and the distance of the remaining ink from the deformed model. 
Understanding Normal and Impaired Word Reading: Computational Principles in Quasi-Regular Domains| Abstract We develop a connectionist approach to processing in quasi-regular domains, as exemplified by English word reading.  A consideration of the shortcomings of a previous implementation (Seidenberg & McClelland,
Learning Sparse Topographic Representations with Products of Student-t Distributions| Abstract We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs.  We encourage the system to find sparse features by using a Studentt distribution to model each filter output.  If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map.  Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters.  Once the model has been learned it can be used as a prior to derive the "iterated Wiener filter" for the purpose of denoising images. 
Does the Wake-sleep Algorithm Produce Good Density Estimators?| Abstract The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a relatively efficient method of fitting a multilayer stochastic generative model to high-dimensional data.  In addition to the top-down connections in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule.  We use a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit. 
Learning to Parse Images| Abstract We describe a class of probabilistic models that we call credibility networks.  Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recognition simultaneously, removing the need for ad hoc segmentation heuristics.  Promising results in the problem of segmenting handwritten digits were obtained. 
Developing Population Codes by Minimizing Description Length| Abstract The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately.  We show how MDL can be used to develop highly redundant population codes.  Each hidden unit has a location in a low-dimensional implicit space.  If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center of this bump.  So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump.  The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes.  Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs.  Most existing unsupervised learning algorithms can be understood using the Minimum Description Length (MDL) principle (Rissanen, 1989).  Given an ensemble of input vectors, the aim of the learning algorithm is to find a method of coding each input vector that minimizes the total cost, in bits, of communicating the input vectors to a receiver.  There are three terms in the total description length: ffl The code-cost is the number of bits required to communicate the code that the algorithm assigns to each input vector.  ffl The model-cost is the number of bits required to specify how to reconstruct input vectors from codes (e. g. , the hidden-to-output weights).  ffl The reconstruction-error is the number of bits required to fix up any errors that occur when the input vector is reconstructed from its code.  Formulating the problem in terms of a communication model allows us to derive an objective function for a network (note that we are not actually sending the bits).  For example, in competitive learning (vector quantization), the code is the identity of the winning hidden unit, so by limiting the system to H units we limit the average code-cost to at most log 2 H bits.  The reconstruction-error is proportional to the squared difference between the input vector and the weight-vector of the winner, and this is what competitive learning algorithms minimize.  The model-cost is usually ignored.  The representations produced by vector quantization contain very little information about the input (at most log 2 H bits).  To get richer representations we must allow many hidden units to be active at once and to have varying activity levels.  Principal components analysis (PCA) achieves this for linear mappings from inputs to codes.  It can be viewed as a version of MDL in which we limit the code-cost by only having a few hidden units, and ignoring the model-cost and the accuracy with which the hidden activities must be coded.  An autoencoder (see Figure 2) that tries to reconstruct the input vector on its output units will perform a version of PCA if the output units are linear.  We can obtain novel and interesting unsupervised learning algorithms using this MDL approach by considering various alternative methods of communicating the hidden activities.  The algorithms can all be implemented by backpropagating the derivative of the code-cost for the hidden units in addition to the derivative of the reconstruction-error backpropagated from the output units.  Any method that communicates each hidden activity separately and independently will tend to lead to factorial codes because any mutual information between hidden units will cause redundancy in the communicated message, so the pressure to keep the message short will squeeze out the redundancy.  In (Zemel, 1993) and (Hinton and Zemel, 1994), we present algorithms derived from this MDL approach aimed at developing factorial codes.  Although factorial codes are interesting, they are not robust against hardware failure nor do they resemble the population codes found in some parts of the brain.  Our aim in this paper is to show how the MDL approach can be used to develop population codes in which the activities of hidden units are highly correlated.  For a more complete discussion of the details of this algorithm, see (Zemel, 1993).  Unsupervised algorithms contain an implicit assumption about the nature of the structure or constraints underlying the input set.  For example, competitive learning algorithms are suited to datasets in which each input can be attributed to one of a set of possible causes.  In the algorithm we present here, we assume that each input can be described as a point in a low-dimensional continuous constraint space.  For instance, a complex shape may require a detailed representation, but a set of images of that shape from multiple viewpoints can be concisely represented by first describing the shape, and then encoding each instance as a point in the constraint space spanned by the viewing parameters.  Our goal is to find and represent the constraint space underlying high-dimensional data samples. 
Generative Models for Discovering Sparse Distributed Representations| Abstract We describe a hierarchical, generative model that can be viewed as a non-linear generalization of factor analysis and can be implemented in a neural network.  The model uses bottom-up, top-down and lateral connections to perform Bayesian perceptual inference correctly.  Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information.  We demonstrate that the network learns to extract sparse, distributed, hierarchical representations. 
Recognizing Handwritten Digits Using Hierarchical Products of Experts| [1] describes a learning algorithm for probabilistic generative models that are composed of a number of experts.  Each expert specifies a probability distribution over the visible variables and the experts are combined by multiplying these distributions together and renormalizing. 
Autoencoders, Minimum Description Length and Helmholtz Free Energy| Abstract An autoencoder network uses a set of recognition weights to convert an input vector into a code vector.  It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector.  We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle.  The aim is to minimize the information required to describe both the code vector and the reconstruction error.  We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector.  Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors.  We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length.  Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights.  We demonstrate that this approach can be used to learn factorial codes. 
Using EM for Reinforcement Learning| Abstract We discsus Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent.  We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters.  The proof is based on a mapping between the RPP and a form of the expectation-maximisation procedure of Dempster, Laird & Rubin (1976). 
Global Coordination of Local Linear Models| Abstract High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. 
Modelling the Manifolds of Images of Handwritten Digits| Abstract This paper describes two new methods for modelling the manifolds of digitised images of handwritten digits.  The models allow a priori information about the structure of the manifolds to be combined with empirical data.  Accurate modelling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models.  One of the methods is grounded in principal components analysis, the other in factor analysis.  Both methods are based on locally linear, low-dimensional approximations to the underlying data manifold.  Links with other methods that model the manifold are discussed. 
Free Energy Coding| Abstract In this paper, we introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol.  It may seem that the most sensible codeword to use in this case is the shortest one.  However, in the proposed free energy approach, random codeword selection yields an effective codeword length that can be less than the shortest codeword length.  If the random choices are Boltzmann distributed, the effective length is optimal for the given source code.  The expectation-maximization parameter estimation algorithms minimize this effective codeword length.  We illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method. 
Probabilistic Sequential Independent Components Analysis| Abstract Under-complete models, which derive lower-dimensional representations of input data, are valuable in domains in which the number of input dimensions is very large, such as data consisting of a temporal sequence of images.  We present the "under-complete product of experts" (UPoE), where each expert models a one dimensional projection of the data.  Maximum-likelihood learning rules for this model constitute a tractable and exact algorithm for learning under-complete independent components.  The learning rules for this model coincide with approximate learning rules proposed earlier for under-complete ICA models.  We also derive an efficient sequential learning algorithm from this model, and discuss its relationship to sequential ICA, projection pursuit density estimation, and feature induction algorithms for additive random field models.  We demonstrate the efficacy of these novel algorithms on high-dimensional continuous datasets. 
Glove-TalkII: A neural network interface which maps gestures to parallel formant speech synthesizer controls| Abstract Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface.  Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer.  The mapping allows the hand to act as an artificial vocal tract that produces speech in real time.  This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume.  Currently, the best version of GloveTalkII uses several input devices (including a Cyberglove, a ContactGlove, a 3-space tracker, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks.  The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network.  The gating network and the consonant network are trained with examples from the user.  The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training examples from the user.  Volume, fundamental frequency and stop consonants are produced with a fixed mapping from the input devices.  One subject has trained to speak intelligibly with Glove-TalkII.  He speaks slowly but with far more naturalsounding pitch variations than a text-to-speech synthesizer. 
Approximate Contrastive Free Energies for Learning in Undirected Graphical Models| Abstract We present a novel class of learning algorithms for undirected graphical models, based on the contrastive free energy (CF).  In particular we study the naive mean field, TAP and Bethe approximations to the contrastive free energy.  The main advantage of CFlearning is the fact that it eliminates the need to infer equilibrium statistics for which mean field type approximations are particularly unsuitable.  Instead, learning decreases the distance between the data distribution and the distribution with onestep reconstructions clamped on the visible nodes.  We test the learning algorithm on the classification of digits. 
Minimizing Description Length in an Unsupervised Neural Network| Abstract An autoencoder network uses a set of recognition weights to convert an input vector into a representation vector.  It then uses a set of generative weights to convert the representation vector into an approximate reconstruction of the input vector.  We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle.  The aim is to minimize the information required to describe both the representation vector and the reconstruction error.  This information is minimized by choosing representation vectors stochastically according to a Boltzmann distribution.  Unfortunately, if the representation vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible representation vectors.  We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution.  This approximation corresponds to using a suboptimal encoding scheme and therefore gives an upper bound on the minimal description length.  Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights.  We demonstrate that this approach can be used to learn distributed representations in which many different hidden causes combine to produce each observed data vector.  Such representations can be exponentially more efficient in their use of hardware than standard vector quantization or mixture models. 
Using generative models for handwritten digit| Abstract We describe a method of recognizing handwritten digits by fitting generative models that are built from deformable B-splines with Gaussian "ink generators" spaced along the length of the spline.  The splines are adjusted using a novel elastic matching procedure based on the Expectation Maximization (EM) algorithm that maximizes the likelihood of the model generating the data.  This approach has many advantages.  (1) After identifying the model most likely to have generated the data, the system not only produces a classification of the digit but also a rich description of the instantiation parameters which can yield information such as the writing style.  (2) During the process of explaining the image, generative models can perform recognition driven segmentation.  (3) The method involves a relatively small number of parameters and hence training is relatively easy and fast.  (4) Unlike many other recognition schemes it does not rely on some form of pre-normalization of input images, but can handle arbitrary scalings, translations and a limited degree of image rotation.  We have demonstrated our method of fitting models to images does not get trapped in poor local minima.  The main disadvantage of the method is it requires much more computation than more standard OCR techniques. 
The wake-sleep algorithm for unsupervised neural networks| Abstract We describe an unsupervised learning algorithm for a multilayer network of stochastic neurons.  Bottom-up "recognition" connections convert the input into representations in successive hidden layers and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above.  In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below.  In the "sleep" phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.  Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections.  The wake-sleep algorithm finesses both these problems.  When there is no teaching signal to be matched, some other goal is required to force the hidden units to extract underlying structure.  In the wake-sleep algorithm the goal is to learn representations that are economical to describe but allow the input to be reconstructed accurately.  Each input vector could be communicated to a receiver by first sending its hidden representation and then sending the difference between the input vector and its top-down reconstruction from the hidden representation.  The aim of learning is to minimize the "description length" which is the total number of bits that would be required to communicate the input vectors in this way [1].  No communication actually takes place, but minimizing the description length that would be required forces the network to learn economical representations that capture the underlying regularities in the data [2].  The neural network has two quite different sets of connections.  The bottom-up "recognition" connections are used to convert the input vector into a representation in one or more layers of hidden units.  The top-down "generative" connections are then used to reconstruct an approximation to the input vector from its underlying representation.  The training algorithm for these two sets of connections can be used with many different types of stochastic neuron, but for simplicity we use only stochastic binary units that have states of or # .  The state of unit # is ### and the probability that it is on is: ### # ### fiff # 25510 - ##### # ##### # ##### # # (1) where # # is the bias of the unit and # # # is the weight on a connection from unit # .  Sometimes the units are driven by the generative weights and other times by the recognition weights, but the same equation is used in both cases.  In the "wake" phase the units are driven bottom-up using the recognition weights, producing a representation of the input vector in the first hidden layer, a representation of this representation in the second hidden layer and so on.  All of these layers of representation combined are called the "total representation" of the input, and the binary state of each hidden unit, , in total representation ! is fiff" # .  This total representation could be used to communicate the input vector, $ , to a receiver.  According to Shannon's coding theorem, it requires #&%('*),+ bits to communicate an event that has probability + under a distribution agreed by the sender and receiver.  We assume that the receiver knows the top-down generative weights [3] so these can be used to create the agreed probability distributions required for communication.  First, the activity of each unit, - , in the top hidden layer is communicated using the distribution #. # "/10 ### "/ # which is obtained by applying Eq.  1 to the single generative bias weight of unit - .  Then the activities of the units in each lower layer are communicated using the distribution #(# " # 0 #2# " # # obtained by applying Eq.  1 to the already communicated activities in the layer above, # " / , and the generative weights, # / # .  The description length of the binary state of unit is: 3 # # " # # # # # " # %('*)4# " # #5# # # " # # %('*)6# #7# " # # (2) The description length for input vector $ using the total representation ! is simply the cost of describing all the hidden states in all the hidden layers plus the cost of describing the input vector given the hidden states 3 # ! 0 $ # # 3 # ! #8# 3 # $#9 ! # #;:<}=@?A: # =B< 3 # # " # #8# :DC 3 # #DE C 9F! # (3)
Feudal Reinforcement Learning| Abstract One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time.  This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them.  Sub-managers need not initially understand their managers' commands.  They simply learn to maximise their reinforcement in the context of the current command.  We illustrate the system using a simple maze task. .  As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map. 
Parameter Estimation for Linear Dynamical Systems| Abstract Linear systems have been used extensively in engineering to model and control the behavior of dynamical systems.  In this note, we present the Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (Shumway and Stoffer, 1982).  We also point out the relationship between linear dynamical systems, factor analysis, and hidden Markov models. 
A Bayesian Unsupervised Learning Algorithm that Scales| Abstract A persistent worry with computational models of unsupervised learning is that learning will become more difficult as the
Adaptive Elastic Models for Hand-Printed Character Recognition| Abstract Hand-printed digits can be modeled as splines that are governed by about 8 control points.  For each known digit, the control points have preferred "home" locations, and deformations of the digit are generated by moving the control points away from their home locations.  Images of digits can be produced by placing Gaussian ink generators uniformly along the spline.  Real images can be recognized by finding the digit model most likely to have generated the data.  For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image.  The model with the lowest total energy wins.  If a uniform noise process is included in the model of image generation, some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image.  The digit models learn by modifying the home locations of the control points. 
Learning Population Codes by Minimizing Description Length| Abstract The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately.  We show how MDL can be used to develop highly redundant population codes.  Each hidden unit has a location in a low-dimensional implicit space.  If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center of this bump.  So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump.  The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes. 
Glove-TalkII: Mapping Hand Gestures to Speech Using Neural Networks| Abstract Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface.  Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer.  The mapping allows the hand to act as an artificial vocal tract that produces speech in real time.  This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume.  Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a 3-space tracker, a keyboard and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks.  The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network.  The gating network and the consonant network are trained with examples from the user.  The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training examples from the user.  Volume, fundamental frequency and stop consonants are produced with a fixed mapping from the input devices.  One subject has trained to speak intelligibly with Glove-TalkII.  He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations. 
Glove-Talk: A neural network interface between a data-glove and a speech synthesizer| Abstract To illustrate the potential of multilayer neural networks for adaptive interfaces, we used a VPL DataGlove connected to a DECtalk speech synthesizer via five neural networks to implement a hand-gesture to speech system.  Using minor variations of the standard back-propagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single "speaker" in a simple training phase.  With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1% of the time, and no word is produced about 5% of the time.  Adaptive control of the speaking rate and word stress is also available.  The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask.  The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user. 
In Network: Computation in Neural Systems| Abstract We describe a method of incrementally constructing a hierarchical generative model of an ensemble of binary data vectors.  The model is composed of stochastic, binary, logistic units.  Hidden units are added to the model one at a time with the goal of minimizing the information required to describe the data vectors using the model.  In addition to the top-down generative weights that define the model, there are bottom-up recognition weights that determine the binary states of the hidden units given a data vector.  Even though the stochastic generative model can produce each data vector in manyways, the recognition model is forced to pick just one of these ways.  The recognition model therefore underestimates the ability of the generative model to predict the data, but this underestimation greatly simplifies the process of searching for the generative and recognition weights of a new hidden unit. 
NeuroAnimator: Fast Neural Network Emulation and Control of Physics-based Models| Abstract: Animation through the numerical simulation of physicsbased graphics models offers unsurpassed realism, but it can be computationally demanding.  Likewise, the search for controllers that enable physics-based models to produce desired animations usually entails formidable computational cost.  This paper demonstrates the possibility of replacing the numerical simulation and control of dynamic models with a dramatically more efficient alternative.  In particular, we propose the NeuroAnimator, a novel approach to creating physically realistic animation that exploits neural networks.  NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physicsbased models in action.  Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation.  Furthermore, by exploiting the network structure of the NeuroAnimator, we introduce a fast algorithm for learning controllers that enables either physics-based models or their neural network emulators to synthesize motions satisfying prescribed animation goals.  We demonstrate NeuroAnimators for a variety of physics-based models. 
Multiple Relational Embedding| Abstract We introduce a way to learn a low-dimensional embedding of data that is constrained by knowledge in the form of multiple different types of similarity relations.  Our method finds different, possibly overlapping, representations by individually reweighting the dimensions of a common underlying latent space.  When applied to a single similarity relation that is based on Euclidean distances between the input data points, the method reduces to simple dimensionality reduction.  In the case that additional information is available about the data set or about subsets of it, it allows us to use this information to clean up or otherwise improve the embedding.  We demonstrate the potential usefulness of this form of semi-supervised dimensionality reduction on a few simple examples. 
``Coaching" variables for regression and classification| Abstract In a regression or classification setting where we wish to predict Y from x1 ; x2 ; .  .  .  xp , we suppose that an additional set of "coaching" variables z1 ; z2 ; .  .  .  zm are available in our training sample.  These might be variables that are difficult to measure, and they will not be available when we predict Y from x1 ; x2 ; .  .  .  xp in the future.  We consider two methods of making use of the coaching variables in order to improve the prediction of Y from x1 ; x2 ; .  .  .  xp .  The relative merits of these approaches are discussed and compared in a number of examples. 
Learning Algorithms for Networks with Internal and External Feedback| the inputs have to remain stationary are considered to be static algorithms, although the settling process is a dynamic one based on internal feedback.  If a given network type can be employed for dynamic problems, and if there exists a corresponding learning algorithm, then we sometimes speak of a dynamic network.  The credit assignment problem.  If a neural network is supposed to learn externally posed tasks then it faces Minsky's fundamental credit assignment problem: If performance is not sufficient, then which component of the network at which time did in which way contribute to the failure? How should critical components change behavior to increase future performance?
Self Supervised Boosting| Abstract Boosting algorithms and successful applications thereof abound for classification and regression learning problems, but not for unsupervised learning.  We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of "non-data" generated from the model's current estimate of the data density.  Training in each boosting round proceeds in three stages: first we sample non-data from the model's current Boltzmann distribution.  Next, a feature is trained to improve classification performance between data and non-data.  Finally, a coefficient is learned which determines the importance of this feature relative to ones already in the pool.  Non-data only needs to be generated once to learn each new feature.  The validity of the approach is demonstrated on binary digits and continuous synthetic data. 
The Microarchitecture of the Pentium| ABSTRACT This paper describes the Intel NetBurst^TM microarchitecture of Intel's new flagship Pentium 4 processor.  This microarchitecture is the basis of a new family of processors from Intel starting with the Pentium 4 processor.  The Pentium 4 processor provides a substantial performance gain for many key application areas where the end user can truly appreciate the difference.  In this paper we describe the main features and functions of the NetBurst microarchitecture.  We present the frontend of the machine, including its new form of instruction cache called the Execution Trace Cache.  We also describe the out-of-order execution engine, including the extremely low latency double-pumped Arithmetic Logic Unit (ALU) that runs at 3GHz.  We also discuss the memory subsystem, including the very low latency Level 1 data cache that is accessed in just two clock cycles.  We then touch on some of the key features that allow the Pentium 4 processor to have outstanding floating-point and multi-media performance.  We provide some key performance numbers for this processor, comparing it to the Pentium III processor.  INTRODU TION The Pent um 4 processor is Intel's new flagship microprocessor that was introduced at 1. 5GHz in November of 2000.  It implements the new Intel NetBurst microarchitecture that features significantly higher clock rates and world-class performance.  It includes several important new features and innovations that will allow the Intel Pentium 4 processor to deliver industry-leading performance for the next several years.  This paper provides an in-depth examination of the features and functions of the Intel NetBurst microarchitecture.  The Pentium 4 processor is designed to deliver performance across applications where end users can truly appreciate and experience its performance.  For example, it allows a much better user experience in areas such as Internet audio and streaming video, image processing, video content creation, speech recognition, 3D applications and games, multi-media, and multi-tasking user environments.  The Pentium 4 processor enables realtime MPEG2 video encoding and near real-time MPEG4 encoding, allowing efficient video editing and video conferencing.  It delivers world-class performance on 3D applications and games, such as Quake 3 # , enabling a new level of realism and visual quality to 3D applications.  The Pentium 4 processor has 42 million transistors implemented on Intel's 0. 18u CMOS process, with six levels of aluminum interconnect.  It has a die size of 217 mm 2 and it consumes 55 watts of power at 1. 5GHz.  Its 3. 2 GB/second system bus helps provide the high data bandwidths needed to supply data to today's and tomorrow's demanding applications.  It adds 144 new 128-bit Single Instruction Multiple Data (SIMD) instructions called SSE2 (Streaming SIMD Extension 2) that improve performance for multi-media, content creation, scientific, and engineering applications.  # Other brands and names are the property of their respective owners.  OVERVIEW OF THE NETBURST ^TM MICROARCHITECTURE A fast processor requires balancing and tuning of many microarchitectural features that compete for processor die cost and for design and validation efforts.  Figure 1 shows the basic Intel NetBurst microarchitecture of the Pentium 4 processor.  As you can see, there are four main sections: the in-order front end, the out-of-order execution engine, the integer and floating-point execution units, and the memory subsystem. 
Wormholes Improve Contrastive Divergence| Abstract In models that define probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model's distribution.  If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3].  But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another.  We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution.  If the model is approximately correct, these long-range moves have a reasonable acceptance rate. 
Distinguishing Text from Graphics in On-line Handwritten Ink| Abstract We present a system that separates text from graphics strokes in handwritten digital ink.  It utilizes not just the characteristics of the strokes, but also the information provided by the gaps between the strokes, as well as the temporal characteristics of the stroke sequence.  It is built using machine learning techniques that infer the internal parameters of the system from real digital ink, collected using a Tablet PC. 
Variational Learning in Nonlinear Gaussian Belief Networks| Abstract We view perceptual tasks such as vision and speech recognition as inference problems where the goal is to estimate the posterior distribution over latent variables (e. g. , depth in stereo vision) given the sensory input.  The recent flurry of research in independent component analysis exemplifies the importance of inferring the continuousvalued latent variables of input data.  The latent variables found by this method are linearly related to the input, but perception requires nonlinear inferences such as classification and depth estimation.  In this paper, we present a unifying framework for stochastic neural networks with nonlinear latent variables.  Nonlinear units are obtained by passing the outputs of linear Gaussian units through various nonlinearities.  We present a general variational method that maximizes a lower bound on the likelihood of a training set and give results on two visual feature extraction problems.  We also show how the variational method can be used for pattern classification and compare the performance of these nonlinear networks with other methods on the problem of handwritten digit recognition. 
Switching State-Space Models| Abstract Weintroduce a statistical model for times series data with nonlinear dynamics which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of those regimes.  This model combines and generalizes two of the most widely used stochastic time series models---the hidden Markov model and the linear dynamical system---and is related to models that are widely used in the control and econometrics literatures.  It can also be derived by extending the mixture of experts neural network model (Jacobs et al. , 1991) to its fully dynamical version, in which both expert and gating networks are recurrent.  Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) alogithm cannot be applied.  However, we presentavariational approximation which maximizes a lower bound on the log likelihood and makes use of both the forward{backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. 
Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task| Abstract The problem of reinforcement learning in large factored Markov decision processes is explored.  The Q-value of a state-action pair is approximated by the free energy of a product of experts network.  Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs.  Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling.  The algorithm is tested on a co-operative multi-agent task.  The product of experts model is found to perform comparably to table-based Q-learning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation. 
GTM Through Time| Abstract The standard GTM (generative topographic mapping) algorithm assumes that the data on which it is trained consists of independent, identically distributed (i. i. d. ) vectors.  For time series, however, the i. i. d.  assumption is a poor approximation.  In this paper we show how the GTM algorithm can be extended to model time series by incorporating it as the emission density in a hidden Markov model.  Since GTM has discrete hidden states we are able to find a tractable EM algorithm, based on the forward-backward algorithm, to train the model.  We illustrate the performance of GTM through time using flight recorder data from a helicopter. 
Variational Learning for Switching State-Space Models| Abstract Weintroduce a new statistical model for time series which iteratively segments data into regimes with
Neighbourhood Components Analysis| Abstract In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm.  The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set.  It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification.  Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them.  The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction. 
A Mode-Hopping MCMC sampler| Abstract One of the main shortcomings of Markov chain Monte Carlo samplers is their inability to mix between modes of the target distribution.  In this paper we show that advance knowledge of the location of these modes can be incorporated into the MCMC sampler by introducing mode-hopping moves that satisfy detailed balance.  The proposed sampling algorithm explores local mode structure through local MCMC moves (e. g.  diffusion or Hybrid Monte Carlo) but in addition also represents the relative strengths of the different modes correctly using a set of global moves.  This "mode-hopping" MCMC sampler can be viewed as a generalization of the darting method [1]. 
The Helmholtz machine| Abstract Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning.  One fruitful approach is to build a parameterised stochastic generative model, independent draws from which are likely to produce the patterns.  For all but the simplest generative models, each pattern can be generated in exponentially many ways.  It is thus intractable to adjust the parameters to maximize the probability of the observed patterns, We describe a way of finessing this combinatorial explosion by maximising an easily computed lower bound on the probability of the observations.  Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways. 
Rate-coded Restricted Boltzmann Machines for Face Recognition| Abstract We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual.  Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known.  Our method compares favorably with other methods in the literature.  The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation.  The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations. 
Spiking Boltzmann Machines| Abstract We first showhow to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions.  Then we showhow the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately.  Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generativemodel.  1 Population codes and energy landscapes A perceived object is represented in the brain by the activities of many neurons, but there is no general consensus on how the activities of individual neurons combine to represent the multiple properties of an object.  We start by focussing on the case of a single object that has multiple instantiation parameters such as position, velocity, size and orientation.  We assume that each neuron has an ideal stimulus in the space of instantiation parameters and that its activation rate or probability of activation falls off monotonically in all directions as the actual stimulus departs from this ideal.  The semantic problem is to define exactly what instantiation parameters are being represented when the activities of manysuch neurons are specified.  Hinton, Rumelhart and McClelland (1986) consider binary neurons with receptive fields that are convex in instantiation space.  They assume that when an object is presentitactivates all of the neurons in whose receptive fields its instantiation parameters lie.  Consequently, if it is known that only one object is present, the parameter values of the object must lie within the feasible region formed by the intersection of the receptive fields of the active neurons.  This will be called a conjunctive distributed representation.  Assuming that each receptive field occupies only a small fraction of the whole space, an interesting property of this type of ``coarse coding" is that the bigger the receptive fields, the more accurate the representation.  However, large receptive fields lead to a loss of resolution when several objects are presentsimultaneously.  When the sensory input is noisy,itis impossible to infer the exact parameters of objects so it makes sense for a perceptual system to represent the probabilitydistribution across parameters rather than just a single best estimate or a feasible region.  The full probability distribution is essential for correctly combining inforE(x) P(x) Figure 1: a) Energy landscape over a onedimensional space.  Each neuron adds a dimple (dotted line) to the energy landscape (solid line).  b) The corresponding probability density.  Where dimples overlap the corresponding probability density becomes sharper.  Since the dimples decay to zero, the location of a sharp probability peak is not affected by distantdimples and multimodal distributions can be represented.  mation from different times or different sources.  One obvious way to represent this distribution (Anderson and van Essen, 1994) is to allow each neuron to represent a fairly compact probability distribution over the space of instantiation parameters and to treat the activity levels of neurons as (unnormalized) mixing proportions.  The semantics of this disjunctive distributed representation is precise, but the percepts it allows are not because it is impossible to represent distributions that are sharper than the individual receptive fields and, in high-dimensional spaces, the individual fields must be broad in order to cover the space.  Disjunctive representations are used in Kohonen's self-organizing map which is why it is restricted to very low dimensional latent spaces.  The disjunctive model can be viewed as an attempt to approximate arbitrary smooth probability distributions by adding together probability distributions contributed byeach active neuron.  Coarse coding suggests a multiplicative approachinwhich the addition is done in the domain of energies (negative log probabilities).  Each active neuron contributes an energy landscape over the whole space of instantiation parameters.  The activity level of the neuron multiplies its energy landscape and the landscapes for all neurons in the population are added (Figure 1).  If, for example, each neuron has a full covariance Gaussian tuning function, its energy landscape is a parabolic bowl whose curvature matrix is the inverse of the covariance matrix.  The activitylevel of the neuron scales the inverse covariance matrix.  If there are k instantiation parameters then only k + k(k +1)=2realnumbers are required to span the space of means and inverse covariance matrices.  So the real-valued activities of O(k 2 ) neurons are sufficient to represent arbitrary full covariance Gaussian distributions over the space of instantiation parameters.  Treating neural activities as multiplicative coefficients on additivecontributions to energy landscapes has a number of advantages.  Unlike disjunctive codes, vague distributions are represented bylow activities so significantbiochemical energy is only required when distributions are quite sharp.  A central operation in Bayesian inference is to combine a prior term with a likelihood term or to combine two conditionally independent likelihood terms.  This is trivially achieved by adding two energy landscapes 1 .  1 We thank Zoubin Ghahramani for pointing out that another important operation, convolving a probability distribution with Gaussian noise, is a difficult non-linear operation on the energy landscape. 
A comparison of statistical learning methods on the GUSTO database| Abstract A battery of modern,
Learning Distributed Representations by Mapping Concepts and Relations into a Linear Space| Abstract Linear Relational Embedding is a method of learning a distributed representation of concepts from data consisting of binary relations between concepts.  Concepts are represented as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept.  A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent.  On a task involving family relationships, learning is fast and leads to good generalization. 
Inferring Motor Programs from Images of Handwritten Digits| Abstract We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program.  We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits.  The inferred motor programs can be used directly for digit classification, but they can also be used in other ways.  By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods.  We can also use the motor programs as additional, highly informative outputs which reduce overfitting when training a feed-forward classifier.  1 Overview The idea that patterns can be recognized by figuring out how they were generated has been around for at least half a century [1, 2] and one of the first proposed applications was the recognition of handwriting using a generative model that involved pairs of opposing springs [3, 4].  The "analysis-by-synthesis" approach is attractive because the true generative model should provide the most natural way to characterize a class of patterns.  The handwritten 2's in figure 1, for example, are very variable when viewed as pixels but they have very similar motor programs.  Despite its obvious merits, analysis-by-synthesis has had few successes, partly because it is computationally expensive to invert non-linear generative models and partly because the underlying parameters of the generative model are unknown for most large data sets.  For example, the only source of information about how the MNIST digits were drawn is the images themselves.  We describe a simple generative model in which a pen is controlled by two pairs of opposing springs whose stiffnesses are specified by a motor program.  If the sequence of stiffnesses is specified correctly, the model can produce images which look just like the MNIST digits.  Using a separate network for each digit class, we show that backpropagation can be used to learn a "recognition" network that maps images to the motor programs required to produce them.  An interesting aspect of this learning is that the network creates its own training data, so it does not require the training images to be labelled with motor programs.  Each recognition network starts with a single example of a motor program and grows an "island of competence" around this example, progressively extending the region over which it can map small changes in the image to the corresponding small changes in the motor program (see figure 2).  Figure 1: An MNIST image of a 2 and the additional images that can be generated by inferring the motor program and then adding random noise to it.  The pixels are very different, but they are all clearly twos.  Fairly good digit recognition can be achieved by using the 10 recognition networks to find 10 motor programs for a test image and then scoring each motor program by its squared error in reconstructing the image.  The 10 scores are then fed into a softmax classifier.  Recognition can be improved by using PCA to model the distribution of motor trajectories for each class and using the distance of a motor trajectory from the relevant PCA hyperplane as an additional score.  Each recognition network is solving a difficult global search problem in which the correct motor program must be found by a single, "open-loop" pass through the network.  More accurate recognition can be achieved by using this open-loop global search to initialize an iterative, closed-loop local search which uses the error in the reconstructed image to revise the motor program.  This requires reconstruction errors in pixel space to be mapped to corrections in the space of spring stiffnesses.  We cannot backpropagate errors through the generative model because it is just a hand-coded computer program.  So we learn "generative" networks, one per digit class, that emulate the generator.  After learning, backpropagation through these generative networks is used to convert pixel reconstruction errors into stiffness corrections.  Our final system gives 1. 82% error on the MNIST test set which is similar to the 1. 7% achieved by a very different generative approach [5] but worse than the 1. 51% produced by the best backpropagation networks or the 1. 4% produced by support vector machines [6].  Recognition of test images is quite slow because it uses ten different recognition networks followed by iterative local search.  There is, however, a much more efficient way to make use of our ability to extract motor programs.  They can be treated as additional output labels when using backpropagation to train a single, multi-layer, discriminative neural network.  These additional labels act as a very informative regularizer that reduces the error rate from 1. 51% to 1. 34% in a network with 500 units in the first hidden layer and 1000 units in the second 1 .  This is a new method of improving performance that can be used in conjunction with other tricks such as preprocessing the images, enhancing the training set or using convolutional neural nets [7, 8].  2 A simple generative model for drawing digits The generative model uses two pairs of opposing springs at right angles.  One end of each spring is attached to a frictionless horizontal or vertical rail that is 39 pixels from the center of the image.  The other end is attached to a "pen" that has significant mass.  The springs themselves are weightless and have zero rest length.  The pen starts at the equilibrium position defined by the initial stiffnesses of the four springs.  It then follows a trajectory that is determined by the stiffness of each spring at each of the 16 subsequent time steps in the motor program.  The mass is large compared with the rate at which the stiffnesses change, so the system is typically far from equilibrium as it follows the smooth trajectory.  On each time step, the momentum is multiplied by 0. 9 to simulate viscosity.  A coarse1 Due to the extreme proximity of the NIPS deadline, only two network architectures were tried and this result was obtained by monitoring the real test error during training.  We promise to do it properly later using a validation set to find a good architecture and stopping criterion. 
Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights| Abstract Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases.  So during learning, it is importanttokeep the weights simple by penalizing the amount of information they contain.  The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-off between the expected squared error of the network and the amount of information in the weights.  We describe a method of computing the derivatives of the expected squared error and of the amountof information in the noisy weights in a network that contains a layer of non-linear hidden units.  Provided the output units are linear, the exact derivatives can be computed efficiently without time-consuming Monte Carlo simulations.  The idea of minimizing the amountof information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights. 
Experiments on Learning by Back Propagation| Abstract Rumelhart, Hinton and Williams [Rumelhart et al.  86] describe a learning procedure for layered networks of deterministic, neuron-like units.  This paper describes further research on the learning procedure.  We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets.  We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise.  The speed of learning is strongly dependent on the shape of the surface formed by the error measure in "weight space. " We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space.  The main drawback of the learning procedure is the way it scales as the size of the task and the network increases.  We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units.  Additional results illustrate the effects on learning speed of the amount of interaction between the weights.  A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure.  Finally, we discuss the relationship between our iterative networks and the "analog" networks described by Hopfield and Tank [Hopfield and Tank 85].  The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search. 
A New View of the EM Algorithm that Justifies Incremental and Other Variants| Abstract We present a new view of the EM algorithm for maximum likelihood estimation in situations with unobserved variables.  In this view, both the E and the M steps of the algorithm are seen as maximizing a joint function of the model parameters and of the distribution over unobserved variables.  From this perspective, it is easy to justify an incremental variant of the algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step.  This variant is shown empirically to give faster convergence in a mixture estimation problem.  A wide range of other variant algorithms are also seen to be possible. 
Exponential Family Harmoniums with an Application to Information Retrieval| Abstract Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields.  Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables.  In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models.  Inference in these "exponential family harmoniums" is fast while learning is performed by minimizing contrastive divergence.  A member of this family is then studied as an alternative probabilistic model for latent semantic indexing.  In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords. 
Efficient Parametric Projection Pursuit Density Estimation| Abstract Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality.  We present the \undercomplete product of experts" (UPoE), where each expert models a one dimensional projection of the data.  The UPoE may be interpreted as a parametric probabilistic model for projection pursuit.  Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA.  We also derive an ecient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models. 
A VIEW OF THE EM ALGORITHM THAT JUSTIFIES INCREMENTAL, SPARSE, AND OTHER VARIANTS| Abstract.  The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved.  We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables.  From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step.  This variant is shown empirically to give faster convergence in a mixture estimation problem.  A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible. 
SMEM Algorithm for Mixture Models| Abstract We present a split and merge EM (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models.  In the case of mixture models, local maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space.  To escape from such configurations we repeatedly perform simultaneous split and merge operations using a new criterion for efficiently selecting the split and merge candidates.  We apply the proposed algorithm to the training of Gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held-out test data.  We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems. 
Extracting Distributed Representations of Concepts and Relations from Positive and Negative Propositions| Abstract Linear Relational Embedding (LRE) was introduced (Paccanaro and Hinton, 1999) as a means of extracting a distributed representation of concepts from relational data.  The original formulation cannot use negative information and cannot properly handle data in which there are multiple correct answers.  In this paper we propose an extended formulation of LRE that solves both these problems.  We present results in two simple domains, which show that learning leads to good generalization. 
A Desktop Input Device and Interface for Interactive 3D Character Animation| Abstract We present a novel input device and interface for interactively controlling the animation of graphical human character from a desktop environment.  The trackers are embedded in a new physical design, which is both simple yet also provides significant benefits, and establishes a tangible interface with coordinate frames inherent to the character.  A layered kinematic motion recording strategy accesses subsets of the total degrees of freedom of the character.  We present the experiences of three novice users with the system, and that of a long-term user who has prior experience with other complex continuous interfaces. 
Adaptive mixtures of local experts|
Learning and relearning in Boltzmann machines|
Learning representations by back-propagating errors"|
How learning can guide evolution|
Learning Internal Representations by Error Propagation,volume 1 of Computational models of cognition and perception,|
Learning Translation Invariant Recognition in Massively Parallel Networks|
A time-delay neural network architecture for isolated word recognition|
Optimal perceptual Inference|
Learning internal represntations by backpropagating errors",|
Self-organizing neural network that discovers surfaces in random-dot stereograms|
Modeling the manifolds of images of handwritten digits|
Switching statespace models," 6 King's College Road,|
The microarchitecture of the pentium 4 processor,"|
A Learning Algorithm for Boltzmann Machines|
A new view on the EM algorithm that justifies incremental and other variants|
Implementing semantic networks in parallel hardware|
Solving random-dot stereograms using the heat equation|
Phoneme Recognition using Time-Delay Neural Networks IEEE,|
Boltzmann machines: Constraint satisfaction networks that learn,Tech|
Connectionist Learning Procedures|
Using Generative Models for Handwritten Digit Recognition|
Adaptive mixture of local experts|
The wake-sleep algorithm for self-organizing neural networks|
On contrastive divergence learning|
A view of the EM algorithm that justifies incremental, sparse, and others variants|
The development of the time-delay neural network architecture for speech recognition|
Modelling the manifolds of images of handwritten digits,|
A 0|18 m CMOS IA32 microprocessor with a 4 GHz integer execution unit,. 
Distributed Representations|
A hierarchical community of experts|
How neural networks learn from experience|
Schemata and sequential thought processes in PDF models|
Hyper-threading technology architecture and microarchitecture|
Learning distributed representations of concepts|
TRAFFIC: Recognizing Objects Using Hierarchical Reference Frame Transformations|
Simplifying neural networks by soft weight sharing,|
Special issue on connectionist symbolic processing|
Learning and relearning in Boltzman machines|
A General Framework for Parallel Distributed Processing|
A learning algorithm for Bolzmann machines|
Learning and relearning in Bolzmann machines,"|
Symbols among neurons,|
An Alternative Model for Mixtures of Experts|
Discovering Viewpoint-Invariant Relationships That Characterize Objects|
Spatial coherence as an internal teacher for a neural network|
Zemel RS 1995 The Helmholtz machine Neural Comput|
Building adaptive interfaces with neural networks: The glove-talk pilot study|
A Parallel Computation that Assigns Canonical Object-Based Frames of Reference|
Parallel computations for controlling an arm|
Wa i be l|
Recognizing Handwritten Digits Using Mixtures of Linear Models|
Representing part-whole hierarchies in connectionist networks|
A Distributed Connectionist Production System|
Frames of reference and mental imagery|
Some demonstrations of the effects of structural descriptions in mental imagery|
How learning guides evolution|
Local Physical Models for Interactive Character Animation|
Mean field networks that can learn to discriminate temporally distorted strings|
Adaptive Soft Weight Tying using Gaussian Mixtures|
Cascaded redundancy reduction|
Mapping Part-Whole Hierarchies into Connectionist Networks|
Vol| 1 of Computational models of cognition and perception,. 
Learning Symmetry Groups with Hidden Units: Beyond the Perception|
Deterministic Boltzmann learning performs steepest descent in weightspace|
Intel's Multi-Threading Technology|
Learning internal rpresentations by error propagation,|
Introduction|
Lesioning a connectionist network: Investigations of acquired dyslexia|
Lesioning an attractor network: Investigations of acquired dyslexia|
Learning internal reperesentations by error propagation|
Keeping neural networks simple|
The appeal of parallel distributed processing|
A 0|18 CMOS IA-32 processor with a 4-GHz integer execution unit. 
Learning to Recognize Shapes in a Parallel Network,|
A learning algorithm for Boltzman Machines,|
Connectionist Architectures for Artificial Intelligence|
"Delve,"|
Unsupervised learning|
Why the islands move|
Models of information processing in the brain|
Generalized darting monte carlo|
"986) in: Learning internal representations by error propagation, eds|
Relaxation and its Role in Vision,|
Learning internal representations by error propagation (Institute for Cognitive Science|
A modified gating network for the mixtures of experts architecture,|
A Mobile Robot that Learns its Place|
Learning internal representations byback-7312 -62274 errors|
M and Zemel R S 1995 The helmholtz machine Neural Computation 7|
Minimum Description Length and Helmholtz Free Energy Advances in Neural Information Processing Systems 6|
Dayan P, Frey B J and Neal R M 1995 The wake-sleep algorithm for unsupervised neural networks Science 268|
The EM Algorithm for Mixtures of Factor Analyzers",|
Shape Recognition and Illusory Conjunctions|
Glove-talkii: An adaptive gesture-to-format interface|
Sejnowski T J 1985 A Learning Algorithm for Boltzmann Machines|
A Distributed Connectionists Production System,|
In Press)| Generative models for discovering sparse distributed representations. 
Neighbourhood component analysis|
A time delay neural network for speech recognition|
Adaptave elastic models for hand-printed character recognition|
Towards Neurally Plausible Bayesian Networks| Plenary talk,. 
---,|
Helmholtz machines|
Adaptive mixtures of experts|
Experiments on discovering high order features with mean field modules|
Learning sets of filters using back propagation|
Deterministic boltzmann learning in networks with asymmetric connectivity|
Analyzing cooperative computation|
Product of experts|
Separating figure from ground with a Boltzmann machine|
Learning mixture-models of spatial coherence|
Learning lnternal representation by error backpropagation|
Learning to use spike timing in a restricted Boltzmann machine|
GloveTalk: A neural network interface between a DataDlove and a speech synthesizer|
Shape recognition and illusory conjunctions, in:|
Optimal perceptual learning|
A soft decision-directed LMS algorithm for blind equalization|
Assessing learning procedures using DELVE|
Hyper-Threading Technology Microarchitecture and Performance,|
Learning interval representation by error propagation,|
The DELVE manual|
Products of hidden markov models,|
The Pentium 4 processor|
Learning to Make Coherent Predictions in Domains with Discontinuities|
The wake-sleep algorithm for unsupervised learning neural networks,|
Connectionist Learning Procedures in Artificial neural networks concept learning,|
and the PDP Research Groups (eds),|
Simplifying Neural Networks by Soft Weight-Sharing| Computational Neuroscience Laboratory,. 
GloveTalkII: An Adaptive Gesture-to-Formant Interface|
Cytochrome P450 induction and histopathology in preemergent pink salmon from oiled spawning sites in Prince William Sd|
editors,|
A Learning Algorith, for Bolzmann Machines|
A framework for PDP|
A timeminima than Elman towers (in fact, they also did better delay neural network architecture for isolated word than Jordan towers)| recognition,. 
Modeling High-Dimensional Data by Combining Simple Experts|
Extracting distributed representation of concepts from relational data using linear relational embedding|
G-maximization: an unsupervised learning procedure for discovering regularities|
NeoroAnimator: Fast Neoral Network Emulation and control of PhysicsBased Models,|
Trace-based instruction caching|
Learning Internal Representations by Error Propagation in Parallel Distributed|
To appear)| Simplifying neural networks by soft weight-sharing. 
Learning internal represenations by error propagation|
Discovering High Order Features with Mean Field Modules|
Shape Representation in Parallel Systems|
Distributed representations| In Parallel distributed processing: explorations in the microstructure of cognition,. 
Massively Parallel Architectures for AI: NETL, Thistle, and Boltzmann Machines|
Information Processing Systems 2,|
Evaluation of Adaptive Mixtures of Competing Experts|
Imagery|
Scene-based and viewer-centered representations for comparing shapes|
Modeling the manifolds of handwritten digits|
Global coordination of linear models|
Evaluation of adaptive mixtures of competing experts, in:|
"Separating figure from ground with a parallel network"|
Symbol Among Neurons|
PDP: Computational models of cognition and perception, I, chapter Learning internal representations by error propagation,|
The EM algorithm for mixture of factor analyzers,|
variables|
Learning in Boltzmann machines|
An alternative model for mixtures of experts, to appear on Advances in Neural Information Processing Systems 7,|
Learning in graphical models, chapter A view of the EM algorithm that justifies incrememntal, sparse and other variants,|
An alternative model for mixture of experts|
Xerion neural network simulator|
Connectionist symbol processing|
Navigating without a map by directly transforming sensory input to location|
Parallel visual computation|
and Patrice Roussel| The Microarchitecture of the Pentium 4 Processor. 
Invited talk| In Workshop \The future and prospects of neural networks" at ICANN'99,. 
The DELVE Manual| DELVE can be found at. 
Neural Networks for Artificial Intelligence|
1st Quarter)|
Distributed representation|
Training Many Small Hidden Markov Models|
\Learning and Unlearning in Bolzmann Machines"|
`Distributed representations', Technical report,|
Parsons L M, 1988 \Scene-based and viewer-centered representations for comparing shapes"|
A tractable probabilistic model for projection pursuit|
Adaptive Mixtures of|
On contrastive divergence (CD) learning|
The appeal of parallel distributed processing| In Rumelhart, McClelland and the PDP Research Group, (eds. ). 
Recognizing handwritten ditits using mixtures of linear models|
The bootstrap Widrow-Hoff rule as a cluster-formation algorithm|
Storage and Processing of Information in Distributed Associative Memory Systems," Teuvo Kohonen, Erkki Oja, and Pekka Lehtio, in Parallel Models of Associative Memory, eds|
Energy-based models for sparse overcomplete representations| Technical report, University of Toronto, Department of Computer Science. 

Algorithm Selection using Reinforcement Learning| Abstract Many computational problems can be solved by multiple algorithms, with different algorithms fastest for different problem sizes, input distributions, and hardware characteristics.  We consider the problem of algorithm selection: dynamically choose an algorithm to attack an instance of a problem with the goal of minimizing the overall execution time.  We formulate the problem as a kind of Markov decision process (MDP), and use ideas from reinforcement learning to solve it.  This paper introduces a kind of MDP that models the algorithm selection problem by allowing multiple state transitions.  The well known Q-learning algorithm is adapted for this case in a way that combines both Monte-Carlo and Temporal Difference methods.  Also, this work uses, and extends in a way to control problems, the Least-Squares Temporal Difference algorithm (LSTD(0)) of Boyan.  The experimental study focuses on the classic problems of order statistic selection and sorting.  The encouraging results reveal the potential of applying learning methods to traditional computational problems. 
Neural Maps for Mobile Robot Navigation| Abstract Neural maps have been recently proposed [4] as an alternative method for mobile robot path planning.  However, these proposals are mostly theoretical and primarily concerned with biological plausibility.  This paper addresses the applicability of neural maps to mobile robot navigation with focus on efficient implementations.  It is suggested that neural maps offer a promising alternative compared to the traditional distance transform and harmonic function methods.  Applications of neural maps are presented for both global and local navigation.  Experimental results (both simulated and real-world on a Nomad 200 mobile robot) demonstrate the validity of the approach.  Our work reveals that a key issue for success of the method is the organization of the map that needs to be optimized for the situation at hand. 
2D DNA Self-Assembly for Satisfiability| Abstract DNA self-assembly has been proposed as a way to cope with huge combinatorial NP-HARD problems, such as satisfiability.  However, the algorithmic designs for DNA self-assembly proposed so far are highly dependent on the instance to be solved.  The required work (DNA synthesis, tile construction, encoding, etc. ) can be done only after the instance is given.  This paper presents an algorithmic design for solving satisfiability problems using two-dimensional DNA self-assembly (tiling).  The main driving factor in this work was the design and encoding of the algorithm in a general way that minimizes the dependency on particular instances.  In effect, a large amount of work and preparation can be done in advance as a batch process in the absence of any particular instance.  In practice, it is likely that the total time (from the time an instance is given, to the time a solution is returned) will be decreased significantly and laboratory procedures will be simplified.  1 The Satisfiability (SAT) Problem The Boolean Satisfiability (SAT) problem is the most known representative of the NP-HARD class of problems.  The non-polynomial (usually exponential) time required for optimal solutions to these problems, implies that solution of large instances becomes intractably difficult, if not practically impossible.  A SAT instance consists of a number of Boolean variables x 1 ;x 2 ; :::; xm and a number of clauses C 1 ;C 2 ; :::; C n .  Each clause is a disjunction of distinct literals, whereby a literal is a single variable x i itself or its complement # x i .  A solution (satisfying assignment) is an assignment of binary values to the variables x i , such that the conjunction of all clauses is satisfied.  Boolean formulas represented in this format are said to be in Conjunction Normal Form (CNF); a conjunction of disjunctions.  An example with 5 variables and 8 clauses is given below: (x 1 + x 2 +fix 3 )(fix 2 + x 4 )(fix 1 +fix 5 )(fix 1 + x 2 + x 3 +fix 4+x 5 )(fix 3 )(x 2 + x 5 )(fix 5 )(x 1 ) In this case, the assignment(x 1 ;x 2 ;x 3 ;x 4 ;x 5 )=(1;1;0;1;0) is satisfying.  So far, there is no restriction on the number of literals that a single clause can contain.  However, even if we require that each clause has exactly k literals (k # 3), the problem (known as kCNF-SAT) is still intractable.  Moreover, it is proven that anySAT instance can be turned into an equivalent kCNFSAT instance [GaJo79].  From these variations, most notable is the 3CNF-SAT variation, because of the small-sized clauses.  Notice that for a given number of m variables, there can be # m k # # 2 k possible clauses in a kCNF-SAT formula (choose k variables and for each one either leave intact or negate).  In particular, for k = 3, there are # m 3 # # 2 3 = 4 3 #m(m , 1)(m , 2) = O(m 3 ) possible clauses in a 3CNF-SAT formula.  2 DNA Computation and Satisfiability The basic idea is to exploit the massive parallelism present in DNA operations in order to emulate a non-deterministic device that solves the SAT problem in polynomial time.  Consider a particular assignment to the boolean variables in the formula.  On a conventional computer it is fairly easy to check whether this particular assignment is a solution to the problem, i. e.  an assignment that satisfies all the clauses in the formula.  In fact, this can be done in time linear to the size of the formula.  It is the huge (exponential) number of different assignments that makes the problem difficult.  This work proposes a way to perform this checking procedure on molecular substrate using 2D DNA self-assembly.  By creating billions of copies of the participating DNA structures (tiles, in our case), we expect that this procedure will run in parallel on all possible assignments.  The assignments will be created dynamically as part of the assembly.  In effect, that will make the computation time linear to the size of the formula, while pushing the exponential dimension of the problem into the large number of DNA assemblies, and thus into the space (volume) occupied by the DNA molecules.  If there is a satisfying assignment, we expect that at least one of these parallel checks will discover it.  This section reviews the main proposals for biomolecular solutions to the SAT problem, briefly describes the general DNA structures and methods that are used here and delineates our work.  2. 1 Related Work Lipton was the first to propose a DNA model for satisfiability.  His proposal [Lipt95] is based on Adleman's elimination method, whereby the whole combinatorial space of solutions is created and subsequently the \good" ones are extracted by a series of separation steps.  Later, Hagiya et al.  [Hagi97] presented an approachtoevaluate and learn #-formulas (a particular form of Boolean formulas) using a technique that is commonly known as Whiplash PCR.  This method was improved by Winfree [Winf98].  Finally, a proposal for CNF-SAT using hairpin DNA tiles and linear assembly can be found in [WiER98].  A common feature of these approaches and a potential practical problem is that construction of the participating DNA structures cannot really begin until the particular problem instance at hand is given.  This \instance-speci#c" design implies large total computation time (encoding/DNA computation/decoding).  In addition, all required man/machine resources need to be employed again and again as new instances are provided.  Moreover, the speci#cityofthe encoding step will increase the likelihood of encoding errors, which if occur will render the whole computation useless.  A possible solution to these problems, would be an algorithmic design that requires minimal encoding for a given instance (some straightforward description of the input), whereas the main algorithm is coded in preconstructed \instance-independent" DNA molecules that can be created and even tested off-line in a batch fashion.  The DNA self-assembly technique we are going to use is known as DNA tiling computation and was proposed by Eric Winfree [Winf98].  Basic components for DNA tiling have been prototyped and tested by Nadrian Seeman and his colleagues.  In particular, double crossover molecules have been shown to be rigid and able to form planar lattices [WLWS98].  Winfree showed how to solve the Hamiltonian Path problem using 2-dimensional DNA tiling [Winf98].  Further, LaBean, Winfree and Reif [LaWR99] have been experimenting with parallel XOR and addition operations using DNA tiling. 
Mobile Robot Local Navigation with a Polar Neural Map| Table 1 Algorithm for path construction. 
Learning to Select Branching Rules in the DPLL Procedure for Satisfiability| Abstract The DPLL procedure is the most popular complete satisfiability (SAT) solver.  While its worst case complexity is exponential, the actual running time is greatly affected by the ordering of branch variables during the search.  Several branching rules have been proposed, but none is the best in all cases.  This work investigates the use of automated methods for choosing the most appropriate branching rule at each node in the search tree.  We consider a reinforcement-learning approach where a value function, which predicts the performance of each branching rule in each case, is learned through trial runs on a typical problem set of the target class of SAT problems.  Our results indicate that, provided sufficient training on a given class, the resulting strategy performs as well as (and, in some cases, better than) the best branching rule for that class. 
Value Function Approximation in Zero-Sum Markov Games| Abstract This paper investigates value function approximation in the context of zero-sum Markov games, which can be viewed as a generalization of the Markov decision process (MDP) framework to the two-agent case.  We generalize error bounds from MDPs to Markov games and describe generalizations of reinforcement learning algorithms to Markov games.  We present a generalization of the optimal stopping problem to a two-player simultaneous move Markov game.  For this special problem, we provide stronger bounds and can guarantee convergence for LSTD and temporal difference learning with linear value function approximation.  We demonstrate the viability of value function approximation for Markov games by using the Least squares policy iteration (LSPI) algorithm to learn good policies for a soccer domain and a flow control problem. 
Universal Access to Mobile Computing Devices through Speech Input| Abstract This paper presents results on a user interface model for providing universal access to mobile computing devices.  The model uses a continuous speech understanding engine to provide access to a virtual keyboard and mouse through speech input.  This research has been targeted towards users with permanent motor disabilities.  However, these results also apply to able-bodied users with temporary, taskinduced motor disabilities, such as users performing alphanumeric data entry through a cellular phone keypad.  The proposed solution might complement (or even replace) miniaturized keyboards and other physical keyboard alternatives, such as stylus-type "soft" keyboards.  Since it only requires a microphone (and perhaps a speaker for feedback) which are already included in many mobile devices, it may allow such devices to shrink considerably in size, as alphanumeric input is no longer bound to a physical area.  The paper describes the underlying architecture employed by the system.  It presents empirical results addressing the effectiveness of this interface over alternative input methods for alphanumeric data entry.  Finally, it discusses implications and future directions. 
A Listening Keyboard for Users with Motor Impairments---A Usability Study| Abstract.  Computer users with motor impairments find it difficult and, in many cases, impossible to access PC functionality through the physical keyboard-and-mouse interface.  Studies show that even able-bodied users experience similar difficulties when interacting with mobile devices; this is due to the reduced size/usability of the input interfaces.  Advances in speech recognition have made it possible to design speech interfaces for alphanumeric data entry and indirect manipulation (cursor control).  Although several related commercial applications exist, such systems do not provide a complete solution for arbitrary keyboard and mouse access, such as the access needed for, say, typing, compiling, and executing a C++ program.  We carried out a usability study to support the development of a speech user interface for arbitrary keyboard access and mouse control.  The study showed that speech interaction with an ideal listening keyboard is better for users with motor impairments than handstick, in terms of task completion time (37% better), typing rate (74% better), and error rates (63% better).  We believe that these results apply to both permanent and task-induced motor impairments.  In particular, a follow-up experiment showed that handstick approximates conventional modes of alphanumeric input available on mobile devices (e. g. , PDAs, cellular phones, and personal organizers).  These modes of input include miniaturized keyboards, stylus "soft" keyboards, cellular phone numberpads, and handwriting recognition software.  This result suggests that a listening keyboard would be an effective mode for alphanumeric input on future mobile devices.  This study contributed to the development of SUITEKeys---a speech user interface for arbitrary keyboard and mouse access available for MS platforms as freeware. 
Learning in Zero-Sum Team Markov Games Using Factored Value Functions| Abstract We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents collaborating against an opposing team of agents.  Our method requires full observability and communication during learning, but the learned policies can be executed in a distributed manner.  The value function is represented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth.  This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally intensive representations with extensive coordination between agents.  Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants.  The approach is demonstrated with an example that shows the efficiency gains over naive enumeration. 
Reinforcement Learning as Classification: Leveraging Modern Classifiers| Abstract The basic tools of machine learning appear in the inner loop of most reinforcement learning algorithms, typically in the form of Monte Carlo methods or function approximation techniques.  To a large extent, however, current reinforcement learning algorithms draw upon machine learning techniques that are at least ten years old and, with a few exceptions, very little has been done to exploit recent advances in classification learning for the purposes of reinforcement learning.  We use a variant of approximate policy iteration based on rollouts that allows us to use a pure classification learner, such as a support vector machine (SVM), in the inner loop of the algorithm.  We argue that the use of SVMs, particularly in combination with the kernel trick, can make it easier to apply reinforcement learning as an "outof-the-box" technique, without extensive feature engineering.  Our approach opens the door to modern classification methods, but does not preclude the use of classical methods.  We present experimental results in the pendulum balancing and bicycle riding domains using both SVMs and neural networks for classifiers. 
Approximate Policy Iteration using Large-Margin Classifiers| Abstract We present an approximate policy iteration algorithm that uses rollouts to estimate the value of each action under a given policy in a subset of states and a classifier to generalize and learn the improved policy over the entire state space.  Using a multiclass support vector machine as the classifier, we obtained successful results on the inverted pendulum and the bicycle balancing and riding domains. 
Coordinated Reinforcement Learning| Abstract We present several new algorithms for multiagent reinforcement learning.  A common feature of these algorithms is a parameterized, structured representation of a policy or value function.  This structure is leveraged in an approach we call coordinated reinforcement learning, by which agents coordinate both their action selection activities and their parameter updates.  Within the limits of our parametric representations, the agents will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space.  Our methods differ from many previous reinforcement learning approaches to multiagent coordination in that structured communication and coordination between agents appears at the core of both the learning algorithm and the execution architecture.  Our experimental results, comparing our approach to other RL methods, illustrate both the quality of the policies obtained and the additional benefits of coordination. 
Spatial Knowledge in Humans, Animals and Robots| Abstract Humans, animals and robots are physically existing agents situated in the real world.  Their common ability to extract, store and use spatial information is crucial for their successful operation.  On the other hand, their idiosyncracies seem to be reflected on their spatial knowledge.  The paper attempts a discussion around the cognitive map, a term coined to describe exactly the body of spatial knowledge held by an agent.  The topic is discussed at both a global and an individual level, occasionally interleaved with the author's personal opinions. 
Using Markov Decision Processes and Reinforcement Learning to Solve the "Remote Versus Local Execution" Problem| Abstract The problem of remote versus local execution requires the construction of a decision to execute a user job locally or to move that job to a remote server based on information of the machines involved, such that the overall user latency is minimized.  We present an approach for solving the remote vs. 
Model-Free Least-Squares Policy Iteration| Abstract We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration.  Our method is model-free and completely off policy.  We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efficient use of sample experiences compared to pure temporal difference algorithms.  LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems.  Moreover, approximations learned by LSTD are strongly influenced by the visitation distribution over states.  Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues.  The result is an off-policy method which can use (or reuse) data collected from any source.  We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trials. 
Least-Squares Policy Iteration|
D DNA self-assembly for satisfiability|
Reinforcement Learning for Algorithm Selection|
Speech Input for Mobile Computing Devices,|
Selecting the right algorithm|
Least-Squares Methods in Reinforcement Learning for Control|

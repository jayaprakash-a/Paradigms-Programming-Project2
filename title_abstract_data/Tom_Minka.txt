Expectation-Propagation for the Generative Aspect Model| Abstract The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents.  Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning.  This paper demonstrates that the simple variational methods of Blei et al.  (2001) can lead to inaccurate inferences and biased learning for the generative aspect model.  We develop an alternative approach that leads to higher accuracy at comparable cost.  An extension of ExpectationPropagation is used for inference and then embedded in an EM algorithm for learning.  Experimental results are presented for both synthetic and real data sets. 
Expectation propagation for signal detection in flat-fading channels| Abstract In this paper, we propose a new Bayesian receiver for signal detection in flat-fading channels.  First, the detection problem is formulated as an inference problem in a hybrid dynamic system that has both continuous and discrete variables.  Then, an expectation propagation algorithm is proposed to address the inference problem.  As an extension of belief propagation, expectation propagation efficiently approximates a Bayesian estimation by iteratively propagating information between different nodes in the dynamic system and projecting exact messages into the exponential family.  Compared to sequential Monte Carlo filters and smoothers, the new method has much lower computational complexity since it makes analytically deterministic approximation instead of Monte Carlo approximations.  Our simulations demonstrate that the new receiver achieves accurate detection without the aid of any training symbols or decision feedbacks. 
Automatic Choice of Dimensionality for PCA| Abstract A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained.  By interpreting PCA as density estimation, we show how to use Bayesian model selection to estimate the true dimensionality of the data.  The resulting estimate is simple to compute yet guaranteed to pick the correct dimensionality, given enough data.  The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly.  But after choosing an appropriate parameterization and applying Laplace's method, an accurate and practical estimator is obtained.  In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster. 
Bayesian linear regression| Abstract This note derives the posterior, evidence, and predictive density for linear multivariate regression under zero-mean Gaussian noise.  Many Bayesian texts, such as Box & Tiao (1973), cover linear regression.  This note contributes to the discussion by paying careful attention to invariance issues, demonstrating model selection based on the evidence, and illustrating the shape of the predictive density.  Piecewise regression, basis function regression, and automatic relevance determination are also discussed. 
Bayesian Color Constancy with Non-Gaussian Models| Abstract We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process.  The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation.  The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reflectance model when estimating the illuminants of a set of test images.  This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base. 
A family of algorithms for approximate Bayesian inference| Abstract One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense.  This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible.  This method, ``Expectation Propagation," unifies and generalizes two previous techniques: assumeddensity filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks.  The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence.  Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation.  Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks.  Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope.  Expectation Propagation also extends belief propagation in the opposite direction---propagating richer belief states which incorporate correlations between variables.  This framework is demonstrated in a variety of statistical models using synthetic and real-world data.  On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes.  For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known.  The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time.  Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection. 
Bayesian Conditional Random Fields| Abstract We propose Bayesian Conditional Random Fields (BCRFs) for classifying interdependent and structured data, such as sequences, images or webs.  BCRFs are a Bayesian approach to training and inference with conditional random fields, which were previously trained by maximizing likelihood (ML) (Lafferty et al. , 2001).  Our framework eliminates the problem of overfitting, and offers the full advantages of a Bayesian treatment.  Unlike the ML approach, we estimate the posterior distribution of the model parameters during training, and average over this posterior during inference.  We apply an extension of EP method, the power EP method, to incorporate the partition function.  For algorithmic stability and accuracy, we flatten the approximation structures to avoid two-level approximations.  We demonstrate the superior prediction accuracy of BCRFs over conditional random fields trained with ML or MAP on synthetic and real datasets. 
Document Image Decoding using Iterated Complete Path Search| Abstract The computation time of Document Image Decoding can be significantly reduced by
Document Image Decoding Using Iterated Complete Path Search with Subsampled Heuristic Scoring| Abstract It has been shown that the computation time of Document Image Decoding can be significantly reduced by employing heuristics in the search for the best decoding of a text line.  In the Iterated Complete Path (ICP)
An Optimized Interaction Strategy for Bayesian Relevance Feedback| Abstract A new algorithm and systematic evaluation is presented for searching a database via relevance feedback.  It represents a new image display strategy for the PicHunter system [2, 1].  The algorithm takes feedback in the form of relative judgments ("item A is more relevant than item B") as opposed to the stronger assumption of categorical relevance judgments ("item A is relevant but item B is not").  It also exploits a learned probabilistic model of human behavior to make better use of the feedback it obtains.  The algorithm can be viewed as an extension of indexing schemes like the k-d tree to a stochastic setting, hence the name "stochastic-comparison search. " In simulations, the amount of feedback required for the new algorithm scales like log 2 jDj, where jDj is the size of the database, while a simple query-by-exampleapproach scales like jDj a , where a ! 1 depends on the structure of the database.  This theoretical advantage is reflected by experiments with real users on a database of 1500 stock photographs. 
Learning How to Learn is Learning with Point Sets| Abstract It has been proposed that learning how to learn be understood in terms of "learning a prior," from a Bayesian point-of-view (Baxter, 1996b).  This paper presents an alternative interpretation: learning how to learn is ordinary learning but on point sets, rather than points.  The idea behind learning how to learn is to partition the data, learn a model for the partitions, and then apply this model to new partitions.  Ordinary learning methods do the same thing but with individual data points as the partitions.  The partitioning for learning how to learn may be recovered automatically and may be applied recursively, leading to "task clustering" models.  Virtually all existing approaches fit naturally into this unifying framework, including learning a distance metric and learning internal representations. 
MIT Media Laboratory Technical Report 365 Also appears as MIT thesis for the degree of Master of Engineering in Electrical Engineering and Computer Science Supervised by Rosalind W| Picard.  Abstract Digital libraries of images and video are rapidly growing in size and availability.  To avoid the expense and limitations of text, there is considerable interest in navigation by perceptual and other automatically extractable attributes.  Unfortunately, the relevance of an attribute for a query is not always obvious.  Queries which go beyond explicit color, shape, and positional cues must incorporate multiple features in complex ways.  This dissertation uses machine learning to automatically select and combine features to satisfy a query, based on positive and negative examples from the user.  The learning algorithm does not just learn during the course of one session: it learns continuously, across sessions.  The learner improves its learning ability by dynamically modifying its inductive bias, based on experience over multiple sessions.  Experiments demonstrate the ability to assist image classification, segmentation, and annotation (labeling of image regions).  The common theme of this work, applied to computer vision, database retrieval, and machine learning, is building in enough flexibility to allow adaptation to changing goals. 
Inferring a Gaussian distribution| Abstract A common question in statistical modeling is \which out of a continuum of models are likely to have generated this data?" For the
Deriving quadrature rules from Gaussian processes| Abstract Quadrature rules are often designed to achieve zero error on a small set of
Bayesian inference of a uniform distribution| Abstract This note derives the posterior, the evidence, and the predictive density for a uniform distribution, given a conjugate parameter prior.  These provide various Bayesian answers to the \taxicab" problem: viewing a city from the train, you see a taxi numbered x.  Assuming taxis are consecutively numbered, how many taxis are in the city?
The `summation hack' as an outlier model| Abstract The `summation hack' is the ad-hoc replacement of a product by a sum in a probabilistic expression.  This hack is usually explained as a device to cope with outliers, with no formal derivation.  This note shows that the hack does make sense probabilistically, and can be best thought of as replacing an outlier-sensitive likelihood with an outlier-tolerant one.  This interpretation exposes the hack as an assumption about the outliers, allowing us to determine when it makes sense to use the hack. 
Psychophysical studies of the performance of an image database retrieval system| ABSTRACT We describe psychophysical experiments conducted to study PicHunter, a content-based image retrieval (CBIR) system.  Experiment 1 studies the importance of using (a) semantic information, (b) memory of earlier input and (c) relative, rather than absolute, judgements of image similarity.  The target testing paradigm is used in which a user must search for an image identical to a target.  We find that the best performance comes from a version of PicHunterthat uses only semantic cues, with memory and relative similarity judgements.  Second best is use of both pictorial and semantic cues, with memory and relative similarity judgements.  Most reports of CBIR systems provide only qualitative measures of performance based on how similar retrieved images are to a target.  Experiment 2 puts PicHunterinto this context with a more rigorous test.  We first establish a baseline for our database by measuring the time required to find an image that is similar to a target when the images are presented in random order.  Although PicHunter'sperformance is measurably better than this, the test is weak because even random presentation of images yields reasonably short search times.  This casts doubt on the strength of results given in other reports where no baseline is established. 
T Media Laboratory Perceptual Computing Section Technical Report No| 349 Submitted to Special Issue of Pattern Recognition on Image Database: Classification and Retrieval.  Abstract Digital library access is driven by features, but features are often context-dependent and noisy, and their relevance for a query is not always obvious.  This paper describes an approach for utilizing many data-dependent, user-dependent, and task-dependent features in a semi-automated tool.  Instead of requiring universal similarity measures or manual selection of relevant features, the approach provides a learning algorithm for selecting and combining groupings of the data, where groupings can be induced by highly specialized and context-dependent features.  The selection process is guided by a rich example-based interaction with the user.  The inherent combinatorics of using multiple features is reduced by a multistage grouping generation, weighting, and collection process.  The stages closest to the user are trained fastest and slowly propagate their adaptations back to earlier stages.  The weighting stage adapts the collection stage's search space across uses, so that, in later interactions, good groupings are found given few examples from the user.  Described is an interactive-time implementation of this architecture for semi-automatic within-image segmentation and across-image labeling, driven by concurrently active color models, texture models, or manually-provided groupings. 
Bayesian inference, entropy, and the multinomial distribution| Abstract Instead of maximum-likelihood or MAP, Bayesian inference encourages the use of
BAYESIAN SPECTRUM ESTIMATION OF UNEVENLY SAMPLED NONSTATIONARY DATA| ABSTRACT Spectral estimation methods typically assume stationarity and uniform spacing between samples of data.  The non-stationarity of real data is usually accommodated by windowing methods, while the lack of uniformlyspaced samples is typically addressed by methods that "fill in" the data in some way.  This paper presents a new approach to both of these problems: we use a non-stationary Kalman filter within a Bayesian framework to jointly estimate all spectral coefficients instantaneously.  The new method works regardless of how the signal samples are spaced.  We illustrate the method on several data sets, showing that it provides more accurate estimation than the Lomb-Scargle method and several classical spectral estimation methods. 
The Bayesian Image Retrieval System, PicHunter: Theory, Implementation and Psychophysical Experiments| Abstract| This paper presents the theory, design principles, implementation, and performance results of PicHunter, a prototype content-based image retrieval (CBIR) system that has been developed over the past three years.  In addition, this document presents the rationale, design, and results of psychophysical experiments that were conducted to address some key issues that arose during PicHunter's development.  The PicHunter project makes four primary contributions to research on content-based image retrieval.  First, PicHunter represents a simple instance of a general Bayesian framework we describe for using relevance feedback to direct a search.  With an explicit model of what users would do, given what target image they want, PicHunter uses Bayes's rule to predict what is the target they want, given their actions.  This is done via a probability distribution over possible image targets, rather than by refining a query.  Second, an entropy-minimizing display algorithm is described that attempts to maximize the information obtained from a user at each iteration of the search.  Third, PicHunter makes use of hidden annotation rather than a possibly inaccurate/inconsistent annotation structure that the user must learn and make queries in.  Finally, PicHunter introduces two experimental paradigms to quantitatively evaluate the performance of the system, and psychophysical experiments are presented that support the theoretical claims. 
Linear regression with errors in both variables: A proper Bayesian approach| Abstract Linear regression with errors in both variables is a common modeling problem with a 100-year literature, yet we have still not achieved the widespread use of a complete and correct solution.  Much of this is due to a confusion between joint and conditional modeling and an unhealthy aversion to priors.  This paper expands on the proper Bayesian method of Zellner (1971) and Gull (1989), deriving specific parameter estimators and giving an analysis of their performance.  They are shown to perform favorably compared to total least squares.  The main findings are: ffl Linear regression, which is a conditional linear model, is different from principal components analysis, which is a joint linear model, ffl It is important to use a proper prior on both the variance of the noise and the value of the true inputs, ffl Special attention needs to be paid to the case when the scatter of the data is small compared to the noise, which can happen when inputs are correlated, ffl Maximum-likelihood estimation can be unreliable on this problem; it is better to use a Bayesian parameter estimate, and ffl This estimate provides a desirable shrinkage effect that countless penalized regression methods have been searching for. 
From Hidden Markov Models to Linear Dynamical Systems| Abstract Hidden Markov Models (HMMs) and Linear Dynamical Systems (LDSs) are based on the same assumption: a hidden state variable, of which we can make noisy measurements, evolves with Markovian dynamics.  Both have the same independence diagram and consequently the learning and inference algorithms for both have the same structure.  The only difference is that the HMM uses a discrete state variable with arbitrary dynamics and arbitrary measurements while the LDS uses a continuous state variable with linearGaussian dynamics and measurements.  We show how the forward-backward equations for the HMM, specialized to linear-Gaussian assumptions, lead directly to Kalman filtering and Rauch-Tung-Streibel smoothing.  We also investigate the most general possible modeling assumptions which lead to ecient recursions in the case of continuous state variables. 
Independence Diagrams| The presentation is based on Pearl (1988). 
Modeling user subjectivity in image libraries| Abstract In addition to the problem of which image analysis models to use in digital libraries, e. g.  wavelet, Wold, color histograms, is the problem of how to combine these models with their different strengths.  Most present systems place the burden of combination on the user, e. g.  the user specifies 50% texture features, 20% color features, etc.  This is a problem since most users do not know how to best pick the settings for the given data and search problem.  This paper addresses this problem, describing research in progress for a system that (1) automatically infers which combination of models best represents the data of interest to the user and (2) learns continuously during interaction with each user.  In particular, these two components -- inference and learning -- provide a solution that adapts to the subjective and hard-to-predict behaviors frequently seen when people query or browse image libraries. 
Estimating a Dirichlet distribution|
Interactive Learning with a "Society of Models"|
Expectation-Propogation for the Generative Aspect Model|
Novelty and redundancy detection in adaptive filtering|
Algorithms for maximumlikelihood logistic regression|
An Image Database Browser that Learns from User Interaction",|
Interactive learning sing a `society of models',|
Vision text re for annotation,|
Tree-structured Approximations by Expectation Propagation|
Using lower bounds to approximate integrals|
Psychophysical evaluation for the performance of content-based image retrieval systems|
Expectation-Maximization as lower bound maximization|
Automatic choice of dimensionality for PCA|
Modelling user subjectivity in image libraries|
The EP energy function and minimization schemes|
Expectation Propagation for approximative Bayesian inference|
Pathologies of Orthodox Statistics,|
Variational Bayes for mixture models: Reversing EM|
Discriminative projections|
iVision texture for annotation,j|
Bayesian model averaging is not model combination|
Exemplar-based likelihoods using the pdf projection theorem|
Homepage|". 
Pattern Recognition, Interactive Learning using a Society of Models,|
A comparison of numerical optimizers for logistic regression|
Algorithms for maximum-likelihood logistic regression (Technical Report)|
A family of algorithm for approximate Bayesian inference|
Toward optimal search of image databases,"|
A society of models for video and image libraries|
Expectation propagation for infinite mixtures|

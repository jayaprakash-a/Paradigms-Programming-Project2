Managing the 802|11 Energy/Performance Tradeoff with Machine Learning.  Abstract---This paper addresses the problem of managing the tradeoff between energy consumption and performance in wireless devices implementing the IEEE 802. 11 standard [1].  To save energy, the 802. 11 specification proposes a power-saving mode (PSM), where a device can sleep to save energy, periodically waking up to receive packets from a neighbor (e. g. , an access point) that may have buffered packets for the sleeping device.  Previous work has shown that a fixed polling time for waking up degrades the performance of Web transfers [2], because network activity is bursty and time-varying.  We apply a new online machine learning algorithm to this problem and show, using ns-2 simulation and trace analysis, that it is able to adapt well to network activity.  The learning process makes no assumptions about the underlying network activity being stationary or even Markov.  Our learning power-saving algorithm, LPSM, guides the learning using a "loss function" that combines the increased latency from potentially sleeping too long and the wasted use of energy in waking up too soon.  In our ns-2 simulations, LPSM saved 7%-20% more energy than 802. 11 in powersaving mode, with an associated increase in average latency by a factor of 1. 02, and not more than 1. 2.  LPSM is straightforward to implement within the 802. 11 PSM framework. 
Physical network models and multi-source data integration| Abstract We develop a new framework for inferring models of transcriptional regulation.  The models in this approach, which we call physical models, are constructed on the basis of verifiable molecular attributes of the underlying biological system.  The attributes include, for example, the existence of protein-protein and protein-DNA interactions in gene regulatory processes, the directionality of signal transduction in protein-protein interactions, as well as the signs of the immediate effects of these interactions (e. g. , whether an upstream gen activates or represses the downstream genes).  Each attribute is included as a variable in the model, and the variables define a collection of annotated random graphs.  Possible configurations of these variables (realizations of the underlying biological system) are constrained by the available data sources.  Some of the data sources such as factor-binding data (location data) involve measurements that are directly tied to the variables in the model.  Other sources such as gene knock-outs are functional in nature and provide only indirect evidence about the (physical) variables.  We associate each knock-out effect in the deletion mutant data with a set of causal paths (molecular cascades) that could in principle explain the effect, resulting in aggregate constraints about the physical variables in the model.  The most likely setting of all the variables is found by the max-product algorithm.  By testing our approach on datasets related to the pheromone response pathway in S.  cerevisiae, we demonstrate that the resulting transcriptional models are consistent with previous studies about the pathway.  Moreover, we show that the approach is capable of predicting gene knock-out effects with high degree of accuracy in a cross-validation setting.  The method also implicates likely molecular cascades responsible for each observed knock-out effect.  The inference results are robust against variations in the model parameters.  We can extend the approach to include other data sources (solve the corresponding data association problems), including, for example, time course expression profiles.  We also discuss coordinated regulation and the use of automated experiment design. 
Continuous Representations of Time Series Gene Expression Data| Abstract We present algorithms for time-series gene expression analysis that permit the principled estimation of unobserved time-points, clustering, and dataset alignment.  Each expression profile is modeled as a cubic spline (piecewise polynomial) that is estimated from the observed data and every time point influences the overall smooth expression curve.  We constrain the spline coefficients of genes in the same class to have similar expression patterns, while also allowing for gene specific parameters.  We show that unobserved time-points can be reconstructed using our method with 10-15% less error when compared to previous best methods.  Our clustering algorithm operates directly on the continuous representations of gene expression profiles, and we demonstrate that this is particularly effective when applied to nonuniformly sampled data.  Our continuous alignment algorithm also avoids difficulties encountered by discrete approaches.  In particular, our method allows for control of the number of degrees of freedom of the warp through the specification of parameterized functions, which helps to avoid overfitting.  We demonstrate that our algorithm produces stable low-error alignments on real expression data and further show a specific application to yeast knockout data that produces biologically meaningful results. 
Tutorial on variational approximation methods| Abstract We provide an introduction to the theory and use of variational methods for inference and estimation in the context of graphical models.  Variational methods become useful as ecient approximate methods when the structure of the graph model no longer admits feasible exact probabilistic calculations.  The emphasis of this tutorial is on illustrating how inference and estimation problems can be transformed into variational form along with describing the resulting approximation algorithms and their properties insofar as these are currently known. 
Using Term Informativeness for Named Entity Detection| ABSTRACT Informal communication (e-mail, bulletin boards) poses a difficult learning environment because traditional grammatical and lexical information are noisy.  Other information is necessary for tasks such as named entity detection.  How topic-centric, or informative, a word is can be valuable information.  It is well known that informative words are best modeled by "heavy-tailed" distributions, such as mixture models.  However, informativeness scores do not take full advantage of this fact.  We introduce a new informativeness score that directly utilizes mixture model likelihood to identify informative words.  We use the task of extracting restaurant names from bulletin board posts as a way to determine effectiveness.  We find that our "mixture score" is weakly effective alone and highly effective when combined with Inverse Document Frequency.  We compare against other informativeness criteria and find that only Residual IDF is competitive against our combined IDF/Mixture score. 
Distributed Information Regularization on Graphs| Abstract We provide a principle for semi-supervised learning based on optimizing the rate of communicating labels for unlabeled points with side information.  The side information is expressed in terms of identities of sets of points or regions with the purpose of biasing the labels in each region to be the same.  The resulting regularization objective is convex, has a unique solution, and the solution can be found with a pair of local propagation operations on graphs induced by the regions.  We analyze the properties of the algorithm and demonstrate its performance on document classification tasks. 
Partially labeled classification with Markov random walks| Abstract To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples.  The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner.  We develop and compare several estimation criteria/algorithms suited to this representation.  This includes in particular multi-way classification with an average margin criterion which permits a closed form solution.  The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification.  We also extend this basic regularization by adapting time scales for individual examples.  We demonstrate the approach on synthetic examples and on text classification problems. 
K-ary Clustering with Optimal Leaf Ordering for Gene Expression Data| Abstract.  A major challenge in gene expression analysis is effective data organization and visualization.  One of the most popular tools for this task is hierarchical clustering.  Hierarchical clustering allows a user to view relationships in scales ranging from single genes to large sets of genes, while at the same time providing a global view of the expression data.  However, hierarchical clustering is very sensitive to noise, it usually lacks of a method to actually identify distinct clusters, and produces a large number of possible leaf orderings of the hierarchical clustering tree.  In this paper we propose a new hierarchical clustering algorithm which reduces susceptibility to noise, permits up to k siblings to be directly related, and provides a single optimal order for the resulting tree.  Our algorithm constructs a k-ary tree, where each node can have up to k children, and then optimally orders the leaves of that tree.  By combining k clusters at each step our algorithm becomes more robust against noise.  By optimally ordering the leaves of the tree we maintain the pairwise relationships that appear in the original method.  Our k-ary construction algorithm runs in O(n 3 ) regardless of k and our ordering algorithm runs in O(4 k+o(k) n 3 ).  We present several examples that show that our k-ary clustering algorithm achieves results that are superior to the binary tree results. 
Approximating Posterior Distributions in Belief Networks Using Mixtures| Abstract Exact inference in densely connected Bayesian networks is computationally intractable, and so there is considerable interest in developing effective approximation schemes.  One approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution.  While this leads to a tractable algorithm, the mean field distribution is assumed to be factorial and hence unimodal.  In this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions.  We derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks.  Our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased. 
Learning Without State-Estimation in Partially Observable Markovian Decision Processes| Abstract Reinforcement learning (RL) algorithms provide a sound theoretical basis for building learning control architectures for embedded agents.  Unfortunately all of the theory and much of the practice (see Barto et al. , 1983, for an exception) of RL is limited to Markovian decision processes (MDPs).  Many realworld decision tasks, however, are inherently non-Markovian, i. e. , the state of the environment is only incompletely known to the learning agent.  In this paper we consider only partially observable MDPs (POMDPs), a useful class of non-Markovian decision processes.  Most previous approaches to such problems have combined computationally expensive state-estimation techniques with learning control.  This paper investigates learning in POMDPs without resorting to any form of state estimation.  We present results about what TD(0) and Q-learning will do when applied to POMDPs.  It is shown that the conventional discounted RL framework is inadequate to deal with POMDPs.  Finally we develop a new framework for learning without state-estimation in POMDPs by including stochastic policies in the search space, and by defining the value or utility of a distribution over states. 
Stable Mixing of Complete and lncomplete lnformation| 2001 massachusetts institute of technology, cambridge, ma 02139 usa --- www. ai. mit. edu massachusetts institute of technology --- artificial intelligence laboratory @ MIT Abstract An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete.  Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations.  We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting.  This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data.  The approach readily applies to any graphical model in O(n 3 ) time where n is the number of parameters.  We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems. 
Tractable Bayesian Learning of Tree Belief Networks| Abstract In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time.  This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form.  Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al. , 1995) constrain the tree parameter priors to be a compactly parametrized product of Dirichlet distributions.  Besides allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures. 
A Discriminative Framework for Detecting Remote Protein Homologies| Abstract A new method for detecting remote protein homologies is introduced and shown to perform well in classifying protein domains by SCOP superfamily.  The method is a variant of support vector machines using a new kernel function.  The kernel function is derived from a generative statistical model for a protein family, in this case a hidden Markov model.  This general approach of combining generative models like HMMs with discriminative methods such as support vector machines may have applications in other areas of biosequence analysis as well. 
An Introduction to Variational Methods for Graphical Models| Abstract.  This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields).  We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms.  We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient.  Inference in the simpified model provides bounds on probabilities of interest in the original model.  We describe a general framework for generating variational transformations based on convex duality.  Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case. 
Kernel Expansions with Unlabeled Examples| Abstract Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance.  We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification.  This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples.  The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution.  We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework.  We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy. 
Learning in Graphical Models, Kluwer Academic Publishers| Abstract.  Mean field methods provide computationally efficient approximations to posterior probability distributions for graphical models.  Simple mean field methods make a completely factorized approximation to the posterior, which is unlikely to be accurate when the posterior is multimodal.  Indeed, if the posterior is multi-modal, only one of the modes can be captured.  To improve the mean field approximation in such cases, we employ mixture models as posterior approximations, where each mixture component is a factorized distribution.  We describe efficient methods for optimizing the parameters in these models. 
Fast Learning by Bounding Likelihoods in Sigmoid Type Belief Networks| Abstract Sigmoid type belief networks, a class of probabilistic neural networks, provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and supervised learning problems.  Often the parameters used in these networks need to be learned from examples.  Unfortunately, estimating the parameters via exact probabilistic calculations (i. e, the EM-algorithm) is intractable even for networks with fairly small numbers of hidden units.  We propose to avoid the infeasibility of the E step by bounding likelihoods instead of computing them exactly.  We introduce extended and complementary representations for these networks and show that the estimation of the network parameters can be made fast (reduced to quadratic optimization) by performing the estimation in either of the alternative domains.  The complementary networks can be used for continuous density estimation as well. 
Large-Margin Matrix Factorization| Abstract We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations.  The approach is inspired by, and has strong connections to, large-margin linear discrimination.  We show how to learn low-norm factorizations by solving a semi-definite program, and present generalization error bounds based on analyzing the Rademacher complexity of low-norm factorizations. 
Bias-Corrected Bootstrap and Model Uncertainty| Abstract The bootstrap has become a popular method for exploring model (structure) uncertainty.  Our experiments with artificial and realworld data demonstrate that the graphs learned from bootstrap samples can be severely biased towards too complex graphical models.  Accounting for this bias is hence essential, e. g. , when exploring model uncertainty.  We find that this bias is intimately tied to (well-known) spurious dependences induced by the bootstrap.  The leading-order bias-correction equals one half of Akaike's penalty for model complexity.  We demonstrate the effect of this simple bias-correction in our experiments.  We also relate this bias to the bias of the plug-in estimator for entropy, as well as to the difference between the expected test and training errors of a graphical model, which asymptotically equals Akaike's penalty (rather than one half). 
Using the Fisher Kernel Method to Detect Remote Protein Homologies| Abstract A new method, called the Fisher kernel method, for detecting remote protein homologies is introduced and shown to perform well in classifying protein domains by SCOP superfamily.  The method is a variant of support vector machines using a new kernel function.  The kernel function is derived from a hidden Markov model.  The general approach of combining generative models like HMMs with discriminative methods such as support vector machines may have applications in other areas of biosequence analysis as well. 
On the Dirichlet Prior and Bayesian Regularization| Abstract A common objective in learning a model from data is to recover its network structure, while the model parameters are of minor interest.  For example, we may wish to recover regulatory networks from high-throughput data sources.  In this paper we examine how Bayesian regularization using a product of independent Dirichlet priors over the model parameters affects the learned model structure in a domain with discrete variables.  We show that a small scale parameter { often interpreted as "equivalent sample size" or "prior strength" { leads to a strong regularization of the model structure (sparse graph) given a suciently large data set.  In particular, the empty graph is obtained in the limit of a vanishing scale parameter.  This is diametrically opposite to what one may expect in this limit, namely the complete graph from an (unregularized) maximum likelihood estimate.  Since the prior affects the parameters as expected, the scale parameter balances a trade-off between regularizing the parameters vs.  the structure of the model.  We demonstrate the benefits of optimizing this trade-off in the sense of predictive accuracy. 
Reinforcement Learning with Soft State Aggregation| Abstract It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems.  Unfortunately almost all of the theory of reinforcement learning assumes lookup table representations.  In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approximator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation.  Preliminary empirical results are also presented. 
A New Class of upper Bounds on the Log Partition Function| Abstract Bounds on the log partition function are important in a variety of contexts, including approximate inference, model fitting, decision theory, and large deviations analysis [11, 5, 4].  We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model.  In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe free energy, but distinguished by the following desirable properties: (i) they are convex, and have a unique global minimum; and (ii) the global minimum gives an upper bound on the log partition function.  The global minimum is defined by stationary conditions very similar to those de#ning fixed points of belief propagation (BP) or tree-based reparameterization [see 13, 14].  As with BP fixed points, the elements of the minimizing argument can be used as approximations to the marginals of the original model.  The analysis described here can be extended to more structured approximations (e. g. , region graph and variants [15, 10]). 
Linear Dependent Dimensionality Reduction| Abstract We formulate linear dimensionality reduction as a semi-parametric estimation problem,
Bayesian parameter estimation via variational methods| Abstract We consider a logistic regression model with a Gaussian prior distribution over the parameters.  We show that an accurate variational transformation can be used to obtain a closed form approximation to the posterior distribution of the parameters thereby yielding an approximate posterior predictive model.  This approach is readily extended to binary graphical model with complete observations.  For graphical models with incomplete observations we utilize an additional variational transformation and again obtain a closed form approximation to the posterior.  Finally, we show that the dual of the regression problem gives a latent variable density model, the variational formulation of which leads to exactly solvable EM updates. 
Exploiting Generative Models in Discriminative Classifiers| Abstract Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences.  On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches.  An ideal classifier should combine these two complementary approaches.  In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models.  We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis. 
Recursive Algorithms for Approximating Probabilities in Graphical Models| Abstract We develop a recursive node-elimination formalism for efficiently approximating large probabilistic networks.  No constraints are set on the network topologies.  Yet the formalism can be straightforwardly integrated with exact methods whenever they are/become applicable.  The approximations we use are controlled: they maintain consistently upper and lower bounds on the desired quantities at all times.  We show that Boltzmann machines, sigmoid belief networks, or any combination (i. e. , chain graphs) can be handled within the same framework.  The accuracy of the methods is verified experimentally. 
Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems| Abstract Increasing attention has been paid to reinforcement learning algorithms in recent years, partly due to successes in the theoretical analysis of their behavior in Markov environments.  If the Markov assumption is removed, however, neither generally the algorithms nor the analyses continue to be usable.  We propose and analyze a new learning algorithm to solve a certain class of non-Markov decision problems.  Our algorithm applies to problems in which the environment is Markov, but the learner has restricted access to state information.  The algorithm involves a Monte-Carlo policy evaluation combined with a policy improvement method that is similar to that of Markov decision problems and is guaranteed to converge to a local maximum.  The algorithm operates in the space of stochastic policies, a space which can yield a policy that performs considerably better than any deterministic policy.  Although the space of stochastic policies is continuous---even for a discrete action space---our algorithm is computationally tractable. 
Weighted Low-Rank Approximations| Abstract We study the common problem of approximating a target matrix with a matrix of lower rank.  We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closedform solution in general.  We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to nonGaussian noise models such as logistic models.  Finally, we apply the methods developed to a collaborative filtering task. 
Generalized Low-Rank Approximations| Abstract We study the frequent problem of approximating a target matrix with a matrix of lower rank.  We provide a simple and efficient (EM) algorithm for solving weighted low rank approximation problems, which, unlike simple matrix factorization problems, do not admit a closed form solution in general.  We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low rank representation, and extend the formulation to non-Gaussian noise models such as classification (collaborative filtering). 
A new approach to analyzing gene expression time series data| Abstract We present algorithms for time-series gene expression analysis that permit the principled estimation of unobserved timepoints, clustering, and dataset alignment.  Each expression profile is modeled as a cubic spline (piecewise polynomial) that is estimated from the observed data and every time point influences the overall smooth expression curve.  We constrain the spline coefficients of genes in the same class to have similar expression patterns, while also allowing for gene specific parameters.  We show that unobserved time-points can be reconstructed using our method with 10-15% less error when compared to previous best methods.  Our clustering algorithm operates directly on the continuous representations of gene expression profiles, and we demonstrate that this is particularly effective when applied to non-uniformly sampled data.  Our continuous alignment algorithm also avoids difficulties encountered by discrete approaches.  In particular, our method allows for control of the number of degrees of freedom of the warp through the specification of parameterized functions, which helps to avoid overfitting.  We demonstrate that our algorithm produces stable low-error alignments on real expression data and further show a specific application to yeast knockout data that produces biologically meaningful results. 
Fast optimal leaf ordering for hierarchical clustering| ABSTRACT We present the first practical algorithm for the optimal linear leaf ordering of trees that are generated by hierarchical clustering.  Hierarchical clustering has been extensively used to analyze gene expression data, and we show how optimal leaf ordering can reveal biological structure that is not observed with an existing heuristic ordering method.  For a tree with n leaves, there are 2 n-1 linear orderings consistent with the structure of the tree.  Our optimal leaf ordering algorithm runs in time O(n 4 ), and we present further improvements that make the running time of our algorithm practical. 
Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms| Abstract.  An important application of reinforcement learning (RL) is to
Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching| Abstract In previous work [10], we presented a class of upper bounds on the log partition function of an arbitrary undirected graphical model based on solving a convex variational problem.  Here we develop a class of local message-passing algorithms, which we call tree-reweighted belief propagation, for ef#ciently computing the value of these upper bounds, as well as the associated pseudomarginals.  We also consider the uses of our bounds for the problem of maximum likelihood (ML) parameter estimation.  For a completely observed model, our analysis gives rise to a concave lower bound on the log likelihood of the data.  Maximizing this lower bound yields an approximate ML estimate which, in analogy to the moment-matching of exact ML estimation, can be interpreted in terms of pseudo-moment-matching.  We present preliminary results illustrating the behavior of this approximate ML estimator. 
Mean Field Theory for Sigmoid Belief Networks| Abstract We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics.  Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence.  We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits. 
Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices| Abstract We prove generalization error bounds for predicting entries in a partially observed matrix by fitting the observed entries with a low-rank matrix.  In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of finite pseudodimension such that the sums of functions from this class have unbounded pseudodimension. 
Using Unlabeled Data to Improve Text| The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the NSF, DARPA, the U. S.  government or any other entity. 
Exact MAP Estimates by (Hyper)tree Agreement| Abstract We describe a method for computing provably exact maximum a posteriori (MAP) estimates for a subclass of problems on graphs with cycles.  The basic idea is to represent the original problem on the graph with cycles as a convex combination of tree-structured problems.  A convexity argument then guarantees that the optimal value of the original problem (i. e. , the log probability of the MAP assignment) is upper bounded by the combined optimal values of the tree problems.  We prove that this upper bound is met with equality if and only if the tree problems share an optimal configuration in common.  An important implication is that any such shared configuration must also be the MAP configuration for the original problem.  Next we develop a tree-reweighted max-product algorithm for attempting to find convex combinations of tree-structured problems that share a common optimum.  We give necessary and sufficient conditions for a fixed point to yield the exact MAP estimate.  An attractive feature of our analysis is that it generalizes naturally to convex combinations of hypertree-structured distributions. 
Combining Location and Expression Data for Principled Discovery of Genetic Regulatory Network Models| We develop principled methods for the automatic induction (discovery) of genetic regulatory network models from multiple data sources and data modalities.  Models of regulatory networks are represented as Bayesian networks, allowing the models to compactly and robustly capture probabilistic multivariate statistical dependencies between the various cellular factors in these networks.  We build on previous Bayesian network validation results by extending the validation framework to the context of model induction, leveraging heuristic simulated annealing search algorithms and posterior model averaging.  Using expression data in isolation yields results inconsistent with location data so we incorporate genomic location data to guide the model induction process.  We combine these two data modalities by allowing location data to influence the model prior and expression data to influence the model likelihood.  We demonstrate the utility of this approach by discovering genetic regulatory models of thirty-three variables involved in S.  cerevisiae pheromone response.  The models we automatically generate are consistent with the current understanding regarding this regulatory network, but also suggest new directions for future experimental investigation. 
Using Graphical Models and Genomic Expression Data to Statistically Validate Models of Genetic Regulatory Networks| We propose a model-driven approach for analyzing genomic expression data that permits genetic regulatory networks to be represented in a biologically interpretable computational form.  Our models permit latent variables capturing unobserved factors, describe arbitrarily complex (more than pair-wise) relationships at varying levels of refinement, and can be scored rigorously against observational data.  The models that we use are based on Bayesian networks and their extensions.  As a demonstration of this approach, we utilize 52 genomes worth of Affymetrix GeneChip expression data to correctly differentiate between alternative hypotheses of the galactose regulatory network in S.  cerevisiae.  When we extend the graph semantics to permit annotated edges, we are able to score models describing relationships at a finer degree of specification. 
Variational Probabilistic Inference and the QMR-DT Network| Abstract We describe a variational approximation method for efficient inference in large-scale probabilistic models.  Variational methods are deterministic procedures that provide approximations to marginal and conditional probabilities of interest.  They provide alternatives to approximate inference methods based on stochastic sampling or search.  We describe avariational approach to the problem of diagnostic inference in the \Quick Medical Reference" (QMR) network.  The QMR network is a large-scale probabilistic graphical model built on statistical and expert knowledge.  Exact probabilistic inference is infeasible in this model for all but a small set of cases.  Weevaluate our variational inference algorithm on a large set of diagnostic test cases, comparing the algorithm to a state-of-the-art stochastic sampling method. 
Maximum likelihood estimation of optimal scaling factors for expression array normalization| ABSTRACT Data from expression arrays must be comparable before it can be analyzed rigorously on a large scale.  Accurate normalization improves the comparability of expression data because it seeks to account for sources of variation obscuring the underlying variation of interest.  Undesirable variation in reported expression levels originates in the preparation and hybridization of the sample as well as in the manufacture of the array itself, and may differ depending on the array technology being employed.  Published research to date has not characterized the degree of variation associated with these sources, and results are often reported without tight statistical bounds on their significance.  We analyze the distributions of reported levels of exogenous control species spiked into samples applied to 1280 Affymetrix arrays.  We develop a model for explaining reported expression levels under an assumption of primarily multiplicative variation.  To compute the scaling factors needed for normalization, we derive maximum likelihood and maximum a posteriori estimates for the parameters characterizing the multiplicative variation in reported spiked control expression levels.  We conclude that the optimal scaling factors in this context are weighted geometric means and determine the appropriate weights.  The optimal scaling factor estimates so computed can be used for subsequent array normalization. 
Computing upper and lower bounds on likelihoods in intractable networks| Abstract We present deterministic techniques for computing upper and lower bounds on marginal probabilities in sigmoid and noisy-OR networks.  These techniques become useful when the size of the network (or clique size) precludes exact computations.  We illustrate the tightness of the bounds by numerical experiments. 
Variational methods and the QMR-DT database| Abstract We describe variational approximation methods for efficient probabilistic reasoning, applying these methods to the problem of diagnostic inference in the QMR-DT database.  The QMR-DT database is a large-scale belief network based on statistical and expert knowledge in internal medicine.  The size and complexity of this network render exact probabilistic diagnosis infeasible for all but a small set of cases.  This has hindered the development of the QMRDT network as a practical diagnostic tool and has hindered researchers from exploring and critiquing the diagnostic behavior of QMR.  In this paper we describe how variational approximation methods can be applied to the QMR network, resulting in fast diagnostic inference.  We evaluate the accuracy of our methods on a set of standard diagnostic cases and compare to stochastic sampling methods. 
Feature Selection and Dualities in Maximum Entropy Discrimination| Abstract Incorporating feature selection into a classification or regression method often carries a number of advantages.  In this paper we formalize feature selection specifically from a discriminative perspective of improving classification/regression accuracy.  The feature selection method is developed as an extension to the recently proposed maximum entropy discrimination (MED) framework.  We describe MED as a flexible (Bayesian) regularization approach that subsumes, e. g. , support vector classification, regression and exponential family models.  For brevity, we restrict ourselves primarily to feature selection in the context of linear classification/regression methods and demonstrate that the proposed approach indeed carries substantial improvements in practice.  Moreover, we discuss and develop various extensions of feature selection, including the problem of dealing with example specific but unobserved degrees of freedom -- alignments or invariants. 
Online Learning of Non-stationary Sequences| Abstract We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts.  We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts.  On the basis of the performance bounds we provide the optimal a priori discretization for learning the parameter that governs the switching dynamics.  We demonstrate the new algorithm in the context of wireless networks. 
Probabilistic kernel regression models| Abstract We introduce a class of flexible conditional probability models and techniques for classification/regression problems.  Many existing methods such as generalized linear models and support vector machines are subsumed under this class.  The flexibility of this class of techniques comes from the use of kernel functions as in support vector machines, and the generality from dual formulations of standard regression models. 
Tree-based reparameterization for approximate inference on loopy graphs| Abstract We develop a tree-based reparameterization framework that provides a new conceptual view of a large class of iterative algorithms for computing approximate marginals in graphs with cycles.  It includes belief propagation (BP), which can be reformulated as a very local form of reparameterization.  More generally, we consider algorithms that perform exact computations over spanning trees of the full graph.  On the practical side, we find that such tree reparameterization (TRP) algorithms have convergence properties superior to BP.  The reparameterization perspective also provides a number of theoretical insights into approximate inference, including a new characterization of fixed points; and an invariance intrinsic to TRP/BP.  These two properties enable us to analyze and bound the error between the TRP/BP approximations and the actual marginals.  While our results arise naturally from the TRP perspective, most of them apply in an algorithm-independent manner to any local minimum of the Bethe free energy.  Our results also have natural extensions to more structured approximations [e. g. , 1, 2]. 
Information Regularization with Partially Labeled Data| Abstract Classification with partially labeled data requires using a large number of unlabeled examples (or an estimated marginal P (x)), to further constrain the conditional P (y|x) beyond a few available labeled examples.  We formulate a regularization approach to linking the marginal and the conditional in a general way.  The regularization penalty measures the information that is implied about the labels over covering regions.  No parametric assumptions are required and the approach remains tractable even for continuous marginal densities P (x).  We develop algorithms for solving the regularization problem for finite covers, establish a limiting differential equation, and exemplify the behavior of the new regularization approach in simple cases. 
Maximum Entropy Discrimination| Abstract We present a general framework for discriminative estimation based on the maximum entropy principle and its extensions.  All calculations involve distributions over structures and/or parameters rather than specific settings and reduce to relative entropy projections.  This holds even when the data is not separable within the chosen parametric class, in the context of anomaly detection rather than classification, or when the labels in the training set are uncertain or incomplete.  Support vector machines are naturally subsumed under this class and we provide several extensions.  We are also able to estimate exactly and eciently discriminative distributions over tree structures of class-conditional models within this framework.  Preliminary experimental results are indicative of the potential in these techniques. 
Convergence of Stochastic Iterative Dynamic Programming Algorithms| Abstract Increasing attention has recently been paid to algorithms based on dynamic programming (DP) due to the suitability of DP for learning problems involving control.  In stochastic environments where the system being controlled is only incompletely known, however, a unifying theoretical account of the behavior of these methods has been missing.  In this paper we relate DP-based learning algorithms to powerful techniques of stochastic approximation via a new convergence theorem, enabling us to establish a class of convergent algorithms to which both TD(#) and Q-learning belong. 
On the Convergence of Stochastic Iterative Dynamic Programming Algorithms| Abstract Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments.  These algorithms, including the TD( ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP).  In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem.  The theorem establishes a general class of convergent algorithms to which both TD( ) and Q-learning belong.  An important component of many real world learning problems is the temporal credit assignment problem---the problem of assigning credit or blame to individual components of a temporally-extended plan of action, based on the success or failure of the plan as a whole.  To solve such a problem, the learner must be equipped with the ability to assess the long-term consequences of particular choices of action and must be willing to forego an immediate payoff for the prospect of a longer term gain.  Moreover, because most real world problems involving prediction of the future consequences of actions involve substantial uncertainty, the learner must be prepared to make use of a probability calculus for assessing and comparing actions.  There has been increasing interest in the temporal credit assignment problem, due principally to the development of learning algorithms based on the theory of dynamic programming (DP) (Barto, Sutton,
A variational approach to Bayesian logistic regression models and their extensions| Abstract We consider a logistic regression model with a Gaussian prior distribution over the parameters.  We show that accurate variational techniques can be used to obtain a closed form posterior distribution over the parameters given the data thereby yielding a posterior predictive model.  The results are straightforwardly extended to (binary) belief networks.  For the belief networks we also derive closed form parameter posteriors in the presence of missing values.  We show #nally that the dual of the regression problem gives a latent variable density model the variational formulation of which leads to exactly solvable EM updates. 
Serial regulation of transcriptional regulators in the yeast cell cycle|
Computational discovery of gene modules and regulatory networks|
in press)| Convergence results for single-step on-policy reinforcement learning algorithms. 
Model-free reinforcement learning for non-Markovian decision problems|
Continuation Methods for Mixing Heterogenous Sources|
Tree consistency and bounds on the maxproduct algorithm and its generalizations|
Bayesian Methods for Elucidating Genetic Regulatory Networks|
Tree-based reparameterization framework for analysis of sum-product and related algorithms|
Exact MAP estimates via agreement on (hyper)trees: Linear programming and message-passing approaches|
An Algorithm for Protecting Knowledge Discovery Data|
Tree-based reparameterization framework for approximate estimation of stochastic processes on graphs with cycles|
Semi-)predictive discretization during model selection|
Optimization by direct search and systematic reduction of the size of the search region,|
Variational methods and the QMR-DT database| MIT Computational Cognitive Science. 
Probabilistic kernel methods|
Machine learning seminar notes,|
Continuous representations of time-series gene expression data|
Comparing the continuous representation of time-series expression profiles to identify dierentially expressed genes|
Bayesian network approach to cell signaling pathway modeling|
Tree-based reparameterization analysis of sum--product and its generalizations|
On Information Regularization|
Advances in Neural Information Processing Systems 11|
Optimisation of Non-Linear Functions Subject to Equality Constraints|
"Direct Search for Complex Systems,"|
Expoloting generative models in discriminating classifiers|
Comparison ofsinteringbehaviourandresultingmicrostructureofpre-heatedandreactionsintered Nd-dopedBa 2|
Information Modeling and Knowledge Bases: Foundations,|

Max-norm Projections for Factored MDPs| Abstract Markov Decision Processes (MDPs) provide a coherent mathematical framework for planning under uncertainty. 
Solving Factored MDPs with Continuous and Discrete Variables| Abstract Although many real-world stochastic planning problems are more naturally formulated by hybrid models with both discrete and continuous variables, current state-of-the-art methods cannot adequately address these problems.  We present the first framework that can exploit problem structure for modeling and solving hybrid problems efficiently.  We formulate these problems as hybrid Markov decision processes (MDPs with continuous and discrete state and action variables), which we assume can be represented in a factored way using a hybrid dynamic Bayesian network (hybrid DBN).  This formulation also allows us to apply our methods to collaborative multiagent settings.  We present a new linear program approximation method that exploits the structure of the hybrid MDP and lets us compute approximate value functions more efficiently.  In particular, we describe a new factored discretization of continuous variables that avoids the exponential blow-up of traditional approaches.  We provide theoretical bounds on the quality of such an approximation and on its scale-up potential.  We support our theoretical arguments with experiments on a set of control problems with up to 28-dimensional continuous state space and 22-dimensional action space. 
Using Probabilistic Models for Data Management in Acquisitional Environments| Abstract Traditional database systems, particularly those focused on capturing and managing data from the real world, are poorly equipped to deal with the noise, loss, and uncertainty in data.  We discuss a suite of techniques based on probabilistic models that are designed to allow database to tolerate noise and loss.  These techniques are based on exploiting correlations to predict missing values and identify outliers.  Interestingly, correlations also provide a way to give approximate answers to users at a significantly lower cost and enable a range of new types of queries over the correlation structure itself.  We illustrate a host of applications for our new techniques and queries, ranging from sensor networks to network monitoring to data stream management.  We also present a unified architecture for integrating such models into database systems, focusing in particular on acquisitional systems where the cost of capturing data (e. g. , from sensors) is itself a significant part of the query processing cost. 
Efficient Solution Algorithms for Factored MDPs| Abstract This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). 
Generalizing Plans to New Environments in Relational MDPs| Abstract A longstanding goal in planning research is the ability to generalize plans developed for some set of environments to a new but similar environment, with minimal or no replanning.  Such generalization can both reduce planning time and allow us to tackle larger domains than the ones tractable for direct planning.  In this paper, we present an approach to the generalization problem based on a new framework of relational Markov Decision Processes (RMDPs).  An RMDP
A Robust Architecture for Distributed Inference in Sensor Networks| Abstract--- Many inference problems that arise in sensor networks require the computation of a global conclusion that is consistent with local information known to each node.  A large class of these problems--including probabilistic inference, regression, and control problems---can be solved by message passing on a data structure called a junction tree.  In this paper, we present a distributed architecture for solving these problems that is robust to unreliable communication and node failures.  In this architecture, the nodes of the sensor network assemble themselves into a junction tree and exchange messages between neighbors to solve the inference problem efficiently and exactly.  A key part of the architecture is an efficient distributed algorithm for optimizing the choice of junction tree to minimize the communication and computation required by inference.  We present experimental results from a prototype implementation on a 97-node Mica2 mote network, as well as simulation results for three applications: distributed sensor calibration, optimal control, and sensor field modeling.  These experiments demonstrate that our distributed architecture can solve many important inference problems exactly, efficiently, and robustly. 
Concurrent Hierarchical Reinforcement Learning| Abstract We describe a language for partially specifying policies in domains consisting of multiple subagents working together to maximize a common reward function.  The language extends ALisp with constructs for concurrency and dynamic assignment of subagents to tasks.  During learning, the subagents learn a distributed representation of the Q-function for this partial policy.  They then coordinate at runtime to find the best joint action at each step.  We give examples showing that programs in this language are natural and concise.  We also describe online and batch learning algorithms for learning a linear approximation to the Q-function, which make use of the coordination structure of the problem. 
Distributed Regression: an Efficient Framework for Modeling Sensor Network Data| ABSTRACT We present distributed regression, an efficient and general framework for in-network modeling of sensor data.  In this framework, the nodes of the sensor network collaborate to optimally fit a global function to each of their local measurements.  The algorithm is based upon kernel linear regression, where the model takes the form of a weighted sum of local basis functions; this provides an expressive yet tractable class of models for sensor network data.  Rather than transmitting data to one another or outside the network, nodes communicate constraints on the model parameters, drastically reducing the communication required.  After the algorithm is run, each node can answer queries for its local region, or the nodes can efficiently transmit the parameters of the model to a user outside the network.  We present an evaluation of the algorithm based upon data from a 48-node sensor network deployment at the Intel Research - Berkeley Lab, demonstrating that our distributed algorithm converges to the optimal solution at a fast rate and is very robust to packet losses. 
Copyright 2004, Intel Corporation, All rights reserved| Distributed Regression: an Efficient Framework for Modeling Sensor Network Data.  DISCLAIMER: THIS DOCUMENT IS PROVIDED TO YOU "AS IS" WITH NO WARRANTIES WHATSOEVER, INCLUDING ANY WARRANTY OF MERCHANTABILITY NON-INFRINGEMENT,
Optimal Nonmyopic Value of Information in Graphical Models -Efficient Algorithms and Theoretical Limits| Abstract Many real-world decision making tasks require us to choose among several expensive observations.  In a
Algorithm-Directed Exploration for Model-Based Reinforcement Learning in Factored MDPs| Abstract One of the central challenges in reinforcement learning is to balance the exploration/exploitation tradeoff while scaling up to large problems.  Although model-based reinforcement learning has been less prominent than value-based methods in addressing these challenges, recent progress has generated renewed interest in pursuing modelbased approaches: Theoretical work on the exploration/exploitation tradeoff has yielded provably sound model-based algorithms such as E 3 and Rmax , while work on factored MDP representations has yielded model-based algorithms that can scale up to large problems.  Recently the benefits of both achievements have been combined in the Factored E 3 algorithm of Kearns and Koller.  In this paper, we address a significant shortcoming of Factored E 3 : namely that it requires an oracle planner that cannot be feasibly implemented.  We propose an alternative approach that uses a practical approximate planner, approximate linear programming, that maintains desirable properties.  Further, we develop an exploration strategy that is targeted toward improving the performance of the linear programming algorithm, rather than an oracle planner.  This leads to a simple exploration strategy that visits states relevant to tightening the LP solution, and achieves sample efficiency logarithmic in the size of the problem description.  Our experimental results show that the targeted approach performs better than using approximate planning for implementing either Factored E 3 or Factored Rmax . 
Greedy Linear Value-Approximation for Factored Markov Decision Processes| Abstract Significant recent work has focused on using linear representations to approximate value functions for factored Markov decision processes (MDPs). 
Distributed Planning in Hierarchical Factored MDPs| We present a principled and efficient planning algorithm for collaborative multiagent dynamical systems.  All computation, during both the planning and the execution phases, is distributed among the agents; each agent only needs to model and plan for a small part of the system.  Each of these local subsystems is small, but once they are combined they can represent an exponentially larger problem.  The subsystems are connected through a subsystem hierarchy.  Coordination and communication between the agents is not imposed, but derived directly from the structure of this hierarchy.  A globally consistent plan is achieved by a message passing algorithm, where messages correspond to natural local reward functions and are computed by local linear programs; another message passing algorithm allows us to execute the resulting policy.  When two portions of the hierarchy share the same structure, our algorithm can reuse plans and messages to speed up computation. 
Multiagent Planning with Factored MDPs| Abstract We present a principled and efficient planning algorithm for cooperative multiagent dynamic systems.  A striking feature of our method is that the coordination and communication between the agents is not imposed, but derived directly from the system dynamics and function approximation architecture.  We view the entire multiagent system as a single, large Markov decision process (MDP), which we assume can be represented in a factored way using a dynamic Bayesian network (DBN).  The action space of the resulting MDP is the joint action space of the entire set of agents.  Our approach is based on the use of factored linear value functions as an approximation to the joint value function.  This factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme.  We provide a simple and efficient method for computing such an approximate value function by solving a single linear program, whose size is determined by the interaction between the value function structure and the DBN.  We thereby avoid the exponential blowup in the state and action space.  We show that our approach compares favorably with approaches based on reward sharing.  We also show that our algorithm is an efficient alternative to more complicated algorithms even in the single agent case. 
Context-Specific Multiagent Coordination and Planning with Factored MDPs| Abstract We present an algorithm for coordinated decision making in cooperative multiagent settings, where the agents' value function can be represented as a sum of context-specific value rules.  The task of finding an optimal joint action in this setting leads to an algorithm where the coordination structure between agents depends on the current state of the system and even on the actual numerical values assigned to the value rules.  We apply this framework to the task of multiagent planning in dynamic systems, showing how a joint value function of the associated Markov Decision Process can be approximated as a set of value rules using an efficient linear programming algorithm.  The agents then apply the coordination graph algorithm at each iteration of the process to decide on the highest-value joint action, potentially leading to a different coordination pattern at each step of the plan. 
Using robotics to fold proteins and dock ligands| Abstract The problems of protein folding and ligand docking have been explored largely using molecular dynamics or Monte Carlo methods.  These methods are very compute intensive because they often explore a much wider range of energies, conformations and time than necessary.  In addition, Monte Carlo methods often get trapped in local minima.  We initially showed that robotic motion planning permitted one to determine the energy of binding and dissociation of ligands from protein binding sites (Singh et al. , 1999).  The robotic motion planning method maps complicated three-dimensional conformational states into a much simpler, but higher dimensional space in which conformational rearrangements can be represented as linear paths.  The dimensionality of the conformation space is of the same order as the number of degrees of conformational freedom in three-dimensional space.  We were able to determine the relative energy of association and dissociation of a ligand to a protein by calculating the energetics of interaction for a few thousand conformational states in the vicinity of the protein and choosing the best path from the roadmap.  More recently, we have applied roadmap planning to the problem of protein folding (Apaydin et al. , 2002a).  We represented multiple conformations of a protein as nodes in a compact graph with the edges representing the probability of moving between neighboring states.  Instead of using Monte Carlo simulation to simulate thousands of possible paths through various conformational states, we were able to use Markov methods to calculate the steady state occupancy of each conformation, needing to calculate the energy of each conformation
Coordinated Reinforcement Learning| Abstract We present several new algorithms for multiagent reinforcement learning.  A common feature of these algorithms is a parameterized, structured representation of a policy or value function.  This structure is leveraged in an approach we call coordinated reinforcement learning, by which agents coordinate both their action selection activities and their parameter updates.  Within the limits of our parametric representations, the agents will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space.  Our methods differ from many previous reinforcement learning approaches to multiagent coordination in that structured communication and coordination between agents appears at the core of both the learning algorithm and the execution architecture.  Our experimental results, comparing our approach to other RL methods, illustrate both the quality of the policies obtained and the additional benefits of coordination. 
Max-Margin Markov Networks| Abstract In typical classification tasks, we seek a function which assigns a label to a single object. 
Model-Driven Data Acquisition in Sensor Networks| Abstract Declarative queries are proving to be an attractive paradigm for interacting with networks of wireless sensors.  The metaphor that "the sensornet is a database"
Stochastic Roadmap Simulation: An Efficient Representation and Algorithm for Analyzing Molecular Motion| Abstract Classic techniques for simulating molecular motion, such as the Monte Carlo and molecular dynamics methods, generate individual motion pathways one at a time and spend most of their time trying to escape from the local minima of the energy landscape of a molecule.  Their high computational cost prevents them from being used to analyze many pathways.  We introduce Stochastic Roadmap Simulation (SRS), a new approach for exploring the kinetics of molecular motion by simultaneously examining multiple pathways encoded compactly in a graph, called a roadmap.  A roadmap is computed by sampling a molecule's conformation space at random.  The computation does not suffer from the localminima problem encountered with existing methods.  Each path in the roadmap represents a potential motion pathway and is associated with a probability indicating the likelihood that the molecule follows this pathway.  By viewing the roadmap as a Markov chain, we can efficiently compute kinetic properties of molecular motion over the entire molecular energy landscape.  We also prove that, in the limit, SRS converges to the same distribution as Monte Carlo simulation.  To test the effectiveness of our approach, we apply it to the computation of the transmission coefficients for protein folding, an important order parameter that measures the "kinetic distance" of a protein's conformation to its native state.  Our computational studies show that SRS obtains more accurate results and achieves several orders-of-magnitude reduction in computation time, compared with Monte Carlo simulation. 
Robust Combination of Local Controllers| Abstract Finding solutions to high dimensional Markov Decision Processes (MDPs) is a dicult problem, especially in the presence of uncertainty or if the actions and time measurements are continuous.  Frequently this diculty can be alleviated by the availability of problem-specific knowledge.  For example, it may be relatively easy to design controllers that are good locally, though having no global guarantees.  We propose a nonparametric method to combine these local controllers to obtain globally good solutions.  We apply this formulation to two types of problems: motion planning (stochastic shortest path problems) and discounted-cost MDPs.  For motion planning, we argue that only considering the expected cost of a path may be overly simplistic in the presence of uncertainty.  We propose an alternative: finding the minimum cost path, subject to the constraint that the robot must reach the goal with high probability.  For this problem, we prove that a polynomial number of samples is sucient to obtain a high probability path.  For discounted MDPs, we consider various problem formulations that explicitly deal with model uncertainty.  We provide empirical evidence of the usefulness of these approaches using the control of a robot arm. 
Solving factored POMDPs with linear value functions|
Max-norm projection for factored MDPs|
Stochastic conformational roadmaps for computing ensemble properties of molecular motion|
Intel lab data| Web Page.  http://berkeley. intel-research. 
Exploiting Correlated Attributes in Acquisitional Query Processing|
SARA (System Architecture Apprentice)",|
Distributed inference in sensor networks|
Planning under uncertainty in complex structured environments|
Outdoor Position Estimation for Planetary Rovers,"|
Information-theoretic features for reinforcement learning|
Exploiting correlated attributes in acquisitional query processing|

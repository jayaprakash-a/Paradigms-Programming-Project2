MINIMUM RISK ACOUSTIC CLUSTERING FOR MULTILINGUAL ACOUSTIC MODEL COMBINATION| ABSTRACT In this paper we describe procedures for combining multiple acoustic models, obtained using training corpora from different languages, in order to improve ASR performance in languages for which large amounts of training data are not available.  We treat these models as multiple sources of information whose scores are combined in a log-linear model to compute the hypothesis likelihood.  The model combination can either be performed in a static way, with constant combination weights, or in a dynamic way, with parameters that can vary for different segments of a hypothesis.  The aim is to optimize the parameters so as to achieve minimum word error rate.  In order to achieve robust parameter estimation in the dynamic combination case, the parameters are defined to be piecewise constant on different phonetic classes that form a partition of the space of hypothesis segments.  The partition is defined, using phonological knowledge, on segments that correspond to hypothesized phones.  We examine different ways to define such a partition, including an automatic approach that gives a binary tree structured partition which tries to achieve the minimum WER with the minimum number of classes. 
LARGE-VOCABULARY AUDIO-VISUAL SPEECH RECOGNITION: A SUMMARY OF THE JOHNS HOPKINS SUMMER 2000 WORKSHOP| Abstract - We report a summary of the Johns Hopkins Summer 2000 Workshop on audio-visual automatic speech recognition (ASR) in the large-vocabulary, continuous speech domain.  Two problems of audio-visual ASR were mainly addressed: Visual feature extraction and audio-visual information fusion.  First, image transform and model-based visual features were considered, obtained by means of the discrete cosine transform (DCT) and active appearance models, respectively.  The former were demonstrated to yield superior automatic speechreading.  Subsequently, a number of feature fusion and decision fusion techniques for combining the DCT visual features with traditional acoustic ones were implemented and compared.  Hierarchical discriminant feature fusion and asynchronous decision fusion by means of the multi-stream hidden Markov model consistently improved ASR for both clean and noisy speech.  Compared to an equivalent audio-only recognizer, introducing the visual modality reduced ASR word error rate by 7% relative in clean speech, and by 27% relative at an 8. 5 dB SNR audio condition. 
USE OF WORD LEVEL SIDE INFORMATION TO IMPROVE SPEECH RECOGNITION| ABSTRACT Word level information obtained from the output of a speech recognizer has been used in the past to extract confidence features for the hypothesized words.  This work describes a post-recognition process which treats these word-level features as independent knowledge sources and combines them in one log linear model for the posterior probability of a word sequence.  This model is used for rescoring the hypotheses.  The parameters of the model are optimized using a discriminative model combination approach, where a simplex optimization method, known as amoeba search, is used to minimize the non-smooth function of empirical error rate on training data.  The method is evaluated on the SWITCHBOARD database.  After training 20 new parameters, we obtain a significant word error rate reduction over the baseline system.  A correlation measure between features and word accuracy is defined to help analyze and explain the results. 
PROSODIC KNOWLEDGE SOURCES FOR AUTOMATIC SPEECH RECOGNITION| ABSTRACT In this work, different prosodic knowledge sources are integrated into a state-of-the-art large vocabulary speech recognition system.  Prosody manifests itself on different levels in the speech signal: within the words as a change in phone durations and pitch, inbetween the words as a variation in the pause length, and beyond the words, correlating with higher linguistic structures and nonlexical phenomena.  We investigate three models, each exploiting a different level of prosodic information, in rescoring N-best hypotheses according to how well recognized words correspond to prosodic features of the utterance.  Experiments on the Switchboard corpus show word accuracy improvements with each prosodic knowledge source.  A further improvement is observed with the combination of all models, demonstrating that they each capture somewhat different prosodic characteristics of the speech signal. 
Limited-Domain Speech-to-Speech Translation between English and Pashto| Abstract This paper describes a prototype system for near-real-time spontaneous, bidirectional translation between spoken English and Pashto, a language presenting many technological challenges because of its lack of resources, including both data and expert knowledge.  Development of the prototype is ongoing, and we propose to demonstrate a fully functional version which shows the basic capabilities, though not yet their final depth and breadth. 
Iterative Statistical Language Model Generation for Use with an Agent-Oriented Natural Language Interface| Abstract We describe a method for developing a statistical language model (SLM) with high keyword spotting accuracy for a natural language interface (NLI).  The NLI is based on the Adaptive Agent Oriented Software Architecture (AAOSA).  Our experience shows that this method provides for rapid development of an SLM that is well suited to the requirements of the agent-oriented NLI.  Experiment results show a comparatively low equal error rate of 13. 2% for a vocabulary of 2400 keywords.  This result is a robust free-form speech-based NLI with a high task completion rate. 
An Efficient Repair Procedure For Quick Transcriptions| Abstract We describe an efficient procedure for automatic repair of quickly transcribed (QT) speech.  QT speech, typically closed captioned data from television broadcasts, usually has a significant number of deletions and misspellings, and has a characteristic absence of disfluencies such as filled pauses (for example, um, uh).  Errors of these kinds often throw an acoustic model training program out of alignment and make it hard for it to resynchronize.  At best the erroneous utterance is discarded and does not benefit the training procedure.  At worst, it could misalign and end up sabotaging the training data.  The procedure we propose in this paper aims to cleanse such quick transcriptions so that they align better with the acoustic evidence and thus provide for better acoustic models for automatic speech recognition (ASR).  Results from comparing our transcripts with those from careful transcriptions on the same corpus, and from comparable state-of-the-art methods are also presented and discussed. 
Morphology-Based Language Modeling for Arabic Speech Recognition| Abstract Language modeling is a difficult problem for languages with rich morphology.  In this paper we investigate the use of morphology-based language models at different stages in a speech recognition system for conversational Arabic.  Classbased and single-stream factored language models using morphological word representations are applied within an N-best list rescoring framework.  In addition, we explore the use of factored language models in first-pass recognition, which is facilitated by two novel procedures: the data-driven optimization of a multi-stream language model structure, and the conversion of a factored language model to a standard word-based model.  We evaluate these techniques on a large-vocabulary recognition task and demonstrate that they lead to perplexity and word error rate reductions. 
Building an ASR System for Noisy Environments: SRI's 2001 SPINE Evaluation System| ABSTRACT We describe SRI's recognition system as used in the 2001 DARPA Speech in Noisy Environments (SPINE) evaluation.  The SPINE task involves recognition of speech in simulated military environments.  The task had some unique challenges, including segmentation of foreground speech from noisy background, the need for robust acoustic models to handle noisy speech, and development of language models from limited training data.  In developing the SRI evaluation system for this task, we addressed each of these challenges using a combination of state-of-the-art techniques, including several types of feature normalization,
WEIGHTING SCHEMES FOR AUDIO-VISUAL FUSION IN SPEECH RECOGNITION| ABSTRACT In this work we demonstrate an improvement in the state-of-theart large vocabulary continuous speech recognition (LVCSR) performance, under clean and noisy conditions, by the use of visual information, in addition to the traditional audio one.  We take a decision fusion approach for the audio-visual information, where the single-modality (audio- and visual- only) HMM classifiers are combined to recognize audio-visual speech.  More specifically, we tackle the problem of estimating the appropriate combination weights for each of the modalities.  Two different techniques are described: The first uses an automatically extracted estimate of the audio stream reliability in order to modify the weights for each modality (both clean and noisy audio results are reported), while the second is a discriminative model combination approach where weights on pre-defined model classes are optimized to minimize WER (clean audio only results). 
Speech Translation for Low-Resource Languages: The Case of Pashto| Abstract We present a number of challenges and solutions that have arisen in the development of a speech translation system for American English and Pashto, highlighting those specific to a very low resource language.  In particular, we address issues posed by Pashto in the areas of written representation, corpus creation, speech recognition, speech synthesis, and grammar development for translation. 
CROSS-DIALECTAL ACOUSTIC DATA SHARING FOR ARABIC SPEECH RECOGNITION| ABSTRACT The automatic recognition of Arabic dialectal speech is a challenging task since Arabic dialects are essentially spoken varieties, for which only sparse resources (transcriptions and standardized acoustic data) are available to date.  In this paper we describe the use of acoustic data from Modern Standard Arabic (MSA) to improve the recognition of Egyptian Conversational Arabic (ECA).  The cross-dialectal use of data is complicated by the fact that MSA is written without short vowels and other diacritics and thus has incomplete phonetic information.  This problem is addressed by automatically vowelizing MSA data before combining it with ECA data.  We described the vowelization procedure as well as speech recognition experiments and show that our technique yields improvements over our baseline system. 
NOVEL APPROACHES TO ARABIC SPEECH RECOGNITION: REPORT FROM THE 2002 JOHNS-HOPKINS SUMMER WORKSHOP| ABSTRACT Although Arabic is currently one of the most widely spoken languages in the world, there has been relatively little speech recognition research on Arabic compared to other languages.  Moreover, most previous work has concentrated on the recognition of formal rather than dialectal Arabic.  This paper reports on our project at the 2002 Johns Hopkins Summer Workshop, which focused on the recognition of dialectal Arabic.  Three problems were addressed: (a) the lack of short vowels and other pronunciation information in Arabic texts; (b) the morphological complexity of Arabic; and (c) the discrepancies between dialectal and formal Arabic.  We present novel approaches to automatic vowel restoration, morphology-based language modeling and the integration of outof-corpus language model data, and report significant word error rate improvements on the LDC Arabic CallHome task. 
Development of Phrase Translation Systems for Handheld Computers: From Concept to Field| Abstract We describe the development and conceptual evolution of handheld spoken phrase translation systems, beginning with an initial unidirectional system for translation of English phrases, and later extending to a limited bidirectional phrase translation system between English and Pashto, a major language of Afghanistan.  We review the challenges posed by such projects, such as the constraints imposed by the computational platform, to the limitations of the phrase translation approach when dealing with nave respondents.  We discuss our proposed solutions and their experimental evaluation. 

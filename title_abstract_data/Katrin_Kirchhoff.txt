POS Tagging of Dialectal Arabic: A Minimally Supervised Approach| Abstract Natural language processing technology for the dialects of Arabic is still in its infancy, due to the problem of obtaining large amounts of text data for spoken Arabic.  In this paper we describe the development of a part-of-speech (POS) tagger for Egyptian Colloquial Arabic.  We adopt a minimally supervised approach that only requires raw text data from several varieties of Arabic and a morphological analyzer for Modern Standard Arabic.  No dialect-specific tools are used.  We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 
Syllable-level desynchronisation of phonetic features for speech recognition| Abstract This paper describes a novel approach to speech recognition which is based on phonetic features as basic recognition units and the delayed synchronisation of these features within a higher-level prosodic domain, viz.  the syllable.  The object of this approach is to avoid a rigid segmentation of the speech signal as it is usually carried out by standard segment-based recognition systems.  The architectural setup of the system will be described, as well as evaluation tests carried out on a medium-sized corpus of spontaneous speech (German).  Syllable and phoneme recognition results will be given and compared to recognition rates obtained by a standard triphone-based HMM recogniser trained and tested on the same data set. 
LOW-RESOURCE NOISE-ROBUST FEATURE POST-PROCESSING ON AURORA 2|0.  ABSTRACT We present a highly effective and extremely simple noiserobust front end based on novel post-processing of standard MFCC features.  It performs remarkably well on the Aurora 2. 0 noisydigits database without requiring any increase in model complexity.  Compared to the Aurora 2. 0 baseline system, our technique improves the average word error rate by 45% in the multicondition training case, (matched training/testing conditions) and 60% in the clean training case (mismatched training/testing conditions) --- this is an improvement that rivals some of the best known results on this database.  Our method, moreover, improves the performances in all cases, regardless of clean or noisy speech, matched or mis-matched environments.  Our technique is entirely general because it makes no assumptions about the existence, type, or level of noise in the speech signal.  Moreover, its simplicity means that it should be easy to integrate with other techniques in order to yield further improvements. 
The Vocal Joystick: A Voice-Based Human-Computer Interface for Individuals with Motor Impairments| Abstract We present a novel voice-based humancomputer interface designed to enable individuals with motor impairments to use vocal parameters for continuous control tasks.  Since discrete spoken commands are ill-suited to such tasks, our interface exploits a large set of continuous acousticphonetic parameters like pitch, loudness, vowel quality, etc.  Their selection is optimized with respect to automatic recognizability, communication bandwidth, learnability, suitability, and ease of use.  Parameters are extracted in real time, transformed via adaptation and acceleration, and converted into continuous control signals.  This paper describes the basic engine, prototype applications (in particular, voice-based web browsing and a controlled trajectory-following task), and initial user studies confirming the feasibility of this technology. 
In International Conference of Spoken Language Processing,| ABSTRACT Robust speech recognition under varying acoustic conditions may be achieved by exploiting multiple sources of information in the speech signal.  In addition to an acoustic signal representation, we use an articulatory representation consisting of pseudoarticulatory features as an additional information source.  Hybrid ANN/HMM recognizers using either of these representations are evaluated on a continuous numbers recognition task (OGI Numbers95) under clean, reverberant and noisy conditions.  An error analysis of preliminary recognition results shows that the different representations produce qualitatively different errors, which suggests a combination of both representations.  We investigate various combination possibilities at the phoneme estimation level and show that significant improvements can been achieved under all three acoustic conditions. 
In International Congress of Phonetic Sciences| ABSTRACT Coarticulation in speech is one of the most difficult problems for automatic speech recognition (ASR) systems.  The degree of coarticulation is assumed to vary with contextual conditions, such as differences in speaking rate, stress, etc.  In the past, coarticulation has been studied using only limited data sets and using acousticphonetic methods such as formant analysis.  We propose a method that statistically analyzes the degree of coarticulatory influence on features typically used for automatic speech recognition systems (LPCs, MFCCs, RASTA, and compressed subband spectral envelopes).  This method computes the Conditional Mutual Information (CMI) between time/feature-position pairs under a variety of coarticulatory conditions.  We applied this method on a twohour subset of the Switchboard database and analyzed CMI for various speaking rate, stress, and vowel category conditions.  Results show that CMI is indeed larger for those phonetic conditions believed to possess more coarticulation. 
INTERNATIONAL COMPUTER SCIENCE INSTITUTE| Abstract Whereas most state-of-the-art speech recognition systems use spectral or cepstral representations of the speech signal, there have also been some promising attempts at using articulatory information.  These attempts have been motivated by two major assumptions: first, coarticulation can be modeled more naturally due to the inherently asynchronous nature of articulatory information.  Second, it is assumed that the overall patterns in the speech signal caused by articulatory gestures are more robust to noise and speaker-dependent acoustic variation than spectral parameters.  A third assumption can be made, viz.  that acoustic and articulatory representations of speech can supply mutually complementary information to a speech recognizer, in which case the combination of these representations might be beneficial.  Previously, articulatory-based speech recognizers have exclusively been developed for clean speech; the potential of an articulatory representation of the speech signal for noisy test conditions, by contrast, has not been explored.  Moreover, there have barely been attempts at systematically combining articulatory information with standard acoustic recognizers.  This paper investigates the second and third of the above assumptions by reporting speech recognition results on a variety of acoustic test conditions for individual acoustic and articulatory speech recognizers, as well as for a combined system.  On a continuous numbers recognition task, the acoustic system generally performs equal to, or slightly better than, the articulatory system, whereas the articulatory system shows a statistically significant improvement on noisy speech with a low signal-to-noise ratio.  The combined system nearly always performs significantly better than either of the individual systems. 
COMBINING ARTICULATORY AND ACOUSTIC INFORMATION FOR SPEECH RECOGNITION IN NOISY AND REVERBERANT ENVIRONMENTS| ABSTRACT Robust speech recognition under varying acoustic conditions may be achieved by exploiting multiple sources of information in the speech signal.  In addition to an acoustic signal representation, we use an articulatory representation consisting of pseudoarticulatory features as an additional information source.  Hybrid ANN/HMM recognizers using either of these representations are evaluated on a continuous numbers recognition task (OGI Numbers95) under clean, reverberant and noisy conditions.  An error analysis of preliminary recognition results shows that the different representations produce qualitatively different errors, which suggests a combination of both representations.  We investigate various combination possibilities at the phoneme estimation level and show that significant improvements can been achieved under all three acoustic conditions. 
The Impact of Response Wording in Error Correction Subdialogs| Abstract Spoken human-machine dialogs are prone to communication failures due to imperfect speech recognition and understanding.  In order to recover from these failures, users typically engage in error correction subdialogs.  Lengthy error correction subdialogs should be avoided since they increase the overall task completion time and decrease user satisfaction.  This study analyzes a large corpus of human-computer dialogs and identifies properties of system responses that affect user frustration and recognition error rates in error correction subdialogs. 
Improved Language Modeling for Statistical Machine Translation| Abstract Statistical machine translation systems use a combination of one or more translation models and a language model.  While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention.  Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system.  In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model. 
THE 2001 GMTK-BASED SPINE ASR SYSTEM| ABSTRACT This paper provides a detailed description of the University of Washington automatic speech recognition (ASR) system for the 2001 DARPA SPeech In Noisy Environments (SPINE) task.  Our system makes heavy use of the graphical modeling toolkit (GMTK), a general purpose graphical modeling-based ASR system that allows arbitrary parameter tying, flexible deterministic and stochastic dependencies between variables, and a generalized maximum likelihood parameter estimation algorithm.  In our SPINE system, GMTK was used for acoustic model training whereas feature extraction, speaker adaptation, and first-pass decoding were performed by HTK.  Our integrated GMTK/HTK system demonstrates the relative merits provided by each tool.  Novel aspects of our SPINE system include the capturing of correlations among feature vectors via a globally-shared factored sparse inverse covariance matrix and generalized EM training. 
Morphology-Based Language Modeling for Arabic Speech Recognition| Abstract Language modeling is a difficult problem for languages with rich morphology.  In this paper we investigate the use of morphology-based language models at different stages in a speech recognition system for conversational Arabic.  Classbased and single-stream factored language models using morphological word representations are applied within an N-best list rescoring framework.  In addition, we explore the use of factored language models in first-pass recognition, which is facilitated by two novel procedures: the data-driven optimization of a multi-stream language model structure, and the conversion of a factored language model to a standard word-based model.  We evaluate these techniques on a large-vocabulary recognition task and demonstrate that they lead to perplexity and word error rate reductions. 
DYNAMIC CLASSIFIER COMBINATION IN HYBRID SPEECH RECOGNITION SYSTEMS USING UTTERANCE-LEVEL CONFIDENCE VALUES| ABSTRACT A recent development in the hybrid HMM/ANN speech recognition paradigm is the use of several subword classifiers, each of which provides different information about the speech signal.  Although the combining methods have obtained promising results, the strategies so far proposed have been relatively simple.  In most cases frame-level subword unit probabilities are combined using an unweighted product or sum rule.  In this paper, we argue and empirically demonstrate that the classifier combination approach can benefit from a dynamically weighted combination rule, where the weights are derived from higher-than-frame-level confidence values. 
Robust Speech Recognition Using Articulatory Information Der Technischen Fakultat der Universitat Bielefeld zur Erlangung des Grades eines Doktor--Ingenieur vorgelegt von| Abstract Current automatic speech recognition systems make use of a single source of information about their input, viz.  a preprocessed form of the acoustic speech signal, which encodes the timefrequency distribution of signal energy.  The goal of this thesis is to investigate the benefits of integrating articulatory information into state-of-the art speech recognizers, either as a genuine alternative to standard acoustic representations, or as an additional source of information.  Articulatory information is represented in terms of abstract articulatory classes or "features", which are extracted from the speech signal by means of statistical classifiers.  A higher-level classifier then combines the scores for these features and maps them to standard subword unit probabilities.  The main motivation for this approach is to improve the robustness of speech recognition systems in adverse acoustic environments, such as background noise.  Typically, recognition systems show a sharp decline of performance under these conditions.  We argue and demonstrate empirically that the articulatory feature approach can lead to greater robustness by enhancing the accuracy of the bottom-up acoustic modeling component in a speech recognition system.  The second focus point of this thesis is to provide detailed analyses of the different types of information provided by the acoustic and the articulatory representations, respectively, and to develop strategies to optimally combine them.  To this effect we investigate combination methods at the levels of feature extraction, subword unit probability estimation, and word recognition.  The feasibility of this approach is demonstrated with respect to two different speech recognition tasks.  The first of these is an American English corpus of telephone-bandwidth speech; the recognition domain is continuous numbers.  The second is a German database of studio-quality speech consisting of spontaneous dialogues.  In both cases recognition performance will be tested not only under clean acoustic conditions but also under deteriorated conditions. 
Automatic Detection of Sentence Boundaries, Disfluencies, and Conversational Fillers in Spontaneous Speech| Date: In presenting this thesis in partial fulfillment of the requirements for a Master's degree at the University of Washington, I agree that the Library shall make its copies freely available for inspection.  I further agree that extensive copying of this thesis is allowable only for scholarly purposes, consistent with "fair use" as prescribed in the
MULTI-STREAM LANGUAGE IDENTIFICATION USING DATA-DRIVEN DEPENDENCY SELECTION| ABSTRACT The most widespread approach to automatic language identification in the past has been the statistical modeling of phone sequences extracted from speech signals.  Recently, we have developed an alternative approach to LID based on n-gram modeling of parallel streams of articulatory features, which was shown to have advantages over phone-based systems on short test signals whereas the latter achieved a higher accuracy on longer signals.  Additionally, phone and feature streams can be combined to achieve maximum performance.  Within this "multi-stream" framework two types of statistical dependencies need to be modeled: (a) dependencies between symbols in individual streams and (b) dependencies between symbols in different streams.  The space of possible dependencies is typically too large to be searched exhaustively.  In this paper, we explore the use of genetic algorithms as a method for data-driven dependency selection.  The result is a general framework for the discovery and modeling of dependencies between multiple information sources expressed as sequences of symbols, which has implications for other fields beyond language identification, such as speaker identification or language modeling. 
Directions For Multi-Party Human-Computer Interaction Research| Abstract Research on dialog systems has so far concentrated on interactions between a single user and a machine.  In this paper we identify novel research directions arising from multi-party human computer interaction, i. e.  scenarios where several human participants interact with a dialog system. 
CROSS-DIALECTAL ACOUSTIC DATA SHARING FOR ARABIC SPEECH RECOGNITION| ABSTRACT The automatic recognition of Arabic dialectal speech is a challenging task since Arabic dialects are essentially spoken varieties, for which only sparse resources (transcriptions and standardized acoustic data) are available to date.  In this paper we describe the use of acoustic data from Modern Standard Arabic (MSA) to improve the recognition of Egyptian Conversational Arabic (ECA).  The cross-dialectal use of data is complicated by the fact that MSA is written without short vowels and other diacritics and thus has incomplete phonetic information.  This problem is addressed by automatically vowelizing MSA data before combining it with ECA data.  We described the vowelization procedure as well as speech recognition experiments and show that our technique yields improvements over our baseline system. 
COMBINATION AND JOINT TRAINING OF ACOUSTIC CLASSIFIERS FOR SPEECH RECOGNITION| product rule when jointly training and combining multiple systems using a generalization of the product rule. 
Genetic Triangulation of Graphical Models for Speech and Language Processing| Abstract Graphical models are an increasingly popular approach for speech and language processing.  As researchers design ever more complex models it becomes crucial to find triangulations that make inference problems tractable.  This paper presents a genetic algorithm for triangulation search that is well-suited for speech and language graphical models.  It is unique in two ways: First, it can find triangulations appropriate for graphs with a mix of stochastic and deterministic dependencies.  Second, the search is guided by optimizing the inference speed (CPU runtime) on real data.  We show results on 10 real-world speech and language graphs and demonstrate inference speed-ups over standard triangulation methods. 
MIXED-MEMORY MARKOV MODELS FOR AUTOMATIC LANGUAGE IDENTIFICATION| ABSTRACT Automatic language identification (LID) continues to play an integral part in many multilingual speech applications.  The most widespread approach to LID is the phonotactic approach, which performs language classification based on the probabilities of phone sequences extracted from the test signal.  These probabilities are typically computed using statistical phone n-gram models.  In this paper we investigate the approximation of these standard n-gram models by mixed-memory Markov models with application to both a phone-based and an articulatory feature-based LID system.  We demonstrate significant improvements in accuracy with a substantially reduced set of parameters on a 10-way language identification task. 
DIRECTED GRAPHICAL MODELS OF CLASSIFIER COMBINATION: APPLICATION TO PHONE RECOGNITION| ABSTRACT Classifier combination is a technique that often provides appreciable accuracy gains.  In this paper, we argue that the underlying statistical model of classifier combination should be made explicit.  Using directed graphical models (DGMs), we provide representations of two common combination schemes, the mean and product rules.  We also introduce new DGMs that yield novel combination rules.  We find that these new DGM-inspired rules can achieve significant accuracy gains on the TIMIT phone-classification task relative to existing combination schemes. 
STATISTICAL ACOUSTIC INDICATIONS OF COARTICULATION| ABSTRACT Coarticulation in speech is one of the most difficult problems for automatic speech recognition (ASR) systems.  The degree of coarticulation is assumed to vary with contextual conditions, such as differences in speaking rate, stress, etc.  In the past, coarticulation has been studied using only limited data sets and using acousticphonetic methods such as formant analysis.  We propose a method that statistically analyzes the degree of coarticulatory influence on features typically used for automatic speech recognition systems (LPCs, MFCCs, RASTA, and compressed subband spectral envelopes).  This method computes the Conditional Mutual Information (CMI) between time/feature-position pairs under a variety of coarticulatory conditions.  We applied this method on a twohour subset of the Switchboard database and analyzed CMI for various speaking rate, stress, and vowel category conditions.  Results show that CMI is indeed larger for those phonetic conditions believed to possess more coarticulation. 
NOVEL APPROACHES TO ARABIC SPEECH RECOGNITION: REPORT FROM THE 2002 JOHNS-HOPKINS SUMMER WORKSHOP| ABSTRACT Although Arabic is currently one of the most widely spoken languages in the world, there has been relatively little speech recognition research on Arabic compared to other languages.  Moreover, most previous work has concentrated on the recognition of formal rather than dialectal Arabic.  This paper reports on our project at the 2002 Johns Hopkins Summer Workshop, which focused on the recognition of dialectal Arabic.  Three problems were addressed: (a) the lack of short vowels and other pronunciation information in Arabic texts; (b) the morphological complexity of Arabic; and (c) the discrepancies between dialectal and formal Arabic.  We present novel approaches to automatic vowel restoration, morphology-based language modeling and the integration of outof-corpus language model data, and report significant word error rate improvements on the LDC Arabic CallHome task. 
Multi-Stream Statistical N-Gram Modeling With Application To Automatic Language Identification| Abstract Most state-of-the art automatic language identification systems are based on phonotactic information, i. e.  languages are identified on the basis of probabilities of phone sequences extracted from the acoustic signal.  This approach ignores the potential advantages to be gained from a richer representation of the acoustic signal in terms of parallel streams of subphonemic events.  In this paper we develop an alternative approach to language identification which is based on parallel streams of phonetic features and sparse modeling of statistical dependencies between these streams.  We present results on the OGI-TS database and show that the feature-based system outperforms a comparable phone-based system significantly while using fewer parameters.  Moreover, the feature-based system exhibits a markedly better performance on very short test signals ( # 3 seconds).  The theoretical approach developed here is of significance not only for language identification but also for related work in pronunciation modeling. 
Factored Language Models and Generalized Parallel Backoff|
Factored language models and general parallelized backoff|
Novel Speech Recognition Models for Arabic: JHU Summer Workshop 2002 Final Report|

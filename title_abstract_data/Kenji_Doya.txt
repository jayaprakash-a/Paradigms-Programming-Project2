Adaptive Synchronization of Neural and Physical Oscillators| Abstract Animal locomotion patterns are controlled by recurrent neural networks called central pattern generators (CPGs). 
Temporal Difference Learning in Continuous Time and Space| Abstract A continuous-time, continuous-state version of the temporal difference (TD) algorithm is derived in order to facilitate the application of reinforcement learning to real-world control tasks and neurobiological modeling.  An optimal nonlinear feedback control lawwas also derived using the derivatives of the value function.  The performance of the algorithms was tested in a task of swinging up a pendulum with limited torque.  Both the \critic" that specifies the paths to the upright position and the \actor" that works as a nonlinear feedback controller were successfully implemented by radial basis function (RBF) networks. 
Bifurcations of Recurrent Neural Networks in Gradient Descent Learning| Abstract Asymptotic behavior of a recurrent neural network changes qualitatively at certain points in the parameter space, which are known
What are the Computations of the Cerebellum, the Basal Gangila, and the Cerebral Cortex?| Abstract The classical notion that the cerebellum and the basal ganglia are
Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction on Different Time Scales| Abstract To understand the brain mechanisms involved in reward prediction on different time scales, we developed a Markov decision task that requires prediction of both immediate and future rewards, and analyzed subjects' brain activities using functional MRI.  We estimated the time course of reward prediction and reward prediction error on different time scales from subjects' performance data, and used them as the explanatory variables for SPM analysis.  We found topographic maps of different time scales in medial frontal cortex and striatum.  The result suggests that different cortico-basal ganglia loops are specialized for reward prediction on different time scales. 
Universality of Fully-Connected Recurrent Neural Networks| Abstract It is shown from the universality of multi-layer neural networks that any discretetime or continuous-time dynamical system can be approximated by discrete-time or continuous-time recurrent neural networks, respectively. 
Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter| Abstract When we model a higher order functions, such as learning and memory, we face a difficulty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network.  Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables.  Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables.  In this paper, we apply particle filter for estimating internal parameters and metaparameters of a reinforcement learning model.  We verified the effectiveness of the method using both artificial data and real animal behavioral data. 
Robust Reinforcement Learning| Abstract This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors.  The use of environmental models in RL is quite popular for both off-line learning by simulations and for on-line action planning.  However, the difference between the model and the real environment can lead to unpredictable, often unwanted results.  Based on the theory of H1 control, we consider a differential game in which a `disturbing' agent (disturber) tries to make the worst possible disturbance while a `control' agent (actor) tries to make the best control input.  The problem is formulated as finding a minmax solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance.  We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function.  We tested the paradigm, which we call ``Robust Reinforcement Learning (RRL)," in the task of inverted pendulum.  In the linear domain, the policy and the value function learned by the on-line algorithms coincided with those derived analytically by the linear H1 theory.  For a fully nonlinear swingup task, the control by RRL achieved robust performance against changes in the pendulum weight and friction while a standard RL control could not deal with such environmental changes. 
Responding to modalities with different latencies| Abstract Motor control depends on sensory feedback in multiple modalities with different latencies.  In this paper we consider within the framework of reinforcement learning how different sensory modalities can be combined and selected for real-time, optimal movement control.  We propose an actor-critic architecture with multiple modules, whose output are combined using a softmax function.  We tested our architecture in a simulation of a sequential reaching task.  Reaching was initially guided by visual feedback with a long latency.  Our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory.  In simulations with different latencies for visual and somatosensory feedback, we found that the agent depended more on feedback with shorter latency. 
A unifying computational framework for motor control and social interaction| Recent empirical studies have implicated the use of the motor system during action observation, imitation and social interaction.  In this paper, we explore the computational parallels between the processes that occur in motor control and in action observation, imitation, social interaction and theory of mind.  In particular, we examine the extent to which motor commands acting on the body can be equated with communicative signals acting on other people and suggest that computational solutions for motor control may have been extended to the domain of social interaction. 
BIFURCATIONS IN THE LEARNING OF RECURRENT NEURAL NETWORKS| Abstract Gradient descent algorithms in recurrent neural networks can have problems when the network dynamics experience bifurcations in the course of learning.  The possible hazards caused by the bifurcations of the network dynamics and the learning equations are investigated.  The roles of teacher forcing, preprogramming of network structures, and the approximate learning algorithms are discussed. 
Near Saddle-Node Bifurcation Behavior as Dynamics in Working Memory for Goal-Directed Behavior|
Complementary roles of basal ganglia and cerebellum in learning and motor control|
Memorizing oscillatory patterns in the analog neuron network|
Efficient Nonlinear Control with Actor-Tutor Architecture|
Multiple Model-Based Reinforcement Learning|
Acquisition of Stand-up Behavior by a Real Robot using Hierarchical Reinforcement Learning|
Recurrent networks: Supervised learning|
Adaptive neural oscillator using continuous-time backpropagation learning|
Reinforcement Learning in Continuous Time and Space|
What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?|
Parallel cortico-basal ganglia mechanisms for acquisition and execution of visuomotor sequences-a computational approach|
Parallel neural networks for learning sequential procedures|
Multiple model-based reinforcement learning| Technical report, Kawato Dynamic Brain Project Technical Report,. 
Intelligent Control of a Flying Vehicle using Fuzzy Associative Memory System|
Reinforcement learning of dynamic motor sequence: Learning to stand up|
Metalearning and neuromodulation|
Recognition and imitation of movement patterns by a multiple predictor--controller architecture|
Evidence for effector independent and dependent representations and their differential time course of acquisition during motor sequence learning|
Reward value dependent striate neuron activity of monkey performing trial-and-error behavioral decision task,|
CompetitiveCooperative-ConcurrentReinforcementLearning with Importance Sampling|
The Handbook of Brain Theory and Neural Networks|, chapter Recurrent networks: supervised learning. ,. 
2004]: "A Neural Correlate of Reward-Based Behavioral Learning in Caudate Nucleus: A Functional Magnetic Resonance Imaging Study of a Stochastic Decision Task,|
Dimension Reduction of Biological Neuron Models by Artificial Neural Networks|
Recurrent Networks: Learning Algorithms|
Temporal Difference Learning in Continuous Time and Space|
Metalearning, neuromodulation and emotion|
A neural network model of temporal pattern memory: adaptive neural oscillator using continuous-time back-propagation learning|
Multi-Agent Reinforcement Learning: An Approach Based on the Other Agent's Internal Model|
Extension of MOSAIC and communication: Computational neuroscience of human intelligence #5|
Symbolization of action patterns and imitation learning by module competition|
Reinforcementlearning with multiple representations in the basal ganglia loops for sequential motor control|
Multiple representations in the basal ganglia loops for acquisition and execution of sequential motor control|
Multiple state estimation reinforcement learning for driving model: Driver model of automobile|

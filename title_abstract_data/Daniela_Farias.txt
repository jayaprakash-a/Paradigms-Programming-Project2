Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments| Abstract A reactive environment is one that responds to the actions of an agent rather than evolving obliviously.  In reactive environments, experts algorithms must balance exploration and exploitation of experts more carefully than in oblivious ones.  In addition, a more subtle definition of a learnable value of an expert is required.  A general exploration-exploitation experts method is presented along with a proper definition of value.  The method is shown to asymptotically perform as well as the best available expert.  Several variants are analyzed from the viewpoint of the exploration-exploitation tradeoff, including explore-then-exploit, polynomially vanishing exploration, constant-frequency exploration, and constant-size exploration phases.  Complexity and performance bounds are proven. 
How to Combine Expert (or Novice) Advice when Actions Impact the Environment| Abstract The so-called "experts algorithms" constitute a methodology for choosing actions repeatedly, when the rewards depend both on the choice of action and on the unknown current state of the environment.  An experts algorithm has access to a set of strategies ("experts"), each of which may recommend which action to choose.  The algorithm learns how to combine the recommendations of individual experts so that, in the long run, for any fixed sequence of states of the environment, it does as well as the best expert would have done relative to the same sequence.  This methodology may not be suitable for situations where the evolution of states of the environment depends on past chosen actions, as is usually the case, for example, in a repeated non-zero-sum game.  A new experts algorithm is presented and analyzed in the context of repeated games.  It is shown that asymptotically, under certain conditions, it performs as well as the best available expert.  This algorithm is quite different from previously proposed experts algorithms.  It represents a shift from the paradigms of regret minimization and myopic optimization to consideration of the long-term effect of a player's actions on the opponent's actions or the environment.  The importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated Prisoner's Dilemma game, whereas previous experts algorithms converge to the suboptimal non-cooperative play. 
The Linear Programming Approach to Approximate Dynamic Programming| Abstract The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems.  We study an ecient method based on linear programming for approximating solutions to such problems.  The approach \#ts" a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function.  Wedevelop error bounds that offer performance guarantees and also guide the selection of both basis functions and \state-relevance weights" that influence quality of the approximation.  Experimental results in the domain of queueing network control provide empirical support for the methodology. 
Approximate Dynamic Programming via Linear Programming| Abstract The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large{scale stochastic control problems.  We study an ecient method based on linear programming for approximating solutions to such problems.  The approach \#ts" a linear combination of pre{selected basis functions to the dynamic programming cost{to{go function.  We develop bounds on the approximation error and present experimental results in the domain of queueing network control, providing empirical support for the methodology. 
A Linear Program for Bellman Error Minimization with Performance Guarantees| Abstract We introduce a new algorithm based on linear programming for optimization of average-cost Markov decision processes (MDPs).  The algorithm approximates the differential cost function of a perturbed MDP via a linear combination of basis functions.  The approximation minimizes a version of Bellman error.  We establish an error bound that scales gracefully with the number of states without imposing the (strong) Lyapunov condition required by its counterpart in [8].  We investigate implications of this result in the context of a queueing problem. 
Approximate Linear Programming for Average-Cost Dynamic Programming| Abstract This paper extends our earlier analysis on approximate linear programming as an approach to approximating the cost-to-go function in a discounted-cost dynamic program [6].  In this paper, we consider the average-cost criterion and a version of approximate linear programming that generates approximations to the optimal average cost and differential cost function.  We demonstrate that a naive version of approximate linear programming prioritizes approximation of the optimal average cost and that this may not be well-aligned with the objective of deriving a policy with low average cost.  For that, the algorithm should aim at producing a good approximation of the differential cost function.  We propose a twophase variant of approximate linear programming that allows for external control of the relative accuracy of the approximation of the differential cost function over different portions of the state space via state-relevance weights.  Performance bounds suggest that the new algorithm is compatible with the objective of optimizing performance and provide guidance on appropriate choices for state-relevance weights. 
The linear programming approach to approximate dynamic programming|
The linear programming approach to approximate dynamic programming|
On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming|
Approximate Dynamic Programming via Linear Programming|
How to Combine Expert (and Novice) Advice when Actions Impact the Environment?|
\On Constraint Sampling for Approximate Linear Programming,",|
DNA electrochemical biosensor for the detection of DNA sequences related to the human immunodeficiency virus|

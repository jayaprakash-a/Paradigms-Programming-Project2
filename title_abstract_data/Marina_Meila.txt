Learning Segmentation by Random Walks| Abstract We present a new view of image segmentation by pairwise similarities.  We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix.  This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation.  In particular, we prove that the Normalized Cut method arises naturally from our framework.  Finally, the framework provides a principled method for learning the similarity function as a combination of features. 
Comparing Subspace Clusterings| Abstract We present the first attempt at designing a framework for comparing subspace clusterings.  We propose several subspace clustering distance measures, including generalizations of well-known distance measures for ordinary clusterings.  We describe a set of desirable properties for any measure for comparing subspace clusterings, as well as a systematic comparison of our proposed measures in terms of these properties.  We validate the usefulness of our subspace clustering distance measures by comparing clusterings produced by the algorithms FastDOC, HARP, PROCLUS, ORCLUS, and SSPC. 
Data Centering in Feature Space| Abstract This paper presents a family of methods for data translation in feature space, to be used in conjunction with kernel machines.  The translations are performed using only kernel evaluations in input space.  We use the methods to improve the numerical properties of kernel machines.  Experiments with synthetic and real data demonstrate the effectiveness of data centering and highlight other interesting aspects of translation in feature space. 
An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse Data| Abstract Chow and Liu [2] introduced an algorithm for fitting a multivariate distribution with a tree (i. e.  a density model that assumes that there are only pairwise dependencies between variables) and that the graph of these dependencies is a spanning tree.  The original algorithm is quadratic in the dimesion of the domain, and linear in the number of data points that define the target distribution P .  This paper shows that for sparse, discrete data, fitting a tree distribution can be done in time and memory that is jointly subquadratic in the number of variables and the size of the data set.  The new algorithm, called the acCL algorithm, takes advantage of the sparsity of the data to accelerate the computation of pairwise marginals and the sorting of the resulting mutual informations, achieving speed ups of up to 2-3 orders of magnitude in the experiments. 
A New Signature-Based Method for Efficient 3-D Object Recognition| Abstract This paper considers the problem of shape-based recognition and pose estimation of 3-D free-form objects in scenes that contain occlusion and clutter.  Our approach is based on a novel set of discriminating descriptors called spherical spin images, which encode the shape information conveyed by classes of distributions of surface points constructed with respect to reference points on the surface of an object.  The key to this approach is the relationship that exists between the l 2 metric, which compares n-dimensional signatures in Euclidean space, and the metric of the compact space on which the class representatives (spherical spin images) are defined.  The connection allows us to efficiently utilize the linear correlation coefficient to discriminate scene points which have spherical spin images that are similar to the spherical spin images of points on the object being sought.  The paper also addresses the problem of a compressed spherical-spin-image representation by means of a random projection of the original descriptors that reduces the dimensionality without a significant loss of recognition/localization performance.  Finally, the efficacy of the proposed representation is validated in a comparative study of the two algorithms introduced here that use uncompressed and compressed spherical spin images versus two previous spin image algorithms reported recently in the literature.  The results of 2012 experiments suggest that the performance of our proposed algorithms is significantly better with respect to accuracy and speed than the performance of the other algorithms tested. 
Learning with Mixtures of Trees| Abstract This paper describes the mixtures-of-trees model, a probabilistic model for discrete multidimensional domains.  Mixtures-of-trees generalize the probabilistic trees of Chow and Liu [6] in a different and complementary direction to that of Bayesian networks.  We present efficient algorithms for learning mixtures-of-trees models in maximum likelihood and Bayesian frameworks.  We also discuss additional efficiencies that can be obtained when data are "sparse," and we present data structures and algorithms that exploit such sparseness.  Experimental results demonstrate the performance of the model for both density estimation and classification.  We also discuss the sense in which tree-based classifiers perform an implicit form of feature selection, and demonstrate a resulting insensitivity to irrelevant attributes. 
Tractable Bayesian Learning of Tree Belief Networks| Abstract In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time.  This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form.  Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al. , 1995) constrain the tree parameter priors to be a compactly parametrized product of Dirichlet distributions.  Besides allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures. 
Multiway Cuts and Spectral Clustering| Abstract We look at spectral clustering as optimization.  We show that near some special points called perfect, spectral clustering optimizes simultaneously two criteria: a dissimilarity measure that we call the multiway normalized cut (MNCut) and a cluster coherence measure that we call the gap.  The immediate implication from the user's p. o. v is that spectral clustering will optimize any tradeoff between MNCut and gap which may explain its success in practice.  Finally, we propose new methods for selecting K based on the gap and show their superior performance in experiments. 
Regularized spectral learning UW-Stat Dept TR # 465| Abstract Spectral clustering is a technique for finding groups in data consisting of similarities S ij between pairs of points.  We approach the problem of learning the similarity as a function of other observed features, in order to optimize spectral clustering results on future data.  This paper formulates a new objective for learning in spectral clustering, that balances a clustering accuracy term, the gap, and a stability term, the eigengap with the later in the role of a regularizer.  We derive an algorithm to optimize this objective, and semiautomatic methods to chose the optimal regularization.  Preliminary experiments confirm the validity of the approach. 
Real-Time Particle Filters| Abstract Particle filters estimate the state of dynamical systems from sensor information.  In many real time applications of particle filters, however, sensor information arrives at a significantly higher rate than the update rate of the filter.  The prevalent approach to dealing with such situations is to update the particle filter as often as possible and to discard sensor information that cannot be processed in time.  In this paper we present real-time particle filters, which make use of all sensor information even when the filter update rate is below the update rate of the sensors.  This is achieved by representing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a filter update.  The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation.  Thereby, our approach focuses computational resources (samples) on valuable sensor information.  Experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches. 
Triangulation by Continuous Embedding| Abstract When triangulating a belief network we aim to obtain a junction tree of minimum state space.  According to (Rose, 1970), searching for the optimal triangulation can be cast as a search over all the permutations of the graph's vertices.  Our approach is to embed the discrete set of permutations in a convex continuous domain D.  By suitably extending the cost function over D and solving the continous nonlinear optimization task we hope to obtain a good triangulation with respect to the aformentioned cost.  This paper presents two ways of embedding the triangulation problem into continuous domain and shows that they perform well compared to the best known heuristic. 
A New Paradigm for Recognizing 3-D Object Shapes from Range Data| Abstract Most of the work on 3-D object recognition from range data has used an alignment-verification approach in which a specific 3-D object is matched to an exact instance of the same object in a scene.  This approach has been successfully used in industrial machine vision, but it is not capable of dealing with the complexities of recognizing classes of similar objects.  This paper undertakes this task by proposing and testing a component-based methodology encompassing three main ingredients: 1) a new way of learning and extracting shape-class components from surface shape information; 2) a new shape representation called a symbolic surface signature that summarizes the geometric relationships among components; and 3) an abstract representation of shape classes formed by a hierarchy of classifiers that learn object-class parts and their spatial relationships from examples. 
Spectral Clustering for Microsoft Netscan Data| Abstract We present the results of exploratory data analysis for a data set that consists of crossposting information for 89,687 newsgroups over a period of 3. 4 years.  The data set we use is a part of Microsoft Netscan data.  Our goal is to investigate the community structure of the newsgroup data set with a specific focus on spectral hierarchical clustering.  We present a spectral hierarchical clustering algorithm and discuss existing and novel ways to measure the quality of a hierarchical clustering.  We construct spectral hierarchical clusterings for ten subsets of the data set and evaluate the stability of the results. 
Estimating Dependency Structure as a Hidden Variable| Abstract This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships.  We present a family of efficient algorithms based on the EM and the Minimum Spanning Tree algorithms that learn mixtures of trees in the ML framework.  The method can be extended to take into account priors and, for a wide class of priors that includes the Dirichlet and the MDL priors, it preserves its computational efficiency.  Experimental results demonstrate the excellent performance of the new model both in density estimation and in classification.  Finally, we show that a single tree classifier acts like an implicit feature selector, thus making the classification performance insensitive to irrelevant attributes. 
A Comparison of Spectral Clustering Algorithms| Abstract Spectral Clustering has become quite popular over the last few years and several new algorithms have been published.  In this paper, we compare several of the best-known algorithms from the point of view of clustering quality over artificial and real datasets.  We implement many variations of the existing spectral algorithms and compare their performance to see which features are more important.  We also demonstrate that spectral methods show competitive performance on real dataset with respect to existing methods. 
Clustering by intersection-merging| ABSTRACT We propose Intersection-Merging (IM), a wrapper algorithm for model-based clustering.  The algorithm takes a set of clusterings obtained e. g.  by EM, breaks down the clusterings into subclusters via an intersection step, and then agglomerates them via a merging step.  We introduce two versions of merging: greedy (standard IM) and by simulated annealing (IMSA).  Experiments on several data sets show that both IM and IMSA improve on the starting clusterings under a variety of criteria. 
Maximum Entropy Discrimination| Abstract We present a general framework for discriminative estimation based on the maximum entropy principle and its extensions.  All calculations involve distributions over structures and/or parameters rather than specific settings and reduce to relative entropy projections.  This holds even when the data is not separable within the chosen parametric class, in the context of anomaly detection rather than classification, or when the labels in the training set are uncertain or incomplete.  Support vector machines are naturally subsumed under this class and we provide several extensions.  We are also able to estimate exactly and eciently discriminative distributions over tree structures of class-conditional models within this framework.  Preliminary experimental results are indicative of the potential in these techniques. 
Comparing Clusterings by the Variation of Information|
An Experimental Comparison of Several Clustering and Initialization Methods|
Learning Fine Motion by Markov Mixtures of Experts|
An Experimental Comparison of Model-Based Clustering Methods|
A top-down approach to structure learning|
The Multicut Lemma|
Adaptive real-time particle filters for robot localization|
A unicity theorem for spectral clustering|

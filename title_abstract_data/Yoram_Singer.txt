PrFQ: Probabilistic Fair Queuing| Abstract--Packet scheduling constitutes the core problem in efficient fair allocation of bandwidth to competing flows.  To date, numerous algorithms for packet scheduling have been suggested and tested.  However, only a few of them are currently deployed.  One of the key reasons for rarity of applied packet scheduling methods lies in the complexity of their implementation.  In this paper we describe and experiment with a family of randomized algorithms for packet scheduling.  The algorithms we present are simple to implement and require small amounts of computation time.  Specifically, we present an O(1) probabilistic weighted fair queuing algorithm that emits packets from flows with an improved delay jitter.  We present experimental results with the proposed randomized algorithms which suggest that the algorithms may form a vailable alternative to the currently deployed deterministic fair queuing algorithms. 
State-based Classification of Finger Gestures from Electromyographic Signals| Abstract Electromyographic signals may provide an important new class of user interface for consumer electronics.  In order to make such interfaces effective, it will be crucial to map EMG signals to user gestures in real time.  The mapping from signals to gestures will vary from user to user, so it must be acquired adaptively.  In this paper, we describe and compare three methods for static classification of EMG signals.  We then go on to explore methods for adapting the classifiers over time and for sequential analysis of the gesture stream by combining the static classification algorithm with a hidden Markov model.  We conclude with an evaluation of the combined model on an unsegmented stream of gestures. 
Robust temporal and spectral modeling for query By melody| ABSTRACT Query by melody is the problem of retrieving musical performances from melodies.  Retrieval of real performances is complicated due to the large number of variations in performing a melody and the presence of colored accompaniment noise.  We describe a simple yet effective probabilistic model for this task.  We describe a generative model that is rich enough to capture the spectral and temporal variations of musical performances and allows for tractable melody retrieval.  While most of previous studies on music retrieval from melodies were performed with either symbolic (e. g.  MIDI) data or with monophonic (single instrument) performances, we performed experiments in retrieving live and studio recordings of operas that contain a leading vocalist and rich instrumental accompaniment.  Our results show that the probabilistic approach we propose is effective and can be scaled to massive datasets. 
Beyond Word N-Grams| Abstract We describe, analyze, and experimentally evaluate a new probabilistic model for wordsequence prediction in natural languages, based on prediction suffix trees (PSTs).  By using efficient data structures, we extend the notion of PST to unbounded vocabularies.  We also show how to use a Bayesian approach based on recursive priors over all possible PSTs to efficiently maintain tree mixtures.  These mixtures have provably and practically better performance than almost any single model.  Finally, we evaluate the model on several corpora.  The low perplexity achieved by relatively small PST mixture models suggests that they may be an advantageous alternative, both theoretically and practically, to the widely used n-gram models. 
A new family of online algorithms for category ranking| ABSTRACT We describe a new family of topic-ranking algorithms for multi-labeled documents.  The motivation for the algorithms stems from recent advances in online learning algorithms.  The algorithms we present are simple to implement and are time and memory ecient.  We evaluate the algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000.  On both corpora the algorithms we present outperform adaptations to topic-ranking of Rocchio's algorithm and the Perceptron algorithm.  We also outline the formal analysis of the algorithm in the mistake bound model.  To our knowledge, this work is the first to report performance results with the entire new Reuters corpus. 
Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers| Abstract We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm.  The proposed
An Efficient Boosting Algorithm for Combining Preferences| Abstract.  The problem of combining preferences arises in several applications, such as combining the results of different search engines.  This work describes an efficient algorithm for combining multiple preferences.  We first give a formal framework for the problem.  We then describe and analyze a new boosting algorithm for combining preferences called RankBoost.  We also describe an efficient implementation of the algorithm for a restricted case.  We discuss two experiments we carried out to assess the performance of RankBoost.  In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain.  For this task, we compare the performance of RankBoost to the individual search strategies.  The second experiment is a collaborative-filtering task for making movie recommendations.  Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms. 
A Markovian Lattice Model for the Acquisition of Morphological Structure| Abstract We describe a new formalism for word morphology.  Our model views word generation as a random walk on a lattice of units where each unit is a set of (short) strings.  The model naturally incorporates segmentation of words into morphemes.  We capture the statistics of unit generation using a probabilistic suffix tree (PST) which is a variant of variable length Markov models.  We present an efficient algorithm that learns a PST over the units whose output is a compact stochastic representation of morphological structure.  We demonstrate the applicability of our approach by using the model in an allomorphy decision problem. 
Learning to Order Things| Abstract There are many applications in which it is desirable to order rather than classify instances.  Here we consider the problem of learning how to order instances given feedback in the form of preference judgments, i. e. , statements to the effect that one instance should be ranked ahead of another.  We outline a two-stage approach in which one first learns by conventional means a binary preference function indicating whether it is advisable to rank one instance before another.  Here we consider an on-line algorithm for learning preference functions that is based on Freund and Schapire's "Hedge" algorithm.  In the second stage, new instances are ordered so as to maximize agreement with the learned preference function.  We show that the problem of finding the ordering that agrees best with a learned preference function is NP-complete.  Nevertheless, we describe simple greedy algorithms that are guaranteed to find a good approximation.  Finally, we show how metasearch can be formulated as an ordering problem, and present experimental results on learning a combination of "search experts," each of which is a domain-specific query expansion strategy for a web search engine. 
On the Learnability and Usage of Acyclic Probabilistic Finite Automata| Abstract We propose and analyze a distribution learning algorithm for a subclass of Acyclic Probabilistic Finite Automata (APFA).  This subclass is characterized by a certain distinguishability property of the automata's states.  Though hardness results are known for learning distributions generated by general APFAs, we prove that our algorithm can efficiently learn distributions generated by the subclass of APFAs we consider.  In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made arbitrarily small with high confidence in polynomial time.  We present two applications of our algorithm.  In the first, we show how to model cursively written letters.  The resulting models are part of a complete cursive handwriting recognition system.  In the second application we demonstrate how APFAs can be used to build multiplepronunciation models for spoken words.  We evaluate the APFA based pronunciation models on labeled speech data.  The good performance (in terms of the log-likelihood obtained on test data) achieved by the APFAs and the little time needed for learning suggests that the learning algorithm of APFAs might be a powerful alternative to commonly used probabilistic models. 
Boosting Applied to Tagging and PP Attachment| Abstract Boosting is a machine learning algorithm that is not well known in computational linguistics.  We apply it to part-of-speech tagging and prepositional phrase attachment.  Performance is very encouraging.  We also show how to improve data quality by using boosting to identify annotation errors. 
Logistic Regression, AdaBoost and Bregman Distances| Abstract We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances.  The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other.  For both problems, we give new algorithms and explain their potential advantages over existing methods.  These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once).  We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified.  For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique.  As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost.  We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm.  We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings. 
Multiclass Learning by Probabilistic Embeddings| Abstract We describe a new algorithmic framework for learning multiclass categorization problems.  In this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space.  In this space each instance is assigned the label it is nearest to.  We outline and analyze an algorithm, termed Bunching, for learning the pair of embeddings from labeled data.  A key construction in the analysis of the algorithm is the notion of probabilistic output codes, a generalization of error correcting output codes (ECOC).  Furthermore, the method of multiclass categorization using ECOC is shown to be an instance of Bunching.  We demonstrate the advantage of Bunching over ECOC by comparing their performance on numerous categorization problems. 
An extended abstract of this journal submission| Abstract We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances.  The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other.  For both problems, we give new algorithms and explain their potential advantages over existing methods.  These algorithms can be divided into two types based on whether the parameters are iteratively updated sequentially (one at a time) or in parallel (all at once).  We also describe a parameterized family of algorithms which interpolates smoothly between these two extremes.  For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique.  As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost.  We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with iterative scaling.  We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings. 
Discriminative Binaural Sound Localization| Abstract Time difference of arrival (TDOA) is commonly used to estimate the azimuth of a source in a microphone array.  The most common methods to estimate TDOA are based on finding extrema in generalized crosscorrelation waveforms.  In this paper we apply microphone array techniques to a manikin head.  By considering the entire cross-correlation waveform we achieve azimuth prediction accuracy that exceeds extrema locating methods.  We do so by quantizing the azimuthal angle and treating the prediction problem as a multiclass categorization task.  We demonstrate the merits of our approach by evaluating the various approaches on Sony's AIBO robot. 
The Hierarchical Hidden Markov Model: Analysis and Applications| Abstract.  We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM).  Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech.  We seek a systematic unsupervised approach to the modeling of such structures.  By extendingthe standard forward-backward(BaumWelch) algorithm, we derive an efficient procedure for estimating the model parameters from unlabeled data.  We then use the trained model for automatic hierarchical parsing of observation sequences.  We describe two applications of our model and its parameter estimation procedure.  In the first application we show how to construct hierarchical models of natural English text.  In these models different levels of the hierarchy correspond to structures on different length scales in the text.  In the second application we demonstrate how HHMMs can be used to automatically identify repeated strokes that represent combination of letters in cursive handwriting. 
Improved Output Coding for Classification Using Continuous Relaxation| Abstract Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems.  Previous research on output coding has employed, almost solely, predefined discrete codes.  We describe an algorithm that improves the performance of output codes by relaxing them to continuous codes.  The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines.  We describe experiments with the proposed algorithm, comparing it to standard discrete output codes.  The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes. 
Protein Family Classification Using Sparse Markov Transducers| Abstract We present a method for classifying proteins into families based on short subsequences of amino acids using a new probabilistic model called sparse Markov transducers (SMT).  We classify a protein by estimating probability distributions over subsequences of amino acids from the protein.  Sparse Markov transducers, similar to probabilistic suffix trees, estimate a probability distribution conditioned on an
Large margin hierarchical classification| Abstract We present an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure.  This structure is encoded by a rooted tree which induces a metric over the label set.  Our approach combines ideas from large margin kernel methods and Bayesian analysis.  Following the large margin principle, we associate a prototype with each label in the tree and formulate the learning task as an optimization problem with varying margin constraints.  In the spirit of Bayesian methods, we impose similarity requirements between the prototypes corresponding to adjacent labels in the hierarchy.  We describe new online and batch algorithms for solving the constrained optimization problem.  We derive a worst case loss-bound for the online algorithm and provide generalization analysis for its batch counterpart.  We demonstrate the merits of our approach with a series of experiments on synthetic, text and speech data. 
An Efficient PAC Algorithm for Reconstructing a Mixture of Lines| Abstract.  In this paper we study the learnability of a mixture of lines model which is of great importance in machine vision, computer graphics, and computer aided design applications.  The mixture of lines is a partially-probabilistic model for an image composed of line-segments.  Observations are generated by choosing one of the lines at random and picking a point at random from the chosen line.  Each point is contaminated with some noise whose distribution is unknown, but which is bounded in magnitude.  Our goal is to discover eciently and rather accurately the line-segments that generated the noisy observations.  We describe and analyze an ecient probably approximately correct (PAC) algorithm for solving the problem.  Our algorithm combines techniques from planar geometry with simple large deviation tools and is simple to implement. 
Using Substitution Matrices to Estimate Probability Distributions for Biological Sequences| Abstract Accurately estimating probabilities from observations is important for probabilistic-based approaches to problems in computational biology.  In this paper we present a biologically-motivated method for estimating probability distributions over discrete alphabets from observations using a mixture model of common ancestors.  The method is an extension of substitution matrix-based probability estimation methods.  In contrast to previous such methods, our method has a simple Bayesian interpretation and has the advantage over Dirichlet mixtures that it is both effective and simple to compute for large alphabets.  The method is applied to estimate amino acid probabilities based on observed counts in an alignment and is shown to perform comparably to previous methods.  The method is also applied to estimate probability distributions over protein families and improves protein classification accuracy. 
Shared Context Probabilistic Transducers| Abstract Recently, a model for supervised learning of probabilistic transducers represented by suffix trees was introduced.  However, this algorithm tends to build very large trees, requiring very large amounts of computer memory.  In this paper, we propose a new, more compact, transducer model in which one shares the parameters of distributions associated to contexts yielding similar conditional output distributions.  We illustrate the advantages of the proposed algorithm with comparative experiments on inducing a noun phrase recognizer. 
SIGIR '98 Boosting and Rocchio Applied to Text Filtering| Abstract We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost.  We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label.  We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm.  We then compare AdaBoost and Rocchio over three large text filtering tasks.  Overall both algorithms are comparable and are quite effective.  AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents.  However, on these tasks, Rocchio runs much faster than AdaBoost. 
The Forgetron: A Kernel-Based Perceptron on a Fixed Budget| Abstract The Perceptron algorithm, despite its simplicity, often performs well in online classification tasks.  The Perceptron becomes especially effective when it is used in conjunction with kernels.  However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly.  In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget.  To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores and, on the other hand, entertains a relative mistake bound.  In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach. 
Training Algorithms for Hidden Markov Models using Entropy Based Distance Functions| Abstract We present new algorithms for parameter estimation of HMMs.  By adapting a framework used for supervised learning, we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay "close" to the current estimated parameters.  We use a bound on the relative entropy between the two HMMs as a distance measure between them.  The result is new iterative training algorithms which are similar to the EM (Baum-Welch) algorithm for training HMMs.  The proposed algorithms are composed of a step similar to the expectation step of Baum-Welch and a new update of the parameters which replaces the maximization (re-estimation) step.  The algorithm takes only negligibly more time per iteration and an approximated version uses the same expectation step as Baum-Welch.  We evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data.  For sparse models, i. e.  models with relatively small number of non-zero parameters, the proposed algorithms require significantly fewer iterations.  1 Preliminaries We use the numbers from 0 to N to name the states of an HMM.  State 0 is a special initial state and state N is a special final state.  Any state sequence, denoted by s, starts with the initial state but never returns to it and ends in the final state.  Observations symbols are also numbers in f1; : : : ; Mg and observation sequences are denoted by x.  A discrete output hidden Markov model (HMM) is parameterized by two matrices A and B.  The first matrix is of dimension [N; N ] and a i;j (0 i N \Gamma 1; 1 j N) denotes the probability of moving from state i to state j.  The second matrix is of dimension [N +1;M ] and b i;k is the probability of outputting symbol k at state i.  The set of parameters of an HMM is denoted by ` = (A; B).  (The initial state distribution vector is represented by the first row of A. ) An HMM is a probabilistic generator of sequences.  It starts in the initial state 0.  It then iteratively does the following until the final state is reached.  If i is the current state then a next state j is chosen according to the transition probabilities out of the current state (row i of matrix A).  After arriving at state j a symbol is output according to the output probabilities of that state (row j of matrix B).  Let P (x; sj`) denote the probability (likelihood) that an HMM ` generates the observation sequence x on the path s starting at state 0 and ending at state N : P (x; sj jsj = jxj + 1; s 0 = 0; s jsj = N; `) def = Q jxj t=1 a s t\Gamma 1 ;s t b s t ;x t .  For the sake of brevity we omit the conditions on s and x.  Throughout the paper we assume that the HMMs are absorbing, that is from every state there is a path to the final state with a non-zero probability.  Similar parameter estimation algorithms can be derived for ergodic HMMs.  Absorbing HMMs induce a probability over all state-observation sequences, i. e.  P x;s P (x; sj`) = 1.  The likelihood of an observation sequence x is obtained by summing over all possible hidden paths (state sequences), P (xj`) = P s P (x; sj`).  To obtain the likelihood for a set X of observations we simply multiply the likelihood values for the individual sequences.  We seek an HMM ` that maximizes the likelihood for a given set of observations X , or equivalently, maximizes the log-likelihood, LL(X j`) = 1 jX j P x2X ln P (xj`).  To simplify our notation we denote the generic parameter in ` by ` i , where i ranges from 1 to the total number of parameters in A and B (There might be less if some are clamped to zero).  We denote the total number of parameters of ` by I and leave the (fixed) correspondence between the ` i and the entries of A and B unspecified.  The indices are naturally partitioned into classes corresponding to the rows of the matrices.  We denote by [i] the class of parameters to which ` i belongs and by ` [i] the vector of all ` j s. t.  j 2 [i].  If j 2 [i] then both ` i and ` j are parameters from the same row of one of the two matrices.  Whenever it is clear from the context, we will use [i] to denote both a class of parameters and the row number (i. e.  state) associated with the class.  We now can rewrite P (x; sj`) as Q I i=1 ` n i (x;s) i , where n i (x; s) is the number of times parameter i is used along the path s with observation sequence x.  (Note that this value does not depend on the actual parameters `. ) We next compute partial derivatives of the likelihood and the log-likelihood using this notation.  @ @` i P (x; sj`) = ` n 1 (x;s) 1 \Delta \Delta \Delta ` n i\Gamma 1 (x;s) i\Gamma 1 n i (x; s) ` n i (x;s)\Gamma 1 i \Delta \Delta \Delta ` n I (x;s) I = n i (x; s) ` i I Y i=1 ` n i (x;s) i = n i (x; s) ` i P (x; sj`): (1) @LL(X j`) @` i = 1 jX j X x2X X s @ @` i P (
PART-OF-SPEECH TAGGING USING A VARIABLE MEMORY MARKOV MODEL| Abstract We present a new approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models.  In contrast to fixed-length Markov models, which predict based on fixed-length histories, variable memory Markov models dynamically adapt their history length based on the training data, and hence may use fewer parameters.  In a test of a VMM based tagger on the Brown corpus, 95. 81% of tokens are correctly classified. 
Using mixtures of common ancestors for estimating the probabilities of discrete events in biological sequences| Abstract Accurately estimating probabilities from observations is important for probabilistic-based approaches to problems in computational biology.  In this paper we present a biologically-motivated method for estimating probability distributions over discrete alphabets from observations using a mixture model of common ancestors.  The method is an extension of substitution matrix-based probability estimation methods.  In contrast to previous substitution matrix-based methods, our method has a simple Bayesian interpretation.  The method presented in this paper has the advantage over Dirichlet mixtures that it is both effective and simple to compute for large alphabets.  The method is applied to estimate amino acid probabilities based on observed counts in an alignment and is shown to perform comparable to previous methods.  The method is also applied to estimate probability distributions over protein families and improves protein classification accuracy. 
A Comparison of New and Old Algorithms for a Mixture Estimation Problem| Abstract.  We investigate the problem of estimating the proportion vector which maximizes the likelihood of a given sample for a mixture of given densities.  We adapt a framework developed for supervised learning and give simple derivations for many of the standard iterative algorithms like gradient projection and EM.  In this framework, the distance between the new and old proportion vectors is used as a penalty term.  The square distance leads to the gradient projection update, and the relative entropy to a new update which we call the exponentiated gradient update (EGj ).  Curiously, when a second order Taylor expansion of the relative entropy is used, we arrive at an update EMj which, for j = 1, gives the usual EM update.  Experimentally, both the EMj-update and the EGj-update for j ? 1 outperform the EM algorithm and its variants.  We also prove a polynomial bound on the rate of convergence of the EGj algorithm. 
Using and Combining Predictors That Specialize| Abstract.  We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms, sometimes called "experts. 
The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees| Abstract Prediction suffix trees (PST) provide a popular and effective tool for tasks such as compression, classification, and language modeling.  In this paper we take a decision theoretic view of PSTs.  Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a mistake bound for it.  We then describe a self-bounded enhancement of our learning algorithm for which the learning process automatically grows a bounded-depth PST.  We also prove a similar mistake-bound for the self-bounded algorithm.  The result is an efficient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters.  To our knowledge, this is the first provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any fixed PST determined in hindsight. 
Kernel Design Using Boosting| Abstract The focus of the paper is the problem of learning kernel operators from empirical data.  We cast the kernel design problem as the construction of an accurate kernel from simple (and less accurate) base kernels.  We use the boosting paradigm to perform the kernel construction process.  To do so, we modify the booster so as to accommodate kernel operators.  We also devise an efficient weak-learner for simple kernels that is based on generalized eigen vector decomposition.  We demonstrate the effectiveness of our approach on synthetic data and on the USPS dataset.  On the USPS dataset, the performance of the Perceptron algorithm with learned kernels is systematically better than a fixed RBF kernel. 
On the Learnability and Design of Output Codes for Multiclass Problems| Abstract Output coding is a general framework for solving multiclass categorization problems.  Previous research on output codes has focused on building multiclass machines given predefined output codes.  In this paper we discuss for the first time the problem of designing output codes for multiclass problems.  For the design problem of discrete codes, which have been used extensively in previous works, we present mostly negative results.  We then introduce the notion of continuous codes and cast the design problem of continuous codes as a constrained optimization problem.  We describe three optimization problems corresponding to three different norms of the code matrix.  Interestingly, for the l 2 norm our formalism results in a quadratic program whose dual does not depend on the length of the code.  A special case of our formalism provides a multiclass scheme for building support vector machines which can be solved efficiently.  We give a time and space efficient algorithm for solving the quadratic program.  Preliminary experiments we have performed with synthetic data show that our algorithm is often two orders of magnitude faster than standard quadratic programming packages. 
Online and batch learning of pseudo-metrics| Abstract We describe and analyze an online algorithm for supervised learning of pseudo-metrics.  The algorithm receives pairs of instances and predicts their similarity according to a pseudo-metric.  The pseudo-metrics we use are quadratic forms parameterized by positive semi-definite matrices.  The core of the algorithm is an update rule that is based on successive projections onto the positive semi-definite cone and onto half-space constraints imposed by the examples.  We describe an efficient procedure for performing these projections, derive a worst case mistake bound on the similarity predictions, and discuss a dual version of the algorithm in which it is simple to incorporate kernel operators.  The online algorithm also serves as a building block for deriving a large-margin batch algorithm.  We demonstrate the merits of the proposed approach by conducting experiments on MNIST dataset and on document filtering. 
The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length| Abstract.  We propose and analyze a distribution learning algorithm for variable memory length Markov processes.  These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata.  The learning algorithm is motivated by real applications in man-machine interaction such as handwriting and speech recognition.  Conventionally used fixed memory Markov and hidden Markov models have either severe practical or theoretical drawbacks.  Though general hardness results are known for learning distributions generated by sources with similar structure, we prove that our algorithm can indeed efficiently learn distributions generated by our more restricted sources.  In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time and sample complexity.  We present two applications of our algorithm.  In the first one we apply our algorithm in order to construct a model of the English language, and use this model to correct corrupted text.  In the second application we construct a simple stochastic model for E. coli DNA. 
A Markov Model for the Acquisition of Morphological Structure| Abstract We describe a new formalism for word morphology.  Our model views word generation as a random walk on a trellis of units where each unit is a set of (short) strings.  The model naturally incorporates segmentation of words into morphemes.  We capture the statistics of unit generation using a probabilistic suffix tree (PST) which is a variant of variable length Markov models.  We present an efficient algorithm that learns a PST over the units whose output is a compact stochastic representation of morphological structure.  We demonstrate the applicability of our approach by using the model in an allomorphy decision problem. 
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network| Abstract We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.  Using these ideas together, the resulting tagger gives a 97. 24% accuracy on the Penn Treebank WSJ, an error reduction of 4. 4% on the best previous single automatically learned tagging result. 
Inferring Probabilistic Acyclic Automata Using the Minimum Description Length Principle| Abstract --- The use of Rissanen's minimum description length principle for the construction of probabilistic acyclic automata (PAA) is explored.  We propose a learning algorithm for a PAA that is adaptive both in the structure and the dimension of the model.  The proposed algorithm was tested on synthetic data as well as on real pattern recognition problems. 
Efficient Bayesian Parameter Estimation in Large Discrete Domains| Abstract We examine the problem of estimating the parameters of a multinomial distribution over a large number of discrete outcomes, most of which do not appear in the training data.  We analyze this problem from a Bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcomes constitute only a small subset of the possible outcomes.  We show how to efficiently perform exact inference with this form of hierarchical prior and compare our method to standard approaches. 
Spikernels: Embedding Spiking Neurons in Inner-Product Spaces| Abstract Inner-product operators, often referred to as kernels in statistical learning, define a mapping from some input space into a feature space.  The focus of this paper is the construction of biologically-motivated kernels for cortical activities.  The kernels we derive, termed Spikernels, map spike count sequences into an abstract vector space in which we can perform various prediction tasks.  We discuss in detail the derivation of Spikernels and describe an efficient algorithm for computing its value on any two sequences of neural population spike counts.  We demonstrate the merits of our modeling approach using various kernels for the task of predicting hand movement velocities from cortical recordings.  In all of our experiments all the kernels we tested outperform the standard scalar product used in regression with the Spikernel consistently achieving the best performance. 
A Simple, Fast, and Effective Rule Learner| Abstract We describe SLIPPER, a new rule learner that generates rulesets by repeatedly boosting a simple, greedy, rule-builder.  Like the rulesets built by other rule learners, the ensemble of rules created by SLIPPER is compact and comprehensible.  This is made possible by imposing appropriate constraints on the rule-builder, and by use of a recently-proposed generalization of Adaboost called confidence-rated boosting.  In spite of its relative simplicity, SLIPPER is highly scalable, and an effective learner.  Experimentally, SLIPPER scales no worse than O(n log n), where n is the number of examples, and on a set of 32 benchmark problems, SLIPPER achieves lower error rates than RIPPER 20 times, and lower error rates than C4. 5rules 22 times. 
The Power of Amnesia| Abstract We propose a learning algorithm for a variable memory length Markov process.  Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales.  On short scales it is characterized mostly by the dynamics that generate the process, whereas on large scales, more syntactic and semantic information is carried.  For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures.  On the other hand using long memory models uniformly is not practical even for as short memory as four.  The algorithm we propose is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small.  We demonstrate the algorithm by learning the structure of natural English text and applying the learned model to the correction of corrupted text.  Using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states.  We also show how the algorithm can be applied to intergenic E.  coli DNA base prediction with results comparable to HMM based methods. 
Generalization Error Bounds for Threshold Decision Lists| Abstract In this paper we consider the generalization accuracy of classification methods based on the iterative use of linear classifiers.  The resulting classifiers, which we call threshold decision lists act as follows.  Some points of the data set to be classified are given a particular classification according to a linear threshold function (or hyperplane).  These are then removed from consideration, and the procedure is iterated until all points are classified.  Geometrically, we can imagine that at each stage, points of the same classification are successively chopped off from the data set by a hyperplane.  We analyse theoretically the generalization properties of
Log-Linear Models for Label Ranking| Abstract Label ranking is the task of inferring a total order over a predefined set of labels for each given instance.  We present a general framework for batch learning of label ranking functions from supervised data.  We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent.  This enables us to accommodate a variety of ranking problems.  In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels.  Special cases of our setting are multilabel categorization and hierarchical classification.  We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration.  The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus. 
Boosting for Document Routing| ABSTRACT RankBoost is a recently proposed algorithm for learning ranking functions.  It is simple to implement and has strong justifications from computational learning theory.  We describe the algorithm and present experimental results on applying it to the document routing problem.  The first set of results applies RankBoost to a text representation produced using modern term weighting methods.  Performance of RankBoost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than RankBoost.  RankBoost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics.  Our second set of results examines the behavior of RankBoost when it has to learn not only a ranking function but also all aspects of term weighting from raw data.  Performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking functions are intriguing, and the approach could easily be adapted to mixtures of textual and nontextual data. 
Context-Sensitive Learning Methods for Text Categorization| Abstract Two recently implemented machine learning algorithms, RIPPER and sleeping experts for phrases, are evaluated on a number of large text categorization problems.  These algorithms both construct classifiers that allow the "context" of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification.  However, RIPPER and sleeping experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination.  In spite of these differences, both RIPPER and sleeping experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods.  We view this result as a confirmation of the usefulness of classifiers that represent contextual information. 
Learning Algorithm for Enclosing Points in Bregmanian Spheres| Abstract.  We discuss the problem of finding a generalized sphere that encloses points originating from a single source.  The points contained in such a sphere are within a maximal divergence from a center point.  The divergences we study are known as the Bregman divergences which include as a special case both the Euclidean distance and the relative entropy.  We cast the learning task as an optimization problem and show that it results in a simple dual form which bears interesting algebraic properties.  We then discuss a general algorithmic framework to solve the optimization problem.  Our training algorithm employs an auxiliary function that bounds the dual's objective function and can be used with a broad class of Bregman functions.  As a specific application of the algorithm we give a detailed derivation for the relative entropy.  We analyze the generalization ability of the algorithm by adopting margin-style proof techniques.  We also describe and analyze two schemes of online algorithms for the case when the radius of the sphere is set in advance. 
Pranking with Ranking| Abstract We discuss the problem of ranking instances.  In our framework each instance is associated with a rank or a rating, which is an integer from 1 to k.  Our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance's true rank.  We describe a simple and ecient online algorithm, analyze its performance in the mistake bound model, and prove its correctness.  We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering.  In the experiments we performed, our algorithm outperforms online algorithms for regression and classification applied to ranking. 
Ultraconservative Online Algorithms for Multiclass Problems| Abstract.  In this paper we study online classification algorithms for multiclass problems in the mistake bound model.  The hypotheses we use maintain one prototype vector per class.  Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and then sets the predicted label to be the index of the prototype achieving the highest similarity.  To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness.  Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype.  We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores.  We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm.  The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well.  We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. 
On-Line Portfolio Selection Using Multiplicative Updates| Abstract We present an on-line investment algorithm which achieves almost the same wealth as the best constant-rebalanced portfolio determined in hindsight from the actual market outcomes.  The algorithm employs a multiplicative update rule derived using a framework introduced by Kivinen and Warmuth.  Our algorithm is very simple to implement and requires only constant storage and computing time per stock in each trading period.  We tested the performance of our algorithm on real stock data from the New York Stock Exchange accumulated during a 22-year period.  On this data, our algorithm clearly outperforms the best single stock as well as Cover's universal portfolio selection algorithm.  We also present results for the situation in which the investor has access to additional "side information. "
Unsupervised Models for Named Entity Classification| Abstract This paper discusses the use of unlabeled examples for the problem of named entity classification.  A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier.  However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple "seed" rules.  The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.  We present two algorithms.  The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).  The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). 
Smooth "-Insensitive Regression by Loss Symmetrization| Abstract We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions.  These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss.  The resulting symmetric logistic loss can be viewed as a smooth approximation to the "-insensitive hinge loss used in support vector regression.  We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses.  The first family employs an iterative log-additive update which provides a regression counterpart for recent boosting algorithms.  The second family utilizes an iterative additive update step.  We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the logistic loss.  A byproduct of our work is a new simple form of regularization for boosting-based classification and regression algorithms.  Our regression framework also has implications on classification algorithms, namely, a new additive batch algorithm for the log-loss and exp-loss used in boosting.  We demonstrate the merits of our algorithms in a series of experiments including an experiment that boosts the accuracy of support vector regressors on a benchmark dataset. 
Batch and On-Line Parameter Estimation of Gaussian Mixtures Based on the Joint Entropy| Abstract We describe a new iterative method for parameter estimation of Gaussian mixtures.  The new method is based on a framework developed by Kivinen and Warmuth for supervised on-line learning.  In contrast to gradient descent and EM, which estimate the mixture's covariance matrices, the proposed method estimates the inverses of the covariance matrices.  Furthermore, the new parameter estimation procedure can be applied in both on-line and batch settings.  We show experimentally that it is typically faster than EM, and usually requires about half as many iterations as EM.  We also describe experiments with digit recognition that demonstrate the merits of the on-line version. 
Improved Boosting Algorithms Using Confidence-rated Predictions| Abstract.  We describe several improvements to Freund and Schapire'
Data-Driven Online to Batch Conversions| Abstract Online learning algorithms are typically fast, memory efficient, and simple to implement.  However, many common learning problems fit more naturally in the batch learning setting.  The power of online learning algorithms can be exploited in batch settings by using online-to-batch conversions, techniques which build a new batch algorithm from an existing online algorithm.  We first give a unified overview of three existing online-to-batch conversion techniques which do not use training data in the conversion process.  We then build upon these data-independent conversions to derive and analyze data-driven conversions.  Our conversions find hypotheses with a small risk by explicitly minimizing datadependent generalization bounds.  We experimentally demonstrate the usefulness of our approach, and in particular show that the data-driven conversions consistently outperform the data-independent conversions. 
LEARNING TO ALIGN POLYPHONIC MUSIC| ABSTRACT We describe an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acoustic counterpart.  Our method employs a supervised learning approach by using a training set of aligned symbolic and acoustic representations.  The alignment function we devise is based on mapping the input acousticsymbolic representation along with the target alignment into an abstract vector-space.  Building on techniques used for learning support vector machines (SVM), our alignment function distills to a classifier in the abstract vectorspace which separates correct alignments from incorrect ones.  We describe a simple iterative algorithm for learning the alignment function and discuss its formal properties.  We use our method for aligning MIDI and MP3 representations of polyphonic recordings of piano music.  We also compare our discriminative approach to a generative method based on a generalization of hidden Markov models.  In all of our experiments, the discriminative method outperforms the HMM-based method. 
Decoding Cursive Scripts| Abstract Online cursive handwriting recognition is currently one of the most intriguing challenges in pattern recognition.  This study presents a novel approach to this problem which is composed of two complementary phases.  The first is dynamic encoding of the writing trajectory into a compact sequence of discrete motor control symbols.  In this compact representation we largely remove the redundancy of the script, while preserving most of its intelligible components.  In the second phase these control sequences are used to train adaptive probabilistic acyclic automata (PAA) for the important ingredients of the writing trajectories, e. g.  letters.  We present a new and efficient learning algorithm for such stochastic automata, and demonstrate its utility for spotting and segmentation of cursive scripts.  Our experiments show that over 90% of the letters are correctly spotted and identified, prior to any higher level language model.  Moreover, both the training and recognition algorithms are very efficient compared to other modeling methods, and the models are `on-line' adaptable to other writers and styles. 
Adaptive Mixtures of Probabilistic Transducers| Abstract We describe and analyze a mixture model for supervised learning of probabilistic transducers.  We devise an on-line learning algorithm that efficiently infers the structure and estimates the parameters of each probabilistic transducer in the mixture.  Theoretical analysis and comparative simulations indicate that the learning algorithm tracks the best transducer from an arbitrarily large (possibly infinite) pool of models.  We also present an application of the model for inducing a noun phrase recognizer. 
Leveraging the margin more carefully| Abstract Boosting is a popular approach for building accurate classifiers.  Despite the initial popular belief, boosting algorithms do exhibit overfitting and are sensitive to label noise.  Part of the sensitivity of boosting algorithms to outliers and noise can be attributed to the unboundedness of the margin-based loss functions that they employ.  In this paper we describe two leveraging algorithms that build on boosting techniques and employ a bounded loss function of the margin.  The first algorithm interleaves the expectation maximization (EM) algorithm with boosting steps.  The second algorithm decomposes a non-convex loss into a difference of two convex losses.  We prove that both algorithms converge to a stationary point.  We also analyze the generalization properties of the algorithms using the Rademacher complexity.  We describe experiments with both synthetic data and natural data (OCR and text) that demonstrate the merits of our framework, in particular robustness to outliers. 
Learning to Query the Web| Abstract The World Wide Web (WWW) is filled with "resource directories"---i. e. , documents that collect together links to all known documents on a specific topic.  Keeping resource directories up-to-date is difficult because of the rapid growth in online documents.  We propose using machine learning methods to address this problem.  In particular, we propose to treat a resource directory as a list of positive examples of an unknown concept, and then use machine learning methods to construct from these examples a definition of the unknown concept.  If the learned definition is in the appropriate form, it can be translated into a query, or series of queries, for a WWW search engine.  This query can be used at a later date to detect any new instances of the concept.  We present experimental results with two implemented systems, and two learning methods.  One system is interactive, and is implemented as an augmented WWW browser; the other is a batch system, which can collect and label documents without any human intervention.  The learning methods are the RIPPER rule learning system, and a rule-learning version of a new online weight allocation algorithm called the sleeping experts prediction algorithm.  The experiments are performed on real data obtained from the WWW. 
Online Classification on a Budget| Abstract Online algorithms for classification often require vast amounts of memory and computation time when employed in conjunction with kernel functions.  In this paper we describe and analyze a simple approach for an on-the-fly reduction of the number of past examples used for prediction.  Experiments performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine (SVM) although the latter, being a batch algorithm, accesses each training example multiple times. 
A Temporal Kernel-Based Model for Tracking Hand-Movements from Neural Activities| Abstract We devise and experiment with a dynamical kernel-based system for tracking hand movements from neural activity.  The state of the system corresponds to the hand location, velocity, and acceleration, while the system's input are the instantaneous spike rates.  The system's state dynamics is defined as a combination of a linear mapping from the previous estimated state and a kernel-based mapping tailored for modeling neural activities.  In contrast to generative models, the activity-to-state mapping is learned using discriminative methods by minimizing a noise-robust version of the ` 1 norm.  We use this approach to predict hand trajectories on the basis of neuronal activity in motor cortex of behaving monkeys and find that the proposed approach is more accurate than both a static approach based on support vector regression and the Kalman filter. 
On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines|
Boosting and Rocchio Applied to Text Filtering|
BoosTexter: A Boosting-based System for Text Categorization|
Update Rules for Parameter Estimation in Bayesian Networks|
Switching Portfolios|
An Efficient Extension to Mixture Techniques for Prediction and Decision Trees|
To appear| Boostexter: A boosting-based system for text categorization. 
Learning Probabilistic Automata with Variable Memory Length|
Adaptive Mixture of Probabilistic Transducers|
Smooth e-Intensive Regression by Loss Symmetrization|
The Hierarchical Hidden Markov Model: Analysis and Applicaitons|
Adaptive mixtures of probalistic transducers|
Learning Gaussian Mixtures Based on the Relative Entropy"; yet unpublished|

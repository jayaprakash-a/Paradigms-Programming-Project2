A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning| Abstract We consider the situation in semi-supervised learning, where the "label sampling" mechanism stochastically depends on the true response (as well as potentially on the features).  We suggest a method of moments for estimating this stochastic dependence using the unlabeled data.  This is potentially useful for two distinct purposes: a.  As an input to a supervised learning procedure which can be used to "de-bias" its results using labeled data only and b.  As a potentially interesting learning task in itself.  We present several examples to illustrate the practical usefulness of our method. 
Model selection via the AUC| Abstract We present a statistical analysis of the AUC as an evaluation criterion for classification scoring models.  First, we consider signi#cance tests for the difference between AUC scores of two algorithms on the same test set.  We derive exact moments under simplifying assumptions and use them to examine approximate practical methods from the literature.  We then compare AUC to empirical misclassification error when the prediction goal is to minimize future error rate.  We show that the AUC may be preferable to empirical error even in this case and discuss the tradeoff between approximation error and estimation error underlying this phenomenon. 
Customer lifetime value modeling and its use for customer retention planning| ABSTRACT We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry.  We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ.  We then describe how we build on this approach to estimate the effects of retention on Lifetime Value.  Our solution has been successfully implemented in Amdocs' Business Insight (BI) platform, and we illustrate its usefulness in real-world scenarios. 
Margin Maximizing Loss Functions| Abstract Margin maximizing properties play an important role in the analysis of classification models, such as boosting and support vector machines.  Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built.  We formulate and prove a sufficient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes.  This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss.  We also generalize it to multi-class classification problems, and present margin maximizing multiclass versions of logistic regression and support vector machines. 
Sparsity and smoothness via the fused lasso| Abstract The lasso (Tibshirani 1996) penalizes a least squares regression by the sum of the absolute values (L 1 norm) of the coecients.  The form of this penalty encourages sparse solutions, that is, having many coecients equal to zero.  In this paper we propose the \fused lasso", a generalization of the lasso designed for problems with features that come in a natural order.  The fused lasso penalizes both the L 1 norm of the coecients and their successive differences.  Thus it encourages both sparsity of the coecients and sparsity of their differences, that is, local constancy of the coecient profile.  The fused lasso is especially useful when the number of features p # N , the sample size.  The technique is also extended to the \hinge" loss function, that underlies the support vector classifier.  We illustrate the methods on examples from protein mass spectroscopy and gene expression data. 
1-norm Support Vector Machines| Abstract The standard 2-norm SVM is known for its good performance in twoclass classication.  In this paper, we consider the 1-norm SVM.  We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features.  We also propose an efcient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 
The Entire Regularization Path for the Support Vector Machine| Abstract In this paper we argue that the choice of the SVM cost parameter can be critical.  We then derive an algorithm that can fit the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as fitting one SVM model. 
Boosting Density Estimation| Abstract Several authors have suggested viewing boosting as a gradient descent search for a good fit in function space.  We apply gradient-based boosting methodology to the unsupervised learning problem of density estimation.  We show convergence properties of the algorithm and prove that a strength of weak learnability property applies to this problem as well.  We illustrate the potential of this approach through experiments with boosting Bayesian networks to learn density models. 
Discovery of Fraud Rules for Telecommunications - Challenges and Solutions|
Ranking-Methods for Flexible Evaluation and Efficient Comparison of 2-Class Models|
Evaluation of prediction models for marketing campaigns|
Boosting as a Regularized Path to a Maximum Margin Classifier|
Piecewise linear regularized solution paths|
L1 norm support vector machines|
KDD-Cup 99: Knowledge Discovery In a Charitable Organization's Donor Database|
KDD-Cup 2000 Question 1 Winner's Report,|

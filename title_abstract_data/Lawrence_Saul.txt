A variational principle for model-based interpolation| Abstract Given a multidimensional data set and a model of its density, we consider how to define the optimal interpolation between two points.  This is done by assigning a cost to each path through space, based on two competing goals---one to interpolate through regions of high density, the other to minimize arc length.  From this path functional, we derive the Euler-Lagrange equations for extremal motion; given two points, the desired interpolation is found by solving a boundary value problem.  We show that this interpolation can be done efficiently, in high dimensions, for Gaussian, Dirichlet, and mixture models. 
Distance Metric Learning for Large Margin Nearest Neighbor Classification| Abstract We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classification by semidefinite programming.  The metric is trained with the goal that k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin.  On seven data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification---for example, achieving a test error rate of 1. 8% on the MNIST handwritten digits.  Our approach has many parallels to support vector machines, including a convex objective function based on the hinge loss and the potential to work in nonlinear feature spaces by using the "kernel trick".  On the other hand, our framework requires no modification for problems with large numbers of classes. 
Hierarchical Distributed Representations for Statistical Language Modeling| Abstract Statistical language models estimate the probability of a word occurring in a given context.  The most common language models rely on a discrete enumeration of predictive contexts (e. g. , n-grams) and consequently fail to capture and exploit statistical regularities across these contexts.  In this paper, we show how to learn hierarchical, distributed representations of word contexts that maximize the predictive value of a statistical language model.  The representations are initialized by unsupervised algorithms for linear and nonlinear dimensionality reduction [14], then fed as input into a hierarchical mixture of experts, where each expert is a multinomial distribution over predicted words [12].  While the distributed representations in our model are inspired by the neural probabilistic language model of Bengio et al.  [2, 3], our particular architecture enables us to work with significantly larger vocabularies and training corpora.  For example, on a large-scale bigram modeling task involving a sixty thousand word vocabulary and a training corpus of three million sentences, we demonstrate consistent improvement over class-based bigram models [10, 13].  We also discuss extensions of our approach to longer multiword contexts. 
Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization| Abstract An auditory "scene", composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources.  Pitch is known to be an important cue for auditory scene analysis.  In this paper, with the goal of building agents that operate in human environments, we describe a real-time system to identify the presence of one or more voices and compute their pitch.  The signal processing in the front end is based on instantaneous frequency estimation, a method for tracking the partials of voiced speech, while the pattern-matching in the back end is based on nonnegative matrix factorization, an unsupervised algorithm for learning the parts of complex objects.  While supporting a framework to analyze complicated auditory scenes, our system maintains real-time operability and state-of-the-art performance in clean speech. 
Mixed Memory Markov Models: Decomposing Complex Stochastic Processes as Mixtures of Simpler Ones| Abstract.  We study Markov models whose state spaces arise from the Cartesian product of two or more discrete random variables.  We show how to parameterize the transition matrices of these models as a convex combination---or mixture---of simpler dynamical models.  The parameters in these models admit a simple probabilistic interpretation and can be fitted iteratively by an Expectation-Maximization (EM) procedure.  We derive a set of generalized Baum-Welch updates for factorial hidden Markov models that make use of this parameterization.  We also describe a simple iterative procedure for approximately computing the statistics of the hidden states.  Throughout, we give examples where mixed memory models provide a useful representation of complex stochastic processes. 
Learning a kernel matrix for nonlinear dimensionality reduction| Abstract We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold.  Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled.  The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors.  The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding.  The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification.  We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods. 
Maximum Likelihood and Minimum Classification Error Factor Analysis for Automatic Speech Recognition| Abstract Hidden Markov models (HMMs) for automatic speech recognition rely on high dimensional feature vectors to summarize the short-time properties of speech.  Correlations between features can arise when the speech signal is non-stationary or corrupted by noise.  We investigate how to model these correlations using factor analysis, a statistical method for dimensionality reduction.  Factor analysis uses a small number of parameters to model the covariance structure of high dimensional data.  These parameters can be chosen in two ways: (i) to maximize the likelihood of observed speech signals, or (ii) to minimize the number of classification errors.  We derive an Expectation-Maximization (EM) algorithm for maximum likelihood estimation and a gradient descent algorithm for improved class discrimination.  Speech recognizers are evaluated on two tasks, one small-sized vocabulary (connected alpha-digits) and one medium-sized vocabulary (New Jersey town names).  We find that modeling feature correlations by factor analysis leads to significantly increased likelihoods and word accuracies.  Moreover, the rate of improvement with model size often exceeds that observed in conventional HMMs. 
Inference in Multilayer Networks via Large Deviation Bounds| Abstract We study probabilistic inference in large, layered Bayesian networks represented as directed acyclic graphs.  We show that the intractability of exact inference in such networks does not preclude their effective use.  We give algorithms for approximate probabilistic inference that exploit averaging phenomena occurring at nodes with large numbers of parents.  We show that these algorithms compute rigorous lower and upper bounds on marginal probabilities of interest, prove that these bounds become exact in the limit of large networks, and provide rates of convergence. 
Modeling Distances in Large-Scale Networks by Matrix Factorization| ABSTRACT In this paper, we propose a model for representing and predicting distances in large-scale networks by matrix factorization.  The model is useful for network distance sensitive applications, such as content distribution networks, topologyaware overlays, and server selections.  Our approach overcomes several limitations of previous coordinates-based mechanisms, which cannot model sub-optimal routing or asymmetric routing policies.  We describe two algorithms --singular value decomposition (SVD) and nonnegative matrix factorization (NMF)---for representing a matrix of network distances as the product of two smaller matrices.  With such a representation, we build a scalable system---Internet Distance Estimation Service (IDES)---that predicts large numbers of network distances from limited numbers of measurements.  Extensive simulations on real-world data sets show that IDES leads to more accurate, efficient and robust predictions of latencies in large-scale networks than previous approaches. 
An Introduction to Locally Linear Embedding| Abstract Many problems in information processing involve some form of dimensionality reduction.  Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data.  LLE attempts to discover nonlinear structure in high dimensional data by exploiting the local symmetries of linear reconstructions.  Notably, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations--though capable of generating highly nonlinear embeddings---do not involve local minima.  We illustrate the method on images of lips used in audiovisual speech synthesis. 
Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifold| Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data.  The data, assumed to lie on a nonlinear manifold, is mapped into a single global coordinate system of lower dimensionality.  The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem.  Notably, the optimizations in LLE--though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima.  We describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance.  The algorithm is applied to manifolds of known structure, as well as real data sets consisting of images of faces, digits, and lips.  We provide extensive illustrations of the algorithm's performance. 
Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines| Abstract We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs).  The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane.  The updates optimize the traditionally proposed objective function for SVMs.  They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration.  They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration.  We analyze the asymptotic convergence of the updates and show that the coefficients of non-support vectors decay geometrically to zero at a rate that depends on their margins.  In practice, the updates converge very rapidly to good classifiers. 
Multiplicative Updates for Classification by Mixture Models| Abstract We investigate a learning algorithm for the classification of nonnegative data by mixture models.  Multiplicative update rules are derived that directly optimize the performance of these models as classifiers.  The update rules have a simple closed form and an intuitive appeal.  Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm---its guarantee of monotonic improvement, and its absence of tuning parameters---with the added advantage of optimizing a discriminative objective function.  The algorithm reduces as a
Statistical Signal Processing with Nonnegativity Constraints| Abstract Nonnegativity constraints arise frequently in statistical learning and pattern recognition.  Multiplicative updates provide natural solutions to optimizations involving these constraints.  One well known set of multiplicative updates is given by the ExpectationMaximization algorithm for hidden Markov models, as used in automatic speech recognition.  Recently, we have derived similar algorithms for nonnegative deconvolution and nonnegative quadratic programming.  These algorithms have applications to low-level problems in voice processing, such as fundamental frequency estimation, as well as high-level problems, such as the training of large margin classifiers.  In this paper, we describe these algorithms and the ideas that connect them. 
An Introduction to Variational Methods for Graphical Models| Abstract.  This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields).  We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms.  We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient.  Inference in the simpified model provides bounds on probabilities of interest in the original model.  We describe a general framework for generating variational transformations based on convex duality.  Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case. 
Fast Learning by Bounding Likelihoods in Sigmoid Type Belief Networks| Abstract Sigmoid type belief networks, a class of probabilistic neural networks, provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and supervised learning problems.  Often the parameters used in these networks need to be learned from examples.  Unfortunately, estimating the parameters via exact probabilistic calculations (i. e, the EM-algorithm) is intractable even for networks with fairly small numbers of hidden units.  We propose to avoid the infeasibility of the E step by bounding likelihoods instead of computing them exactly.  We introduce extended and complementary representations for these networks and show that the estimation of the network parameters can be made fast (reduced to quadratic optimization) by performing the estimation in either of the alternative domains.  The complementary networks can be used for continuous density estimation as well. 
Global Coordination of Local Linear Models| Abstract High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. 
A Generalized Linear Model for Principal Component Analysis of Binary Data| Abstract We investigate a generalized linear model for dimensionality reduction of binary data.  The model is related to principal component analysis (PCA) in the same way that logistic regression is related to linear regression.  Thus we refer to the model as logistic PCA.  In this paper, we derive an alternating least squares method to estimate the basis vectors and generalized linear coefficients of the logistic PCA model.  The resulting updates have a simple closed form and are guaranteed at each iteration to improve the model's likelihood.  We evaluate the performance of logistic PCA---as measured by reconstruction error rates---on data sets drawn from four real world applications.  In general, we find that logistic PCA is much better suited to modeling binary data than conventional PCA. 
Real Time Voice Processing with Audiovisual Feedback: Toward Autonomous Agents with Perfect Pitch| Abstract We have implemented a real time front end for detecting voiced speech and estimating its fundamental frequency.  The front end performs the signal processing for voice-driven agents that attend to the pitch contours of human speech and provide continuous audiovisual feedback.  The algorithm we use for pitch tracking has several distinguishing features: it makes no use of FFTs or autocorrelation at the pitch period; it updates the pitch incrementally on a sample-by-sample basis; it avoids peak picking and does not require interpolation in time or frequency to obtain high resolution estimates; and it works reliably over a four octave range, in real time, without the need for postprocessing to produce smooth contours.  The algorithm is based on two simple ideas in neural computation: the introduction of a purposeful nonlinearity, and the error signal of a least squares fit.  The pitch tracker is used in two real time multimedia applications: a voice-to-MIDI player that synthesizes electronic music from vocalized melodies, and an audiovisual Karaoke machine with multimodal feedback.  Both applications run on a laptop and display the user's pitch scrolling across the screen as he or she sings into the computer. 
Large Deviation Methods for Approximate Probabilistic Inference| Abstract We study two-layer belief networks of binary random variables in which the conditional probabilities Pr[childjparents] depend monotonically on weighted sums of the parents.  In large networks where exact probabilistic inference is intractable, we showhow to compute upper and lower bounds on many probabilities of interest.  In particular, using methods from large deviation theory,we derive rigorous bounds on marginal probabilities suchasPr[children] and prove rates of convergence for the accuracy of our bounds as a function of network size.  Our results apply to networks with generic transfer function parameterizations of the conditional probability tables, such as sigmoid and noisy-OR.  They also explicitly illustrate the types of averaging behavior that can simplify the problem of inference in large networks. 
Markov Processes on Curves| Abstract We study the classification problem that arises when two variables---one continuous (x), one discrete (s)---evolve jointly in time.  We suppose that the vector x traces out a smooth multidimensional curve, to each point of which the variable s attaches a discrete label.  The trace of s thus partitions the curve into different segments whose boundaries occur where s changes value.  We consider how to learn the mapping between the trace of x and the trace of s from examples of segmented curves.  Our approach is to model the conditional random process that generates segments of constant s along the curve of x.  We suppose that the variable s evolves stochastically as a function of the arc length traversed by x.  Since arc length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions are invariant to nonlinear warpings (or reparameterizations) of time.  We show how to estimate the parameters of these models---known as Markov processes on curves (MPCs)---from labeled and unlabeled data.  We then apply these models to two problems in automatic speech recognition, where x are acoustic feature trajectories and s are phonetic alignments. 
Semisupervised alignment of manifolds| Abstract In this paper, we study a family of semisupervised learning algorithms for "aligning" different data sets that are characterized by the same underlying manifold.  The optimizations of these algorithms are based on graphs that provide a discretized approximation to the manifold.  Partial alignments of the data sets---obtained from prior knowledge of their manifold structure or from pairwise correspondences of subsets of labeled examples--are completed by integrating supervised signals with unsupervised frameworks for manifold learning.  As an illustration of this semisupervised setting, we show how to learn mappings between different data sets of images that are parameterized by the same underlying modes of variability (e. g. , pose and viewing angle).  The curse of dimensionality in these problems is overcome by exploiting the low dimensional structure of image manifolds. 
Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech| Abstract An eigenvalue method is developed for analyzing periodic structure in speech.  Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA).  Our method---called periodic component analysis (#CA)---uses constructive interference to enhance periodic components of the frequency spectrum and destructive interference to cancel noise.  The front end emulates important aspects of auditory processing, such as cochlear filtering, nonlinear compression, and insensitivity to phase, with the aim of approaching the robustness of human listeners.  The method avoids the inefficiencies of autocorrelation at the pitch period: it does not require long delay lines, and it correlates signals at a clock rate on the order of the actual pitch, as opposed to the original sampling rate.  We derive its cost function and present some experimental results. 
Attractor Dynamics in Feedforward Neural Networks| We study the probabilistic generative models parameterized by feedforward neural networks.  An attractor dynamics for probabilistic inference in these models is derived from a mean field approximation for large, layered sigmoidal networks.  Fixed points of the dynamics correspond to solutions of the mean field equations, which relate the statistics of each unit to those of its Markov blanket.  We establish globalconvergence of the dynamics by providing a Lyapunov function and show that the dynamics generate the signals required for unsupervised learning.  Our results for feedforwardnetworks provide a counterpart to those of Cohen-Grossberg and Hopfield for symmetric networks. 
Mean Field Theory for Sigmoid Belief Networks| Abstract We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics.  Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence.  We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits. 
Unsupervised Learning of Image Manifolds by Semidefinite Programming| Abstract Can we detect low dimensional structure in high dimensional data sets of images and video? The problem of dimensionality reduction arises often in computer vision and pattern recognition.  In this paper, we propose a new solution to this problem based on semidefinite programming.  Our algorithm can be used to analyze high dimensional data that lies on or near a low dimensional manifold.  It overcomes certain limitations of previous work in manifold learning, such as Isomap and locally linear embedding.  We illustrate the algorithm on easily visualized examples of curves and surfaces, as well as on actual images of faces, handwritten digits, and solid objects. 
Hidden Markov decision trees \Lambda| Abstract We study a time series model that can be viewed as a decision tree with Markov temporal structure.  The model is intractable for exact calculations, thus we utilize variational approximations.  We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence.  We present simulation results for artificial data and the Bach chorales. 
Nonlinear Dimensionality Reduction by Semidefinite Programming and Kernel Matrix Factorization| Abstract We describe an algorithm for nonlinear dimensionality reduction based on semidefinite programming and kernel matrix factorization.  The algorithm learns a kernel matrix for high dimensional data that lies on or near a low dimensional manifold.  In earlier work, the kernel matrix was learned by maximizing the variance in feature space while preserving the distances and angles between nearest neighbors.  In this paper, adapting recent ideas from semi-supervised learning on graphs, we show that the full kernel matrix can be very well approximated by a product of smaller matrices.  Representing the kernel matrix in this way, we can reformulate the semidefinite program in terms of a much smaller submatrix of inner products between randomly chosen landmarks.  The new framework leads to order-of-magnitude reductions in computation time and makes it possible to study much larger problems in manifold learning. 
NONNEGATIVE DECONVOLUTION FOR TIME OF ARRIVAL ESTIMATION| ABSTRACT The interaural time difference (ITD) of arrival is a primary cue for acoustic sound source localization.  Traditional estimation techniques for ITD based upon cross-correlation are related to maximum-likelihood estimation of a simple generative model.  We generalize the time difference estimation into a deconvolution problem with nonnegativity constraints.  The resulting nonnegative least squares optimization can be efficiently solved using a novel iterative algorithm with guaranteed global convergence properties.  We illustrate the utility of this algorithm using simulations and experimental results from a robot platform. 
Multiplicative Updates for Large Margin Classifiers| Abstract.  Various problems in nonnegative quadratic programming arise in the training of large margin classifiers.  We derive multiplicative updates for these problems that converge monotonically to the desired solutions for hard and soft margin classifiers.  The updates differ strikingly in form from other multiplicative updates used in machine learning.  In this paper, we provide complete proofs of convergence for these updates and extend previous work to incorporate sum and box constraints in addition to nonnegativity. 
Robust numeric recognition in spoken language dialogue| Abstract This paper addresses the problem of automatic numeric recognition and understanding in spoken language dialogue.  We show that accurate numeric understanding in uent unconstrained speech demands maintaining robustness at several dierent levels of system design,
Aggregate and mixed-order Markov models for statistical language processing|
Exploiting Tractable Substructures in Intractable Networks|
Boltzmann Chains and Hidden Markov Models|
Hidden Markov Decision Trees|
A Variational Principle for Model-based Morphing|
"Nonlinear dimensionality reduction by locally linear embedding,"|
Locally linear embedding|
Automatic Segmentation of Continuous Trajectories with Invariance to Nonlinear Warpings of Time|
Markov Processes on Curves for Automatic Speech Recognition|
Modeling Acoustic Correlations by Factor Analysis|
LLE Code Page| Internet URL. 

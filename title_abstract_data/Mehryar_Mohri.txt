Rapid Unit Selection from a Large Speech Corpus for Concatenative Speech Synthesis| ABSTRACT Concatenative Text-to-Speech (TTS) systems such as those described by Hunt and Black [6] can select at synthesis time from a very large number of recorded units.  The selected units are chosen to minimize a combination of target and join costs for a given sentence.  However, the join costs, in particular, can be quite expensive to compute, even when this computation has been optimized.  If possible, we would avoid this computation by precomputing and caching all the possible join costs, but their number is prohibitive.  Although the search space of possible joins is large, we have found that only a small fraction are selected in practice.  By synthesizing a large quantity of text and logging the units actually selected, we were able to gather usage statistics and construct a practical and efficient cache of concatenation costs.  Use of this cache dramatically decreased the runtime of the AT&T Next-Generation TTS system [1] with negligible effect on speech quality.  Experiments show that by caching 0. 7% of the possible joins, 99% of the join cost computations can be avoided. 
Dynamic Compilation of Weighted Context-Free Grammars| Abstract Weighted context-free grammars are a convenient formalism for representing grammatical constructions and their likelihoods in a variety of language-processing applications.  In particular, speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances.  In many of those applications, the actual languages described are regular, but context-free representations are much more concise and easier to create.  We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages.  The resulting automata can then be combined with other speech recognition components.  Our method allows the recognizer to dynamically activate or deactivate grammar rules and substitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation.  We also report experimental results showing the practicality of the approach. 
An Efficient Compiler for Weighted Rewrite Rules| Abstract Context-dependent rewrite rules are used in many areas of natural language and speech processing.  Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs).  We describe a new algorithm for compiling rewrite rules into FSTs.  We show the algorithm to be simpler and more efficient than existing algorithms.  Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights.  We have extended the algorithm to allow for this.  1.  Motivation Rewrite rules are used in many areas of natural language and speech processing, including syntax, morphology, and phonology 1 .  In interesting applications, the number of rules can be very large.  It is then crucial to give a representation of these rules that leads to efficient programs.  Finite-state transducers provide just such a compact representation (Mohri, 1994).  They are used in various areas of natural language and speech processing because their increased computational power enables one to build very large machines to model interestingly complex linguistic phenomena.  They also allow algebraic operations such as union, composition, and projection which are very useful in practice (
Context-Free Recognition with Weighted Automata| Abstract We introduce the definition of language recognition with weighted automata, a generalization of the classical definition of recognition with unweighted acceptors.  We show that, with our definition of recognition, weighted automata can be used to recognize a class of languages that strictly includes regular languages.  The class of languages accepted depends on the weight set which has the algebraic structure of a semiring.  We give a generic linear time algorithm for recognition with weighted automata and describe examples with various weight sets illustrating the recognition of several classes of context-free languages.  We prove, in particular, that the class of languages equivalent to the language of palindromes can be recognized by weighted automata over the (+; \Delta)-semiring, and that the class of languages equivalent to the Dyck language of first order D 0\Lambda 1 can be recognized by weighted automata over the real tropical semiring. 
Margin-Based Ranking Meets Boosting in the Middle| AdaBoost and RankBoost will produce the same result, explaining the empirical observations. 
Statistical Modeling for Unit Selection in Speech Synthesis| Abstract Traditional concatenative speech synthesis systems use a number of heuristics to define the target and concatenation costs, essential for the design of the unit selection component.  In contrast to these approaches, we introduce a general statistical modeling framework for unit selection inspired by automatic speech recognition.  Given appropriate data, techniques based on that framework can result in a more accurate unit selection, thereby improving the general quality of a speech synthesizer.  They can also lead to a more modular and a substantially more efficient system.  We present a new unit selection system based on statistical modeling.  To overcome the original absence of data, we use an existing high-quality unit selection system to generate a corpus of unit sequences.  We show that the concatenation cost can be accurately estimated from this corpus using a statistical n-gram language model over units.  We used weighted automata and transducers for the representation of the components of the system and designed a new and more efficient composition algorithm making use of string potentials for their combination.  The resulting statistical unit selection is shown to be about 2. 6 times faster than the last release of the AT&T Natural Voices Product while preserving the same quality, and offers much flexibility for the use and integration of new and more complex components.  1 Motivation A concatenative speech synthesis system (Hunt and Black, 1996; Beutnagel et al. , 1999a) consists of three components.  The first component, the textanalysis frontend, takes text as input and outputs a sequence of feature vectors that characterize the acoustic signal to synthesize.  The first element of each of these vectors is the predicted phone or halfphone; other elements are features such as the phonetic context, acoustic features (e. g. , pitch, duration), or prosodic features. 
Journal of Automata, Languages and Combinatorics| ABSTRACT Weighted automata and transducers are powerful devices used in many large-scale applications.  The eciency of these applications is substantially increased when the automata or transducers used are deterministic.  There exists a general determinization algorithm for weighted automata and transducers that is an extension of the classical subset construction used in the case of unweighted finite automata [14]. 
Minimization algorithms for sequential transducers| Abstract We present general algorithms for minimizing sequential finite-state transducers that output strings or numbers.  The algorithms are shown to be efficient since in the case of acyclic transducers and for output strings they operate in O(S + jEj + jV j + (jEj \Gamma jV j + jF j) \Delta (jP max j + 1)) steps, where S is the sum of the lengths of all output labels of the resulting transducer, E the set of transitions of the given transducer, V the set of its states, F the set of final states, and P max one of the longest of the longest common prefixes of the output paths leaving each state of the transducer.  The algorithms apply to a larger class of transducers which includes subsequential transducers. 
p-Subsequentiable Transducers| Abstract.  p-subsequential transducers are ecient finite-state transducers with p final outputs used in a variety of applications.  Not all transducers admit equivalent p-subsequential transducers however.  We briefly describe an existing generalized determinization algorithm for psubsequential transducers and give the first characterization of p-subsequentiable transducers, transducers that admit equivalent p-subsequential transducers.  Our characterization shows the existence of an ecient algorithm for testing p-subsequentiability.  We have fully implemented the generalized determinization algorithm and the algorithm for testing psubsequentiability.  We report experimental results showing that these algorithms are practical in large-vocabulary speech recognition applications.  The theoretical formulation of our results is the equivalence of the following three properties for finite-state transducers: determinizability in the sense of the generalized algorithm, p-subsequentiability, and the twins property. 
An optimal pre-determinization algorithm for weighted transducers| Abstract We present a general algorithm, pre-determinization, that makes an arbitrary weighted transducer over the tropical semiring or an arbitrary unambiguous weighted transducer over a cancellative commutative semiring determinizable by inserting in it transitions labeled with special symbols.  After determinization, the special symbols can be removed or replaced with #-transitions.  The resulting transducer can be significantly more efficient to use.  We report empirical results showing that our algorithm leads to a substantial speed-up in large-vocabulary speech recognition.  Our pre-determinization algorithm makes use of an efficient algorithm for testing a general twins property, a sufficient condition for the determinizability of all weighted transducers over the tropical semiring and unambiguous weighted transducers over cancellative commutative semirings.  Based on the transitions marked by this test of the twins property, our pre-determinization algorithm inserts new transitions just when needed to guarantee that the resulting transducer has the twins property and thus is determinizable.  It also uses a single-source shortest-paths algorithm over the min-max semiring for carefully selecting the positions for insertion of new transitions to benefit from the subsequent application of determinization.  These positions are proved to be optimal in a sense that we describe. 
Chapter 2 WEIGHTED GRAMMAR TOOLS: THE GRM LIBRARY| Abstract We describe the algorithmic and software design principles of a general grammar library designed for use in spoken-dialogue systems, speech synthesis, and other speech processing applications.  The library is a set of general-purpose software tools for constructing and modifying weighted finite-state acceptors and transducers representing grammars.  The tools can be used in particular to compile weighted contextdependent rewrite rules into weighted finite-state transducers, read and compile, when possible, weighted context-free grammars into weighted automata, and dynamically modify the compiled grammar automata.  The dynamic modifications allowed include: grammar switching, dynamic modification of rules, dynamic activation or non-activation of rules, and the use of dynamic lists.  Access to these features is essential in spoken-dialogue applications.  2. 1 Motivation We describe the algorithmic and software design principles of a general grammar library (GRM library) designed for use in spoken-dialogue systems, speech synthesis, and other speech processing applications.  Most grammars used in speech processing applications are weighted.  The use of grammar weights is crucial for building robust spoken-dialogue systems.  In speech recognition, grammar weights are combined with acoustic weights to rank different hypotheses for a given speech utterance [Rabiner and Juang 1993, Jelinek 1998].  Due to the high variability of the input speech signal, a robust grammar admits any hypothesis as a possible transcription with a certain degree of probability. 
Generalized Algorithms for Constructing Statistical Language Models| Abstract Recent text and speech processing applications such as speech mining raise new and more general problems related to the construction of language models.  We present and describe in detail several new and efficient algorithms to address these more general problems and report experimental results demonstrating their usefulness.  We give an algorithm for computing efficiently the expected counts of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton; describe a new technique for creating exact representations of # -gram language models by weighted automata whose size is practical for offline use even for a vocabulary size of about 500,000 words and an # -gram order ##### ; and present a simple and more general technique for constructing class-based language models that allows each class to represent an arbitrary weighted automaton.  An efficient implementation of our algorithms and techniques has been incorporated in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities.  1 Motivation Statistical language models are crucial components of many modern natural language processing systems such as speech recognition, information extraction, machine translation, or document classification.  In all cases, a language model is used in combination with other information sources to rank alternative hypotheses by assigning them some probabilities.  There are classical techniques for constructing language models such as #gram models with various smoothing techniques (see Chen and Goodman (1998) and the references therein for a survey and comparison of these techniques).  In some recent text and speech processing applications, several new and more general problems arise that are related to the construction of language models.  We present new and efficient algorithms to address these more general problems.  Counting.  Classical language models are constructed by deriving statistics from large input texts.  In speech mining applications or for adaptation purposes, one often needs to construct a language model based on the output of a speech recognition system.  But, the output of a recognition system is not just text.  Indeed, the word error rate of conversational speech recognition systems is still too high in many tasks to rely only on the one-best output of the recognizer.  Thus, the word lattice output by speech recognition systems is used instead because it contains the correct transcription in most cases.  A word lattice is a weighted finite automaton (WFA) output by the recognizer for a particular utterance.  It contains typically a very large set of alternative transcription sentences for that utterance with the corresponding weights or probabilities.  A necessary step for constructing a language model based on a word lattice is to derive the statistics for any given sequence from the lattices or WFAs output by the recognizer.  This cannot be done by simply enumerating each path of the lattice and counting the number of occurrences of the sequence considered in each path since the number of paths of even a small automaton may be more than four billion.  We present a simple and efficient algorithm for computing the expected count of any given sequence in a WFA and report experimental results demonstrating its efficiency.  Representation of language models by WFAs.  Classical sical # -gram language models admit a natural representation by WFAs in which each state encodes a left context of width less than # .  However, the size of that representation makes it impractical for offline optimizations such as those used in large-vocabulary speech recognition or general information extraction systems.  Most offline representations of these models are based instead on an approximation to limit their size.  We describe a new technique for creating an exact representation of # -gram language models by WFAs whose size is practical for offline use even in tasks with a vocabulary size of about 500,000 words and for ##### .  Class-based models.  In many applications, it is natural and convenient to construct class-based language models, that is models based on classes of words (Brown et al. , 1992).  Such models are also often more robust since they may include words that belong to a class but that were not found in the corpus.  Classical class-based models are based on simple classes such as a list of words.  But new clustering algorithms allow one to create more general and more complex classes that may be regular languages.  Very large and complex classes can also be defined using regular expressions.  We present a simple and more general approach to class-based language models based on general weighted context-dependent rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996).  Our approach allows us to deal efficiently with more complex classes such as weighted regular languages.  We have fully implemented the algorithms just mentioned and incorporated them in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities (Allauzen et al. , 2003).  In the following, we will present in detail these algorithms and briefly describe the corresponding GRM utilities.  2 Preliminaries Definition 1 A system ############# ### ### is a semiring (Kuich and Salomaa, 1986) if: ######### ### is a commutative monoid with identity element # ; ######### ### is a monoid with identity element # ; # distributes over # ; and # is an annihilator for # : for all ####### #!# #"# #####$# # .  Thus, a semiring is a ring that may lack negation.  Two semirings often used in speech processing are: the log semiring %&#'##(*),+. -0/1####2 3 45##6###-0# #1# (Mohri, 2002) which is isomorphic to the familiar real or probability semiring ##(879##6##5:;# #<#=### via a }@?1A morphism with, for all #B# C##D(E)F+. -0/ : #;# 2 3G4 CH##ID}@?1AJ#LKNMPOQ#RIS#P#Q6,KTMPOQ#UI#CT#U# and the convention that: KTMPOQ#UI#-## # # and ID}@?1A####1#V#W- , and the tropical semiring XY#Y#Z([7\) +. -0/1#G]!^@_`##6###-##G#1# which can be derived from the log semiring using the Viterbi approximation.  Definition 2 A weighted finite-state transducer a over a semiring # is an 8-tuple ab#c#ed###f###g!#Gh##GiH# jk# lQ#UmP# where: d is the finite input alphabet of the transducer; f is the finite output alphabet; g is a finite set of states; h0nog the set of initial states; ipnqg the set of final states; jrn0gs:D#td#)u+#v=/w#x:y##fz)u+wvN/. #{:k#\:Vg a finite set of transitions; l#|<h#}~# the initial weight function; and m#|#i}# the final weight function mapping i to # .  A Weighted automaton '##ed###g!#Gh##GiH# jk# lQ#UmP# is defined in a similar way by simply omitting the output labels.  We denote by #####n#d# the set of strings accepted by an automaton and similarly by ##Z# the strings described by a regular expression .  Given a transition F##j , we denote by # = its input label, x = its origin or previous state and # = its destination state or next state, " N its weight, P = its output label (transducer case).  Given a state iD#`g , we denote by jV i# the set of transitions leaving i .  A path ^#'1~x=NU.  is an element of jk with consecutive transitions: # L#~ S#x wZ , #P#=NN=# .  We extend # and to paths by setting: #V## and F#. ~# .  A cycle is a path whose origin and destination states coincide: # ;##.  We denote by #Zi# i. # the set of paths from i to iand by #Zi#U#Gi1# and #Zi#U#U#Gi5# the set of paths from i to i#with input label &#&d and output label (transducer case).  These definitions can be extended to subsets $# kng , by: ####GQ# -##)8#5xtUL5`#Zi#U#Gi. -# .  The labeling functions (and similarly ) and the weight function can also be extended to paths by defining the label of a path as the concatenation of the labels of its constituent transitions, and the weight of a path as the # -product of the weights of its constituent transitions: # S## ~ #NNU# , " ### ~ #=N##0" .  We also extend to any finite set of paths by setting: " ;9#" .  The output weight associated by to each input string y##d# is: S t#ZB#[# °5¼{½@¾=² 5#lQ##x## <#m#L# # S t#ZB# is defined to be # when #Zh##UQ# i9#k#'.  Similarly, the output weight associated by a transducer a to a pair of input-output string #L#U<# is: a; #LQ#G<## °5¼8½@¾=² wQl#x L##," P#,mB#Z# L# aS #L#U<### # when #Zh##U#U#Gi9###.  A successful path in a weighted automaton or transducer is a path from an initial state to a final state.  is unambiguous if for any string ##yd# there is at most one successful path labeled with .  Thus, an unambiguous transducer defines a function.  For any transducer a , denote by !1#Za## the automaton obtained by projecting a on its output, that is by omitting its input labels.  Note that the second operation of the tropical semiring and the log semiring as well as their identity elements are identical.  Thus the weight of a path in an automaton over the tropical semiring does not change if is viewed as a weighted automaton over the log semiring or viceversa.  3 Counting This section describes a counting algorithm based on general weighted automata algorithms.  Let # ##gk#Gh##GiH##d## . #G##l##Gm# be an arbitrary weighted automaton over the probability semiring and let be a regular expression defined over the alphabet d .  We are interested in counting the occurrences of the sequences #####L# in while taking into account the weight of the paths where they appear.  3. 1 Definition When is deterministic and pushed, or stochastic, it can be viewed as a probability distribution over all strings
A General Weighted Grammar Library| Abstract.  We present a general weighted grammar software library, the GRM Library, that can be used in a variety of applications in text, speech, and biosequence processing.  The underlying algorithms were designed to support a wide variety of semirings and the representation and use of very large grammars and automata of several hundred million rules or transitions.  We describe several algorithms and utilities of this library and point out in each case their application to several text and speech processing tasks. 
Distribution kernels based on moments of counts| Abstract Many applications in text and speech processing require the analysis of distributions of variable-length sequences.  We recently introduced a general kernel framework, rational kernels,
The Design Principles of a Weighted Finite-State Transducer Library| Abstract We describe the algorithmic and software design principles of an object-oriented library for weighted finite-state transducers.  By taking advantage of the theory of rational power series, we were able to achieve high degrees of generality, modularity and irredundancy, while attaining competitive efficiency in demanding speech processing applications involving weighted automata of more than 10 7 states and transitions.  Besides its mathematical foundation, the design also draws from important ideas in algorithm design and programming languages: dynamic programming and shortest-paths algorithms over general semirings, object-oriented programming, lazy evaluation and memoization. 
GENERALIZED OPTIMIZATION ALGORITHM FOR SPEECH RECOGNITION TRANSDUCERS| ABSTRACT Weighted transducers provide a common representation for the components of a speech recognition system.  In previous work, we showed that these components can be combined off-line into a single compact recognition transducer that maps directly HMM state sequences to word sequences [11].  The construction of that recognition transducer and its efficiency of use critically depend on the use of a general optimization algorithm, determinization.  However, not all weighted automata and transducers used in largevocabulary speech recognition are determinizable.  We present a general algorithm that can make an arbitrary weighted transducer determinizable and generalize our previous optimization technique for building an integrated recognition transducer to deal with arbitrary weighted transducers used in speech recognition.  We report experimental results in a large-vocabulary speech recognition task, How May I Help You (HMIHY), showing that our generalized technique leads to a recognition transducer that performs as well as our original solution in the case of classical n-gram models while inserting less special symbols, and that it leads to a substantial improvement of the recognition speed, factor of 2:6, in the same task when using a class-based language model.  1.  MOTIVATION Weighted transducers are finite-state transducers in which each transition carries a weight in addition to the usual input and output symbols [14, 7].  They provide a common representation for several components of a speech recognition system: language models, pronunciation dictionaries, context-dependency and HMM models [11].  General weighted transducer algorithms can be used to combine and optimize these representations and to build off-line a single efficient recognition transducer that integrates all of these components, directly mapping from HMM state sequences to word sequences [11].  The size of that integrated recognition transducer is practical since it has been shown empirically to be close to that of the language model used.  The construction of that recognition transducer and its efficiency of use critically depend on the use of a general optimization algorithm, determinization [9, 12].  Determinization outputs a transducer equivalent to the input that is deterministic, i. e. , one that has a unique initial state and that has no two transitions leaving the same state with the same input label.  This considerably reduces the number of paths needed to be explored by the decoder and thus substantially improves the efficiency of recognition.  However, not all weighted automata and transducers used in speech recognition are determinizable.  A solution to this problem was provided in the special case where n-gram statistical language models are used [11].  But that solution does not cover some important cases such as that of class-based language models which can lead to non-determinizable weighted transducers.  Other language models created either directly or by approximation of weighted context-free grammars can also create similar non-determinism that could make our previous technique inapplicable.  We present a general algorithm that makes an arbitrary weighted transducer determinizable by inserting in it transitions labeled with special symbols just when needed and at the optimal positions to fully benefit from the application of determinization.  Those special symbols can be removed, or mapped to the empty string after application of determinization.  The algorithm generalizes our previous optimization technique for building an integrated recognition transducer to deal with arbitrary weighted transducers used in speech recognition.  Our experiments in a large-vocabulary speech recognition task, How May I Help You (HMIHY), show that our new and generalized technique leads to a recognition transducer that performs as well as our original solution in the case where classical n-gram models are used, while inserting less special symbols.  We also report experiments with a class-based language model in the same task using our generalized optimization technique.  The experiments show an improvement of the recognition speed by a factor of 2:6 over the system used without application of determinization.  We first introduce some preliminary definitions and notation necessary for the presentation of our symbol insertion algorithm and experimental results. 
Weighted Automata Kernels -- General Framework and Algorithms| Abstract Kernel methods have found in recent years wide use in statistical learning techniques due to their good performance and their computational efficiency in high-dimensional feature space.  However, text or speech data cannot always be represented by the fixed-length vectors that the traditional kernels handle.  We recently introduced a general kernel framework based on weighted transducers, rational kernels, to extend kernel methods to the analysis of variable-length sequences and weighted automata [5] and described their application to spoken-dialog applications.  We presented a constructive algorithm for ensuring that rational kernels are positive definite symmetric, a property which guarantees the convergence of discriminant classification algorithms such as Support Vector Machines, and showed that many string kernels previously introduced in the computational biology literature are special instances of such positive definite symmetric rational kernels [4].  This paper reviews the essential results given in [5, 3, 4] and presents them in the form of a short tutorial. 
A Weight Pushing Algorithm for Large Vocabulary Speech Recognition| Abstract Weighted finite-state transducers provide a general framework for the representation of the components of speech recognition systems; language models, pronunciation dictionaries, contextdependent models, HMM-level acoustic models, and the output word or phone lattices can all be represented by weighted automata and transducers.  In general, a representation is not unique and there may be different weighted transducers realizing the same mapping.  In particular, even when they have exactly the same topology with the same input and output labels, two equivalent transducers may differ by the way the weights are distributed along each path.  We present a weight pushing algorithm that modifies the weights of a given weighted transducer in a way such that the transition probabilities form a stochastic distribution.  This results in an equivalent transducer whose weight distribution is more suitable for pruning and speech recognition.  We demonstrate substantial improvements of the speed of our recognition system in several tasks based on the use of this algorithm.  We report a ### speedup at ### word accuracy with a simple single-pass fiff # ###-word vocabulary North American Business News (NAB) recognition system on the DARPA Eval '95 test set.  With the same technique, we report a #### speedup at ### word accuracy in rescoring NAB word lattices with more accurate 2nd-pass models.  We finally report a #### speedup at ### word accuracy for ### # ### first name-last name pairs recognition. 
Chapter 9 REGULAR APPROXIMATION OF CONTEXT-FREE GRAMMARS THROUGH TRANSFORMATION| Abstract We present an algorithm for approximating context-free languages with regular languages.  The algorithm is based on a simple transformation that applies to any context-free grammar and guarantees that the result can be compiled into a finite automaton.  The resulting grammar contains at most one new nonterminal for any nonterminal symbol of the input grammar.  The result thus remains readable and if necessary modifiable.  We extend the approximation algorithm to the case of weighted context-free grammars.  We also report experiments with several grammars showing that the size of the minimal deterministic automata accepting the resulting approximations is of practical use for applications such as speech recognition. 
Rational Kernels: Theory and Algorithms| Abstract Many classification algorithms were originally designed for fixed-size vectors.  Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and more generally weighted automata.  An approach widely used in statistical learning techniques such as Support Vector Machines (SVMs) is that of kernel methods, due to their computational efficiency in high-dimensional feature spaces.  We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that extend kernel methods to the analysis of variable-length sequences or more generally weighted automata.  We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm.  Not all rational kernels are positive definite and symmetric (PDS), or equivalently verify the Mercer condition, a condition that guarantees the convergence of training for discriminant classification algorithms such as SVMs.  We present several theoretical results related to PDS rational kernels.  We show that under some general conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We give the proof of several characterization results that can be used to guide the design of PDS rational kernels.  We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels.  Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  Rational kernels can be combined with SVMs to form efficient and powerful techniques for a variety of classification tasks in text and speech processing, or computational biology.  We describe examples of general families of PDS rational kernels that are useful in many of these applications and report the result of our experiments illustrating the use of
General Indexation of Weighted Automata -Application to Spoken Utterance Retrieval| Abstract Much of the massive quantities of digitized data widely available, e. g. , text, speech, handwritten sequences, are either given directly, or, as a result of some prior processing, as weighted automata.  These are compact representations of a large number of alternative sequences and their weights reflecting the uncertainty or variability of the data.  Thus, the indexation of such data requires indexing weighted automata.  We present a general algorithm for the indexation of weighted automata.  The resulting index is represented by a deterministic weighted transducer that is optimal for search: the search for an input string takes time linear in the sum of the size of that string and the number of indices of the weighted automata where it appears.  We also introduce a general framework based on weighted transducers that generalizes this indexation to enable the search for more complex patterns including syntactic information or for different types of sequences, e. g. , word sequences instead of phonemic sequences.  The use of this framework is illustrated with several examples.  We applied our general indexation algorithm and framework to the problem of indexation of speech utterances and report the results of our experiments in several tasks demonstrating that our techniques yield comparable results to previous methods, while providing greater generality, including the possibility of searching for arbitrary patterns represented by weighted automata.  1 Motivation Much of the massive quantities of digitized data widely available is highly variable or uncertain.  This uncertainty affects the interpretation of the data and its computational processing at various levels, e. g. , natural language texts are abundantly ambiguous, speech and hand-written sequences are highly variable and hard to recognize in presence of noise, biological sequences may be altered or incomplete.  Searching or indexing such data requires dealing with a large number of ranked or weighted alternatives.  These may be for example the different parses of an input text, the various responses to a search engine or information extraction query, or the best hypotheses of a speech or hand-written recognition system.  In most cases, alternative sequences can be compactly represented by weighted automata.  The weights may be probabilities or some other weights used to rank different hypotheses.  This motivates our study of the general problem of indexation of weighted automata.  This is more general than the classical indexation problems since, typically, there are many distinct hypotheses or alternatives associated with the same index, e. g. , a specific input speech or hand-written sequence may have a large number of different transcriptions according to the system and models used.  Moreover, the problem requires taking into consideration the weight of each alternative, which does not have a counterpart in classical indexation problems.  We describe a general indexation algorithm for weighted automata.  The resulting index is represented by a deterministic weighted transducer that is optimal for search: the search for an input string takes time linear in the sum of the size of that string and the number of indices of the weighted automata where it appears.  In some cases, one may wish to search using sequences in some level, e. g.  word sequences, different from the level of the sequences of the index, e. g.  phonemic sequences.  One may also wish to search for complex sequences including both words and parts-of-speech, or restrict the search by either restricting the weights or probabilities or the lengths or types of sequences.  We describe a general indexation framework covering all these cases.  Our framework is based on the use of filtering weighted transducers for restriction or other transducers mapping between distinct information levels or knowledge structures.  We illustrate the use of this framework with several examples that demonstrate its relevance to a number of indexation tasks.  We applied our framework and algorithms to the particular problem of speech indexation.  In recent years, spoken document retrieval systems have made large archives of broadcast news searchable and browsable.  Most of these systems use automatic speech recognition to convert speech into text, which is then indexed using standard methods.  When a user presents the system with a query, documents that are relevant to the query are found using text-based information retrieval techniques.  As speech indexation and retrieval systems move beyond the domain of broadcast news to more challenging spoken communications, the importance for the indexed material to contain more than just a simple text representation of the communication is becoming clear.  Indexation and retrieval techniques must be extended to handle more general representations including for example syntactic information.  In addition to the now familiar retrieval systems or search engines, other applications such as data mining systems can be used to automatically identify useful patterns in large collections of spoken communications.  Information extraction systems can be used to gather high-level information such as named-entities.  For a given input speech utterance, a large-vocabulary speech recognition system often generates a lattice, a weighted automaton representing a range of alternative hypotheses with some associated weights or probabilities used to rank them.  When the accuracy of a system is relatively low as in many conversational speech recognition tasks, it is not safe to rely only on the best hypothesis output by the system.  It is then preferable to use instead the full lattice output by the recognizer.  We report the results of our experiments in several tasks demonstrating that our techniques yield comparable results to the previous methods of Saraclar and Sproat (2004), while providing greater generality, including the possibility of searching for arbitrary patterns represented by weighted automata.  The paper is organized as follows.  Section 2 introduces the notation and the definitions used in the rest of the paper.  Section 3 describes our general indexation algorithm for weighted automata.  The algorithm for searching that index is presented in Section 4 and our general indexation framework is described and illustrated in Section 5.  Section 6 reports the results of our experiments in several tasks. 
Transducer Composition for Context-Dependent Network Expansion| Abstract Context-dependent models for language units are essential in highaccuracy speech recognition. 
Confidence Intervals for the Area under the ROC Curve| Abstract In many applications, good ranking is a highly desirable performance for a classifier.  The criterion commonly used to measure the ranking quality of a classification algorithm is the area under the ROC curve (AUC).  To report it properly, it is crucial to determine an interval of confidence for its value.  This paper provides confidence intervals for the AUC based on a statistical and combinatorial analysis using only simple parameters such as the error rate and the number of positive and negative examples.  The analysis is distribution-independent, it makes no assumption about the distribution of the scores of negative or positive examples.  The results are of practical use and can be viewed as the equivalent for AUC of the standard confidence intervals given in the case of the error rate.  They are compared with previous approaches in several standard classification tasks demonstrating the benefits of our analysis.  1 Motivation In many machine learning applications, the ranking quality of a classifier is critical.  For example, the ordering of the list of relevant documents returned by a search engine or a document classification system is essential.  The criterion widely used to measure the ranking quality of a classification algorithm is the area under an ROC curve (AUC).  But, to measure and report the AUC properly, it is crucial to determine an interval of confidence for its value as it is customary for the error rate and other measures.  It is also important to make the computation of the confidence interval practical by relying only on a small and simple number of parameters.  In the case of the error rate, such intervals are often derived from just the sample size N .  We present an extensive theoretical analysis of the AUC and show that a similar confidence interval can be derived for its value using only simple parameters such as the error rate k/N , the number of positive examples m, and the number of negative examples n = N- m.  Thus, our results extend to AUC the computation of confidence intervals from a small number of readily available parameters.  Our analysis is distribution-independent in the sense that it makes no assumption about the distribution of the scores of negative or positive examples.  The use of the error rate helps determine tight confidence intervals.  This contrasts with existing approaches presented in the statistical literature [11, 5, 2] which are based either on weak distribution-independent assumptions resulting in too loose confidence intervals, or strong distribution-dependent assumptions leading to tight but unsafe confidence intervals.  We show that our results are of practical use.  We also compare them with previous approaches in several standard classification tasks demonstrating the benefits of our analysis.  Our results are also useful for testing the statistical significance of the difference of the AUC values of two classifiers.  The paper is organized as follows.  We first introduce the definition of the AUC, its connection with the Wilcoxon-Mann-Whitney statistic (Section 2), and briefly review some essential aspects of the existing literature related to the computation of confidence intervals for the AUC.  Our computation of the expected value and variance of the AUC for a fixed error rate requires establishing several combinatorial identities.  Section 4 presents some existing identities and gives the proof of novel ones useful for the computation of the variance.  Section 5 gives the reduced expressions for the expected value and variance of the AUC for a fixed error rate.  These can be efficiently computed and used to determine our confidence intervals for the AUC (Section 6).  Section 7 reports the result of the comparison of our method with previous approaches, including empirical results for several standard tasks. 
Rational Kernels| Abstract We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that can be used for analysis of variable-length sequences or more generally weighted automata, in applications such as computational biology or speech recognition.  We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm.  We also describe several general families of positive definite symmetric rational kernels.  These general kernels can be combined with Support Vector Machines to form efficient and powerful techniques for spoken-dialog classification: highly complex kernels become easy to design and implement and lead to substantial improvements in the classification accuracy.  We also show that the string kernels considered in applications to computational biology are all specific instances of rational kernels. 
Article Submitted to Computer Speech and Language Weighted Finite-State Transducers in Speech Recognition| Abstract We survey the use of weighted finite-state transducers (WFSTs) in speech recognition.  We show that WFSTs provide a common and natural representation for HMM models, context-dependency, pronunciation dictionaries, grammars, and alternative recognition outputs.  Furthermore, general transducer operations combine these representations flexibly and efficiently.  Weighted determinization and minimization algorithms optimize their time and space requirements, and a weight pushing algorithm distributes the weights along the paths of a weighted transducer optimally for speech recognition.  As an example, we describe a North American Business News (NAB) recognition system built using these techniques that combines the HMMs, full cross-word triphones, a lexicon of forty thousand words, and a large trigram grammar into a single weighted transducer that is only somewhat larger than the trigram word grammar and that runs NAB in real-time on a very simple decoder.  In another example, we show that the same techniques can be used to optimize lattices for second-pass recognition.  In a third example, we show how general automata operations can be used to assemble lattices from different recognizers to improve recognition performance. 
Finite-State Transducers in Language and Speech Processing| Finite-state machines have been used in various domains of natural language processing.  We consider here the use of a type of transducers that supports very efficient programs: sequential transducers.  We recall classical theorems and give new ones characterizing sequential string-tostring transducers.  Transducers that output weights also play an important role in language and speech processing.  We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms.  Some applications of these algorithms in speech recognition are described and illustrated. 
An Efficient Pre-determinization Algorithm| Abstract.  We present a general algorithm, pre-determinization, that makes an arbitrary weighted transducer over the tropical semiring or an arbitrary unambiguous weighted transducer over a cancellative commutative semiring determinizable by inserting in it transitions labeled with special symbols.  After determinization, the special symbols can be removed or replaced with #-transitions.  The resulting transducer can be significantly more ecient to use.  We report empirical results showing that our algorithm leads to a substantial speed-up in large-vocabulary speech recognition.  Our pre-determinization algorithm makes use of an ecient algorithm for testing a general twins property, a sucient condition for the determinizability of all weighted transducers over the tropical semiring and unambiguous weighted transducers over cancellative commutative semirings.  It inserts new transitions just when needed to guarantee that the resulting transducer has the twins property and thus is determinizable.  It also uses a single-source shortest-paths algorithm over the min-max semiring for carefully selecting the positions for insertion of new transitions to benefit from the subsequent application of determinization.  These positions are proved to be optimal in a sense that we describe. 
Positive Definite Rational Kernels| Abstract.  Kernel methods are widely used in statistical learning techniques.  We recently introduced a general kernel framework based on weighted transducers or rational relations, rational kernels, to extend kernel methods to the analysis of variable-length sequences or more generally weighted automata.  These kernels are ecient to compute and have been successfully used in applications such as spoken-dialog classi#cation.  Not all rational kernels are positive definite and symmetric (PDS) however, a sucient property for guaranteeing the convergence of discriminant classification algorithms such as Support Vector Machines.  We present several theoretical results related to PDS rational kernels.  We show in particular that under some conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels.  Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  1 Motivation Many classification algorithms were originally designed for fixed-length vectors.  Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and even more generally weighted automata.  Indeed, the output of a large-vocabulary speech recognizer for a particular input speech utterance, or that of a complex information extraction system combining several information sources for a specific input query, is typically a weighted automaton compactly representing a large set of alternative sequences.  The weights assigned by the system to each sequence are used to rank different alternatives according to the models the system is based on.  The error rate of such complex systems is still too high in many tasks to rely only on their one-best output, thus it is preferable instead to use the full output weighted automata which contain the correct result in most cases.  Kernel methods [13] are widely used in statistical learning techniques such as Support Vector Machines (SVMs) [2, 4, 14] due to their computational eciency in high-dimensional feature spaces.  Recently, a general kernel framework Semiring Set # # 0 1 Boolean f0; 1g _ ^ 0 1 Probability R+ + # 0 1 Log R [f2 ; +1g # log + +1 0 Tropical R [f2 ; +1g min + +1 0 Table 1.  Semiring examples.  # log is defined by: x # log y = log(e x + e y ).  based on weighted transducers or rational relations, rational kernels, was introduced to extend kernel methods to the analysis of variable-length sequences or more generally weighted automata [3].  It was shown that there are general and ecient algorithms for computing rational kernels.  Rational kernels have been successfully used for applications such as spoken-dialog classification.  Not all rational kernels are positive definite and symmetric (PDS), or equivalently verify the Mercer condition [1], a condition that guarantees the convergence of discriminant classification algorithms such as SVMs.  This motivates the study undertaken in this paper.  We present several theoretical results related to PDS rational kernels.  In particular, we show that under some conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We also study the relationship between rational kernels and some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler [6], and some string kernels used in the context of computational biology [8].  We show that these kernels are all specific instances of rational kernels.  In each case, we explicitly describe the corresponding weighted transducer.  These transducers are often simple and ecient for computing kernels.  Their diagram often provides more insight into the definition of kernels and can guide the design of new kernels.  Our results also include the proof of the fact that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  2 Preliminaries In this section, we present the algebraic definitions and notation necessary to introduce rational kernels.  Definition 1 ([7]).  A system (K ; #; #; 0; 1) is a semiring if: (K ; #; 0) is a commutative monoid with identity element 0; (K ; #; 1) is a monoid with identity element 1; # distributes over #; and 0 is an annihilator for #: for all a 2 K ; a # 0 = 0 # a = 0.  Thus, a semiring is a ring that may lack negation.  Table 1 lists some familiar semirings.  Definition 2.  A weighted finite-state transducer T over a semiring K is an 8-tuple T = (#;
A GENERALIZED CONSTRUCTION OF INTEGRATED SPEECH RECOGNITION TRANSDUCERS| ABSTRACT We showed in previous work that weighted finite-state transducers provide a common representation for many components of a speech recognition system and described general algorithms for combining these representations to build a single optimized and compact transducer integrating all these components, directly mapping from HMM states to words.  This approach works well for certain well-controlled input transducers, but presents some problems related to the efficiency of composition and the applicability of determinization and weight-pushing with more general transducers.  We generalize our prior construction of the integrated speech recognition transducer to work with an arbitrary number of component transducers and, to a large extent, release the constraints imposed to the type of input transducers by providing more general solutions to these problems.  This generalization allowed us to deal with cases where our prior optimization did not apply.  Our experiments in the AT&T HMIHY 0300 task and an AT&T VoiceTone task show the efficiency of our generalized optimization technique.  We report a 1. 6 recognition speed-up in the HMIHY 0300 task, 1. 8 speed-up in a VoiceTone task using a word-based language model, and 1. 7 using a class-based model.  1.  MOTIVATION In previous work, we showed that weighted finite-state transducers provide a common and natural representation for many components of a speech recognition system, e. g. , HMMs, contextdependency, pronunciation dictionaries, and language models [8].  We also described general algorithms for combining these representations flexibly and efficiently and showed that they can be used to build a single optimized transducer that integrates these components, directly mapping from HMM states to words [9, 10].  In this method, weighted transducer composition is used to combine the component transducers, while determinization, minimization and weight-pushing optimize the result in time and space.  The resultant transducer has a standardized representation, unique up to state renumbering.  This approach works well for certain well-controlled input transducers, but presents some problems with more general transducers.  These problems are related to composition, determinization, and weight-pushing.  Composition can use unacceptable amounts of time and space when there are significant delays in matching due to #-transitions.  In simple cases, this can be avoided by a careful construction of the component transducers.  In practice, inexperienced users are often not fully aware of this.  For example, they may place the output labels in the lexicon transducer L at the ends of words rather than at the beginning, which can significantly slow down composition.  For more complex transducers, trying to manually place the input and output labels for the best effect becomes difficult.  Not all transducers are determinizable.  The lexicon transducer, for example, is not determinizable if it contains homophones.  In our prior construction, we added disambiguation symbols at word ends as needed to solve this problem.  For more general transducer inputs, this is insufficient.  A related problem is that # This author's new address is:
AUC Optimization vs| Error Rate Minimization.  Abstract The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classification algorithm.  However, the objective function optimized in most of these algorithms is the error rate and not the AUC value.  We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate.  Our results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable.  Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values.  We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC.  We report the results of our experiments with RankBoost in several datasets demonstrating the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.  1 Motivation In many applications, the overall classification error rate is not the most pertinent performance measure, criteria such as ordering or ranking seem more appropriate.  Consider for example the list of relevant documents returned by a search engine for a specific query.  That list may contain several thousand documents, but, in practice, only the top fifty or so are examined by the user.  Thus, a search engine's ranking of the documents is more critical than the accuracy of its classification of all documents as relevant or not.  More generally, for a binary classifier assigning a real-valued score to each object, a better correlation between output scores and the probability of correct classification is highly desirable.  A natural criterion or summary statistic often used to measure the ranking quality of a classifier is the area under an ROC curve (AUC) [8].  1 However, the objective function optimized by most classification algorithms is the error rate and not the AUC.  Recently, several algorithms have been proposed for maximizing the AUC value locally [4] or maximizing some approximations of the global AUC value [9, 15], but, in general, these algorithms do not obtain AUC values significantly better than those obtained by an algorithm designed to minimize the error rates.  Thus, it is important to determine the relationship between the AUC values and the error rate.  # This author's new address is: Google Labs, 1440 Broadway, New York, NY 10018, corinna@google. com.  1 The AUC value is equivalent to the Wilcoxon-Mann-Whitney statistic [8] and closely related to the Gini index [1].  It has been re-invented under the name of L-measure by [11], as already pointed out by [2], and slightly modified under the name of Linear Ranking by [13, 14].  (1,1) (0,0) False positive rate True positive rate ROC Curve.  AUC=0. 718 True positive rate = correctly classified positive total positive False positive rate = incorrectly classified negative total negative Figure 1: An example of ROC curve.  The line connecting (0, 0) and (1, 1), corresponding to random classification, is drawn for reference.  The true positive (negative) rate is sometimes referred to as the sensitivity (resp.  specificity) in this context.  In the following sections, we give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate.  2 We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC.  We report the results of our experiments with RankBoost in several datasets and demonstrate the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.  2 Definition and properties of the AUC The Receiver Operating Characteristics (ROC) curves were originally developed in signal detection theory [3] in connection with radio signals, and have been used since then in many other applications, in particular for medical decision-making.  Over the last few years, they have found increased interest in the machine learning and data mining communities for model evaluation and selection [12, 10, 4, 9, 15, 2].  The ROC curve for a binary classification problem plots the true positive rate as a function of the false positive rate.  The points of the curve are obtained by sweeping the classification threshold from the most positive classification value to the most negative.  For a fully random classification, the ROC curve is a straight line connecting the origin to (1, 1).  Any improvement over random classification results in an ROC curve at least partially above this straight line.  Fig.  (1) shows an example of ROC curve.  The AUC is defined as the area under the ROC curve and is closely related to the ranking quality of the classification as shown more formally by Lemma 1 below.  Consider a binary classification task with m positive examples and n negative examples.  We will assume that a classifier outputs a strictly ordered list for these examples and will denote by 1X the indicator function of a set X .  Lemma 1 ([8]) Let c be a fixed classifier.  Let x 1 , .  .  .  , xm be the output of c on the positive examples and y 1 , .  .  .  , yn its output on the negative examples.  Then, the AUC, A, associated to c is given by: A = P m i=1 P n j=1 1 x i }y j mn (1) that is the value of the Wilcoxon-Mann-Whitney statistic [8].  Proof.  The proof is based on the observation that the AUC value is exactly the probability P (X } Y ) where X is the random variable corresponding to the distribution of the outputs for the positive examples and Y the one corresponding to the negative examples [7].  The Wilcoxon-Mann-Whitney statistic is clearly the expression of that probability in the discrete case, which proves the lemma [8].  Thus, the AUC can be viewed as a measure based on pairwise comparisons between classifications of the two classes.  With a perfect ranking, all positive examples are ranked higher than the negative ones and A = 1.  Any deviation from this ranking decreases the AUC. 
FULL EXPANSION OF CONTEXT-DEPENDENT NETWORKS IN LARGE VOCABULARY SPEECH RECOGNITION| ABSTRACT We combine our earlier approach to context-dependent network representation with our algorithm for determinizing weighted networks to build optimized networks for large-vocabulary speech recognition combining an n-gram language model, a pronunciation dictionary and context-dependency modeling.  While fullyexpanded networks have been used before in restrictive settings (medium vocabulary or no cross-word contexts), we demonstrate that our network determinization method makes it practical to use fully-expanded networks also in large-vocabulary recognition with full cross-word context modeling.  For the DARPA North American Business News task (NAB), we give network sizes and recognition speeds and accuracies using bigram and trigram grammars with vocabulary sizes ranging from 10,000 to 160,000 words.  With our construction, the fully-expanded NAB context-dependent networks contain only about twice as many arcs as the corresponding language models.  Interestingly, we also find that, with these networks, real-time word accuracy is improved by increasing vocabulary size and n-gram order. 
Weighted determinization and minimization for large vocabulary speech recognition|
A Rational Design for a Weighted Finite-State Transducer Library|
AT&T General-purpose Finite-State Machine Software Tools,|
On some applications of finite-state automata theory to natural language processing|
Weighted automata in text and speech processing|
Integrated context-dependent networks in very large vocabulary speech recognition,|
Fsm library,|
Efficient Algorithms for Testing the Twins Property|
Semiring Frameworks and Algorithms for Shortest-Distance Problems|
Edit-Distance of Weighted Automata|
Weighted grammar tools: the GRM library|
Weighted Finite-State Transducers in Speech Recognition,|
VPQ: a spoken language interface to large scale directory information",|
Voice signatures|
Rational design for a weighted finite-state transducer library|
Network optimizations for large vocabulary speech recognition,|
"A comparison of two LVR search optimization techniques,|
DCD Library Decoder Library|
At&t fsm library -- finite-state machine library|
Rational kernels| Neural Information Processing Systems 16,. 
Generalized algorithms for constructing language models|
Fsm library -- general purpose finitestate machine software tools|
An efficient algorithm for the n-best-strings problem|
General-purpose finite-state machine software tools|
Minimization of Sequential Transducers|
General algebraic frameworks and algorithms for shortest-distance problems,|
AT&T Labs --Research|
Regular approximation of context-free grammars through transformation|
On the Determinizability of Weighted Automata and Transducers|
Minimization of sequential trasducers, in:|
Matching Patterns of An Automaton|
Syntactic analysis by local grammars automata: an efficient algorithm|
On some applications of finite-state automata theory to natural language processing: Representation of morphological dictionaries, compaction, and indexation|
Finite State Devices in Natural Language Processing, chapter On The Use of Sequential Transducers in Natural Language Processing|
Generic epsilon-removal and input epsilon-normalization algorithms for weighted transducers|
General Algebraic Frameworks and Algorithms for Shortest-Distance Problems," Technical Memorandum 981210-10TM, AT&T Labs - Research, 62 pages,|
Finite-State Transducers in Languagee and Speech Processing|
Rational kernels| In Becker et al.  [5]. 
On The Use of Sequential Transducers in Natural Language Processing|
Compact Representations by Finite-State Transducers|
Generic "-Removal Algorithm for Weighted Automata"|
p-Subsequential Transducers|
Robustness in Language and Speech Technology, chapter Regular Approximation of Context-Free Grammars through Transformation,|
String-Matching with Automata|
A general framework for shortest distance problems,|
Weighted Finite-State Transducer Algorithms: An Overview|
Edit-Distance Of Weighted Automata: General Definitions And Algorithms|
AT&T FSM Toolkit v4|0. 
An e@cient compiler for wei hted rewrite rules,|

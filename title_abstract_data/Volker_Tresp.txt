Discovering Structure in Continuous Variables Using Bayesian Networks| Abstract We study Bayesian networks for continuous variables using nonlinear conditional density estimators.  We demonstrate that useful structures can be extracted from a data set in a self-organized way and we present sampling techniques for belief update based on Markov blanket conditional density models. 
Call-Based Fraud Detection in Mobile Communication Networks Using a Hierarchical Regime-Switching Model| Abstract Fraud causes substantial losses to telecommunication carriers.  Detection systems which automatically detect illegal use of the network can be used to alleviate the problem.  Previous approaches worked on features derived from the call patterns of individual users.  In this paper we present a call-based detection system based on a hierarchical regime-switching model.  The detection problem is formulated as an inference problem on the regime probabilities.  Inference is implemented by applying the junction tree algorithm to the underlying graphical model.  The dynamics are learned from data using the EM algorithm and subsequent discriminative training.  The methods are assessed using fraud data from a real mobile communication network. 
Scaling Kernel-Based Systems to Large Data Sets| Abstract.  In the form of the support vector machine and Gaussian processes, kernel-based systems are currently very popular approaches to supervised learning. 
FRAUD DETECTION IN COMMUNICATIONS NETWORKS USING NEURAL AND PROBABILISTIC METHODS| ABSTRACT Fraud detection refers to the attempt to detect illegitimate usage of a communications network.  Three methods to detect fraud are presented.  Firstly, a feed-forward neural network based on supervised learning is used to learn a discriminative function to classify subscribers using summary statistics.  Secondly, Gaussian mixture model is used to model the probability density of subscribers' past behavior so that the probability of current behavior can be calculated to detect any abnormalities from the past behavior.  Lastly, Bayesian networks are used to describe the statistics of a particular user and the statistics of different fraud scenarios.  The Bayesian networks can be used to infer the probability of fraud given the subscribers' behavior.  The data features are derived from toll tickets.  The experiments show that the methods detect over 85 % of the fraudsters in our testing set without causing false alarms. 
Mixture Approximations to Bayesian Networks| Abstract Structure and parameters in a Bayesian network uniquely specify the probability distribution of the modeled domain.  The locality of both structure and probabilistic information are the great benefits of Bayesian networks and require the modeler to only specify local information.  On the other hand this locality of information might prevent the modeler ---and even more any other person--from obtaining a general overview of the important relationships within the domain.  The goal of the work presented in this paper is to provide an "alternative" view on the knowledge encoded in a Bayesian network which might sometimes be very helpful for providing insights into the underlying domain.  The basic idea is to calculate a mixture approximation to the probability distribution represented by the Bayesian network.  The mixture component densities can be thought of as representing typical scenarios implied by the Bayesian model, providing intuition about the basic relationships.  As an additional benefit, performing inference in the approximate model is very simple and intuitive and can provide additional insights.  The computational complexity for the calculation of the mixture approximations critically depends on the measure which defines the distance between the probability distribution represented by the Bayesian network and the approximate distribution.  Both the KLdivergence and the backward KL-divergence lead to inecient algorithms.  Incidentally, the latter is used in recent work on mixtures of mean field solutions to which the work presented here is closely related.  We show, however, that using a mean squared error cost function leads to update equations which can be solved using the junction tree algorithm.  We conclude that the mean squared error cost function can be used for Bayesian networks in which inference based on the junction tree is tractable.  For large networks, however, one may have to rely on mean field approximations. 
A Self-Organizing Map for Clustering Probabilistic Models| Abstract We present a general framework for SelfOrganizing Maps, which store probabilistic models in map units.  We introduce the negative log probability of the data sample as the error function and motivate its use by showing its correspondence to the Kullback-Leibler distance between the unknown true distribution of data and our empirical models.  We present a general winner search procedure based on this probability measure and an update step based on its gradients.  As an application, we derive the learning rules for a particular probabilistic model that is used in user profiling in mobile communications network.  Due to the constrained nature of the parameters of our probabilistic model, we introduce a new parameter space, in which the gradient update step is performed.  In the experiments, we show clustering of user profiles using calling data involving normal users of mobile phones and users that are known to be victims of fraud.  In the summary, we discuss further applications of the approach. 
Neural Control for Rolling Mills: Incorporating Domain Theories to Overcome Data Deficiency| Abstract In a Bayesian framework, we give a principled account of how domainspecific prior knowledge such as imperfect analytic domain theories can be optimally incorporated into networks of locally-tuned units: by choosing a specific architecture and by applying a specific training regimen.  Our method proved successful in overcoming the data deficiency problem in a large-scale application to devise a neural control for a hot line rolling mill.  It achieves in this application significantly higher accuracy than optimally-tuned standard algorithms such as sigmoidal backpropagation, and outperforms the state-of-the-art solution. 
Robust Neural Network Regression for Offline and Online Learning| Abstract We replace the commonly used Gaussian noise model in nonlinear regression by a more flexible noise model based on the Student-tdistribution.  The degrees of freedom of the t-distribution can be chosen such that as special cases either the Gaussian distribution or the Cauchy distribution are realized.  The latter is commonly used in robust regression.  Since the t-distribution can be interpreted as being an infinite mixture of Gaussians, parameters and hyperparameters such as the degrees of freedom of the t-distribution can be learned from the data based on an EM-learning algorithm.  We show that modeling using the t-distribution leads to improved predictors on real world data sets.  In particular, if outliers are present, the t-distribution is superior to the Gaussian noise model.  In effect, by adapting the degrees of freedom, the system can "learn" to distinguish between outliers and non-outliers.  Especially for online learning tasks, one is interested in avoiding inappropriate weight changes due to measurement outliers to maintain stable online learning capability.  We show experimentally that using the t-distribution as a noise model leads to stable online learning algorithms and outperforms state-of-the art online learning methods like the extended Kalman filter algorithm. 
Averaging, Maximum Penalized Likelihood and Bayesian Estimation for Improving Gaussian Mixture Probability Density Estimates| Abstract We apply the idea of averaging ensembles of estimators to probability density estimation.  In particular we use Gaussian mixture models which are important components in many neural network applications.  One variant of averaging is Breiman's "bagging", which recently produced impressive results in classification tasks.  We investigate the performance of averaging using three data sets.  For comparison, we employ two traditional regularization approaches, i. e.  a maximum penalized likelihood approach and a Bayesian approach.  In the maximum penalized likelihood approach we use penalty functions derived from conjugate Bayesian priors such that an EM algorithm can be used for training.  In all experiments, the maximum penalized likelihood approach and averaging improved performance considerably if compared to a maximum likelihood approach.  In two of the experiments, the maximum penalized likelihood approach outperformed averaging.  In one experiment averaging was clearly superior.  Our conclusion is that maximum penalized likelihood gives good results if the penalty term in the cost function is appropriate for the particular problem.  If this is not the case, averaging is superior since it shows greater robustness by not relying on any particular prior assumption.  The Bayesian approach worked very well on a low-dimensional toy problem but failed to give good performance in higherdimensional problems. 
AHIDDENMARKOVMODELFORMETRICAND EVENT-BASED DATA| ABSTRACT The question of data representation is central to any data analysis problem.  Ideally, the representation should faithfully describe the domain to be analyzed and in addition, the model used should be able to process such a representation.  In practice, however, the modeler must often compromise how the problem is described, since the class of possible representations is constrained by the model.  This problem may be circumvented by extending conventional models to handle more unconventional data representations.  These data are often found in industrial environments and especially in telecommunications.  In this paper, we consider an extension of hidden Markov models (HMM) for modeling data streams, which switch between metric and eventbased representations.  In a HMM, the representation of the observed data is constrained by the emission probability density.  Since this density can not change its representation once it is fixed, modeling data streams involving di%erent types of data semantics can be di&cult.  In the extension introduced in this paper, an additional data semantics variable is introduced, which is conditional on the hidden variable.  Furthermore, data itself is conditioned on its semantics, which enables correct interpretation of the observed data.  We briefly review the essentials of HMMs and present our extended architecture.  We proceed by introducing inference and learning rules for the extension.  As an application, we present a HMM for user profiling in mobile communications networks, where the data exhibits switching behavior. 
Missing and Noisy Data in Nonlinear Time-Series Prediction| Abstract We discuss the issue of missing and noisy data in nonlinear time-series prediction.  We derive fundamental equations both for prediction and for training.  Our discussion shows that if measurements are noisy or missing, treating the time series as a static input/output mapping problem (the usual time-delay neural network approach) is suboptimal.  We describe approximations of the solutions which are based on stochastic simulations.  A special case is K-step prediction in which a onestep predictor is iterated K times.  Our solutions provide error bars for prediction with missing or noisy data and for K-step prediction.  Using the K-step iterated logistic map as an example, we show that the proposed solutions are a considerable improvement over simple heuristic solutions.  Using our formalism we derive algorithms for training recurrent networks, for control of stochastic systems and for reinforcement learning problems. 
Scalable Kernel Systems| Abstract.  Kernel-based systems are currently very popular approaches to supervised learning.  Unfortunately, the computational load for training kernel-based systems increases drastically with the number of training data points.  Recently, a number of approximate methods for scaling kernel-based systems to large data sets have been introduced.  In this paper we investigate the relationship between three of those approaches and compare their performances experimentally. 
Mixtures of Gaussian Processes| Abstract We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent.  The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities.  We discuss how Gaussian processes ---in particular in form of Gaussian process classication, the support vector machine and the MGP model--can be used for quantifying the dependencies in graphical models. 
Hierarchical Bayesian Modelling with Gaussian Processes| Abstract We present a novel method for Gaussian process regression in a hierarchical Bayesian framework.  The method is suited for cases where data from separate, but related problem scenarios is available.  In the hierarchical framework, kernel matrices on a fixed set of input points (transduction) can be learned from data using a simple and efficient EM algorithm.  This does not require a parametric form for the kernel function, thus problems that require, for example, nonstationary kernels, can be handled.  We evaluate our approach as a recommendation engine for art images, where the proposed hierarchical Bayesian method leads to excellent prediction performance. 
Nonlinear Time-Series Prediction with Missing and Noisy Data| Abstract We derive solutions for the problem of missing and noisy data in nonlinear timeseries prediction from a probabilistic point of view.  We discuss dierent approximations to the solutions, in particular approximations which require either stochastic simulation or the substitution of a single estimate for the missing data.  We show experimentally that commonly used heuristics can lead to suboptimal solutions.  We show how error bars for the predictions can be derived and we show how our results can be applied to K-step prediction.  We verify our solutions using two chaotic time series and the sun-spot data set.  In particular, we show that for K-step prediction stochastic simulation is superior to simply iterating the predictor. 
Network Structuring And Training Using Rule-based Knowledge| Abstract We demonstrate in this paper how certain forms of rule-based knowledge can be used to prestructure a neural network of normalized basis functions and give a probabilistic interpretation of the network architecture.  We describe several ways to assure that rule-based knowledge is preserved during training and present a method for complexity reduction that tries to minimize the number of rules and the number of conjuncts.  After training, the refined rules are extracted and analyzed. 
A Solution for Missing Data in Recurrent Neural Networks With an Application to Blood Glucose Prediction| Abstract We consider neural network models for stochastic nonlinear dynamical systems where measurements of the variable of interest are only available at irregular intervals i. e.  most realizations are missing.  Difculties arise since the solutions for prediction and maximum likelihood learning with missing data lead to complex integrals, which even for simple cases cannot be solved analytically.  In this paper we propose a specic combination of a nonlinear recurrent neural predictive model and a linear error model which leads to tractable prediction and maximum likelihood adaptation rules.  In particular, the recurrent neural network can be trained using the real-time recurrent learning rule and the linear error model can be trained by an EM adaptation rule, implemented using forward-backward Kalman lter equations.  The model is applied to predict the glucose/insulin metabolism of a diabetic patient where blood glucose measurements are only available a few times a day at irregular intervals.  The new model shows considerable improvement with respect to both recurrent neural networks trained with teacher forcing or in a free running mode and various linear models. 
The Bayesian Committee Support Vector Machine| Abstract.  Empirical evidence indicates that the training time for the support vector machine (SVM) scales to the square of the number of training data points.  In this paper, we introduce the Bayesian committee support vector machine (BC-SVM) and achieve an algorithm for training the SVM which scales linearly in the number of training data points.  We verify the good performance of the BC-SVM using several data sets. 
Averaging Regularized Estimators| Abstract We compare the performance of averaged regularized estimators.  We show that the improvement in performance which can be achieved by averaging depends critically on the degree of regularization which is used in training the individual estimators.  We compare four dierent averaging approaches: simple averaging, bagging, variance-based weighting and variance-based bagging.  In any of the averaging methods the greatest degree of improvement ---if compared to the individual estimators--- is achieved if no or only a small degree of regularization is used.  Here, variance-based weighting and variance-based bagging are superior to simple averaging or bagging.  Our experiments indicate that better performance for both individual estimators and for averaging is achieved in combination with regularization.  With increasing degrees of regularization, the two bagging-based approaches (bagging, variance-based bagging) outperform the individual estimators, simple averaging, as well as variance-based weighting.  Bagging and variance-based bagging seem to be the overall best combining methods over a wide range of degrees of regularization. 
The generalized Bayesian committee machine| ABSTRACT In this paper we introduce the Generalized Bayesian Committee Machine (GBCM) for applications with large data sets.  In particular, the GBCM can be used in the context of kernel based systems such as smoothing splines, kriging, regularization networks and Gaussian process regression which ---for computational reasons--- are otherwise limited to rather small data sets.  The GBCM provides a novel and principled way of combining estimators trained for regression, classification, the prediction of counts, the prediction of lifetimes and other applications which can be derived from the exponential family of distributions.  We describe an online version of the GBCM which only requires one pass through the data set and only requires the storage of a matrix of the dimension of the number of query or test points.  After training, the prediction at additional test points only requires resources dependent on the number of query points but is independent of the number of training data.  We confirm the good scaling behavior using real and experimental data sets. 
Observations on the Nystrom Method for Gaussian Process Prediction| Abstract A number of methods for speeding up Gaussian Process (GP) prediction have been proposed, including the Nystrom method of Williams and Seeger (2001).  In this paper we focus on two issues (1) the relationship of the Nystrom method to the Subset of Regressors method (Poggio and Girosi, 1990; Luo and Wahba, 1997) and (2) understanding in what circumstances the Nystrom approximation would be expected to provide a good approximation to exact GP regression. 
A LEARNING VECTOR QUANTIZATION ALGORITHM FOR PROBABILISTIC MODELS| ABSTRACT In classification problems, it is preferred to attack the discrimination problem directly rather than indirectly by first estimating the class densities and by then estimating the discrimination function from the generative models through Bayes's rule.  Sometimes, however, it is convenient to express the models as probabilistic models, since they are generative in nature and can handle the representation of high-dimensional data like time-series.  In this paper, we derive a discriminative training procedure based on Learning Vector Quantization (LVQ) where the codebook is expressed in terms of probabilistic models.  The likelihood-based distance measure is justified using the Kullback-Leibler distance.  In updating the winner unit, a gradient learning step is taken with regard to the parameters of the probabilistic model.  The method essentially departs from a prototypical representation and incorporates learning in the parameter space of generative models.  As an illustration, we present experiments in the fraud detection domain, where models of calling behavior are used to classify mobile phone subscribers to normal and fraudulent users.  This is an extension of our earlier work in clustering probabilistic models with the Self-Organizing Map (SOM) algorithm to the classification domain. 
Early Brain Damage| Abstract Optimal Brain Damage (OBD) is a method for reducing the number of weights in a neural network.  OBD estimates the increase in cost function if weights are pruned and is a valid approximation if the learning algorithm has converged into a local minimum.  On the other hand it is often desirable to terminate the learning process before a local minimum is reached (early stopping).  In this paper we show that OBD estimates the increase in cost function incorrectly if the network is not in a local minimum.  We also show how OBD can be extended such that it can be used in connection with early stopping.  We call this new approach Early Brain Damage, EBD.  EBD also allows to revive already pruned weights.  We demonstrate the improvements achieved by EBD using three publicly available data sets. 
Seminar fur Statistik Ludwig Maximilians Universitat Munchen Dynamic Neural Regression Models| Abstract We consider sequential or online learning in dynamic neural regression models.  By using a state space representation for the neural network's parameter evolution in time we obtain approximations to the unknown posterior by either deriving posterior modes via the Fisher scoring algorithm or by deriving approximate posterior means with the importance sampling method.  Furthermore, we replace the commonly used Gaussian noise assumption in the neural regression model by a more exible noise model based on the Student t-density.  Since the t-density can be interpreted as being an innite mixture of Gaussians, hyperparameters such as the degrees of freedom of the t-density can be learned from the data based on an online EM-type algorithm.  We show experimentally that our novel methods outperform state-of-the art neural network online learning algorithms like the extended Kalman lter method for both, situations with standard Gaussian noise terms and situations with measurement outliers. 
A Bayesian Committee Machine| Abstract The Bayesian committee machine (BCM) is a novel approach to combining estimators which were trained on different data sets.  Although the BCM can be applied to the combination of any kind of estimators the main foci are Gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data.  Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator.  The BCM also provides a new solution for online learning with potential applications to data mining.  We apply the BCM to systems with fixed basis functions and discuss its relationship to Gaussian process regression.  Finally, we also show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input dependent combination of estimators. 
Local Factorization of Functions| Abstract This paper is concerned with the notion of a local factorization of a function where we are mostly interested in the special case that this function is a probability distribution.  We introduce the notions of local independence and of the local Kullback-Leibler divergence.  We introduce a specific approximate local factorization.  The number of terms required in the approximation is linear in the number of input dimensions and the approximation does not require the calculation of higher derivatives (as in a Taylor expansion) and is not limited to approximations near the mode of a function.  We provide examples where we believe the approximation might be useful as in the approximate calculation of certain integrals. 
Committee Machines| Abstract In this chapter, we describe some of the most important architectures and algorithms for committee machines.  We discuss three reasons for using committee machines.  The first reason is that a committee can achieve a test set performance unobtainable by a single committee member.  As typical representative approaches, we describe simple averaging, bagging and boosting.  Secondly with committee machines, one obtains modular solutions which is sometimes desirable.  The prime example here is the mixture of experts approach whose goal it is to autonomously break up a complex prediction task into subtasks which are modeled by the individual committee members.  The third reason for using committee machines is a reduction in computational complexity.  In the presented Bayesian committee machine, the training data set is partitioned into several smaller data sets and the different committee members are trained on the different sets.  Their predictions are then combined using a covariance-based weighting scheme.  The computational complexity of the Bayesian committee machine approach grows only linearly with the size of the training data set, independent of the learning systems used as committee members. 
Training Neural Networks with Deficient Data| Abstract We analyze how data with uncertain or missing input features can be incorporated into the training of a neural network.  The general solution requires a weighted integration over the unknown or uncertain input although computationally cheaper closed-form solutions can be found for certain Gaussian Basis Function (GBF) networks.  We also discuss cases in which heuristical solutions such as substituting the mean of an unknown input can be harmful. 
Neural Network Models for the Blood Glucose Metabolism of a Diabetic| Abstract We study the application of neural networks to modeling the blood glucose metabolism of a diabetic.  In particular we consider recurrent neural networks and time series convolution neural networks which we compare to linear models and to nonlinear compartment models.  We include a linear error model to take into account the uncertainty in the system and for handling missing blood glucose observations.  Our results indicate that best performance can be achieved by the combination of the recurrent neural network and the linear error model. 
Transductive and Inductive Methods for Approximate Gaussian Process Regression| Abstract Gaussian process regression allows a simple analytical treatment of exact Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data.  In this paper we compare experimentally three of the leading approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, and the Bayesian committee machine.  Furthermore we provide theoretical insight into some of our experimental results.  We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels.  On low noise data sets, the Bayesian committee machine achieves significantly better accuracy, yet at a higher computational cost for large test data sets. 
Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging| Abstract We compare two regularization methods which can be used to improve the generalization capabilities of Gaussian mixture density estimates.  The first method uses a Bayesian prior on the parameter space.  We derive EM (Expectation Maximization) update rules which maximize the a posterior parameter probability.  In the second approach we apply ensemble averaging to density estimation.  This includes Breiman's "bagging", which recently has been found to produce impressive results for classification networks. 
A nonparametric hierarchical bayesian framework for information filtering| ABSTRACT Information filtering has made considerable progress in recent years.  The predominant approaches are content-based methods and collaborative methods.  Researchers have largely concentrated on either of the two approaches since a principled unifying framework is still lacking.  This paper suggests that both approaches can be combined under a hierarchical Bayesian framework.  Individual content-based user profiles are generated and collaboration between various user models is achieved via a common learned prior distribution.  However, it turns out that a parametric distribution (e. g.  Gaussian) is too restrictive to describe such a common learned prior distribution.  We thus introduce a nonparametric common prior, which is a sample generated from a Dirichlet process which assumes the role of a hyper prior.  We describe effective means to learn this nonparametric distribution, and apply it to learn users' information needs.  The resultant algorithm is simple and understandable, and offers a principled solution to combine content-based filtering and collaborative filtering.  Within our framework, we are now able to interpret various existing techniques from a unifying point of view.  Finally we demonstrate the empirical success of the proposed information filtering methods. 
GPPS: A Gaussian Process Positioning System for Cellular Networks| Abstract In this article, we present a novel approach to solving the localization problem in cellular networks.  The goal is to estimate a mobile user's position, based on measurements of the signal strengths received from network base stations.  Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements.  In the localization stage, the user's position can be estimated by maximizing the likelihood of received signal strengths with respect to the position.  We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network. 
Combining Estimators Using Non-Constant Weighting Functions|
A Neural Network Approach for Three-Dimensional Object Recognition|
Representing Probabilistic Rules with Networks of Gaussian Basis Functions|
Network Structuring and Training Using Rule-Based Knowledge|
Some Solutions to the Missing Feature Problem in Vision|
Efficient Methods for Dealing with Missing Data in Supervised Learning|
Removing redundancy and inconsistency in memory-based collaborative filtering|
editors,|
Probabilistic Memory-Based Collaborative Filtering|
Learning vector quantization algorithm for probabilistic models|
Combining Regularized Neural Networks|
Averaging, maximum penalised likelihood and Bayesian estimation for improving Gaussian mixture probability density estimates,|
Die besonderen Eigenschaften Neuronaler Netze bei der Approximation von Funktionen|
Knowing a tree from the forest: art image retrieval using a society of profiles|
Hidden Markov model for metric and event-based data|

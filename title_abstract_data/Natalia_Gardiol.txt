Applying Probabilistic Rules To Relational Worlds| Abstract Being able to represent and reason about the world as though it were composed of \objects" seems like a useful abstraction.  The typical approach to representing a world composed of objects is to use a relational representation; however, other representations, such as deictic representations, have also been studied.  I am interested not only in an agent that is able to represent objects, but in one that is also able to act in order to achieve some task.  This requires the ability to learn a plan of action.  While value-based approaches to learning plans have been studied in the past, both with relational and deictic representations, I believe the shortcomings uncovered by those studies can be overcome by the use of a world model.  Knowledge about how the world works has the advantage of being re-usable across specific tasks.  In general, however, it is dicult to obtain a completely specified model about the world.  This work attempts to characterize an approach to planning in a relational domain when the world model is represented as a potentially incomplete and/or redundant set of uncertain rules. 
Learning with Deictic Representation| Abstract Most reinforcement learning methods operate on propositional representations of the world state.  Such representations are often intractably large and generalize poorly.  Using a deictic representation is believed to be a viable alternative: they promise generalization while allowing the use of existing reinforcement-learning methods.  Yet, there are few experiments on learning with deictic representations reported in the literature.  In this paper we explore the effectiveness of two forms of deictic representation and a naive propositional representation in a simple blocks-world domain.  We find, empirically, that the deictic representations actually worsen performance.  We conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects. 
Hierarchical Memory-Based Reinforcement Learning| Abstract A key challenge for reinforcement learning is how to scale up to large partially observable domains.  In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task.  At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time.  We formalize this idea in a framework for solving partially observable, sequential decision tasks called Hierarchical Short-Term Memory (HSM).  HSM uses a memory-based SMDP Qlearning method to rapidly propagate delayed reward across long decision sequences.  We show that the HSM framework outperforms several related reinforcement learning techniques on a realistic corridor navigation task. 
Hierarchical memory-based reinforcement learning|
Learning with deictic representations|

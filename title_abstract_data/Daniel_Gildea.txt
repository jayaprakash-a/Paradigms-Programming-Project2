Final Report of Johns Hopkins 2003 Summer Workshop on Syntax for Statistical Machine Translation| Abstract In recent evaluations of machine translation systems, statistical systems have outperformed classical approaches based on interpretation, transfer, and generation.  Nonetheless, the output of statistical systems often contains obvious grammatical errors.  This can be attributed to the fact that the syntactic well-formedness is only influenced by local n-gram language models and simple alignment models.  We aim to integrate syntactic structure into statistical models to address this problem.  In the workshop we start with a very strong baseline -- the alignment template statistical machine translation system that obtained the best results in the 2002 and 2003 DARPA MT evaluations.  This model is based on a log-linear modeling framework, which allows for the easy integration of many different knowledge sources (i. e.  feature functions) into an overall model and to train the feature function combination weights discriminatively.  During the workshop, we incrementally add new features representing syntactic knowledge that deal with specific problems of the underlying baseline.  We want to investigate a broad range of possible feature functions, from very simple binary features to sophisticated treeto-tree translation models.  Simple feature functions test if a certain constituent occurs in the source and the target language parse tree.  More sophisticated features are derived from an alignment model where whole sub-trees in source and target can be aligned node by node.  We also plan to investigate features based on projection of parse trees from one language onto strings of another, a useful technique when parses are available for only one of the two languages.  We extend previous tree-based alignment models by allowing partial tree alignments when the two syntactic structures are not isomorphic.  We work with the Chinese-English data from the recent evaluations, as large amounts of sentence-aligned training corpora, as well as multiple reference translations are available.  This will also allow to compare results with the various systems participating in the evaluations.  In addition, an annotated Chinese-English parallel tree-bank is available.  We evaluate the improvement of our system using the BLEU metric.  Using the additional feature functions developed during the workshop the BLEU score improved from 31. 6% for the baseline MT system to 33. 2% using rescoring of a 1000-best list. 
Submitted to Journal of the Acoustical Society of America| Comments appreciated, but do not quote without permission.  Form variation of English function words in conversation.  Abstract Function words (the, that,
Skeletons in the parser: Using a shallow parser to improve deep parsing| Abstract We describe a simple approach for integrating shallow and deep parsing.  We use phrase structure bracketing obtained from the Collins parser as filters to guide deep parsing.  Our experiments demonstrate that our technique yields substantial gains in speed along with modest improvements in accuracy. 
A Dynamic Model of Aspectual Composition| Abstract This paper describes results of a dynamic model of aspectual composition that demonstrates how features necessary for planning and controlling actions can also motivate and ground simple analyses of a number of aspectual phenomena.  Anovel feature of the model is an active computational representation for verb semantics called x-schemas, an extension of the Petri net formalism that can encode goals, resources and other features affecting aspect.  Vexing problems of aspectual composition lend themselves to simple analyses in terms of the context-sensitiveinteraction between verb-speci#c x-schemas and a controller x-schema that captures important regularities in the evolution of events.  The resulting x-schemas can be elaborated and constrained by such factors as tense, temporal modifiers, nominals and pragmatic context, providing a rich representation that supports simulative inference in language understanding. 
Probabilistic Models of Verb-Argument Structure| Abstract We evaluate probabilistic models of verb argument structure trained on a corpus of verbs and their syntactic arguments.  Models designed to represent patterns of verb alternation behavior are compared with generic clustering models in terms of the perplexity assigned to held-out test data.  While the specialized models of alternation do not perform as well, closer examination reveals alternation behavior represented implicitly in the generic models. 
Syntactic Features for Evaluation of Machine Translation| Abstract Automatic evaluation of machine translation, based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems.  We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.  Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments. 
A Smorgasbord of Features for Statistical Machine Translation| Abstract We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation.  Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list.  Feature weights were optimized directly against the BLEU evaluation metric on held-out data.  We present results for a small selection of features at each level of syntactic representation. 
Identifying Semantic Roles Using Combinatory Categorial Grammar| Abstract We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.  This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles. 
The Necessity of Parsing for Predicate Argument Recognition| Abstract Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.  Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.  In this paper, we quantify the eect of parser accuracy on these systems' performance, and examine the question of whether a flatter "chunked" representation of the input can be as eective for the purposes of semantic role identification. 
To appear in ACL-95 Automatic Induction of Finite State Transducers for Simple Phonological Rules| Abstract This paper presents a method for learning phonological rules from sample pairs of underlying and surface forms, without negative evidence.  The learned rules are represented as finite state transducers that accept underlying forms as input and generate surface forms as output.  The algorithm for learning them is an extension of the OSTIA algorithm for learning general subsequential finite state transducers.  Although OSTIA is capable of learning arbitrary s. f. s. t's in the limit, large dictionaries of actual English pronunciations did not give enough samples to correctly induce phonological rules.  We then augmented OSTIA with two kinds of knowledge specific to natural language phonology, biases from "universal grammar".  One bias is that underlying phones are often realized as phonetically similar or identical surface phones.  The other biases phonological rules to apply across natural phonological classes.  The additions helped in learning more compact, accurate, and general transducers than the unmodified OSTIA algorithm.  An implementation of the algorithm successfully learns a number of English postlexical rules. 
Automatic Induction of Finite State Transducers for Simple Phonological Rules| ####################
TOPIC-BASED LANGUAGE MODELS USING EM| ABSTRACT In this paper, we propose a novel statistical language model to capture topic-related long-range dependencies.  Topics are modeled in a latent variable framework in which we also derive an EM algorithm to perform a topic factor decomposition based on a segmented training corpus.  The topic model is combined with a standard language model to be used for on-line word prediction.  Perplexity results indicate an improvement over previously proposed topic models, which unfortunately has not translated into lower word error. 
The Proposition Bank: An Annotated Corpus of Semantic Roles| The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank.  The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.  We discuss the criteria used to define the sets of semantic roles used in the annotation process, and analyze the frequency of syntactic/semantic alternations in the corpus.  We describe an automatic system for semantic role tagging trained on the corpus, and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation, and the contribution of the empty "trace" categories of the Treebank. 
Loosely Tree-Based Alignment for Machine Translation| Abstract We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.  This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. 
FORMS OF ENGLISH FUNCTION WORDS---EFFECTS OF DISFLUENCIES, TURN POSITION, AGE AND SEX, AND PREDICTABILITY| ABSTRACT This study examines the role of several non--phonetic factors in the reduction of ten frequent English function words (I, and, the, that, a , you, to, of, it, and in) in the phoneticallytranscribed portion of the Switchboard corpus of spontaneous telephone conversations.  Using ordinary linear and logistic regression models, we examined the length of the words and whether their vowels were full or reduced.  We show that function words are more likely to be longer or unreduced when they are turn--initial or utterance--final, when the speaker is female (mostly but not completely due to slower rate of speech) and when the word is surprising given the previous or following words.  Finally, focusing on finer details of the effect of planning problems on reduction, we show that filled pauses (uh and um) are the strongest factor in predicting lengthening of a previous function word.  The results bear on issues in speech recognition and models of speech production. 
Syntax-Based Alignment: Supervised or Unsupervised?| Abstract Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.  The trees may be learned directly from parallel corpora (Wu, 1997), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001).  In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data. 
Automatic Labeling of Semantic Roles| We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame.  Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-specific semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.  We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.  These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.  We used various lexical clustering algorithms to generalize across possible fillers of roles.  Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented constituents.  At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and featurecombination methods in the semantic role labeling task.  We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1.  Introduction Recent years have been exhilarating ones for natural language understanding.  The excitement and rapid advances that had characterized other language processing tasks such as speech recognition, part-of-speech tagging, and parsing have finally begun to appear in tasks in which understanding and semantics play a greater role.  For example, there has been widespread commercial deployment of simple speech-based natural language understanding systems that answer questions about flight arrival times, give directions, report on bank balances, or perform simple financial transactions.  More sophisticated research systems generate concise summaries of news articles, answer factbased questions, and recognize complex semantic and dialogue structure.  But the challenges that lie ahead are still similar to the challenge that the field has faced since Winograd (1972):
To appear, Journal of the Acoustical Society of America Effects of disfluencies, predictability, and utterance position on word form variation in English conversation| Abstract Function words, especially frequently occurring ones such as (the, that,
Stochastic Lexicalized Inversion Transduction Grammar for Alignment| Abstract We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.  Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences. 
Corpus Variation and Parser Performance| Abstract Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.  While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.  We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.  This leads us to a technique for pruning parameters to reduce the size of the parsing model. 
Dependencies vs| Constituents for Tree-Based Alignment.  Abstract Given a parallel parsed corpus, statistical treeto-tree alignment attempts to match nodes in the syntactic trees for a given sentence in two languages.  We train a probabilistic tree transduction model on a large automatically parsed Chinese-English corpus, and evaluate results against human-annotated word level alignments.  We find that a constituent-based model performs better than a similar probability model trained on the same trees converted to a dependency representation. 
APPLYING PRONUNCIATION MODELING TECHNIQUES TO FRENCH| ABSTRACT This paper describes the use of data-driven pronunciation modeling techniques to substantially reduce error in a large vocabulary hybrid ANN/HMM speech recognition system for French.  Pronunciations from canonical transcriptions are combined with pronunciations from a text-to-speech system.  Forced alignments are used to calculate pronunciations' probabilities and to eliminate bad pronunciations.  Phonetic rules are used to smooth probabilities and ensure the inclusion of pronunciations that may not be present in the training data.  Issues concerning the choice of phonemes for the system are also discussed. 
Learning Bias and Phonological-Rule Induction| A fundamental debate in the machine learning of language has been the role of prior knowledge in the learning process.  Purely nativist approaches, such as the Principles and Parameters model, build parameterized linguistic generalizations directly into the learning system. 
The functional relation of visual evoked response and reaction time to stimulus intensity|
Semantic roles labeling by maximum entropy model|

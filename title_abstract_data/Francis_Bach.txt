Learning Graphical Models for Stationary Time Series| Abstract Probabilistic graphical models can be extended to time series by considering probabilistic dependencies between entire time series. 
Tree-dependent Component Analysis| Abstract We present a generalization of independent component analysis (ICA), where instead of looking for a linear transform that makes the data components independent, we look for a transform that makes the data components well fit by a tree-structured graphical model.  Treating the problem as a semiparametric statistical problem, we show that the optimal transform is found by minimizing a contrast function based on mutual information, a function that directly extends the contrast function used for classical ICA.  We provide two approximations of this contrast function, one using kernel density estimation, and another using kernel generalized variance.  This tree-dependent component analysis framework leads naturally to an efficient general multivariate density estimation technique where only bivariate density estimation needs to be performed. 
Analyse en Composantes Independantes et Reseaux Bayesiens| Resume -- Une generalisation de l'analyse en composantes independantes (ACI) est introduite: au lieu de determiner une application lineaire qui rend les composantes independantes, nous cherchons une application lineaire qui rend les composantes modelisables par un reseau Bayesien dont la structure est un arbre.  Ce nouveau mod` ele, que nous denommons TCA (Tree-dependent Component Analysis), permet de relaxer l'hypoth` ese d'independance et d'adapter la structure de dependance aux observations.  En particulier, lorsque l'arbre a plusieurs composantes connexes, notre methode permet de trouver des amas de composantes de telle sorte que les composantes ` a l'interieur d'un amas sont dependantes les unes
Kernel Independent Component Analysis| ABSTRACT We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space.  On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence.  On the other hand, building on recent developments in kernel methods, we show that these criteria can be computed efficiently.  Minimizing these criteria leads to flexible and robust algorithms for ICA.  We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms. 
Computing regularization paths for learning multiple kernels| Abstract The problem of learning a sparse conic combination of kernel functions or kernel matrices for classification or regression can be achieved via the regularization by a block 1-norm [1].  In this paper, we present an algorithm that computes the entire regularization path for these problems.  The path is obtained by using numerical continuation techniques, and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter.  Working in the setting of kernel linear regression and kernel logistic regression, we show empirically that the effect of the block 1-norm regularization differs notably from the (non-block) 1-norm regularization commonly used for variable selection, and that the regularization path is of particular value in the block case. 
Thin Junction Trees| Abstract We present an algorithm that induces a class of models with thin junction trees---models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph.  By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process.  This allows both an efficient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efficiently with the final model.  We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection. 
FINDING CLUSTERS IN INDEPENDENT COMPONENT ANALYSIS| ABSTRACT We present a class of algorithms that find clusters in independent component analysis: the data are linearly transformed so that the resulting components can be grouped into clusters, such that components are dependent within clusters and independent between clusters.  In order to find such clusters, we look for a transform that fits the estimated sources to a forest-structured graphical model.  In the nonGaussian, temporally independent case, the optimal transform is found by minimizing a contrast function based on mutual information that directly extends the contrast function used for classical ICA.  We also derive a contrast function in the Gaussian stationary case that is based on spectral densities and generalizes the contrast function of Pham [22] to richer classes of dependency. 
Beyond Independent Components: Trees and Clusters| Abstract We present a generalization of independent component analysis (ICA), where instead of looking for a linear transform that makes the data components independent, we look for a transform that makes the data components well fit by a tree-structured graphical model.  This tree-dependent component analysis (TCA) provides a tractable and flexible approach to weakening the assumption of independence in ICA.  In particular, TCA allows the underlying graph to have multiple connected components, and thus the method is able to find "clusters" of components such that components are dependent within a cluster and independent between clusters.  Finally, we make use of a notion of graphical models for time series due to Brillinger (1996) to extend these ideas to the temporal setting.  In particular, we are able to fit models that incorporate tree-structured dependencies among multiple time series. 
Statistical Convergence of Kernel CCA| Abstract While kernel canonical correlation analysis (kernel CCA) has been applied in many problems, the asymptotic convergence of the functions estimated from a finite sample to the true functions has not yet been established.  This paper gives a rigorous proof of the statistical convergence of kernel CCA and a related method (NOCCO), which provides a theoretical justification for these methods.  The result also gives a sufficient condition on the decay of the regularization coefficient in the methods to ensure convergence. 
Predictive low-rank decomposition for kernel methods| Abstract Low-rank matrix decompositions are essential tools in the application of kernel methods to large-scale learning problems.  These decompositions have generally been treated as black boxes---the decomposition of the kernel matrix that they deliver is independent of the specific learning task at hand--and this is a potentially significant source of inefficiency.  In this paper, we present an algorithm that can exploit side information (e. g. , classification labels, regression responses) in the computation of low-rank decompositions for kernel matrices.  Our algorithm has the same favorable scaling as state-of-the-art methods such as incomplete Cholesky decomposition---it is linear in the number of data points and quadratic in the rank of the approximation.  We present simulation results that show that our algorithm yields decompositions of significantly smaller rank than those found by incomplete Cholesky decomposition. 
Learning Spectral Clustering| Abstract Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity.  In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem.  Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm.  Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix.  We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors. 
Multiple kernel learning, conic duality, and the SMO algorithm| Abstract While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels.  Lanckriet et al.  (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP).  Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-di#erentiable.  We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied.  We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes. 
Learning Graphical Models with Mercer Kernels| Abstract We present a class of algorithms for learning the structure of graphical models from data.  The algorithms are based on a measure known as the kernel generalized variance (KGV), which essentially allows us to treat all variables on an equal footing as Gaussians in a feature space obtained from Mercer kernels.  Thus we are able to learn hybrid graphs involving discrete and continuous variables of arbitrary type.  We explore the computational properties of our approach, showing how to use the kernel trick to compute the relevant statistics in linear time.  We illustrate our framework with experiments involving discrete and continuous data. 
A Probabilistic Interpretation of Canonical Correlation Analysis| Abstract We give a probabilistic interpretation of canonical correlation (
Blind one-microphone speech separation: A spectral learning approach| Abstract We present an algorithm to perform blind, one-microphone speech separation.  Our algorithm separates mixtures of speech without modeling individual speakers.  Instead, we formulate the problem of speech separation as a problem in segmenting the spectrogram of the signal into two or more disjoint sets.  We build feature sets for our segmenter using classical cues from speech psychophysics.  We then combine these features into parameterized affinity matrices.  We also take advantage of the fact that we can generate training examples for segmentation by artificially superposing separately-recorded signals.  Thus the parameters of the affinity matrices can be tuned using recent work on learning spectral clustering [1].  This yields an adaptive, speech-specific segmentation algorithm that can successfully separate one-microphone speech mixtures. 
Dimensionality Reduction for Supervised Learning with Reproducing Kernel Hilbert Spaces| Abstract We propose a novel method of dimensionality reduction for supervised learning problems.  Given a regression or classification problem in which we wish to predict a response variable Y from an explanatory variable X , we treat the problem of dimensionality reduction as that of finding a low-dimensional "effective subspace" of X which retains the statistical relationship between X and Y .  We show that this problem can be formulated in terms of conditional independence.  To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on a reproducing kernel Hilbert space.  This characterization allows us to derive a contrast function for estimation of the effective subspace.  Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X , nor a parametric model of the conditional distribution of Y .  We present experiments that compare the performance of the method with conventional methods. 
On the Path to an Ideal ROC Curve: Considering Cost Asymmetry in Learning Classifiers| Abstract Receiver Operating Characteristic (ROC) curves are a standard way to display the performance of a set of binary classifiers for all feasible ratios of the costs associated with false positives and false negatives.  For linear classifiers, the set of classifiers is typically obtained by training once, holding constant the estimated slope and then varying the intercept to obtain a parameterized set of classifiers whose performances can be plotted in the ROC plane.  In this paper, we consider the alternative of varying the asymmetry of the cost function used for training.  We show that the ROC curve obtained by varying the intercept and the asymmetry---and hence the slope---always outperforms the ROC curve obtained by varying only the intercept.  In addition, we present a path-following algorithm for the support vector machine (SVM) that can compute efficiently the entire ROC curve, that has the same computational properties as training a single classifier.  Finally, we provide a theoretical analysis of the relationship between the asymmetric cost model assumed when training a classifier and the cost model assumed in applying the classifier.  In particular, we show that the mismatch between the step function used for testing and its convex upper bounds usually used for training leads to a provable and quantifiable difference around extreme asymmetries. 
Kernel indepedendent component analysis|
Consistency of kernel canonical correlation|

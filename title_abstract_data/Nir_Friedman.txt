Belief Revision: A Critique \Lambda| Abstract We examine carefully the rationale underlying the approaches to belief change taken in the literature, and highlight what we view as methodological problems.  We argue that to study belief change carefully, we must be quite explicit about the "ontology" or scenario underlying the belief change process.  This is something that has been missing in previous work, with its focus on postulates.  Our analysis shows that we must pay particular attention to two issues that have often been taken for granted: The first is how we model the agent's epistemic state.  (Do we use a set of beliefs, or a richer structure, such as an ordering on worlds? And if we use a set of beliefs, in what language are these beliefs are expressed?) We show that even postulates that have been called "beyond controversy" are unreasonable when the agent's beliefs include beliefs about her own epistemic state as well as the external world.  The second is the status of observations.  (Are observations known to be true, or just believed? In the latter case, how firm is the belief?) Issues regarding the status of observations arise particularly when we consider iterated belief revision, and we must confront the possibility of revising by ' and then by :'. 
Structured Representation of Complex Stochastic Systems| Abstract This paper considers the problem of representing complex systems that evolve stochastically over time. 
Robust temporal and spectral modeling for query By melody| ABSTRACT Query by melody is the problem of retrieving musical performances from melodies.  Retrieval of real performances is complicated due to the large number of variations in performing a melody and the presence of colored accompaniment noise.  We describe a simple yet effective probabilistic model for this task.  We describe a generative model that is rich enough to capture the spectral and temporal variations of musical performances and allows for tractable melody retrieval.  While most of previous studies on music retrieval from melodies were performed with either symbolic (e. g.  MIDI) data or with monophonic (single instrument) performances, we performed experiments in retrieving live and studio recordings of operas that contain a leading vocalist and rich instrumental accompaniment.  Our results show that the probabilistic approach we propose is effective and can be scaled to massive datasets. 
Discovering the Hidden Structure of Complex Dynamic Systems| Abstract Dynamic Bayesian networks provide a compact and natural representation for complex dynamic systems.  However, in many cases, there is no expert available from whom a model can be elicited.  Learning provides an alternative approach for constructing models of dynamic systems.  In this paper, we address some of the crucial computational aspects of learning the structure of dynamic systems, particularly those where some relevant variables are partially observed or even entirely unknown.  Our approach is based on the Structural Expectation Maximization (SEM) algorithm.  The main computational cost of the SEM algorithm is the gathering of expected sufficient statistics.  We propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently.  We also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search.  Our approach is based on the observation that, in dynamic systems, ignoring a hidden variable typically results in a violation of the Markov property.  Thus, our algorithm searches for such violations in the data, and introduces hidden variables to explain them.  We provide empirical results showing that the algorithm is able to learn the dynamics of complex systems in a computationally tractable way. 
Bayesian Q-Learning| Abstract A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good.  The benefit of exploration can be estimated using the classical notion of Value of Information---the expected improvement in future decision quality that might arise from the information acquired by exploration.  Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states.  In this paper, we adopt a Bayesian approach to maintaining this uncertain information.  We extend Watkins' Q-learning by maintaining and propagating probability distributions over the Q-values.  These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation.  We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies. 
On Decision-Theoretic Foundations for Defaults| Abstract In recent years, considerable effort has gone into understanding default reasoning.  Most of this effort concentrated on the question of entailment, i. e. , what conclusions are warranted by a knowledge-base of defaults.  Surprisingly, few works formally examine the general role of defaults.  We argue that an examination of this role is necessary in order to understand defaults, and suggest a concrete role for defaults: Defaults simplify our decisionmaking process, allowing us to make fast, approximately optimal decisions by ignoring certain possible states.  In order to formalize this approach, we examine decision making in the framework of decision theory.  We use probability and utility to measure the impact of possible states on the decisionmaking process.  More precisely, we examine when a consequence relation, which is the set of default inferences made by an inference system, can be compatible with such a decision theoretic setup.  We characterize general properties that such consequence relations must satisfy and contrast them with previous analysis of default consequence relations in the literature.  In particular, we show that such consequence relations must satisfy the
A Structural EM Algorithm for Phylogenetic Inference| Abstract A central task in the study of molecular evolution is the reconstruction of a phylogenetic tree from sequences of current-day taxa.  The most established approach to tree reconstruction is maximum likelihood (ML) analysis.  Unfortunately, searching for the maximum likelihood phylogenetic tree is computationally prohibitive for large data sets.  In this dissertation, we describe a new algorithm that uses Structural EM for learning maximum likelihood phylogenetic trees.  This algorithm is similar to the standard EM method for edge-length estimation, except that during iterations of the Structural EM algorithm the topology is improved as well as the edge length.  Our algorithm performs iterations of two steps.  In the E-Step, we use the current tree topology and edge lengths to compute expected suf#cient statistics, which summarize the data.  In the M-Step, we search for a topology that maximizes the likelihood with respect to these expected suf#cient statistics.  We show that searching for better topologies inside the M-step can be done ef#ciently, as opposed to standard methods for topology search.  We prove that each iteration of this procedure increases the likelihood of the topology, and thus the procedure must converge.  This convergence point, however, can be a sub-optimal one.  To escape from such ^local optim,we further enhance our basic EM procedure by incorporating moves in the flavor of simulated annealing.  We evaluate these new algorithms on both synthetic and real sequence data, and show that for protein sequences even our basic algorithm finds more plausible trees than existing methods for searching maximum likelihood phylogenies.  Furthermore, our algorithms are dramatically faster than such methods, enabling, for the first time, phylogenetic analysis of large protein data sets in the maximum likelihood framework. 
Rich probabilistic models for gene expression| ABSTRACT Clustering is commonly used for analyzing gene expression data.  Despite their successes, clustering methods suffer from a number of limitations.  First, these methods reveal similarities that exist over all of the measurements, while obscuring relationships that exist over only a subset of the data.  Second, clustering methods cannot readily incorporate additional types of information, such as clinical data or known attributes of genes.  To circumvent these shortcomings, we propose the use of a single coherent probabilistic model, that encompasses much of the rich structure in the genomic expression data, while incorporating additional information such as experiment type, putative binding sites, or functional information.  We show how this model can be learned from the data, allowing us to discover patterns in the data and dependencies between the gene expression patterns and additional attributes.  The learned model reveals context-specific relationships, that exist only over a subset of the experiments in the dataset.  We demonstrate the power of our approach on synthetic data and on two real-world gene expression data sets for yeast.  For example, we demonstrate a novel functionality that falls naturally out of our framework: predicting the "cluster" of the array resulting from a gene mutation based only on the gene's expression pattern in the context of other mutations. 
The Information Bottleneck EM Algorithm| Abstract Learning with hidden variables is a central challenge in probabilistic graphical models that has important implications for many real-life problems.  The classical approach is using the Expectation Maximization (EM) algorithm.  This algorithm, however, can get trapped in local maxima.  In this paper we explore a new approach that is based on the Information Bottleneck principle.  In this approach, we view the learning problem as a tradeoff between two information theoretic objectives.  The first is to make the hidden variables uninformative about the identity of specific instances.  The second is to make the hidden variables informative about the observed attributes.  By exploring different tradeoffs between these two objectives, we can gradually converge on a high-scoring solution.  As we show, the resulting, Information Bottleneck Expectation Maximization (IB-EM) algorithm, manages to find solutions that are superior to standard EM methods. 
Building Classifiers Using Bayesian Networks| Abstract Recent work in supervised learning has shown that a
Conditional Logics of Belief Change \Lambda| Abstract The study of belief changehas been an active area in philosophy and AI.  In recent years two special cases of belief change, belief revision and belief update, have been studied in detail.  Belief revision and update are clearly not the only possible notions of belief change.  In this paper we investigate properties of a range of possible belief change operations.  We start with an abstract notion of a belief change system and provide a logical language that describes belief change in such systems.  We then consider several reasonable properties one can impose on such systems and characterize them axiomatically.  We show that both belief revision and update fit into our classification.  As a consequence, we get both a semantic and an axiomatic (proof-theoretic) characterization of belief revision and update (as well as some belief change operations that generalize them), in one natural framework. 
Learning Probabilistic Relational Models| Abstract A large portion of real-world data is stored in commercial relational database systems.  In contrast, most statistical learning methods work only with "flat" data representations.  Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database.  This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases.  PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects.  Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models.  We describe both parameter estimation and structure learning --- the automatic induction of the dependency structure in a model.  Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets.  We present experimental results on both real and synthetic relational databases. 
A structural EM algorithm for phylogenetic inference| ABSTRACT A central task in the study of evolution is the reconstruction of a phylogenetic tree from sequences of current-day taxa.  A well supported approach to tree reconstruction performs maximum likelihood (ML) analysis.  Unfortunately, searching for the maximum likelihood phylogenetic tree is computationally expensive.  In this paper, we describe a new algorithm that uses Structural-EM for learning maximum likelihood trees.  This algorithm is similar to the standard EM method for estimating branch lengths, except that during iterations of this algorithms the topology is improved as well as the branch length.  The algorithm performs iterations of two steps.  In the E-Step, we use the current tree topology and branch lengths to compute expected sucient statistics, which summarize the data.  In the M-Step, we search for a topology that maximizes the likelihood with respect to these expected sucient statistics.  As we show, searching for better topologies inside the M-step can be done eciently, as opposed to standard search over topologies.  We prove that each iteration of this procedure increases the likelihood of the topology, and thus the procedure must converge.  We evaluate our new algorithm on both synthetic and real sequence data, and show that it is both dramatically faster and finds more plausible trees than standard search for maximum likelihood phylogenies. 
Learning Probabilistic Relational Models with Structural Uncertainty| Abstract Most real-world data is stored in relational form.  In
Conditional Logics of Belief Change| Abstract The study of belief change has been an active area in philosophy and AI.  In recent years two special cases of belief change, belief revision and belief update, have been studied in detail.  Belief revision and update are clearly not the only possible notions of belief change.  In this paper we investigate properties of a range of possible belief change operations.  We start with an abstract notion of a belief change system and provide a logical language that describes belief change in such systems.  We then consider several reasonable properties one can impose on such systems and characterize them axiomatically.  We show that both belief revision and update fit into our classification.  As a consequence, we get both a semantic and an axiomatic (proof-theoretic) characterization of belief revision and update (as well as some belief change operations that generalize them), in one natural framework. 
Context-specific Bayesian clustering for gene expression data| Abstract The recent growth in genomic data and measurements of genome-wide expression patterns allows us to apply computational tools to examine gene regulation by transcription factors.  In this work, we present a class of mathematical models that help in understanding the connections between transcription factors and functional classes of genes based on genetic and genomic data.  Such a model represents the joint distribution of transcription factor binding sites and of expression levels of a gene in a unified probabilistic model.  Learning a combined probability model of binding sites and expression patterns enables us to improve the clustering of the genes based on the discovery of putative binding sites and to detect which binding sites and experiments best characterize a cluster.  To learn such models from data, we introduce a new search method that rapidly learns a model according to a Bayesian score.  We evaluate our method on synthetic data as well as on real life data and analyze the biological insights it provides.  Finally, we demonstrate the applicability of the method to other data analysis problems in gene expression data. 
A Simple Hyper-Geometric Approach for Discovering Putative Transcription Factor Binding Sites| Abstract.  A central issue in molecular biology is understanding the regulatory mechanisms that control gene expression.  The recent flood of genomic and post-genomic data opens the way for computational methods elucidating the key components that play a role in these mechanisms.  One important consequence is the ability to recognize groups of genes that are co-expressed using microarray expression data.  We then wish to identify in-silico putative transcription factor binding sites in the promoter regions of these gene, that might explain the co-regulation, and hint at possible regulators.  In this paper we describe a simple and fast, yet powerful, two stages approach to this task.  Using a rigorous hypergeometric statistical analysis and a straightforward computational procedure we find small conserved sequence kernels.  These are then stochastically expanded into PSSMs using an EM-like procedure.  We demonstrate the utility and speed of our methods by applying them to several data sets from recent literature.  We also compare these results with those of MEME when run on the same sets. 
Learning the Structure of Dynamic Probabilistic Networks| Abstract Dynamic probabilistic networks are a compact
A Qualitative Markov Assumption and Its Implications for Belief Change| Abstract The study of belief change has been an active area in philosophyand AI.  In recent years, two special cases of belief change, belief revision and belief update, have been studied in detail.  Roughly speaking, revision treats a surprising observation as a sign that previous beliefs were wrong, while update treats a surprising observation as an indication that the world has changed.  In general, we would expect that an agent making an observation may both want to revise some earlier beliefs and assume that some change has occurred in the world.  We define a novel approach to belief change that allows us to do this, by applying ideas from probability theory in a qualitative settings.  The key idea is to use a qualitative Markov assumption, which says that state transitions are independent.  We show that a recent approach to modeling qualitative uncertainty using plausibility measures allows us to make such a qualitative Markov assumption in a relatively straightforward way, and show how the Markov assumption can be used to provide an attractive belief-change model. 
The Bayesian Structural EM Algorithm| Abstract In recent years there has been a flurry of works on learning Bayesian networks from data.  One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data---that is, in the presence of missing values or hidden variables.  In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection.  That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score.  In this paper, I extend Structural EM to deal directly with Bayesian model selection.  I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof. 
Learning Bayesian Network Structure from Massive Datasets: The "Sparse Candidate" Algorithm| Abstract Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a statistically motivated score.  By and large, existing learning tools address this optimization problem using standard heuristic search techniques.  Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable.  This problem becomes critical when we deal with data sets that are large either in the number of instances, or the number of attributes.  In this paper, we introduce an algorithm that achieves faster learning by restricting the search space.  This iterative algorithm restricts the parents of each variable to belong to a small subset of candidates.  We then search for a network that satisfies these constraints.  The learned network is then used for selecting better candidates for the next iteration.  We evaluate this algorithm both on synthetic and real-life data.  Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures. 
Learning of Bayesian Network Structure from Massive Datasets: The "Sparse Candidate" Algorithm| Abstract Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a statistically motivated score.  By and large, existing learning tools address this optimization problem using standard heuristic search techniques.  Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable.  This problem becomes critical when we deal with data sets that are large both in the number of instances, and the number of attributes.  In this paper, we introduce an algorithm that achieves faster learning by restricting the search space.  This iterative algorithm restricts the parents of each variable to belong to a small subset of candidates.  We then search for a network that satisfies these constraints.  The learned network is then used for selecting better candidates for the next iteration.  We evaluate this algorithm both on synthetic and real-life data.  Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures. 
Tissue classification with gene expression profiles| Abstract Constantly improving gene expression profiling technologies are expected to provide understanding and insight into cancer related cellular processes.  Gene expression data is also expected to significantly aid in the development of efficient cancer diagnosis and classification platforms.  In this work we examine two sets of gene expression data measured across sets of tumor and normal clinical samples.  One set consists of 2,000 genes, measured in 62 epithelial colon samples [1].  The second consists of 100,000 clones, measured in 32 ovarian samples (unpublished, extension of data set described in [26]).  We examine the use of scoring methods, measuring separation of tumors from normals using individual gene expression levels.  These are then coupled with high dimensional classification methods to assess the classification power of complete expression profiles.  We present results of performing leave-one-out cross validation (LOOCV) experiments on the two data sets, employing SVM [8], AdaBoost [13] and a novel clustering based classification technique.  As tumor samples can differ from normal samples in their cell-type composition we also perform LOOCV experiments using appropriately modified sets of genes, attempting to eliminate the resulting bias.  We demonstrate success rate of at least 90% in tumor vs normal classification, using sets of selected genes, with as well as without cellular contamination related members.  These results are insensitive to the exact selection mechanism, over a certain range. 
Learning Module Networks| Abstract Methods for learning Bayesian networks can discover dependency structure between observed variables.  Although these methods are useful in many applications,
Modeling Belief in Dynamic Systems, Part II: Revision and Update| Abstract The study of belief change has been an active area in philosophy and AI.  In recent years two special cases of belief change, belief revision and belief update, have been studied in detail.  In a companion paper (Friedman & Halpern
Modeling Belief in Dynamic Systems| Part II: Revision and Update \Lambda.  Abstract The study of belief change has been an active area in philosophy and AI.  In recent years two special cases of belief change, belief revision and belief update, have been studied in detail.  In a companion paper [Friedman and Halpern 1997
Plausibility Measures: A User's Guide| Abstract We examine a new approach to modeling uncertainty based on plausibility measures, where a plausibility measure just associates with an event its plausibility , an element is some partially ordered set.  This approach is easily seen to generalize other approaches to modeling uncertainty, such as probability measures, belief functions, and possibility measures.  The lack of structure in a plausibility measure makes it easy for us to add structure on an "as needed" basis, letting us examine what is required to ensure that a plausibility measure has certain properties of interest.  This gives us insight into the essential features of the properties in question, while allowing us to prove general results that apply to many approaches to reasoning about uncertainty.  Plausibility measures have already proved useful in analyzing default reasoning.  In this paper, we examine their "algebraic properties", analogues to the use of and # in probability theory.  An understanding of such properties will be essential if plausibility measures are to be used in practice as a representation tool. 
Tissue Classification with Gene Expression Profiles2| Abstract Constantly improving gene expression profiling technologies are expected to provide understanding and insight into cancer related cellular processes.  Gene expression data is also
On the Application of The Bootstrap for Computing Confidence Measures on Features of Induced Bayesian Networks| Abstract In the context of learning Bayesian networks from data, very little work has been published on methods for assessing the quality of an induced model.  This issue, however, has received a great deal of attention in the statistics literature.  In this paper, we take a well-known method from statistics, Efron's Bootstrap, and examine its applicability for assessing a confidence measure on features of the learned network structure.  We also compare this method to assessments based on a practical realization of the Bayesian methodology. 
Plausibility Measures and Default Reasoning| Abstract In recent years, a number of different semantics for defaults have been proposed, such as preferential
A Bayesian Approach to Structure Discovery in Bayesian Networks| Abstract.  In many multivariate domains, we are interested in analyzing the dependency structure of the
Data Perturbation for Escaping Local Maxima in Learning| Abstract Almost all machine learning algorithms---be they for regression, classification or density estimation---seek hypotheses that optimize a score on training data.  In most interesting cases, however, full global optimization is not feasible and local search techniques are used to discover reasonable
Bayesian Network Classification with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting| Abstract In a recent paper,
From promoter sequence to expression: a probabilistic framework| ABSTRACT We present a probabilistic framework that models the process by which transcriptional binding explains the mRNA expression of different genes.  Our joint probabilistic model unifies the two key components of this process: the prediction of gene regulation events from sequence motifs in the gene's promoter region, and the prediction of mRNA expression from combinations of gene regulation events in different settings.  Our approach has several advantages.  By learning promoter sequence motifs that are directly predictive of expression data, it can improve the identification of binding site patterns.  It is also able to identify combinatorial regulation via interactions of different transcription factors.  Finally, the general framework allows us to integrate additional data sources, including data from the recent binding localization assays.  We demonstrate our approach on the cell cycle data of Spellman et al. , combined with the binding localization information of Simon et al.  We show that the learned model predicts expression from sequence, and that it identifies coherent co-regulated groups with significant transcription factor motifs.  It also provides valuable biological insight into the domain via these co-regulated ^modules~ and the combinatorial regulation effects that govern their behavior. 
Class discovery in gene expression data| ABSTRACT Recent studies (Alizadeh et al, [1]; Bittner et al,[5]; Golub et al, [11]) demonstrate the discovery of putative disease subtypes from gene expression data.  The underlying computational problem is to partition the set of sample tissues into statistically meaningful classes.  In this paper we present a novel approach to class discovery and develop automatic analysis methods.  Our approach is based on statistically scoring candidate partitions according to the overabundance of genes that separate the different classes.  Indeed, in biological datasets, an overabundance of genes separating known classes is typically observed.  we measure overabundance against a stochastic null model.  This allows for highlighting subtle, yet meaningful, partitions that are supported on a small subset of the genes.  Using simulated annealing we explore the space of all possible partitions of the set of samples, seeking partitions with statistically significant overabundance of differentially expressed genes.  We demonstrate the performance of our methods on synthetic data, where we recover planted partitions.  Finally, we turn to tumor expression datasets, and show that we find several highly pronounced partitions. 
Challenge: Where is the Impact of Bayesian Networks in Learning?| Abstract Bayesian networks are graphical representations of probability distributions.  Over the last decade, these representations have become the method of choice for representation of uncertainly in artificial intelligence.  Today, they play a crucial role in modern expert systems, diagnosis engines, and decision support systems.  In recent years, there has been much interest in learning Bayesian networks from data.  Learning such models is desirable simply because there is a wide array of off-the-shelf tools that can apply the learned models as described above.  Practitioners also claim that adaptive Bayesian networks have advantages in their own right as a non-parametric method for density estimation, data analysis, pattern classification, and modeling.  Among the reasons cited we find: their semantic clarity and understandability by humans, the ease of acquisition and incorporation of prior knowledge, the ease of integration with optimal decision-making methods, the possibility of causal interpretation of learned models, and the automatic handling of noisy and missing data.  In spite of these claims, methods that learn Bayesian networks have yet to make the impact that other techniques such as neural networks and hidden Markov models have made in applications such as pattern and speech recognition.  In this paper, we challenge the research community to identify and characterize domains where induction of Bayesian networks makes the critical difference, and to quantify the factors that are responsible for that difference.  In addition to formalizing the challenge, we identify research problems whose solution is, in our view, crucial for meeting this challenge. 
First-Order Conditional Logic Revisited| Abstract Conditional logics play an important role in recent attempts to investigate default reasoning.  This paper investigates firstorder conditional logic.  We show that, as for first-order probabilistic logic, it is important not to confound statistical conditionals over the domain (such as "most birds fly"), and subjective conditionals over possible worlds (such as "I believe that Tweety is unlikely to fly").  We then address the issue of ascribing semantics to first-order conditional logic.  As in the propositional case, there are many possible semantics.  To study the problem in a coherent way, we use plausibility structures. 
Learning Probabilistic Models of Relational Structure| We describe the appropriate conditions for using each model and present learning algorithms for each.  We present experimental results showing that the learned models can be used to predict relational structure and, moreover, the observed relational structure can be used to provide better predictions for the attributes in the model. 
A branch-and-bound algorithm for the inference of ancestral amino-acid sequences when the replacement rate varies among sites: Application to the evolution of five gene families| Running head: Branch & bound algorithm for ancestral sequences. 
Belief Revision: A Critique| Abstract The problem of belief change---how
CIS: Compound Importance Sampling Method for Transcription Factor Binding Site p-value Estimation| Abstract Transcriptional regulation is mainly obtained by transcription factors that bind sequencespecific binding
Bayesian Network Classifiers| Abstract.  Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4. 5.  This fact raises the question of whether a classifier with less restrictive assumptions can perform even better.  In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks.  These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence.  Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes.  We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4. 5, naive Bayes, and wrapper methods for feature selection. 
Context-Specific Independence in Bayesian Networks| Abstract Bayesiannetworks provide a languagefor qualitatively representing the conditional independence properties of a distribution.  This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms.  It is well-known, however, that there are certain independencies that we cannot capture qualitatively within the Bayesian network structure: independencies that hold only in certain contexts, i. e. , given a specific assignment of values to certain variables.  In this paper, we propose a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node.  We present a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network.  We then focus on a particular qualitative representation scheme---tree-structured CPTs--for capturing CSI.  We suggest ways in which this representation can be used to support effective inference algorithms.  In particular, we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms, and an alternative algorithm based on cutset conditioning. 
Efficient Bayesian Parameter Estimation in Large Discrete Domains| Abstract We examine the problem of estimating the parameters of a multinomial distribution over a large number of discrete outcomes, most of which do not appear in the training data.  We analyze this problem from a Bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcomes constitute only a small subset of the possible outcomes.  We show how to efficiently perform exact inference with this form of hierarchical prior and compare our method to standard approaches. 
Belief Revision with Unreliable Observations| Abstract Research in belief revision has been dominated by work that lies firmly within the classic
Using Bayesian networks to analyze expression data| Abstract DNA hybridization arrays simultaneously measure the expression level for thousands of genes.  These measurements provide
Data Analysis with Bayesian Networks: A Bootstrap Approach| Abstract In recent years there has been significant progress in algorithms and methods for inducing Bayesian networks from data.  However, in complex data analysis problems, we need to go beyond being satisfied with inducing networks with high scores.  We need to provide confidence measures on features of these networks: Is the existence of an edge between two nodes warranted? Is the Markov blanket of a given node robust? Can we say something about the ordering of the variables? We should be able to address these questions, even when the amount of data is not enough to induce a high scoring network.  In this paper we propose Efron's Bootstrap as a computationally efficient approach for answering these questions.  In addition, we propose to use these confidence measures to induce better structures from the data, and to detect the presence of latent variables. 
Modeling dependencies in protein-DNA binding sites| ABSTRACT The availability of whole genome sequences and high-throughput genomic assays opens the door for in silico analysis of transcription regulation.  This includes methods for discovering and characterizing the binding sites of DNA-binding proteins, such as transcription factors.  A common representation of transcription factor binding sites is a position specific score matrix (PSSM).  This representation makes the strong assumption that binding site positions are independent of each other.  In this work, we explore Bayesian network representations of binding sites that provide different tradeoffs between complexity (number of parameters) and the richness of dependencies between positions.  We develop the formal machinery for learning such models from data and for estimating the statistical significance of putative binding sites.  We then evaluate the ramifications of these richer representations in characterizing binding site motifs and predicting their genomic locations.  We show that these richer representations improve over the PSSM model in both tasks. 
Agglomerative Multivariate Information Bottleneck| Abstract The Information bottleneck method is an unsupervised non-parametric data organization technique.  Given a joint distribution P (A; B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B.  The information bottleneck has already been applied to document classification, gene expression, neural code, and spectral analysis.  In this paper, we introduce a general principled framework for multivariate extensions of the information bottleneck method.  This allows us to consider multiple systems of data partitions that are inter-related.  Our approach utilizes Bayesian networks for specifying the systems of clusters and what information each captures.  We show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations.  We also present a general framework for iterative algorithms for constructing solutions, and apply it to several examples. 
Discovering Hidden Variables: A Structure-Based Approach| Abstract A serious problem in learning probabilistic models is the presence of hidden variables.  These variables are not observed, yet interact with several of the observed variables.  As such, they induce seemingly complex dependencies among the latter.  In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables.  In this paper, we address the related problem of detecting hidden variables that interact with the observed variables.  This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models.  A very natural approach is to search for "structural signatures" of hidden variables --substructures in the learned network that tend to suggest the presence of a hidden variable.  We make this basic idea concrete, and show how to integrate it with structure-search algorithms.  We evaluate this method on several synthetic and real-life datasets, and show that it performs surprisingly well. 
Unsupervised document classification using sequential information maximization| ABSTRACT We present a novel sequential clustering algorithm which is motivated by the Information Bottleneck (IB) method.  In contrast to the agglomerative IB algorithm, the new sequential (sIB) approach is guaranteed to converge to a local maximum of the information, as required by the original IB principle.  Moreover, the time and space complexity are significantly improved.  We apply this algorithm to unsupervised document classification.  In our evaluation, on small and medium size corpora, the sIB is found to be consistently superior to all the other clustering methods we examine, typically by a significant margin.  Moreover, the sIB results are comparable to those obtained by a supervised Naive Bayes classifier.  Finally, we propose a simple procedure for trading cluster's recall to gain higher precision, and show how this approach can extract clusters which match the existing topics of the corpus almost perfectly. 
Being Bayesian about Network Structure| Abstract In many domains, we are interested in analyzing the structure of the underlying
Model based Bayesian Exploration| Abstract Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good.  The benefit of exploration can be estimated using the classical notion of Value of Information --- the expected improvement in future decision quality arising from the information acquired by exploration.  Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states.  In this paper we investigate ways to represent and reason about this uncertainty in algorithms where the system attempts to learn a model of its environment.  We explicitly represent uncertainty about the parameters of the model and build probability distributions over Qvalues based on these.  These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. 
"Ideal Parent" Structure Learning for Continuous Variable Networks| Abstract In recent years, there is a growing interest in learning Bayesian networks
C5|1. 5 Bayesian Classification.  Abstract Bayesian classification addresses the classification problem by learning the distribution of instances given differentclassvalues.  We review the basic notion of Bayesian classification, describe in some detail the naive Bayesian classifier, and briefly discuss some extensions. 
Likelihood Computations Using Value Abstraction| Abstract In this paper, we use evidence-specific value abstraction for speeding Bayesian networks inference.  This is done by grouping variable values and treating the combined values as a single entity.  As we show, such abstractions can exploit regularities in conditional probability distributions and also the specific values of observed variables.  To formally justify value abstraction, we define the notion of safe value abstraction and devise inference algorithms that use it to reduce the cost of inference.  Our procedure is particularly useful for learning complex networks with many hidden variables.  In such cases, repeated likelihood computations are required for EM or other parameter optimization techniques.  Since these computations are repeated with respect to the same evidence set, our methods can provide significant speedup to the learning procedure.  We demonstrate the algorithm on genetic linkage problems where the use of value abstraction sometimes differentiates between a feasible and non-feasible solution. 
Overabundance Analysis and Class Discovery in Gene Expression Data \Lambda| Abstract Recent studies (Alizadeh et al.  2000, Bittner et al.  2000, Golub et al.  1999) demonstrate the discovery of disease subtypes from gene expression data.  In this paper, we propose a principled and systematic approach to address the computational problem of partitioning the set of sample tissues into statistically meaningful classes.  We start by describing a method,
On the Sample Complexity of Learning Bayesian Networks| Abstract In recent years there has been an increasing interest in learning Bayesian networks from data.  One of the most effective methods for learning such networks is based on the minimum description length (MDL) principle.  Previous work has shown that this learning procedure is asymptotically successful: with probability one, it will converge to the target distribution, given a sufficient number of samples.  However, the rate of this convergence has been hitherto unknown.  In this work we examine the sample complexity of MDL based learning procedures for Bayesian networks.  We show that the number of samples needed to learn an ffl-close approximation (in terms of entropy distance) with confidence ffi is O i ( 1 ffl ) 4 3 log 1 ffl log 1 ffi log log 1 ffi j .  This means that the sample complexity is a low-
Gaussian Process Networks| Abstract In this paper we address the problem of learning the structure of a Bayesian network in domains with continuous variables.  This task requires a procedure for comparing different candidate structures.  In the Bayesian framework, this is done by evaluating the marginal likelihood of the data given a candidate structure.  This term can be computed in closed-form for standard parametric families (e. g. , Gaussians), and can be approximated, at some computational cost, for some semi-parametric families (e. g. , mixtures of Gaussians).  We present a new family of continuous variable probabilistic networks that are based on Gaussian Process priors.  These priors are semiparametric in nature and can learn almost arbitrary noisy functional relations.  Using these priors, we can directly compute marginal likelihoods for structure learning.  The resulting method can discover a wide range of functional dependencies in multivariate data.  We develop the Bayesian score of Gaussian Process Networks and describe how to learn them from data.  We present empirical results on artificial data as well as on real-life domains with non-linear dependencies. 
Learning Belief Networks in the Presence of Missing Values and Hidden Variables| Abstract In recent years there has been a flurry of works on learning probabilistic belief networks.  Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values or hidden variables.  However, no method has yet been demonstrated to effectively learn network structure from incomplete data.  In this paper, we propose a new method for learning network structure from incomplete data.  This method is based on an extension of the Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure.  We prove the convergence of this algorithm, and adapt it for learning belief networks.  We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables.  We provide experimental results that show the effectiveness of our procedure in both scenarios. 
Discretizing Continuous Attributes While Learning Bayesian Networks| Abstract We introduce a method for learning Bayesian networks that handles the discretization of continuous variables as an integral part of the learning process.  The main ingredient in this method is a new metric based on the Minimal Description Length principle for choosing the threshold values for the discretization while learning the Bayesian network structure.  This score balances the complexity of the learned discretization and the learned network structure against how well they model the training data.  This ensures that the discretization of each variable introduces just enough intervals to capture its interaction with adjacent variables in the network.  We formally derive the new metric, study its main properties, and propose an iterative algorithm for learning a discretization policy.  Finally, we illustrate its behavior in applications to supervised learning. 
A Knowledge-Based Framework for Belief change, Part I: Foundations| Abstract We propose a general framework in which to study belief change.  We begin by defining belief in terms of knowledge and plausibility: an agent believes ' if he knows that ' is true in all the worlds he considers most plausible.  We then consider some properties defining the interaction between knowledge and plausibility, and show how these properties affect the properties of belief.  In particular, we show that by assuming two of the most natural properties, belief becomes a KD45 operator.  Finally, we add time to the picture.  This gives us a framework in which we can talk about knowledge, plausibility (and hence belief), and time, which extends the framework of Halpern and Fagin [HF89] for modeling knowledge in multi-agent systems.  We show that our framework is quite expressive and lets us model in a natural way a number of different scenarios for belief change.  For example, we show how we can capture an analogue to prior probabilities, which can be updated by "conditioning".  In a related paper, we show how the two best studied scenarios, belief revision and belief update, fit into the framework. 
Generalized Prioritized Sweeping| Abstract Prioritized sweeping is a model-based reinforcement learning method that attempt to focus the agent's limited computational resources to achieve a good estimate of the value of environment states.  The classic account of prioritized sweeping uses an explicit, state-based representation of the value, reward, and model parameters.  Such a representation is unwieldy for dealing with complex environments and there is growing interest in learning with more compact representations.  We claim that classic prioritized sweeping is ill-suited for learning with such representations.  To overcome this deficiency, we introduce generalized prioritized sweeping, a principled method for generating representation-specific algorithms for model-based reinforcement learning.  We then apply this method for several representations, including state-based models and generalized model approximators (such as Bayesian networks).  We describe preliminary experiments that compare our approach with classical prioritized sweeping. 
Image Segmentation in Video Sequences: A Probabilistic Approach|
Modeling Belief in Dynamic Systems, Part I: Foundations|
Being Bayesian About Network Structure| A Bayesian Approach to Structure Discovery in Bayesian Networks. 
On the Complexity of Conditional Logics|
Probabilistic models for identifying regulation networks|
Challenge: What is the Impact of Bayesian Networks on Learning?|
Mois#es Goldszmidt,|
Incorporating Expressive Graphical Models in VariationalApproximations: Chain-graphs and Hidden Variables|
Learning the Dimensionality of Hidden Variables|
Inferring quantitative models of regulatory networks from expression data|
Moises Goldszmidt| Bayesian network classifiers. 

Experiments in high-dimensional text categorization| ABSTRACT We present results for automated text categorization of the Reuters-810000 collection of news stories.  Our experiments use the entire one-year collection of 810,000 stories and the entire subject index.  We divide the data into monthly groups and provide an initial benchmark of text categorization performance on the complete collection.  Experimental results show that e$cient sparse-feature implementations of linear methods and decision trees, using a global unstemmed dictionary, can readily handle applications of this size.  Predictive performance is approximately as strong as the best results for the much smaller older Reuters collections.  Detailed results are provided over time periods.  It is shown that a smaller time horizon does not diminish predictive quality, implying reduced demands for retraining when sample size is large. 
Semi-Automatic Approach for Music Classification| classification Audio categorization is essential when managing a music database, either a professional library or a personal collection.  However, a complete automation in categorizing music into proper classes for browsing and searching is not yet supported by today's technology.  Also, the issue of music classification is subjective to some extent as each user may have his own criteria for categorizing music.  In this technical report, we propose the idea of semi-automatic music classification.  With this approach, a music browsing system is set up which contains a set of tools for separating music into a number of broad types (e. g.  male solo, female solo, string instruments performance, etc. ) using existing music analysis methods.  With results of the automatic process, the user may further cluster music pieces in the database into finer classes and/or adjust misclassifications manually according to his own preferences and definitions.  Such a system may greatly improve the efficiency of music browsing and retrieval, while at the same time guarantee accuracy and user's satisfaction of the results.  Since this semiautomatic system has two parts, i. e.  the automatic part and the manual part, they are described separately in the paper, with detailed descriptions and examples of each step of the two parts included. 
Convergence of Large Margin Separable Linear Classification| Abstract Large margin linear classification methods have been successfully applied to many applications.  For a linearly separable problem, it is known that under appropriate assumptions, the expected misclassification error of the computed "optimal hyperplane" approaches zero at a rate proportional to the inverse training sample size.  This rate is usually characterized by the margin and the maximum norm of the input data.  In this paper, we argue that another quantity, namely the robustness of the input data distribution, also plays an important role in characterizing the convergence behavior of expected misclassification error.  Based on this concept of robustness, we show that for a large margin separable linear classification problem, the expected misclassification error may converge exponentially in the number of training sample size. 
The Consistency of Greedy Algorithms for Classification| Abstract.  We consider a class of algorithms for classification, which are based on sequential greedy minimization of a convex upper bound on the 0 1 loss function.  A large class of recently popular algorithms falls within the scope of this approach, including many variants of Boosting algorithms.  The basic question addressed in this paper relates to the statistical consistency of such approaches.  We provide precise conditions which guarantee that sequential greedy procedures are consistent, and establish rates of convergence under the assumption that the Bayes decision boundary belongs to a certain class of smooth functions.  The results are established using a form of regularization which constrains the search space at each iteration of the algorithm.  In addition to providing general consistency results, we provide rates of convergence for smooth decision boundaries.  A particularly interesting conclusion of our work is that Logistic function based Boosting provides faster rates of convergence than Boosting based on the exponential function used in AdaBoost. 
From #-entropy to KL-complexity: analysis of minimum information complexity density estimation| Abstract We extend the concept of #-entropy to KL-complexity for randomized density estimation methods.  Based on this extension, we develop a general information theoretical inequality that measures the statistical complexity of some deterministic and randomized density estimators.  Consequences of the new inequality will be presented.  In particular, we show that this technique can effortlessly lead to substantial improvements of some classical results concerning the convergence of minimum description length (MDL) and Bayesian posterior distributions.  Moreover, we are able to derive clean finite-sample convergence bounds that are not obtainable using previous approaches. 
Audio Content Analysis for Online Audiovisual Data Segmentation and Classification| Abstract---While current approaches for audiovisual data segmentation and classification are mostly focused on visual cues, audio signals may actually play a more important role in content parsing for many applications.  An approach to automatic segmentation and classification of audiovisual data based on audio content analysis is proposed.  The audio signal from movies or TV programs is segmented and classified into basic types such as speech, music, song, environmental sound, speech with music background, environmental sound with music background, silence, etc.  Simple audio features including the energy function, the average zero-crossing rate, the fundamental frequency, and the spectral peak tracks are extracted to ensure the feasibility of real-time processing.  A heuristic rule-based procedure is proposed to segment and classify audio signals and built upon morphological and statistical analysis of the time-varying functions of these audio features.  Experimental results show that the proposed scheme achieves an accuracy rate of more than 90% in audio classification. 
Text Chunking based on a Generalization of Winnow| Abstract This paper describes a text chunking system based on a generalization of the Winnow algorithm.  We propose a general statistical model for text chunking which we then convert into a classification problem.  We argue that the Winnow family of algorithms is particularly suitable for solving classification problems arising from NLP applications, due to their robustness to irrelevant features.  However in theory, Winnow may not converge for linearly non-separable data.  To remedy this problem, we employ a generalization of the original Winnow method.  An additional advantage of the new algorithm is that it provides reliable confidence estimates for its classification predictions.  This property is required in our statistical modeling approach.  We show that our system achieves state of the art performance in text chunking with less computational cost then previous systems. 
Named Entity Recognition through Classifier Combination| Abstract This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.  When no gazetteer or other additional training resources are used, the combined system attains a performance of 91. 6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data. 
Inactivation and conformational changes of fatty acid synthase from chicken liver during unfolding by sodium dodecyl sulfate| Abstract Fatty acid synthase is an important enzyme participating in energy metabolism in vivo.  The inactivation and conformational changes of the multifunctional fatty acid synthase from chicken liver in SDS solutions have been studied.  The results show that the denaturation of this multifunctional enzyme by SDS occurred in three stages.  At low concentrations of SDS (less than 0. 15 mM) the enzyme was completely inactivated with regard to the overall reaction.  For each component of the enzyme, the loss of activity occurred at higher concentrations of SDS.  Signicant conformational changes (as indicated by the changes of the intrinsic uorescence emission and the ultraviolet dierence spectra) occurred at higher concentrations of SDS.  Increasing the SDS concentration caused only slight changes of the CD spectra, indicating that SDS had no signicant eect on the secondary structure of the enzyme.  The results suggest that the active sites of the multifunctional fatty acid synthase display more conformational exibility than the enzyme molecule as a whole. 
Surface Tortuosity and its Application to Analyzing Cracks in Concrete| Abstract Previous studies of tortuosity were restricted to a curve in 2D or 3D.  We propose several measures of surface tortuosity based on surface normals and principal curvatures, and apply these measures to analyze the tortuosity of threedimensional crack surfaces of concrete.  We show that all crack surfaces are similar in tortuosity despite their different sizes and locations, while distinctive from various geometric surfaces. 
Generating Panorama Photos| ABSTRACT Photo or video mosaicing have drawn a lot of interests in the research field in the past years.  Most of the existing work, however, focuses on how to match the images or video frames.  This paper presents techniques to handle some practical issues when generating panorama photos.  We have found from the experiments that a simple translational motion model gives more robust results than affine model for horizontally panned image sequences.  Realizing the fact that there would always be some misalignments between two images no matter how well the matching is done, we propose a stitching method that finds a line of best agreement between two images, to make the misalignments less visible.  Also shown in this paper are methods on how to correct camera exposure changes and how to blend the stitching line between the images.  We will show panorama photos generated from both still images and video. 
Statistical Analysis of Some Multi-Category Large Margin Classification Methods| Abstract The purpose of this paper is to investigate statistical properties of risk minimization based multicategory classification methods.  These methods can be considered as natural extensions of binary
Learning Bounds for Kernel Regression using Effective Data Dimensionality| Abstract Kernel methods can embed finite dimensional data into infinite dimensional feature spaces.  In spite of
Data-Dependent Bounds for Bayesian Mixture Methods| Abstract We consider Bayesian mixture approaches, where a predictor is constructed by forming a weighted average of hypotheses from some space of functions.  While such procedures are known to lead to optimal predictors in several cases, where suciently accurate prior information is available, it has not been clear how they perform when some of the prior assumptions are violated.  In this paper we establish data-dependent bounds for such procedures, extending previous randomized approaches such as the Gibbs algorithm to a fully Bayesian setting.  The finite-sample guarantees established in this work enable the utilization of Bayesian mixture approaches in agnostic settings, where the usual assumptions of the Bayesian paradigm fail to hold.  Moreover, the bounds derived can be directly applied to non-Bayesian mixture approaches such as Bagging and Boosting. 
Updating an NLP System to Fit New Domains: an empirical study on the sentence segmentation problem| Abstract Statistical machine learning algorithms have been successfully applied to many natural language processing (NLP) problems.  Compared to manually constructed systems, statistical NLP systems are often easier to develop and maintain since only annotated training text is required.  From annotated data, the underlying statistical algorithm can build a model so that annotations for future data can be predicted.  However, the performance of a statistical system can also depend heavily on the characteristics of the training data.  If we apply such a system to text with characteristics different from that of the training data, then performance degradation will occur.  In this paper, we examine this issue empirically using the sentence boundary detection problem.  We propose and compare several methods that can be used to update a statistical NLP system when moving to a different domain. 
On the Dual Formulation of Regularized Linear Systems with Convex Risks| Abstract In this paper, we study a general formulation of linear prediction algorithms including a number of known methods as special cases.  We describe a convex duality for this class of methods and propose numerical algorithms to solve the derived dual learning problem.  We show that the dual formulation is closely related to online learning algorithms.  Furthermore, by using this duality, we show that new learning methods can be obtained.  Numerical examples will be given to illustrate various aspects of the newly proposed algorithms. 
Instrument Classification in Polyphonic Music Based on Timbre Analysis| ABSTRACT While most previous work on musical instrument recognition is focused on the classification of single notes in monophonic music, a scheme is proposed in this paper for the distinction of instruments in continuous music pieces whichmaycontain one or more kinds of instruments.  Highlights of the system include music segmentation into notes, harmonic partial estimation in polyphonic sound, note feature calculation and normalization, note classification using a set of neural networks, and music piece categorization with fuzzy logic principles.  Example outputs of the system are \the music piece is 100% guitar (with 90% likelihood)" and \the music piece is 60% violin and 40% piano, thus a violin/piano duet".  The system has been tested with twelve kinds of musical instruments, and very promising experimental results have been obtained.  An accuracy of about 80% is achieved, and the number can be raised to 90% if misindexings within the same instrument family are tolerated (e. g.  cello, viola and violin).  A demonstration system for musical instrument classification and music timbre retrieval is also presented. 
Active learning using adaptive resampling| ABSTRACT Classification modeling (a. k. a.  supervised learning) is an extremely useful analytical technique for developing predictive and forecasting applications.  The explosive growth in data warehousing and internet usage has made large amounts of data potentially available for developing classification models.  For example, natural language text is widely available in many forms (e. g. , electronic mail, news articles, reports, and web page contents).  Categorization of data is a common activity which can be automated to a large extent using supervised learning methods.  Examples of this include routing of electronic mail, satellite image classification, and character recognition.  However, these tasks require labeled data sets of sufficiently high quality with adequate instances for training the predictive models.  Much of the on-line data, particularly the unstructured variety (e. g. , text), is unlabeled.  Labeling is usually a expensive manual process done by domain experts.  Active learning is an approach to solving this problem and works byidentifying a subset of the data that needs to be labeled and uses this subset to generate classification models.  We present an active learning method that uses adaptive resampling in a natural wayto significantly reduce the size of the required labeled set and generates a classification model that achieves the high accuracies possible with current adaptive resampling methods. 
Regularized Winnow Methods| Abstract In theory, the Winnow multiplicative update has certain advantages over the Perceptron additive update when there are many irrelevant attributes.  Recently, there has been much effort on enhancing the Perceptron algorithm by using regularization, leading to a class of linear classification methods called support vector machines.  Similarly, it is also possible to apply the regularization idea to the Winnow algorithm, which gives methods we call regularized Winnows.  We show that the resulting methods compare with the basic Winnows in a similar way that a support vector machine compares with the Perceptron.  We investigate algorithmic issues and learning properties of the derived methods.  Some experimental results will also be provided to illustrate different methods. 
Approximation Bounds for Some Sparse Kernel Regression Algorithms| Abstract Gaussian processes have been widely applied to regression problems with good performance.  However, they can be computationally expensive.  In order to reduce the computational cost, there have been recent studies on using sparse approximations in Gaussian processes.  In this paper, we investigate properties of certain sparse regression algorithms that approximately solve a Gaussian process.  We obtain approximation bounds, and compare our results with related methods. 
An Experimental Evaluation of Data Flow and Mutation Testing| SUMMARY This paper presents two experimental comparisons of data flow and mutation testing.  These two techniques are widely considered to be effective for unit-level software testing, but can only be analytically compared to a limited extent.  We compare the techniques by evaluating the effectiveness of test data developed for each.  For a number of programs, we develop ten independent sets of test data; five to satisfy the mutation criterion, and five to satisfy the all-uses data flow criterion.  These test sets are developed using automated tools, in a manner consistent with the way a test engineer might be expected to generate test data in practice.  We use these test sets in two separate experiments.  First we measure the effectiveness of the test data that was developed for one technique in terms of the other technique.  Second, we investigate the ability of the test sets to find faults.  We place a number of faults into each of our subject programs, and measure the number of faults that are detected by the test sets.  Our results indicate that while both techniques are effective, mutation-adequate test sets are closer to satisfying the data flow criterion, and detect more faults. 
A Probability Analysis on the Value of Unlabeled Data for Classification Problems| Abstract Recently, there has been increasing interest in using unlabeled data for classification.  In order to understand better the value of using unlabeled data under certain learning models, it is worthwhile to formulate the problem precisely, and to analyze relevant issues carefully.  In this paper, we approach this problem from the statistical point of view, where we know a correct model of the underlying distribution.  Fisher information matrices are used to judge the asymptotic value of unlabeled data.  We apply this methodology to both "passive partially supervised learning" and "active learning", and draw conclusions.  Experiments will also be provided. 
On the Consistency of Instantaneous Rigid Motion Estimation| Abstract Instantaneous camera motion estimation is an important research topic in computer vision.  Although in theory more than five points uniquely determine the solution in an ideal situation, in practice one can usually obtain better estimates by using more image velocity measurements because of the noise present in the velocity measurements.  However, the usefulness of using a large number of observations has never been analyzed in detail.  In this paper, we formulate this problem in the statistical estimation framework.  We show that under certain noise models, consistency of motion estimation can be established: that is, arbitrarily accurate estimates of motion parameters are possible with more and more observations.  This claim does not simply follow from the the general consistency result for maximum likelihood estimates.  Some experiments will be provided to verify our theory.  Our analysis and experiments also indicate that many previously proposed algorithms are inconsistent under even very simple noise models. 
Text Categorization Based on Regularized Linear Classification Methods| Abstract A number of linear classification methods such as the linear least squares fit (LLSF), logistic regression, and support vector machines (SVM's) have been applied to text categorization problems.  These methods share the similarity by finding hyperplanes that approximately separate a class of document vectors from its complement.  However, support vector machines are so far considered special in that they have been demonstrated to achieve the state of the art performance.  It is therefore worthwhile to understand whether such good performance is unique to the SVM design, or if it can also be achieved by other linear classification methods.  In this paper, we compare a number of known linear classification methods as well as some variants in the framework of regularized linear systems.  We will discuss the statistical and numerical properties of these algorithms, with a focus on text categorization.  We will also provide some numerical experiments to illustrate these algorithms on a number of datasets.  1 Background The text categorization problem is to determine predefined categories for an incoming unlabeled message or document containing text based on information extracted from a training set of labeled messages or documents.  Text categorization is an important practical problem for companies that wish to use computers to categorize incoming email, thereby either enabling an automatic machine response to the email or simply assuring that the email reaches the correct human recipient.  But, beyond email, text items to be categorized may come from many sources, including the output of voice recognition software, collections of documents (e. g. , news stories, patents, or case summaries), and the contents of web pages.  The text categorization problem can be reduced to a set of binary classification problems { one for each category { where for each one wishes to determine a method for predicting inclass versus out-of-class membership.  Such supervised learning problems have been widely studied in the past.  Recently, many methods developed for classification problems have been applied to text categorization.  For example, Apte, Damerau, and Weiss [1] applied an inductive rule learning algorithm, SWAP1, to the text categorization problem.  In [25], Yang and Chute proposed a linear least squares fit algorithm to train linear classifiers.  Yang also compared a number of statistical methods for text categorization in [24].  The best performances previously reported in the literature are from weighted resampled decision trees in [23] and (linear) support vector machines in [12, 4].  Integral parts of all these approaches are tokenization, feature selection, and creating numeric vector representations of documents.  The first step, tokenization, is laid out in detail in Figure 1.  This functionality is common to most methods of text categorization.  In the tokenization procedure depicted in Figure 1, one or both of Steps 4 and 5 may be omitted, although keeping them may improve performance.  If both steps are retained, elimination of stopwords (Step 5) may also be done before stemming (Step 4).  Also, the elimination of stopwords (Step 5) may in some instances be subsumed by subsequent feature selection, to be discussed below.  For consistency, the same tokenization procedure would be used both (1) for documents used in training to build categorization rules and (2) for incoming documents to be categorized by a system employing the classifiers obtained in the training phase.  After tokenization, each document is represented by a list of word occurrences.  A program should then be used to carry out feature selection.  (However, feature selection could also be skipped entirely, so that tokens were to be taken by themselves to be the sole features of interest. ) We will not take up the specifics of feature selection, but a number of methods of varying degrees of sophistication have been studied in [27].  Feature selection might be done only once for the entire data set, but experience indicates that better results will be obtained by doing feature selection separately for each category, reflecting the intuition that features indicative of category membership will differ as one moves from category to category.  Feature selection, under the assumption that a separate set of features is to be selected for each category, is laid out in Figure 2.  The output of feature selection would normally be a specification of a separate list of selected features (words) for each category for which we intend to generate a classifier.  The specification would necessarily be detailed enough to permit a computer to identify each occurrence of each feature in the tokenized representation of a document.  After feature selection, each document is represented by a vector of word occurrences for each category where each vector component corresponds to a word feature selected for the category in the previous step.  Figure 3 shows the steps necessary to use a list of features selected for relevance to a particular category to convert a tokenized representation of a document to a numeric vector representing the document.  Because of the vast numbers of different words that may appear in text, generally the numerical vectors of world occurrences one gets are sparse vectors of very high dimensionality.  Thus, text categorization necessitates
Mining Home Video for Photos| More and more home videos have been generated with the ever growing popularity of digital cameras and camcorders.  In many cases of home video, a photo, whether capturing a moment or a scene within the video, provides a complementary representation to the video.  In this paper, a complete solution of video to photo is presented.  The intent of the user is first derived by analyzing video motions.  Then, photos are produced accordingly from the video.  They can be keyframes at video highlights, panorama of the scene, or high-resolution frames.  Methods and results of camera motion mining, intelligent keyframe extraction, video frame stitching and super-resolution enhancement are described. 
Support Vector Classification with Input Data Uncertainty| Abstract This paper investigates a new learning model in which the input data is corrupted with noise.  We present a general statistical framework to tackle this problem.  Based on the statistical reasoning, we propose a novel formulation of support vector classification, which allows uncertainty in input data.  We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efficiently solve it.  Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 
Sequential greedy approximation for certain convex optimization problems| Abstract| A greedy algorithm for a class of convex optimization problems is presented in this paper.  The algorithm is motivated from function approximation using sparse combination of basis functions as well as some of its variants.  We derive a bound on the rate of approximate minimization for this algorithm, and present examples of its application.  Our analysis generalizes a number of earlier studies. 
Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification Methods| Abstract We consider the problem of deriving class-size independent generalization bounds for some regularized discriminative multi-category classification methods.  In particular, we obtain an expected generalization bound for a standard formulation of multi-category support vector machines.  Based on the theoretical result, we argue that the formulation over-penalizes misclassification error, which in theory may lead to poor generalization performance.  A remedy, based on a generalization of multi-category logistic regression (conditional maximum entropy), is then proposed, and its theoretical properties are examined. 
Solving large scale linear prediction problems using stochastic gradient descent algorithms| Abstract Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning.  In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods.  This class of methods, related to online algorithms such as perceptron, are both ecient and very simple to implement.  We obtain numerical rate of convergence for such algorithms, and discuss its implications.  Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings. 
Proceedings of the 43rd Annual Meeting of the ACL,| Abstract In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT).  The model predicts blocks with orientation to handle local phrase re-ordering.  We use a maximum likelihood criterion to train a log-linear block bigram model which uses realvalued features (e. g.  a language model score) as well as binary features based on the block identities themselves, e. g.  block bigram features.  Our training algorithm can easily handle millions of features.  The best system obtains a ##### # % improvement over the baseline on a standard Arabic-English translation task. 
Text categorization for a comprehensive time-dependent benchmark| Abstract We present results for automated text categorization of the Reuters-810000 collection of news stories.  Our experiments use the entire one-year collection of 810,000 stories and the entire subject index.  We divide the data into monthly groups and provide an initial benchmark of text categorization performance on the complete collection.  Experimental results show that ecient sparse-feature implementations of linear methods and decision trees, using a global unstemmed dictionary, can readily handle applications of this size.  Predictive performance is approximately as strong as the best results for the much smaller older Reuters collections.  Detailed results are provided over time periods.  It is shown that a smaller time horizon does not appreciably diminish predictive quality, implying reduced demands for retraining when sample size is large. 
Text Chunking using Regularized Winnow| Abstract Many machine learning methods have recently been applied to natural language processing tasks.  Among them, the Winnow algorithm has been argued to be particularly suitable for NLP problems, due to its robustness to irrelevant features.  However in theory, Winnow may not converge for nonseparable data.  To remedy this problem, a modification called regularized Winnow has been proposed.  In this paper, we apply this new method to text chunking.  We show that this method achieves state of the art performance with significantly less computation than previous approaches. 
MENTAL STATE DETECTION OF DIALOGUE SYSTEM USERS VIA SPOKEN LANGUAGE| ABSTRACT This paper presents an approach to simulate the mental activities of children during their interaction with computers through their spoken language.  The mental activities are categorized into three states: confidence, confusion and frustration.  Two knowledge sources are used in the detection.  One is prosody, which indicates utterance type and user's attitude.  The other is embedded key words/phrases which help interpret the utterances.  Moreover, it is found that children's speech exhibits very different acoustic characteristics from adults.  Given the uniqueness of children's speech, this paper applies a vocal-tract-length-normalization (VTLN)-based technique to compensate for both inter-speaker variability and intraspeaker variability in children's speech.  The detected key words/phrases are then integrated with prosodic information as the cues for the MAP decision of mental states.  Tests on a set of 50 utterances collected from the project experiment showed the classification accuracy was 74%. 
Boosting with Early Stopping: Convergence and Consistency| Abstract Boosting is one of the most significant advances in machine learning for classification and regression.  In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion.  The resulted estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations.  An unusual regularization technique, early stopping, is employed based on CV or a test set.  This paper studies numerical convergence, consistency, and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions.  For general loss functions, we prove the convergence of boosting's greedy optimization to the infinimum of the loss function over the linear span.  Using the numerical convergence result, we find early stopping strategies under which boosting is shown to be consistent based on iid samples, and we obtain bounds on the rates of convergence for boosting estimators.  Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting.  As a side product, these results also reveal the importance of restricting the greedy search step sizes, as known in practice through the works of Friedman and others.  Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with # ! 0 stepsize becomes an L 1 -margin maximizer when left to run to convergence. 
Statistical Behavior and Consistency of Classification Methods based on Convex Risk Minimization| Abstract We study how close the optimal Bayes error rate can be approximately reached using a classification algorithm that computes a classifier by minimizing a convex upper bound of the classification error function.  The measurement of closeness is characterized by the loss function used in the estimation.  We show that such a classification scheme can be generally regarded as a (non maximum-likelihood) conditional in-class probability estimate, and we use this analysis to compare various convex loss functions appeared in the literature.  Furthermore, the theoretical insight allows us to design good loss functions with desirable properties.  Another aspect of our analysis is to demonstrate the consistency of certain classification methods using convex risk minimization.  This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines.  It also shows their limitations and suggests possible improvements. 
A decision-tree-based symbolic rule induction system for text categorization| Abstract We present a decision-tree-based symbolic rule induction system whose purpose is to categorize text documents automatically.  Our method for rule induction involves the novel combination of (1) a fast decision tree induction algorithm especially suited to text data and (2) a new method for converting a decision tree to a rule set that is simplified, but still logically equivalent to, the original tree.  We report experimental results on the use of this system on some practical problems. 
A Robust Risk Minimization based Named Entity Recognition System| Abstract This paper describes a robust linear classification system for Named Entity Recognition.  A similar system has been applied to the CONLL text chunking shared task with state of the art performance.  By using different linguistic features, we can easily adapt this system to other token-based linguistic tagging problems.  The main focus of the current paper is to investigate the impact of various local linguistic features for named entity recognition on the CONLL2003 (Sang and Meulder, 2003) shared task data.  We show that the system performance can be enhanced significantly with some relative simple token-based features that are available for many languages.  Although more sophisticated linguistic features will also be helpful, they provide much less improvement than might be expected. 
An Infinity-sample Theory for Multi-category Large Margin Classification| Abstract The purpose of this paper is to investigate infinity-sample properties of risk minimization based multi-category classification methods.  These methods can be considered as natural extensions to binary large margin classification.  We establish conditions that guarantee the infinity-sample consistency of classifiers obtained in the risk minimization framework.  Examples are provided for two specific forms of the general formulation, which extend a number of known methods.  Using these examples, we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem.  Such conditional probability information will be useful for statistical inferencing tasks beyond classification.  1 Motivation Consider a binary classification problem where we want to predict label y 2 {1} based on observation x.  One of the most significant achievements for binary classification in machine learning is the invention of large margin methods, which include support vector machines and boosting algorithms.  Based on a set of observations (X 1 , Y
A Multistage, Multimethod Approach for Automatic Detection and Classification of Epileptiform EEG| Abstract---An efficient system for detection of epileptic activity in ambulatory electroencephalogram (EEG) must be sensitive to abnormalities while keeping the false-detection rate to a low level.  Such requirements could be fulfilled neither by single stage nor by simple method strategy, due to the extreme variety of EEG morphologies and frequency of artifacts.  The present study proposes a robust system that combines multiple signal-processing methods in a multistage scheme, integrating adaptive filtering, wavelet transform, artificial neural network, and expert system.  The system consists of two main stages: a preliminary screening stage in which data are reduced significantly, followed by an analytical stage.  Unlike most systems that merely focus on sharp transients, our system also takes into account slow waves.  A nonlinear filter for separation of nonstationary and stationary EEG components is also developed in this paper.  The system was evaluated on testing data from 81 patients, totaling more than 800 hours of recordings.  90. 0% of the epileptic events were correctly detected.  The detection rate of sharp transients was 98. 0% and overall false-detection rate was 6. 1%.  We conclude that our system has good performance in detecting epileptiform activities and the multistage multimethod approach is an appropriate way of solving this problem. 
Learning Bounds for a Generalized Family of Bayesian Posterior Distributions| Abstract In this paper we obtain convergence bounds for the concentration of Bayesian posterior distributions (around the true distribution) using a novel method that simplifies and enhances previous results.  Based on the analysis, we also introduce a generalized family of Bayesian posteriors, and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution.  This important and surprising robustness property does not hold for the standard Bayesian posterior in that it may not concentrate when there exist "bad" prior structures even at places far away from the true distribution. 
Real Time Motion Analysis Toward Semantic Understanding of Video Content| ABSTRACT Video motion analysis and its applications have been a classic research topic for decades.  In this paper, we explore the problem of real time video semantics understanding based on motion information.  The work can be divided into two segments: global / camera motion estimation and object motion analysis.  The former involves optical flow analysis and semantic meaning parsing, and the latter involves object detection and tracking.  Although each of these topics has been studied extensively in the literature, a thorough system combining all of them without human intervention, especially under a real time application scenario, is still worthy of further investigation.  In this paper we develop our approach toward such a destination and propose an integral architecture.  The usability and efficiency of the proposed system have been demonstrated through experiments.  Results of this project have numerous applications in digital entertainment, such as video and image summarization, annotation, retrieval and editing. 
HowtogetaChineseName(Entity): Segmentation and Combination Issues| Abstract When building a Chinese named entity recognition system, one must deal with certain language-specific issues such as whether the model should be based on characters or words.  While there is no unique answer to this question, we discuss in detail advantages and disadvantages of each model, identify problems in segmentation and suggest possible solutions, presenting our observations, analysis, and experimental results.  The second topic of this paper is classifier combination.  We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs.  The results demonstrate that classifier combination is an effective technique of improving system performance: experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error. 
Analysis of Spectral Kernel Design based Semi-supervised Learning| Abstract We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design.  This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs.  We examine various theoretical properties of such methods.  In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound.  Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance.  Experiments are used to illustrate the main consequences of our analysis. 
The Value of Unlabeled Data for Classification Problems| Abstract Recently, there has been increasing interest in using unlabeled data for classification.  However, whether these unlabeled data are truly useful is still under debate.  In order to have a better understanding of relevant issues, it is worthwhile to precisely formulate the problem and carefully analyze the value of unlabeled data under certain learning models.  In this paper, we approach this problem from the statistical point of view, where we assume that a correct model of the underlying distribution is given.  We demonstrate that Fisher information matrices can be used to judge the asymptotic value of unlabeled data.  We apply this methodology to both "passive partially supervised learning" and "active learning", and draw conclusions from this analysis.  Experiments will be provided to support our claims. 
Greedy Algorithms for Classification -- Consistency, Convergence Rates, and Adaptivity| Abstract Many regression and classification algorithms proposed over the years can be described as greedy procedures for the stagewise minimization of an appropriate cost function.  Some examples include additive models, matching pursuit, and boosting.  In this work we focus on the classification
An Experimental Evaluation of Data Flow and Mutation Testing|
Heuristic approach for generic audio data segmentation and annotation|
Generalization Error Bounds for Bayesian Mixture Algorithms|
Optimal Surface Smoothing as Filter Design|
Named entity recogintion through classifier combination|
Volume and Surface Area Distributions of Cracks in Concrete|
The Rieman problem for the transportaion equations in gas dynamics,|
Active learning using adaptive aesampling,|
Leave-One-Out Bounds for Kernel Methods|
Recommender Systems Using Linear Classifier|
On the Convergence of MDL Density Estimation|
Joint code and decoder design for implementation-oriented (3, k)-regular LDPC codes,|
\Analysis of Certain Regularized Linear Function Classes with Special Emphasis on Classification",|
Fast, Robust, and Consistent Camera Motion Estimation|
"Heirarchal Classifcation of Audio Data For Archiving and Retrieving" IEEE database|
Theoretical Analysis of a Class of Randomized Regularization Methods|
Analysis of regularized linear functions for classification problems|
Existence of a global smooth solution for a degenerate Goursat problem of gas dynamics,|
Some Sparse Approximation Bounds for Regression Problems|
A Sequential Approximation Bound for Some Sample-Dependent Convex Optimization Problems with Applications in Learning|

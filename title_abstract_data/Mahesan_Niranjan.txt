Flexible Independent Component Analysis| Abstract.  This paper addresses an independent component analysis (ICA) learning algorithm with flexible nonlinearity, so named as flexible ICA, that is able to separate instantaneous mixtures of suband super-Gaussian source signals.  In the framework of natural Riemannian gradient, we employ the parameterized generalized Gaussian density model for hypothesized source distributions.  The nonlinear function in the flexible ICA algorithm is controlled by the Gaussian exponent according to the estimated kurtosis of demixing filter output.  Computer simulation results and performance comparison with existing methods are presented. 
The Application of Bayesian Inference to Linear Prediction of Speech G| Abstract The analysis of a speech segment is conventionally performed through linear prediction and the subsequent minimisation of a data error term in the least squares sense.  The parameters derived as such maximise the likelihood of the data.  In a learning problem, the addition of penalty terms, or regularisers, to the data term facilitates the estimation of the Maximum a Posteriori , or MAP, parameters.  A direct equivalence can be drawn between the type of regulariser used and the prior assumptions regarding the solution.  The Bayesian evidence procedure provides a framework for MAP parameter estimation and model order selection.  In this paper, the use of suitable quadratic regularisers for the determination of linear prediction MAP parameters is addressed.  The application of continuity constraints across successive speech segments will be demonstrated to enhance the tracking of formants for speech embedded in gaussian noise.  The use of variable order models for speech analysis-synthesis is also addressed and its apparent benefits discussed. 
Sequential Monte Carlo Methods to Train Neural Network Models| We discuss a novel strategy for training neural networks using sequential Monte Carlo algorithms and propose a new hybrid gradient descent/ sampling importance resampling algorithm (HySIR).  In terms of computational time and accuracy, the hybrid SIR is a clear improvement over conventional sequential Monte Carlo techniques.  The new algorithm may be viewed as a global optimization strategy that allows us to learn the probability distributions of the network weights and outputs in a sequential framework.  It is well suited to applications involving on-line, nonlinear, and nongaussian signal processing.  We show how the new algorithm outperforms extended Kalman filter training on several problems.  In particular, we address the problem of pricing option contracts, traded in financial markets.  In this context, we are able to estimate the one-step-ahead probability density functions of the options prices. 
A Systolic Array Implementation of a Dynamic Sequential Neural Network for Pattern Recognition| Recently we have developed a sequential algorithm for designing a multi-layer perceptron classifier [1, 2].  Our approach, called Sequential Input Space Partitioning (SISP) algorithm, results in a one pass algorithm and a growing network.  We exploit the fact that class boundary constructed by an MLP classifier is piecewise linear and hence the contribution of each hidden hidden unit to the final decision is essentially local .  We have shown that, in a number of benchmark classification problems, the algorithm achieves performances similar to conventional batch methods of training.  We have also argued that the sequential design has an indirect computational advantage.  This computational advantage comes from the fact that the algorithm sees each data item only once, hence the feasibility of pipelining the training procedures in a true parallel architecture.  In this paper, we show how this one pass algorithm can be pipelined and realised by a systolic array implementation.  The idea is to exploit the fact that the locations of boundary segments are determined by solving localised classification problems.  Training is achieved by updating local covariances using the Recursive Least Squares (RLS) algorithm.  The algorithm is sequential in the sense that training examples are passed only once, and the network will learn and/or expand at the arrival of each example.  The major advantage in this sequential scheme is the feasibility of pipelining the training procedures in a true parallel architecture.  In this paper, we present a systolic array implementation of the SISP algorithm. 
Maximum realisable performance: a principled method for enhancing performance by using multiple classifiers| Abstract A novel method is described for obtaining superior classification performance over a variable range of classification costs.  By analysis of a set of existing classifiers using a receiver operating characteristic (ROC) curve, a set of new realisable classifiers may be obtained by a principled random combination of two of the existing classifiers.  These classifiers lie on the convex hull that contains the original ROC points for the existing classifiers.  This hull is the maximum realisable ROC (MRROC).  A theorem for this method is derived and proved from an observation about ROC data, and experimental results verify that a superior classification system may be constructed using only the existing classifiers and the information of the original ROC data.  This new system is shown to produce the MRROC, and as such provides a powerful technique for improving classification systems in problem domains within which classification costs may not be known a priori [ Lovell et al. , 1997b, Lovell et al. , 1997a ] . 
Realisable Classifiers: Improving Operating Performance on Variable Cost Problems| Abstract A novel method is described for obtaining superior classification performance over a variable range of classification costs.  By analysis of a set of existing classifiers using a receiver operating characteristic (ROC)curve,a set of new realisable classifiers may be obtained by a random combination of two of the existing classifiers.  These classifiers lie on the convex hull that contains the original ROC points for the existing classifiers.  This hull is the maximum realisable ROC (MRROC).  A theorem for this method is derived and proved from an observation about ROC data, and experimental results verify that a superior classification system may be constructed using only the existing classifiers and the information of the original ROC data.  This new system is shown to produce the MRROC, and as such provides a powerful technique for improving classification systems in problem domains within which classification costs may not be known apriori.  Empirical results are presented for artificial data, and for two real world data sets: an image segmentation task and the diagnosis of abnormal thyroid condition. 
THE USE OF RECURRENT NEURAL NETWORKS FOR CLASSIFICATION| Abstract--Recurrent neural networks are widely used for context dependent pattern classification tasks such as speech recognition.  The feedback in these networks is generally claimed to contribute to integrating the context of the input feature vector to be classified.  This paper analyses the use of recurrent neural networks for such applications.  We show that the contribution of the feedback connections is primarily a smoothing mechanism and that this is achieved by moving the class boundary of an equivalent feedforward network classifier.  We also show that when the sigmoidal hidden nodes of the network operate close to saturation, switching from one class to the next is delayed, and within a class the network decisions are insensitive to the order of presentation of the input vectors. 
NON-LINEAR SPEECH TRANSITION VISUALIZATION| ABSTRACT Modelling context effects and segmental transitions in speech recognition systems is very important.  Explicitly modelling segmental transitions in a RNN framework would circumvent these problems.  We present an interesting application of Principal Curves, an algorithm to extract a non-linear summary of p-dimensional data firstly published in 1989 by Hastie/Stuetzle.  The algorithm can be used to visualize non-linear transient characteristics in speech.  We will show that between-phone characteristics found within diphones can be used as discriminant information to distinguish ambiguous phones.  The technique used is explained and illustrated on the examples /bah/, /dah/ and /gah/. 
Expected Attainable Discrimination for feature selection in large scale medical risk prediction problems| Abstract This report investigates the use of expected attainable discrimination (EAD) as a measure to select discrete valued features in two-class prediction problems.  In essence, EAD tells us the performance we could expect to achieve with a simple histogram probability density model of a given dataset.  For discrete valued features, this kind of density model is bias-free but can have large variance.  Given insufficient training data, such a model's test set performance will be lower than that of a suitably biased model.  In light of this, we explore the usefulness of EAD for feature selection. 
IEEE Transactions on Speech and Audio Processing Abstract| Recent studies have shown that nonlinear predictors can achieve about 2 3 improvement in speech prediction over conventional linear predictors.  In this paper, we take advantage of the nonlinear prediction capability of neural networks and apply it to the design of improved predictive speech coders.  Our studies concentrate on the following three aspects: (a) the development of short-term (formant) and long-term (pitch) nonlinear predictive vector quantisers, (b) the analysis of the output variance of the nonlinear predictive filter to an input disturbance, and (c) the design of nonlinear predictive speech coders.  The above studies have resulted in a fully vector-quantised, code-excited, nonlinear predictive speech coder.  Performance evaluations and comparisons with linear predictive speech coding are presented.  These tests have shown the applicability of nonlinear prediction to speech coding and the improvement in coding performance. 
Reducing the variability in cDNA microarray image processing by Bayesian inference| Abstract Motivation: Gene expression levels are obtained from microarray experiments through the extraction of pixel intensities from a scanned image of the slide.  It is widely acknowledged that variabilities can occur in expression levels extracted from the same images by different users with the same software packages.  These inconsistencies arise due to differences in the refinement of the placement of the microarray `grids'.  We introduce a novel automated approach to the refinement of grid placements that is based upon the use of Bayesian inference for determining the size, shape and positioning of the microarray `spots', capturing uncertainty that can be passed to downstream analysis.  Results: Our experiments demonstrate that variability between users can be significantly reduced using the approach.  The automated nature of the approach also saves hours of researchers' time normally spent in refining the grid placement.  Availability: A MATLAB implementation of the algorithm and tiff images of the slides used in our experiments, as well as the code necessary to recreate them are available for noncommercial use from
DYNAMICAL LEARNING WITH THE EM ALGORITHM FOR NEURAL NETWORKS| Abstract In this paper, we derive an EM algorithm for nonlinear state space models.  We use it to estimate jointly the neural network weights, the model uncertainty and the noise in the data.  In the E-step we apply a forward-backward Rauch-Tung-Striebel smoother to compute the network weights.  For the M-step, we derive expressions to compute the model uncertainty and the measurement noise.  We find that the method is intrinsically very powerful, simple and stable. 
Average-Case Learning Curves for Radial Basis Function Networks| Abstract The application of statistical physics to the study of the learning curves of feedforward connectionist networks has, to date, been concerned mostly with networks that do not include hidden layers.  Recent work has extended the theory to networks such as committee machines and parity machines; however these are not networks that are often used in practice and an important direction for current and future research is the extension of the theory to practical connectionist networks.  In this paper we investigate the learning curves of a class of networks that has been widely, and successfully applied to practical problems: the Gaussian radial basis function networks (RBFNs).  We address the problem of learning linear and nonlinear, realizable and unrealizable, target rules from noise-free training examples using a stochastic training algorithm.  Expressions for the generalization error, defined as the expected error for a network with a given set of parameters, are derived for general Gaussian RBFNs, for which all parameters, including centres and spread parameters, are adaptable.  Specializing to the case of RBFNs with fixed basis functions we then study the learning curves for these networks in the limit of high temperature. 
The Use of Feed-forward and Recurrent Neural Networks for System Identification|
VOCAL TRACT MODELLING WITH RECURRENT NEURAL NETWORKS| ABSTRACT In this paper, the speech production system is modelled using the true glottal excitation as the source and a recurrent neural network to represent the vocal tract.  The hidden nodes have multiple delays of one and two samples, making the network equivalent to a parallel formant synthesiser in the linear regions of the hidden node sigmoids.  An ARX model identification is carried out to initialise the neural network parameters.  These parameters are re-estimated in an analysis-bysynthesis framework to minimise the synthesis (output) error.  Unlike other analysis-by-synthesis speech production models such as CELP, the source and filter in this approach are decoupled, enabling manipulation of the source time-scale to achieve high quality pitch changes. 
Extractive Summarization of Voicemail using Lexical and Prosodic Feature Subset Selection| Abstract This paper presents a novel data-driven approach to summarizing spoken audio transcripts utilizing lexical and prosodic features.  The former are obtained from a speech recognizer and the latter are extracted automatically from speech waveforms.  We employ a feature subset selection algorithm, based on ROC curves, which examines different combinations of features at different target operating conditions.  The approach is evaluated on the IBM Voicemail corpus, demonstrating that it is possible and desirable to avoid complete commitment to a single best classifier or feature set. 
DIPHONE MULTI-TRAJECTORY SUBSPACE MODELS| ABSTRACT In this paper we report on the extension of capturing speech transitions embedded in diphones using trajectory models.  The slowly varying dynamics of spectral trajectories carry much discriminant information that is very crudely modelled by traditional approaches such as HMMs.  We improved our methodology of explicitly capturing the trajectory of short time spectral parameter vectors introducing multi-trajectory concepts in a probabilistic framework.  Optimal subspace selection is presented which finds the most discriminant plane for classification.  Using the E-set from the TIMIT database results suggest that discriminant information is preserved in the subspace. 
A Dynamic Neural Network Architecture by sequential Partitioning of the input space| We present a sequential approach to training multilayer perceptron for pattern classi#cation applications.  The network is presented with each item of data only once and its architecture is dynamically adjusted during training.  At the arrival of each example, a decision whether to increase the complexity of the network, or simply train the existing nodes is made based on three heuristic criteria.  These criteria measure the position of the new item of data in the input space with respect to the information currently stored in the network.  During the training process, each layer is assumed to be an independent entity with its particular input space.  By adding nodes to each layer, the algorithm effectively adding a hyperplane to the input space hence adding a partition in the input space for that layer.  When existing nodes are sufficient to accommodate the incoming input, the involved hidden nodes will be trained accordingly.  Each hidden unit in the network is trained in closed form by means of a Recursive Least Squares (RLS) algorithm.  A local covariance matrix of the data is maintained at each node and the closed form solution is recursively updated.  The three criteria are computed from these covariance matrices with minimum computational cost.  The performance of the algorithm is illustrated on two problems.  The first problem is the two dimensional Peterson & Barney vowel data.  The second problem is a 32 dimensional data used for wheat classification.  The sequential nature of the algorithm has an efficient hardware implementation in the form of systolic arrays, and the incremental training idea has better biological plausibility when compared with iterative methods. 
On-line Q-learning using connectionist systems|
Data-dependent kernels in SVM classification of speech patterns,|
A Theoretical Investigation into the Performance of the Hopfield Model,|
Hierarchical Bayesian-Kalman models for regularisation and ARD in sequential learning,|
Nonlinear state space learning with EM and neural networks|
The EM algorithm and neural networks for nonlinear state space estimation|
Sequential Tracking in Pricing Financial Options using Model Based and Neural Network Approaches|
Regularisation in Sequential Learning Algorithms|
Toughening of Epoxy Resin Networks with Functionalized Engineering Thermoplastics",|
A function estimation approach to sequential learning with neural networks|
Sequential Adaptation of Radial Basis Function Networks|
An HMM based Cepstral-domain speech enhancement scheme|
Application of an architecturally dynamic network for speech pattern classification|
Sequential Monte Carlo Methods to Train Neural Network Models|
Subspace models for speech transitions using principal curves,|
Neural networks and radial basis functions for classifying static speech patterns",|
On the practical applicability of VC dimension bounds|
Neural networks and radial basis functions in classifying static speech patterns|
Sequential Bayesian Decoding with a Population of Neurons|
On-line Q-Learning using connexionist systems,|
Pruning with replacement on limited resource allocating networks by F-projections|
CELP coding with adaptive output error model identification,|
Limits on the discrimination possible with discrete valued data, with application to medical risk prediction,|
Using upper bounds on discrimination to select discrete valued features,"|
Nonlinear predictive vector quantization with recurrent neural nets|
On the use of expected attainable discrimination for feature selection in large scale medical risk predication problems|
Design, construction and evaluation of systems to predict risk in obstetrics|
Nonlinear Adaptive Filtering in Nonstationary Environments|
Recursive tracking of formants in speech signals|
Speech Modelling Using Subspace and EM Techniques|

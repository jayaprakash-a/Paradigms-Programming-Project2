Evaluating Multiple Aspects of Coherence in Student Essays| Abstract Criterion SM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e. g. , thesis statements).  We describe a new system that enhances Criterion's capability, by evaluating multiple aspects of coherence in essays.  This system identifies features of sentences based on semantic similarity measures and discourse structure.  A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements.  Intra-sentential quality is evaluated with rule-based heuristics.  Results indicate that the system yields higher performance than a baseline on all three aspects.  1 Overview This work is motivated by a need for advanced discourse analysis capabilities for writing instruction applications.  Criterion SM Online Essay Evaluation Service is an application for writing instruction which includes a capability to annotate sentences in student essays with discourse element labels.  These labels include the categories Thesis Statement, Main Idea, Supporting Idea, and Conclusion (Burstein et al. , 2003b).  Though it accurately annotates sentences with essay-based discourse labels, Criterion does not provide an evaluation of the expressive quality of the sentences that comprise a discourse segment.  What this means is that, on one hand, the system might accurately label a student's essay as having all of the typically expected discourse elements: thesis statement, 3 main ideas, supporting evidence linked to each main idea, and a conclusion.  As teachers have pointed out, however, an essay may have all of these discourse elements, but the quality of individual discourse elements may need improvement.  In this paper, we present a capability that captures expressive quality of sentences in the discourse segments of an essay.  For this work, we have defined expressive quality in terms of four aspects related to global and local essay coherence.  The first two dimensions capture global coherence, and the latter two relate to local coherence: a) relatedness to the essay question (topic), b) relatedness between discourse elements, c) intra-sentential quality, and d) sentence-relatedness within a discourse segment.  Each dimension represents a different aspect of coherence.  Essentially, the goal of the system is to be able to predict whether a sentence in a discourse segment has high or low expressive quality with regard to a particular coherence dimension.  We have deliberately developed an approach to essay coherence that is comprised of multiple dimensions, so that an instructional application may provide appropriate feedback to student writers, based on the system's prediction of high or low for each dimension.  For instance, sentences in the student's thesis statement may have a strong relationship to the essay topic, but may have a number of serious grammatical errors that make it hard to follow.  For this student, we may want to point out that on the one hand, the sentences in the thesis address the topic, but the thesis statement as a discourse segment might be more clearly stated if the grammar errors were fixed.  By contrast, the sentences that comprise the student's thesis statement may be grammatically correct, but only loosely related to the essay topic.  For this student, we would also want the system to provide appropriate feedback to, so that the student could revise the thesis statement text appropriately.  In earlier work, Foltz, Kintsch & Landauer (1998), and Wiemer-Hastings & Graesser (2000) have developed systems that also examine coherence in student writing.  Their systems measure lexical relatedness between text segments by using vector-based similarity between adjacent sentences.  This linear approach to similarity scoring is in line with the TextTiling scheme (Hearst and Plaunt, 1993; Hearst, 1997), which may be used to identify the subtopic structure of a text.  Miltsakaki and Kukich (2000) have also addressed the issue of establishing the coherence of student essays, using the Rough Shift element of Centering Theory.  Again, this previous work looks at the relatedness of adjacent text segments, and does not explore global aspects of text coherence.  Hierarchical models of discourse have been applied to the question of coherence (Mann and Thompson, 1986), but so far these have been more useful in language generation than in determining how coherent a given text is, or in identifying the specific problem, such as the breakdown of coherence in a document.  Our approach differs in fundamental ways from this earlier work that deals with student writing.  First, both Foltz et al.  (1998), Wiemer-Hastings and Graesser (2000), and Miltsakaki and Kukich (2000) assume that text coherence is linear.  They calculate the similarity between adjacent segments of text.  By contrast, our approach considers the discourse structure in the text, following Burstein et al.  (2003b).  Our method considers sentences with regard to their discourse segments, and how the sentences relate to other text segments both inside (such as the essay thesis) and outside (such as the essay topic) of a document.  This allows us to identify cases in which there may be a breakdown in coherence due to more global aspects of essay-based discourse structure.  Second, the previous work has used Latent Semantic Analysis as a semantic similarity measure (Landauer and Dumais, 1997).  We have adapted another vectorbased method of semantic representation: Random Indexing (Kanerva et al. , 2000; Sahlgren, 2001).  Another difference between our system and earlier systems is that we use essays manually annotated on the four coherence dimensions to train our system.  The final system employs a hybrid approach to classify the first two of the four coherence dimensions with a high or low quality rank.  For these dimensions, a support vector machine is used to model features derived from Random Indexing and from essay-based discourse structure information.  A third local coherence dimension component is driven by rule-based heuristics.  A fourth dimension related to coherence within a discourse segment cannot be classified due to a lack of data characterizing low expressive quality.  This is fully explained later in the paper.  2 Protocol Development and Human Annotation 2. 1 Protocol Development The development of this system required a corpus of human annotated essay data for modeling purposes.  In the end, the goal is to have the system make judgments similar to those made by a human with regard to ranking the coherence of an essay on four dimensions.  Therefore, we created a detailed protocol for annotating the expressive quality of essay-based discourse elements in essays with regard to four aspects related to global and local essay coherence.  This protocol was designed for the following purposes: 1.  To yield annotations that are useful for the purpose of providing students with feedback about the expressive relatedness of discourse elements in their essays, given four relatedness dimensions; 2.  To permit human annotators to achieve high levels of consistency during the annotation process; 3.  To produce annotations that have the potential of being derivable by computer programs through training on corpora annotated by humans. 
On the Way to Perfection: Primal Operations for Stable Sets in Graphs| In this paper some operations are described that transform every graph into a perfect graph by replacing nodes with sets of new nodes.  The transformation is done in such a way that every stable set in the perfect graph corresponds to a stable set in the original graph.  These operations yield a purely combinatorial augmentation procedure for finding a maximum weighted stable set in a graph.  Starting with a stable set in a given graph one defines a simplex type tableau whose associated basic feasible solution is the incidence vector of the stable set.  In an iterative fashion, non-basic columns that would lead to pivoting into non-integral basic feasible solutions, are replaced by new columns that one can read off from special graph structures such as odd holes, odd antiholes, and various generalizations.  Eventually, either a pivot leading to an integral basic feasible solution is performed, or the optimality of the current solution is proved. 
Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms| Abstract We provide a worst-case analysis of selective sampling algorithms for learning linear threshold functions.  The algorithms considered in this paper are Perceptron-like algorithms, i. e. , algorithms which can be efficiently run in any reproducing kernel Hilbert space.  Our algorithms exploit a simple margin-based randomized rule to decide whether to query the current label.  We obtain selective sampling algorithms achieving on average the same bounds as those proven for their deterministic counterparts, but using much fewer labels.  We complement our theoretical findings with an empirical comparison on two text categorization tasks.  The outcome of these experiments is largely predicted by our theoretical results: Our selective sampling algorithms tend to perform as good as the algorithms receiving the true label after each classification, while observing in practice substantially fewer labels. 
A New Approximate Maximal Margin Classification Algorithm| Abstract A new incremental learning algorithm is described which approximates the maximal margin hyperplane w. r. t.  norm p # 2 for a set of linearly separable data.  Our algorithm, called alma p (Approximate Large Margin algorithm w. r. t.  norm p), takes O # (p 1) # 2 # 2 # corrections to separate the data with p-norm margin larger than (1 #) #, where # is the (normalized) p-norm margin of the data.  alma p avoids quadratic (or higher-order) programming methods.  It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's Perceptron algorithm.  We performed extensive experiments on both real-world and artificial datasets.  We compared alma 2 (i. e. , alma p with p = 2) to standard Support vector Machines (SVM) and to two incremental algorithms: the Perceptron algorithm and Li and Long's ROMMA.  The accuracy levels achieved by alma 2 are superior to those achieved by the Perceptron algorithm and ROMMA, but slightly inferior to SVM's.  On the other hand, alma 2 is quite faster and easier to implement than standard SVM training algorithms.  When learning sparse target vectors, alma p with p }
Adaptive and Self-Confident On-Line Learning Algorithms| Abstract Most of the performance bounds for on-line learning algorithms are proven assuming a constant learning rate.  To optimize these bounds, the learning rate must be tuned based on quantities that are generally unknown, as they depend on the whole sequence of examples.  In this paper we show that essentially the same optimized bounds can be obtained when the algorithms adaptively tune their learning rates as the examples in the sequence are progressively revealed.  Our adaptive learning rates apply to a wide class of on-line algorithms, including p-norm algorithms for generalized linear regression and Weighted Majority for linear regression with absolute loss.  We emphasize that our adaptive tunings are radically different from previous techniques, such as the so-called doubling trick.  Whereas the doubling trick restarts the on-line algorithm several times using a constant learning rate for each run, our methods save information by changing the value of the learning rate very smoothly.  In fact, for Weighted Majority over a finite set of experts our analysis provides a better leading constant than the doubling trick. 
Margin-Based Algorithms for Information Filtering| Abstract In this work, we study an information filtering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function.  Building on the analysis of a restricted version of our model, we derive a general filtering rule based on the margin of a ridge regression estimator.  While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document filtering problem show that the performance of our rule is close to that of the on-line classifier which is allowed to observe all labels.  These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal filtering rule which knows the hidden linear model. 
Linear Hinge Loss and Average Margin| Abstract We describe a unifying method for proving relative loss bounds for online linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms.  For classification problems the discrete loss is used, i. e. , the total number of prediction mistakes.  We introduce a continuous loss function, called the "linear hinge loss", that can be employed to derive the updates of the algorithms.  We first prove bounds w. r. t.  the linear hinge loss and then convert them to the discrete loss.  We introduce a notion of "average margin" of a set of examples .  We show how relative loss bounds based on the linear hinge loss can be converted to relative loss bounds i. t. o.  the discrete loss using the average margin. 
Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms| Abstract New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method.  This makes them particularly suitable to the classification of microarray expression data, where the goal is to obtain accurate rules depending on few genes only.  Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods.  We report on preliminary experiments with five known DNA microarray datasets.  These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks. 
Asymptotic Behaviour of Nonlinear Parabolic Equations with Monotone Principal Part| We consider nonlinear parabolic equations with subdierential principal part and give conditions under which they posses global attractors in spite of considering non-Lipschitz perturbations.  The case of globally Lipschitz perturbations of a maximal monotone operator has been addressed in [4].  In the case of perturbations which are not globally Lipschitz, the main dicultness is the lack of uniqueness of solutions which at first does not even allow us to define attractors.  We overcome this difucultness for problems enjoying certain regularity and absorption properties that allow uniqueness of solutions after some time has been elapsed.  The results developed here are applied to the case when the subdierential operator is the p-Laplacian to obtain existence of attractors and the existence of periodic solutions.  April, 2000 ICMC-USP
approaches for network flow problems|
Max Horn SAT and the minimum cut problem in directed hypergraphs| Abstract In this paper we consider the Maximum Horn Satisfiability problem, which is reduced to the problem of finding a minimum cardinality cut on a directed hypergraph.  For the latter problem, we propose different ILP formulations, related to three different definitions of hyperpath weight.  We investigate the properties of their linear relaxations, showing that they define a hierarchy.  The weakest relaxation is shown to be equivalent to the relaxation of a well know ILP formulation of Max Horn SAT, and to a max-flow problem on hypergraphs.  The tightest relaxation, which is a disjunctive programming problem, is shown to have integer optimum.  The intermediate relaxation consists in a set covering problem with a possibly exponential number of constraints.  This latter relaxation provides an approximation of the convex hull of the integer solutions which, as proven by the experimental results given, is much tighter than the one known in the literature. 
On the Generalization Ability of On-Line Learning Algorithms| Abstract---In this paper, it is shown how to extract a hypothesis with small risk from the ensemble of hypotheses generated by an arbitrary on-line learning algorithm run on an independent and identically distributed (i. i. d. ) sample of data.  Using a simple large deviation argument, we prove tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic associated with the on-line performance of the ensemble.  Via sharp pointwise bounds on , we then obtain risk tail bounds for kernel Perceptron algorithms in terms of the spectrum of the empirical kernel matrix.  These bounds reveal that the linear hypotheses found via our approach achieve optimal tradeoffs between hinge loss and margin size over the class of all linear functions, an issue that was left open by previous results.  A distinctive feature of our approach is that the key tools for our analysis come from the model of prediction of individual sequences; i. e. , a model making no probabilistic assumptions on the source generating the data.  In fact, these tools turn out to be so powerful that we only need very elementary statistical facts to obtain our final risk bounds. 
Determination of the fluorescence quantum yield by oceanic phytoplankton in their natural habitat| Sun-stimulated chlorophyll a fluorescence has been measured in situ, within the upward and downward light fields, in oceanic waters with chlorophyll concentrations of 0. 04 --3 mg m 23 .  We combined these signals with phytoplankton absorption spectra to derive the fluorescence quantum yield, f~number of photons emitted by fluorescenceynumber of absorbed photons!.  f was derived separately from hyperspectral ~upward and downward! irradiance measurements ~with a LI-COR Instruments spectroradiometer! and from nadir radiance near 683 nm ~with a Biospherical Instruments profiler!.  The contribution of inelastic Raman scattering to the signal in the red band was assessed and subtracted.  Ramancorrected f values derived from the two instruments compared well.  Vertical f profiles were strongly structured, with maximal ~5-- 6%! values at depth, whereas f was }1% in near-surface waters ~measurements made approximately at solar noon!.  These near-surface values are needed for interpretation of remotely sensed fluorescence signals.  This optical study shows that the fluorescence yield of algae in their natural environment can be accurately derived in a nonintrusive way with available instrumentation and adequate protocols.  2000 Optical Society of America
P-Sufficient Statistics for PAC Learning k-term-DNF Formulas through Enumeration| Abstract Working in the framework of PAC-learning theory, we present special statistics for accomplishing in polynomial time proper learning of DNF boolean formulas having a fixed number of monomials.  Our statistics turn out to be near sufficient for a large family of distribution laws---that we call butterfly distributions.  We develop a theory of most powerful learning for analyzing the performance of learning algorithms, with particular reference to trade-offs between power and computational costs.  Focusing attention on sample and time complexity, we prove that our algorithm works as efficiently as the best algorithms existing in the literature --- while the latter only take care of subclasses of our family of distributions. 
Approximate maximal margin classification with respect to an arbitrary norm|
Minireview: The chemistry of melatonin's interaction with reactive species|
Proving relative loss bounds for on-line learning algorithms using bregman divergences|
A Second-Order Perceptron Algorithm|
The Robustness of the p-Norm Algorithms|
Inf'erence grammaticale `a partir d'exemples et de contre-exemples : deux algorithmes optimaux : (big et rig) et une version heuristique (brig)|
Diffuse reflectance of oceanic waters: Its dependence on Sun angle as influenced by the molecular scattering contribution|
New preconditioners for KKT systems of network flow problems,|
Yeast microarrays for genome wide parallel genetic and gene expression analysis|
in:|
Accuracy assessment of bridge modal parameters estimated from ambient|
The impact of sexual abuse on children:|
Sample Size Lower Bounds in PAC Learning by Algorithmic Complexity Theory|
Continuous monitoring of surface optical properties across a geostrophic front: Biogeochemical inferences,|
Diffuse reflectance of oceanic waters|
Diffuse reflectance of oceanic waters| III.  Implication of bidirectionality for the remote-sensing problem,". 
Diffuse reflectance of oceanic shallow waters: influence of water depth and bottom albedo,"|
Bidirectional reflectance of oceanic waters: accounting for Raman emission and varying particle scattering phase function|
Improved Lower Bounds for Learning from Noisy Examples: An Information-Theoretic Approach|
Solving Large MIP Models in Supply Chain Management by Branch & Cut|
The Robustness of the -Norm Algorithms|
Learning Probabilistic Linear-Threshold Classifiers via Selective Sampling|
A Primal Approach to the Stable Set Problem|
Kernal method for document filtering|
Comparison Results for Nonlinear Parabolic Problems with|
SART: A system for supporting operators with contextual knowledge|
The robustness of the p-norm algorithm|
Relativistic Solar Proton Database for the Ground Level Enhancements During Solar Cycle 22,|
Kernel Methods for Document Filtering|

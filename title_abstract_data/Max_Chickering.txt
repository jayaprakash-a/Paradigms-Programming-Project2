Goal-Oriented Clustering| Abstract We introduce goal-oriented clustering, a process that clusters items with the explicit knowledge that the ultimate use of the clusters is prediction.  In this approach, we use data on a set of target variables (those we want to predict) and a set of input variables (those wedonotwant to predict) to learn a graphical (generative) model with a single hidden layer of discrete variables H.  The states of H correspond to clusters.  We describe a generalized EM algorithm for learning the parameters of this class of models and provide a convergence guarantee.  We compare our goal-oriented approach to a standard clustering approach on the task of targeted advertising on a web site. 
Efficient Determination of Dynamic Split Points in a Decision Tree| Abstract We consider the problem of choosing split points for continuous predictor variables in a decision tree.  Previous approaches to this problem typically either (1) discretize the continuous predictor values prior to learning or (2) apply a dynamic method that considers all possible split points for each potential split.  In this paper, we describe a number of alternative approaches that generate a small number of candidate split points dynamically with little overhead.  We argue that these approaches are preferable to pre-discretization, and provide experimental evidence that they yield probabilistic decision trees with the same prediction accuracy as the traditional dynamic approach.  Furthermore, because the time to grow a decision tree is proportional to the number of split points evaluated, our approach is significantly faster than the traditional dynamic approach. 
Large-Sample Learning of Bayesian Networks is NP-Hard| Abstract In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data.  Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest structure for which the model is able to represent the generative distribution exactly.  Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset.  We show that identifying high-scoring structures is NPhard, even when any combination of one or more of the following hold: the generative distribution is perfect with respect to some DAG containing hidden variables; we are given an independence oracle; we are given an inference oracle; we are given an information oracle; we restrict potential solutions to structures in which each node has at most k parents, for all k # 3.  Our proof relies on a new technical result that we establish in the appendices.  In particular, we provide a method for constructing the local distributions in a Bayesian network such that the resulting joint distribution is provably perfect with respect to the structure of the network. 
Finding Optimal Bayesian Networks| Abstract In this paper, we derive optimality results for greedy Bayesian-network search algorithms that perform single-edge modifications at each step and use asymptotically consistent scoring criteria.  Our results extend those of Meek (1997) and Chickering (2002), who demonstrate that in the limit of large datasets, if the generative distribution is perfect with respect to a DAG defined over the observable variables, such search algorithms will identify this optimal (i. e.  generatve) DAG model.  We relax their assumption about the generative distribtion, and assume only that this distribution satisfies the composition property over the observable variables, which is a more realistic assumption for real domains.  Under this assumption, we guarantee that the search algorithms identify an inclusion-optimal model; that is, a model that (1) contains the generative distribution and (2) has no sub-model that contains this distribution.  In addition, we show that the composition property is guaranteed to hold whenever the dependence relationships in the generative distribution can be characterized by paths between singleton elements in some generative graphical model (e. g.  a DAG, a chain graph, or a Markovnetwork) even when we allow the generative model to include unobserved variables, and the observed data to be subject to selection bias. 
Optimal Structure Identification With Greedy Search| Abstract In this paper we prove the so-called "Meek Conjecture".  In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G = H.  As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a twophase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG.  We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored eciently using local functions of the nodes in the domain.  Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes. 
Learning Bayesian Networks From Dependency Networks: A Preliminary Study| Abstract In this paper we describe how to learn Bayesian networks from a summary of complete data in the form of a dependency network rather than from data directly.  This method allows us to gain the advantages of both representations: scalable algorithms for learning dependency networks and convenient inference with Bayesian networks.  Our approach is to use a dependency network as an "oracle" for the statistics needed to learn a Bayesian network.  We show that the general problem is NP-hard and develop a greedy search algorithm.  We conduct a preliminary experimental evaluation and find that the prediction accuracy of the Bayesian networks constructed from our algorithm almost equals that of Bayesian networks learned directly from the data. 
Targeted Internet Advertising Using Predictive Clustering and Linear Programming| Abstract Many Internet sites have the freedom to choose which advertisements go on which page, subject to the constraint that all paid-for ads must be shown.  To increase revenue, these sites can choose ads so as to maximize click-through rates.  We present a two-step system that does so.  In step one, the system applies predictive clustering to the pages so that the clusters are good predictors of ad click-through.  This predictive clustering is accomplished by GEM applied to a regularized log loss on click-through prediction.  In step two, the system measures click-through rates of each cluster, and then uses a linear program to optimize overall click-through rate on the site.  Using data from the msnbc. com Internet site, we show that this approach increases overall click-through rate by 38%, a dramatic improvement over the use of hand-assigned clusters. 
Learning Bayesian Networks: Search Methods and Experimental Results| Abstract We discuss Bayesian approaches for learning Bayesian networks from data.  First, we review a metric for computing the relative posterior probability of a network structure given data developed by Heckerman et al.  (1994a,b,c).  We see that the metric has a property useful for inferring causation from data.  Next, we describe search methods for identifying network structures with high posterior probabilities.  We describe polynomial algorithms for finding the highestscoring network structures in the special case where every node has at most k = 1 parent.  We show that the general case (k } 1) is NP-hard, and review heuristic search algorithms for this general case.  Finally, we describe a methodology for evaluating learning algorithms, and use this methodology to evaluate various scoring metrics and search procedures. 
Dependency Networks for Inference, Collaborative Filtering, and Data Visualization|
A Transformational Characterization of Equivalent Bayesian Network Structures|
Learning Bayesian Networks: The Combination of Knowledge and Statistical Data|
Learning Bayesian networks is NP-Complete|
Efficient Approximations for the Marginal Likelihood of Bayesian Networks with Hidden Variables|
A new characterization of equivalent Bayesian network structures|
Learning Mixtures of DAG Models|
Dependency Networks for Collaborative Filtering and Data Visualization|
The WinMine toolkit|
Learning Bayesian Networks from Data|
Learning Bayesian networks: The|
Efficient approximations for the marginal likelihood of -|
Learning equivalence classes of Bayesian network structures|
Efficent approximations for the marginal likelihood of Bayesian networks with hidden variables|
Dependency networks for density estimation, collaborative filtering, and data visualization|
A Decision Theoretic Approach to Targeted Advertising|
On Finding a Cycle Basis with a Shortest Maximal Cycle|
Learning bayesian network: The combination of knowledge and statistical data|
Best-First Minimax Search: Othello Results|
Learning mixtures of Bayes networks|
Learning equivalence classes of Bayesian network structure,|
Autoregressive Tree Models for Time-Series Analysis|
Fast Learning from Sparse Data|
Best-First Minimax Search|
"Learning mixtures of Bayesian networks,"|
Targeted advertising with inventory management|
Learning Bayesian networks is NP-Complete|
Optimal structure identification with greedy search|
`Learning equivalence classes of Bayesian networks structures',|
Learning Bayesian networks from dependency networks|
A comparison of scientific and engineering criteria for Bayesian model selection|
nd| WinMine Toolkit Home Page. 
combination of knowledge and statistical data (|
High-performance asw at an affordable price|

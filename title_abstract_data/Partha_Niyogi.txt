Feature Selection in MLPs and SVMs based on Maximum Output Information| Abstract--- We present feature selection algorithms for multilayer Perceptrons (MLPs) and multi-class Support Vector Machines (SVMs), using mutual information between class labels and classifier outputs, as an objective function.  This objective function involves inexpensive computation of information measures only on discrete variables; provides immunity to prior class probabilities; and brackets the probability of error of the classifier.  The Maximum Output Information (MOI) algorithms employ this function for feature subset selection by greedy elimination and directed search.  The output of the MOI algorithms is a feature subset of user-defined size and an associated trained classifier(MLP/SVM).  These algorithms compare favorably with a number of other methods in terms of performance on various artificial and real-world data sets. 
A Geometric Perspective on Speech Sounds| Abstract In order to effectively approach high dimensional pattern recognition problems, one seeks to understand and exploit any inherent low dimensional structure.  Recently, a number of manifold learning algorithms have been motivated by a geometric point of view that models high dimensional data as lying near a low dimensional submanifold of the original space.  Our paper has two main goals: (i) to investigate this manifold assumption for natural speech data.  It seems intuitive that a human speech producing apparatus with few degrees of freedom would not produce sounds that fill up the acoustic space.  We formalize this intuition by considering a concatenated acoustic tube model of the vocal tract and showing that the sounds generated by such a system lie on a low dimensional curved submanifold of the ambient acoustic space.  To the extent that this model captures the essence of human speech production, the manifold assumption is true of natural speech data.  (ii) to explore the implications of this geometric point of view towards human speech.  We show that the manifold structure of speech sounds may be exploited for dimensionality reduction, semi-supervised learning, and speech representation with sometimes striking perfomance improvements in simulated and real speech data.  The non-linear geometry of speech sounds suggests new interpretations of phenomena such as the perceptual magnet effect or quantal theory. 
Locality Preserving Projections| Abstract Many problems in information processing involve some form of dimensionality reduction.  In this paper, we introduce Locality Preserving Projections (LPP).  These are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set.  LPP should be seen as an alternative to Principal Component Analysis (PCA) -- a classical linear technique that projects the data along the directions of maximal variance.  When the high dimensional data lies on a low dimensional manifold embedded in the ambient space,
Tensor Subspace Analysis| Abstract Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces.  The typical linear subspace learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projection (LPP).  All of these methods consider an n 1 n 2 image as a high dimensional vector in R n1n2 , while an image represented in the plane is intrinsically a matrix.  In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA).  TSA considers an image as the second order tensor in R n1 # R n2 , where R n1 and R n2 are two vector spaces.  The relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by TSA.  TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace.  We compare our proposed approach with PCA, LDA and LPP methods on two standard databases.  Experimental results demonstrate that TSA achieves better recognition rate, while being much more efficient. 
Discriminative Gaussian Mixture Models for Speaker Identification| Abstract Gaussian mixture models are commonly used for the text independent speaker identification task. 
Active Learning by Sequential Optimal Recovery| Abstract In most classical frameworks for learning from examples, it is assumed that examples are randomly drawn and presented to the learner.  In this paper, we consider the possibility of a more active learner who is allowed to choose his/her own examples.  Our investigations are carried out in a function approximation setting.  In particular, using arguments from optimal recovery (Micchelli and Rivlin, 1976), we develop an adaptive sampling strategy (equivalent to adaptive approximation) for arbitrary approximation schemes.  We provide a general formulation of the problem and show how it can be regarded as sequential optimal recovery.  We demonstrate the application of this general formulation to two special cases of functions on the real line 1) monotonically increasing functions and 2) functions with bounded derivative.  An extensive investigation of the sample complexity of approximating these functions is conducted yielding both theoretical and empirical results on test functions.  Our theoretical results (stated in PAC-style), along with the simulations demonstrate the superiority of our active scheme over both passive learning as well as classical optimal recovery.  The analysis of active function approximation is conducted in a worst-case setting, in contrast with other Bayesian paradigms obtained from optimal design (Mackay, 1992). 
Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering| Abstract Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space.  The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering.  Several applications are considered.  In many areas of artificial intelligence, information retrieval and data mining, one is often confronted with intrinsically low dimensional data lying in a very high dimensional space.  For example, gray scale n \Theta n images of a fixed object taken with a moving camera yield data points in R n 2 .  However, the intrinsic dimensionality of the space of all images of the same object is the number of degrees of freedom of the camera -- in fact the space has the natural structure of a manifold embedded in R n 2 .  While there is a large body of work on dimensionality reduction in general, most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside.  Recently, there has been some interest (Tenenbaum et al, 2000; Roweis and Saul, 2000) in the problem of developing low dimensional representations of data in this particular context.  In this paper, we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction.  The core algorithm is very simple, has a few local computations and one sparse eigenvalue problem.  The solution reflects the intrinsic geometric structure of the manifold.  The justification comes from the role of the Laplacian operator in providing an optimal embedding.  The Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace-Beltrami operator defined on the manifold.  The embedding maps for the data come from approximations to a natural map that is defined on the entire manifold.  The framework of analysis presented here makes this connection explicit.  While this connection is known to geometers and specialists in spectral graph theory (for example, see [1, 2]) to the best of our knowledge we do not know of any application to data representation yet.  The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner.  The locality preserving character of the Laplacian Eigenmap algorithm makes it relatively insensitive to outliers and noise.  A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data.  Connections to spectral clustering algorithms developed in learning and computer vision (see Shi and Malik, 1997) become very clear.  Following the discussion of Roweis and Saul (2000), and Tenenbaum et al (2000), we note that the biological perceptual apparatus is confronted with high dimensional stimuli from which it must recover low dimensional structure.  One might argue that if the approach to recovering such low-dimensional structure is inherently local, then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception.  1 The Algorithm Given k points x 1 ; : : : ; x k in R l , we construct a weighted graph with k nodes, one for each point, and the set of edges connecting neighboring points to each other.  1.  Step 1.  [Constructing the Graph] We put an edge between nodes i and j if x i and x j are "close".  There are two variations: (a) ffl-neighborhoods.  [parameter ffl 2 R] Nodes i and j are connected by an edge if jjx i \Gamma x j jj 2 ! ffl.  Advantages: geometrically motivated, the relationship is naturally symmetric.  Disadvantages: often leads to graphs with several connected components, difficult to choose ffl.  (b) n nearest neighbors.  [parameter n 2 N] Nodes i and j are connected by an edge if i is among n nearest neighbors of j or j is among n nearest neighbors of i.  Advantages: simpler to choose, tends to lead to connected graphs.  Disadvantages: less geometrically intuitive.  2.  Step 2.  [Choosing the weights] Here as well we have two variations for weighting the edges: (a) Heat kernel.  [parameter t 2 R].  If nodes i and j are connected, put W ij = e\Gamma jjx i \Gamma x j jj 2 t The justification for this choice of weights will be provided later.  (b) Simple-minded.  [No parameters].  W ij = 1 if and only if vertices i and j are connected by an edge.  A simplification which avoids the necessity of choosing t.  3.  Step 3.  [Eigenmaps] Assume the graph G, constructed above, is connected, otherwise proceed with Step 3 for each connected component. 
Regression and Regularization on Large Graphs| Abstract We consider the problem of labeling a partially labeled graph.  This setting may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings.  It is also of potential practical importance, when the data is abundant, but labeling is expensive or requires human assistance.  Our approach develops a framework for regularization on such graphs.  Within this framework two algorithms are developed.  The algorithms are very simple and involve solving a single, usually sparse, system of linear equations.  Using the notion of algorithmic stability, we derive bounds on the generalization error and relate it to the structural invariants of the graph.  Some experimental comparisons to existing algorithms suggest that this approach is competitive. 
Robust Acoustic Object Detection| Abstract We consider a novel approach to the problem of detecting phonological objects like phonemes, syllables, or words, directly from the speech signal.  We begin by defining local features in the time-frequency plane with built in robustness to intensity variations and time warping.  Global templates of phonological objects correspond to the coincidence in time and frequency of patterns of the local features.  These global templates are constructed by using the statistics of the local features in a principled way.  The templates have clear phonetic interpretability, are easily adaptable, have built in invariances, and display considerable robustness in the face of additive noise and clutter from competing speakers.  We provide a detailed evaluation of the performance of some diphone detectors and a word detector based on this approach. 
DISTINCTIVE FEATURE DETECTION USING SUPPORT VECTOR MACHINES| ABSTRACT An important aspect of distinctive feature based approaches to automatic speech recognition is the formulation of a framework for robust detection of these features.  We discuss the application of the support vector machines (SVM) that arise when the structural risk minimization principle is applied to such feature detection problems.  In particular, we describe the problem of detecting stop consonants in continuous speech and discuss an SVM framework for detecting these sounds.  In this paper we use both linear and nonlinear SVMs for stop detection and present experimental results to show that they perform better than a cepstral features based hidden Markov model (HMM) system, on the same task. 
Laplacian Eigenmaps for Dimensionality Reduction and Data Representation| Abstract One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data.  We consider the problem of constructing a representation for data lying on a low dimensional manifold embedded in a high dimensional space.  Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high dimensional data.  The algorithm provides a computationally ecient approach to non-linear dimensionality reduction that has locality preserving properties and a natural connection to clustering.  Some potential applications and illustrative examples are discussed. 
The Voicing Feature for Stop Consonants: Recognition Experiments with Continuously Spoken Alphabets| Abstract We consider the possibility of incorporating phonetic features into a statistically based speech recognizer.  We develop a two pass strategy for recognition with a Hidden Markov Model (HMM) based first pass followed by a second pass that performs an alternative analysis using class-specific features.  For the voiced/voiceless distinction on stops for an alphabet recognition task, we show that a perceptually and linguistically motivated acoustic feature exists (the voice onset time).  We perform acousticphonetic analyses demonstrating that this feature provides superior separability to the traditional spectral features.  Further, the voice onset time can be automatically extracted from the speech signal.  We describe several such algorithms that can be incorporated into our two pass recognition strategy to reduce error rates by as much as 53 % over a baseline HMM recognition system. 
A Dynamical Systems Model for Language Change| Abstract This paper formalizes linguists' intuitions about language change, proposing a dynamical systems model for language change derived from a model for language acquisition.  Linguists must explain not only how languages are learned but also how and why they have evolved along certain trajectories and not others.  While the language learning problem has focused on the behavior of individuals and how they acquire a particular grammar from a class of grammars G, here we consider a population of such learners and investigate the emergent, global population characteristics of linguistic communities over several generations.  We argue that language change follows logically from specific assumptions about grammatical theories and learning paradigms.  Roughly, as the end product of two types of learning misconvergence over several generations, individual language learner behavior leads to emergent, population language community characteristics.  In particular, we show that any triple {G, A, P} of grammatical theory, learning algorithm, and initial sentence distributions can be transformed into a dynamical system whose evolution depicts a population's evolving linguistic composition.  We explicitly show how this transformation can be carried out for memoryless learning algorithms and parameterized grammatical theories.  As the simplest case, we formalize the example of two grammars (languages) differing by exactly one binary parameter, and show that even this situation leads directly to a quadratic (nonlinear) dynamical system, including regions with chaotic behavior.  We next apply the computational model to some some actual data, namely the observed historical loss of "Verb Second" from Old French to modern French.  Thus, the formal model allows one to pose new questions about language phenomena that one otherwise could not ask, such as: (1) whether languages/grammars correctly follow observed historical trajectories---an evolutionary criteria for the adequacy of grammatical theories; (2) what are the logically possible dynamical change envelopes given a posited grammatical theory---rates and shapes of linguistic change, including the possibilities for the past and the future; (3) what can be the effect of quantified variation in initial conditions (e. g. , population differences resulting from socio-political facts); (4) intrinsically interesting mathematical questions regarding linguistic dynamical systems. 
MULTIPLE CLASSIFIERS BY CONSTRAINED MINIMIZATION| ABSTRACT The paper describes an approach to combining multiple classifiers in order to improve classification accuracy.  Since individual classifiers in the ensemble should somehow be uncorrelated to yield higher classification accuracy than a single classifier, we propose to train classifiers by minimizing the correlation between their classification errors.  A simple combination strategy for three classifiers is then proposed and its achievable error rate is analyzed and compared to individual single classifier performance.  The proposed approach has been evaluated on artificial data and a nasal/oral vowel classification task.  Theoretical analyses and experimental results illustrate the effectiveness of the proposed approach. 
Generalization Bounds for Function Approximation from Scattered Noisy Data| We consider the problem of approximating functions from scattered data using linear superpositions of non-linearly parameterized functions.  We show how the total error (generalization error) can be decomposed into two parts: an approximation part that is due to the finite number of parameters of the approximation scheme used; and an estimation part that is due to the finite number of data available.  We bound each of these two parts under certain assumptions and prove a general bound for a class of approximation schemes that include radial basis functions and multilayer perceptrons. 
Almost-everywhere Algorithmic Stability and Generalization Error| Abstract We explore in some detail the notion of algorithmic stability as a viable framework for analyzing the generalization error of learning algorithms.  We introduce the new notion of training stability of a learning algorithm and show that, in a general setting, it is sufficient for good bounds on generalization error.  In the PAC setting, training stability is both necessary and sufficient for learnability.  The approach based on training stability makes no reference to VC dimension or VC entropy.  There is no need to prove uniform convergence, and generalization error is bounded directly via an extended McDiarmid inequality.  As a result it potentially allows us to deal with a broader class of learning algorithms than Empirical Risk Minimization.  We also explore the relationships among VC dimension, generalization error, and various notions of stability.  Several examples of learning algorithms are considered. 
Linear Manifold Regularization for Large Scale Semi-supervised Learning| Abstract The enormous wealth of unlabeled data in many applications of machine learning is beginning to pose challenges to the designers of semi-supervised learning methods.  We are interested in developing linear classification algorithms to efficiently learn from massive partially labeled datasets.  In this paper,
On the Relationship Between Generalization Error, Hypothesis Complexity, and Sample Complexity for Radial Basis Functions| Abstract Feedforward networks together with their training algorithms are a class of regression techniques that can be used to learn to perform some task from a set of examples.  The question of generalization of network performance from a finite training set to unseen data is clearly of crucial importance.  In this article we first show that the generalization error can be decomposed in two terms: the approximation error, due to the insufficient representational capacity of a finite sized network, and the estimation error, due to insufficient information about the target function because of the finite number of samples. 
Semi-supervised learning on manifolds| Abstract We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy.  Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner.  The central idea of our approach is that classification functions are naturally defined only on the submanifold in question rather than the total ambient space.  Using the Laplace Beltrami operator one produces a basis for a Hilbert space of square integrable functions on the submanifold.  To recover such a basis, only unlabeled examples are required.  Once such a basis is obtained, training can be performed using the labeled data set.  Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace Beltrami operator by the graph Laplacian.  We provide details of the algorithm, its theoretical justification, and several practical applications for image, speech, and text classification. 
Detecting and Interpreting Acoustic Features by Support Vector Machines| Abstract An important aspect of distinctive feature based approaches to automatic speech recognition is the formulation of a framework for the robust detection of these features.  We discuss the application of the Support Vector Machines (SVM) that arise when the structural risk minimization principle is applied to such feature detection problems.  In particular, we consider in some detail the problem of detecting stop consonants in continuous speech.  We track dynamic acoustic properties and discuss an SVM framework for detecting these sounds.  In this paper, we use both linear and non-linear SVMs and present experimental results to illustrate the factors upon which superior performance depends.  We also relate the detectors to perceptual phenomena like categorical perception and the perceptual magnet effect.  We show how the detectors operate by comparing sounds in a transformed space leading to many different distance metrics that may then be defined.  Only one of these is compatible with the perceptual magnet effect. 
Statistical Learning: Stability is Sufficient for Generalization and Necessary and Sufficient for Consistency of Empirical Risk Minimization| Abstract Solutions of learning problems by Empirical Risk Minimization (ERM) -and almost-ERM when the minimizer does not exist -- need to be consistent, so that they may be predictive.  They also need to be well-posed in the sense of being stable, so that they might be used robustly.  We propose a statistical form of leave-one-out stability, called CVEEE loo stability.  Our main new results are two.  We prove that for bounded loss classes CVEEE loo stability is (a) sufficient for generalization, that is convergence in probability of the empirical error to the expected error, for any algorithm satisfying it and, (b) necessary and sufficient for generalization and consistency of ERM.  Thus CVEEE loo stability is a weak form of stability that represents a sufficient condition for generalization for general learning algorithms while subsuming the classical conditions for consistency of ERM.  We discuss alternative forms of stability.  In particular, we conclude that for ERM a certain form of well-posedness is equivalent to consistency. 
Evolutionary Consequences of Language Learning| Abstract Linguists' intuitions about language change can be captured by a dynamical systems model derived from the dynamics of language acquisition.  Rather than having to posit a separate model for diachronic change, as has sometimes been done by drawing on assumptions from population biology (cf.  CavalliSforza and Feldman, 1973; 1981; Kroch, 1990), this new model dispenses with these independent assumptions by showing how the behavior of individual language learners leads to emergent, global population characteristics of linguistic communities over several generations.  As the simplest case, we formalize the example of two grammars and show that even this situation leads directly to a nonlinear (quadratic) dynamical system.  We study this one parameter model in a variety of situations for different kinds of acquisition algorithms and maturational times, showing how different learning theories can have very different evolutionary consequences.  This allows us to formulate an evolutionary criterion for the adequacy of grammatical and learning theories.  An application of the computational model to the historical loss of Verb Second from Old French to Modern French is described showing the how otherwise adequate grammatical theories might fail the evolutionary criterion. 
An Approach to Data Reduction and Clustering with Theoretical Guarantees| Abstract We discuss the problem of representing a dataset of n points by a smaller number of representative points.  We introduce a novel objective function over the data set that yields a global minimum solution by ecient means.  This leads to a new approach to data reduction or unsupervised learning.  We derive an exact expression for the minimum training error and bounds on the generalization error of the scheme.  This shows how to precisely characterize overfitting in terms of an approximation-estimation tradeoff for unsupervised learning in this particular context.  As a result we are able to choose the optimal number of clusters. 
Phase Transitions in Language Evolution| Abstract Language is transmitted from one generation to the next via learning by individuals.  By taking this point of view one is able to link the linguistic behavior of successive generations and therefore study how language evolves over generational time scales.  We provide a brief overview of this approach to the study of language evolution, its formalization as a dynamical system, and the analogical connections to the methodological principles of evolutionary biology.  We show how the interplay between learning and evolution can be quite subtle and how phase transitions arise in many such models of language evolution.  Such phase transitions may provide a suitable theoretical construct with which explanations for rapid language change or evolution may be given.  Some illustrative examples are provided. 
Detecting Stop Consonants in Continuous Speech| Abstract We consider the problem of implementing a detector for stop consonants in continuously spoken speech.  We pose the problem as one of finding an optimal filter (linear or non-linear) that operates on a particular appropriately chosen representation, and ideally outputs a 1 when a stop occurs and 0 otherwise.  We discuss the performance of several variants of a canonical stop detector and consider its implications for human and machine speech recognition. 
The interaction of stability and weakness in AdaBoost| Abstract We provide an analysis of AdaBoost within the framework of algorithmic stability.  In particular, we show that AdaBoost is a stabilitypreserving operation: if the \input" (the weak learner) to AdaBoost is stable, then the \output" (the strong learner) is almost-everywhere stable.  Because classifier combination schemes such as AdaBoost have greatest effect when the weak learner is weak, we discuss weakness and its implications.  We also show that the notion of almost-everywhere stability is sucient for good bounds on generalization error.  These bounds hold even when the weak learner has infinite VC dimension. 
Manifold Regularization: A Geometric Framework for Learning from Examples| Abstract We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution.  We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. 
Measuring the Functional Load of Phonological Contrasts| Frequency counts are a measure of how much use a language makes of a linguistic unit, such as a phoneme or word.  However, what is often important is not the units themselves, but the contrasts between them.  A measure is therefore needed for how much use a language makes of a contrast, i. e.  the functional load (FL) of the contrast.  We generalize previous work in linguistics and speech recognition and propose a family of measures for the FL of several phonological contrasts, including phonemic oppositions, distinctive features, suprasegmentals, and phonological rules.  We then test it for robustness to changes of corpora. 
Comparing support vector machines with Gaussian kernels to radial basis functions classifiers,|
Evolution of universal grammar|
General conditions for predictivity in learning theory|
Face Recognition using Laplacianfaces,|
Regularization and Semi-supervised Learning on Large Graphs|
"A Detection Framework for Locating Phonetic Events,|
Incorporating voice onset time to improve letter recognition accuracies|
On the relationship between hypothesis complexity, sample complexity, and generalization error for radial basis functions|
Using Manifold Stucture for Partially Labeled Classification|
Finding the homology of submanifolds with high confidence from random samples|
Beyond the point cloud: from Transductive to Semi-supervised Learning Under submission,|
A language learning model for finite parameter spaces|
The evolutionary dynamics of grammar acquisition|
Computational and evolutionary aspects of language|
Incorporating prior information in machine learning by creating virtual examples,|
Formalizing triggers: A learning model for finite spaces|
Detecting and interpreting acoustic features with support vector machines|
Regularization and regression on large graphs|
Active Learning for Function Approximation|
The informational complexity of learning from examples|
A Markov Model for Finite Parameter Spaces|
Formalizing triggers: A learning model for finite parameter spaces|
A Markov Language Learning Model for Finite Parameter Spaces|
Learning from triggers|
Formal models for learning finite parameter spaces|
Locality Preserving Projection",|
Generalization bounds for function approximation from scattered noisy data|
Stability and generalization of bipartite ranking algorithms,|
Comparing support vector machines with gaussian kernels to radial basis function classifiers|
Laplacian eigenmaps for dimensionality reduction and data representation| (Technical Report).  Computer Science Department,. 
Evolution of universal grammer|
Detecting and implementing acoustic features by support vector machines|
An active formulation for approximation of real valued functions|
The Logical Problem of Language Change|
Evolution of Universal|
Feature based representation for audio-visual speech recongition,"|
Towards a theoeretical foundation for Laplacian-based manifold methods,|
The voicing feature for stop consonants: Acoustic phonetic analyses and automatic speech recognition experiments|
Questioning the role of communicative efficiency in language evolution|

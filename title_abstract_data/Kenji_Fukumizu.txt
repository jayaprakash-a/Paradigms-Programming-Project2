A network of chaotic elements for information processing| Abstract A Globally Coupled Map (GCM) model is a network of chaotic elements that are globally coupled with each other.  In this paper, first, a modified GCM model called the "Globally Coupled Map using the Symmetric map (S-GCM)" is proposed.  The SGCM is designed for information-processing applications.  The S-GCM has attractors called "cluster frozen attractors," each of which is taken to represent information.  This paper also describes the following characteristics of the S-GCM which are important to information-processing applications: (a) The S-GCM falls into one of the cluster frozen attractors over a wide range of parameters.  This means that the information representation is stable over parameters; (b) Represented information can be preserved or broken by controlling parameters; (c) The cluster partitioning is restricted, i. e. , the representation of information has a limitation.  Finally, our techniques for applying the S-GCM to information processing are shown, considering these characteristics.  Two associative memory systems are proposed and their performance is compared with that of the Hopfield network. 
Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting| Abstract We show the existence of critical points as lines for the likelihood function of mixture-type models.  They are given by embedding a critical point for models with less components.  A sufficient condition that the critical line gives local maxima or saddle points is also derived.  Based on this fact, a component-split method is proposed for mixture of Gaussian components, and its effectiveness is verified through experiments. 
Likelihood Ratio of Unidentiable Models and Multilayer Neural Networks| Abstract This paper discusses the behavior of the maximum likelihood estimator, when the true parameter cannot be identied uniquely.  Among many statistical models with unidentiability, neural network models are the main concern of this paper.  The set of unidentiable true parameters is formulated as a conic singularity of the model, which is embeddedinaninnite dimensional space of probability density functions.  Ithasbeenknowninsomemodelswithunidentiability the asymptotics of the likelihood ratio of MLE has an unusually larger order.  Following Hartigans idea, the likelihood ratio of MLE is described by the supremum of an empirical process over a set of functions, and a useful su!cient condition of such larger orders is derived.  This result is applied to neural network models, and a larger order is observed if the true function is realized by a network with a smaller number of hidden units than the model.  A stronger lower bound of the order of likelihood ratio is also derived on condition that there are at least two redundant hidden units to realize the true function. 
Statistical Convergence of Kernel CCA| Abstract While kernel canonical correlation analysis (kernel CCA) has been applied in many problems, the asymptotic convergence of the functions estimated from a finite sample to the true functions has not yet been established.  This paper gives a rigorous proof of the statistical convergence of kernel CCA and a related method (NOCCO), which provides a theoretical justification for these methods.  The result also gives a sufficient condition on the decay of the regularization coefficient in the methods to ensure convergence. 
Asymptotic Theory of Locally Conic Models and its Application to Multilayer Neural Networks| Abstract.  This paper discusses the maximum likelihood estimation in a statistical model with unidentiability, using the framework of conic singularity.  The likelihood ratio may diverge in unidentiable cases, though in regular cases it converges to a ! 2 distribution.  A useful su!cient condition of such divergence is obtained, and is applied to neural networks.  The exact order for multilayer perceptrons is also discussed. 
Dimensionality Reduction for Supervised Learning with Reproducing Kernel Hilbert Spaces| Abstract We propose a novel method of dimensionality reduction for supervised learning problems.  Given a regression or classification problem in which we wish to predict a response variable Y from an explanatory variable X , we treat the problem of dimensionality reduction as that of finding a low-dimensional "effective subspace" of X which retains the statistical relationship between X and Y .  We show that this problem can be formulated in terms of conditional independence.  To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on a reproducing kernel Hilbert space.  This characterization allows us to derive a contrast function for estimation of the effective subspace.  Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X , nor a parametric model of the conditional distribution of Y .  We present experiments that compare the performance of the method with conventional methods. 
Probabilistic design of layered neural networks based on their unified framework|
Local Minima and Plateaus in Hierarchical Structures of Multilayer Perceptrons|
Likelihood Ratio of Unidentifiable Models and Multilayer Neural Networks| The Annals of Stastistics.  To be published. 
Active Learning in Multilayer Perceptrons|
999) Generalization error of linear neural networks in unidentiable cases|
"996) A regularity condition of the information matrix of a multilayer perceptron network|
Generalization error of linear neural networks in unidentifiable cases|
A regularity condition of information matrix of a multilayer perceptron network|
Likelihood ratio of unidentifiable models and multilayer neural networks, Research Memorandum 780|
Statistical analysis of unidentifiable models and its application to multilayer neural networks, Memo at Post-|
Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons|
Adaptive natural gradient learning algorithms for various stochastic models,|
"A globally coupled map model for information processing|
Associative memory using spatiotemporal chaos|
Statistical active learning in multilayer perceptrons|
A kernel method for hierarchical and non-hierarchical clustering|
Consistency of kernel canonical correlation|

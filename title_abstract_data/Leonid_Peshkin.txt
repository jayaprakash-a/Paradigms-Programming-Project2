Learning Policies with External Memory| Abstract In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations.  In this paper, we explore a stigmergic approach, in which the agent's actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent.  In this case, we need to learn a reactive policy in a highly non-Markovian domain.  We explore two algorithms: sarsa(#), which has had empirical success in partially observable domains, and vaps, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains.  We compare the performance of these two algorithms on benchmark problems. 
Context-based policy search: transfer of experience across problems| Abstract An important question in reinforcement learning is how generalization may be performed.  This problem is especially important if the learning agent receives only partial information about the state of the environment.  Typically, the bias required for generalization is chosen by the experimenter.  Here, we investigate a way for the learning method to extract bias from learning one problem and apply it in subsequent problems.  We use a gradient-based policy search method, and look for controllers that consist of a context component and an action component.  Empirical results on a two-agent coordination problem are reported.  It was found that learning a bias made it possible to address problems that were not solved otherwise. 
Factored Particles for Scalable Monitoring| Abstract Exact monitoring in dynamic Bayesian networks is intractable, so approximate algorithms are necessary.  This paper presents a new family of approximate monitoring algorithms that combine the best qualities of the particle filtering and Boyen-Koller methods.  Our algorithms maintain an approximate representation the belief state in the form of sets of factored particles, that correspond to samples of clusters of state variables.  Empirical results show that our algorithms outperform both ordinary particle filtering and the Boyen-Koller algorithm on large systems. 
Bayesian Nets in Syntactic Categorization of Novel Words| Abstract This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of-Speech (PoS) tags to novel text.  This task is particularly challenging for non-standard corpora, such as Internet lingo, where a large proportion of words are unknown.  Previous work reveals that PoS tags depend on a variety of morphological and contextual features.  Representing these dependencies in a DBN results into an elegant and effective PoS tagger. 
Segmentation of yeast DNA using hidden Markov models| Abstract Motivation: Compositionally homogeneous segments of genomic DNA often correspond to meaningful biological units.  Simple sliding window analysis is usually insucient for compositional segmentation of natural sequences.  Hidden Markov Models (HMM) with a small number of states are a natural language for description of compositional properties of chromosome-size DNA sequences.  Results: The algorithms were applied to yeast Saccharomyces cerevisiae chromosomes (YC) I, III, IV, VI and IX.  The optimal number of HMM states is found to be four.  The optimal 4-state HMMs for all chromosomes are very similar, as well as the reconstructed segmentations.  In most cases the models with k+1 states are obtained by \splitting" one of the states in the model with k states, and the corresponding increase of the level of detail in segmentation.  The high AT states usually correspond to intergenic regions.  We also explore the model's likelihood landscape and analyze the dynamics of the optimization process, thus addressing the problem of reliability of the obtained optima and eciency of the algorithms.  # To whom correspondence should be addressed.  Availability: The system is available on request from the first author. 
Learning from Scarce Experience| Abstract Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems.  Typically, with each change of the target policy, its value is estimated from the results of following that very policy.  This requires a large number of interactions with the environment as different polices are considered.  We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy.  The algorithms combine estimation and optimization stages.  The former utilizes experience to build a non-parametric representation of an optimized function.  The latter performs optimization on this estimate.  We show positive empirical results and provide the sample complexity bound. 
Bayesian Information Extraction Network| Abstract Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various aspects of language in one model.  Many existing algorithms developed for learning and inference in DBNs are applicable to probabilistic language modeling.  To demonstrate the potential of DBNs for natural language processing, we employ a DBN in an information extraction task.  We show how to assemble wealth of emerging linguistic instruments for shallow parsing, syntactic and semantic tagging, morphological decomposition, named entity recognition etc.  in order to incrementally build a robust information extraction system.  Our method outperforms previously published results on an established benchmark domain.  1 Information Extraction Information extraction (IE) is the task of filling in template information from previously unseen text which belongs to a pre-defined domain.  The resulting database is suited for formal queries and filtering.  IE systems generally work by detecting patterns in the text that help identify significant information.  Researchers have shown [Freitag and McCallum, 1999; Ray and Craven, 2001] that a probabilistic approach allows the construction of robust and well-performing systems.  However, the existing probabilistic systems are generally based on Hidden Markov Models (HMMs).  Due to this relatively impoverished representation, they are unable to take advantage of the wide array of linguistic information used by many non-probabilistic IE systems.  In addition, existing HMM-based systems model each target category separately, failing to capture relational information, such as typical target order, or the fact that each element only belongs to a single category.  This paper shows how to incorporate a wide array of knowledge into a probabilistic IE system, based on dynamic Bayesian networks (DBN)---a rich probabilistic representation that generalizes HMMs.  Let us illustrate IE by describing seminar announcements which got established as one of the most popular benchmark domains in the field [Califf and Mooney, 1999; Freitag and McCallum, 1999; Soderland, 1999; Roth and Yih, 2001; Ciravegna, 2001] .  People receive dozens of seminar announcements weekly and need to manually extract information and paste it into personal organizers.  The goal of an IE system is to automatically identify target fields such as location and topic of a seminar, date and starting time, ending time and speaker.  Announcements come in many formats, but usually follow some pattern.  We often find a header with a gist in the form "PostedBy:
Learning to Cooperate via Policy Search|
Learning Finite-State Controllers for Partially Observable Environments|
Exploration in gradient-based reinforcement learning|
Part-of-Speech Tagging with Dynamical Bayesian Network|
Solving Very Large Weakly Coupled Markov Decision Processes|
Reinforcement learning for adaptive routing|
Searching for finite-state POMDP controllers|
Baysian information extraction network|
Architectures for policy search|

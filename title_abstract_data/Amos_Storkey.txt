Increasing the Capacity of a Hopfield Network without Sacrificing Functionality| Abstract.  Hopfield networks are commonly trained by one of two algorithms.  The simplest of these is the Hebb rule, which has a low absolute capacity of n=(2 ln n), where n is the total number of neurons.  This capacity can be increased to n by using the pseudo-inverse rule.  However, capacity is not the only consideration.  It is important for rules to be local (the weight of a synapse depends ony on information available to the two neurons it connects), incremental (learning a new pattern can be done knowing only the old weight matrix and not the actual patterns stored) and immediate (the learning process is not a limit process).  The Hebbian rule is all of these, but the pseudo-inverse is never incremental, and local only if not immediate.  The question addressed by this paper is, `Can the capacity of the Hebbian rule be increased without losing locality, incrementality or immediacy?' Here a new algorithm is proposed.  This algorithm is local, immediate and incremental.  In addition it has an absolute capacity significantly higher than that of the Hebbian method: n= p 2 ln n.  In this paper the new learning rule is introduced, and a heuristic calculation of the absolute capacity of the learning algorithm is given.  Simulations show that this calculation does indeed provide a good measure of the capacity for finite network sizes.  Comparisons are made between the Hebb rule and this new learning rule. 
The Basins of Attraction of a New Hopfield Learning Rule| Abstract The nature of the basins of attraction of a Hopfield network is as important as the capacity.  Here a new learning rule is re-introduced.  This learning rule has a higher capacity than that of the Hebb rule, and still keeps important functionality, such as incrementality and locality, which the pseudo-inverse lacks.  However the basins of attraction of the fixed points of this learning rule have not yet been studied.  Three important characteristics of basins of attraction are considered: indirect and direct basins of attraction, distribution of sizes of basins of attraction and the shape of the basins of attraction.  The results for the new learning rule are compared with those of the Hebb rule.  The size of direct and indirect basins of attractions are generally larger for the new rule than for the Hebb rule, the distribution of sizes is more even, and the shape of the basins more round. 
Workshop Programme Computer-Supported Mathematical Theory Development| Mathematical reasoning tools, such as computer algebra systems, theorem provers, decision procedures, etc. , are increasingly employed in mathematics and engineering.  Also large repositories of formalized mathematics are currently emerging.  It is nevertheless the case that the actual pragmatics of mathematics is still to be characterized as mainly pen and paper based.  One reason is that still no convincing systems exist that provide a sufficiently integrated support for the usual work phases of a mathematician, e. g.  from initial conception and organization of ideas up to the final publication in a journal article.  A special focus of the workshop is on computer-support for the development of mathematical theories.  Mathematical theory development describes the formulation, organization, manipulation, and maintenance of mathematical content.  Support for adequate interaction with the (human) mathematician is mandatory in this context.  Thus, computer-supported mathematical theory development comprises: .  the formulation of mathematical statements in a computer-processable form, .  computer-support in processing mathematical content; depending on the content, this can mean "proving", "computing", "solving", "visualizing", "checking", "simulating", "conjecturing", etc.  .  the systematic organization and maintenance in and the powerful retrieval of mathematical knowledge from computer-accessible media, .  the management of change in the development of mathematical knowledge, .  the publication and presentation of mathematical material using new and/or wellestablished computer-based publication or presentation formats, and .  the interaction between the human mathematician and the supporting software.  The workshop addresses the design and implementation of frameworks aiming at integrated support for the entire process of theory development.  Clearly, there is still a big gap between the systems envisioned and the systems already available and this gap has to be overcome in the future.  Therefore also partial solutions are welcome if their relevance for the bigger vision can be illustrated. 
Formalizing Abstract Mathematics: Issues and Progress| Editorial This report contains the proceedings of the IJCAR 2004 Workshop 7 on ComputerSupported Mathematical Theory Development, held July 5, 2004 in Cork, Ireland.  Mathematical reasoning tools, such as computer algebra systems, theorem provers, decision procedures, etc. , are increasingly employed in mathematics and engineering.  Also large repositories of formalized mathematics are currently emerging.  It is nevertheless the case that the actual pragmatics of mathematics is still to be characterized as mainly pen and paper based.  One reason is that still no convincing systems exist that provide a sufficiently integrated support for the usual work phases of a mathematician, e. g.  from initial conception and organization of ideas up to the final publication in a journal article.  A special focus of the workshop is on computer-support for the development of mathematical theories.  Mathematical theory development describes the formulation, organization, manipulation, and maintenance of mathematical content.  Support for adequate interaction with the (human) mathematician is mandatory in this context.  Thus, computer-supported mathematical theory development comprises: .  the formulation of mathematical statements in a computer-processable form, .  computer-support in processing mathematical content; depending on the content, this can mean "proving", "computing", "solving", "visualizing", "checking", "simulating", "conjecturing", etc.  .  the systematic organization and maintenance in and the powerful retrieval of mathematical knowledge from computer-accessible media, .  the management of change in the development of mathematical knowledge, .  the publication and presentation of mathematical material using new and/or wellestablished computer-based publication or presentation formats, and .  the interaction between the human mathematician and the supporting software.  The workshop addresses the design and implementation of frameworks aiming at integrated support for the entire process of theory development.  Clearly, there is still a big gap between the systems envisioned and the systems already available and this gap has to be overcome in the future.  Therefore also partial solutions are welcome if their relevance for the bigger vision can be illustrated. 
Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data| Abstract Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT).  However there are many situations where doing fast Fourier transforms without complete data would be desirable.  In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coefficients.  Furthermore efficient generalised belief propagation methods between clusters of four nodes enable the Fourier coefficients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points.  This method is compared with a number of common approaches such as setting missing data to zero or to interpolation.  It is tested on generated data and for a Fourier analysis of a damaged audio signal. 
A Hopfield learning rule with high capacity storage of time-correlated patterns| Abstract A new local and incremental learning rule is examined for its ability to store patterns from a time series in an attractor neural network.  This learning rule has a higher capacity than the Hebb rule, and suffers significantly less capacity loss as the correlation between patterns increases. 
Dynamic Trees: A Structured Variational Method Giving Efficient Propagation Rules| Abstract Dynamic trees are mixtures of tree structured belief networks.  They solve some of the problems of fixed tree networks at the cost of making exact inference intractable.  For this reason approximate methods such as sampling or mean field approaches have been used.  However, mean field approximations assume a factorised distribution over node states.  Such a distribution seems unlikely in the posterior, as nodes are highly correlated in the prior.  Here a structured variational approach is used, where the posterior distribution over the non-evidential nodes is itself approximated by a dynamic tree.  It turns out that this form can be used tractably and eciently.  The result is a set of update rules which can propagate information through the network to obtain both a full variational approximation, and the relevant marginals.  The propagation rules are more ecient than the mean field approach and give noticeable quantitative and qualitative improvement in the inference.  The marginals calculated give better approximations to the posterior than loopy propagation on a small toy problem. 
Dynamic Positional Trees for Structural Image Analysis| Abstract Dynamic positional trees are a significant extension of dynamic trees, incorporating movable nodes.  This addition makes sequence tracking viable within the model, but requires a new formulation to incorporate the prior over positions.  The model is implemented using a structured variational procedure, and is illustrated on synthetic raytraced images and image sequences.  We consider the problem of structural image analysis and in particular the inference of scene properties from image data.  We are especially concerned with image decomposition, that is obtaining the characteristic parts of an image and the relationships between them.  The components of an image are not independent of each other; certain objects are expected to occur together, and objects are made up of different subcomponents.  One way of thinking of this problem is by analogy with parsing a language; we are interested in parsing images.  However, the important characteristics and structure in an image is significantly different from linguistic data.  Those familiar with work on dynamic trees will be aware that they have been developed in the context of single static images [15, 1, 13].  It would be desirable if the benefits of the dynamic tree approach could also be made available for image sequences.  Introducing a sequence model into the basic dynamic tree formalism is not straightforward as a change in the position of an object is reflected in a change in the connectivity structure of the dynamic tree.  This change would be hard to predict from the previous time slice and would be an inelegant representation of the dynamics: the connectivity structure is supposed to represent the structural characteristics of an object, most of which will be preserved during movement.  Here the dynamic tree is modified to incorporate position variables, resulting in a model where object movement can be represented in terms of a change in position components of the nodes representing that object.  The structure of the remainder of the paper is as follows.  The first section of this paper develops some of the issues surrounding image analysis in general and then outlines the form of the dynamic positional tree, and the rationale behind its design.  This leads in to a more formal definition of the dynamic tree model in section 2, and we discuss related models in section 3.  Defining a Bayesian model is one thing, being able to implement it is another.  In section 4, we take a variational approach to the implementation problem and give a set of structured variational approximations which can be calculated eciently, and which have the structural information we need.  The resulting set of update equations are given in section 5.  Illustrations of the approach appear in section 7, after a brief discussion of the issue of sequences (section 6). 
Gaussian Processes for Switching Regimes| Abstract.  It has been shown that Gaussian processes are a competitive tool for nonparametric regression and classification.  Furthermore they are equivalent to neural networks in the limit of an infinite number of neurons.  Here we show that the versatility of Gaussian processes at defining different textural characteristics can be used to recognise different regimes in a signal switching between different sources. 
Dynamic Structure Super-Resolution| Abstract The problem of super-resolution involves generating feasible higher resolution images, which are pleasing to the eye and realistic, from a given low resolution image.  This might be attempted by using simple filters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images.  In this paper we describe an approach which lies between the two extremes.  It is a generic unsupervised method which is usable in all domains, but goes beyond simple smoothing methods in what it achieves.  We use a dynamic tree-like architecture to model the high resolution data.  Approximate conditioning on the low resolution image is achieved through a mean field approach. 
Palimpsest memories: a new high-capacity forgetful learning rule for Hopfield networks| illustrate the performance of the learning rule. 
MFDTs: Mean Field Dynamic Trees| Abstract Tree structured belief networks are attractive for image segmentation tasks.  However, networks with fixed architectures are not very suitable as they lead to blocky artefacts, and led to the introduction of Dynamic Trees (DTs) in [6].  The Dynamic Trees architecture provides a prior distribution over tree structures, and in [6] simulated annealing (SA) was used to search for structures with high posterior probability.  In this paper we introduce a mean field approach to inference in DTs.  We find that the mean field method captures the posterior better than just using the maximum a posteriori solution found by SA. 
Scientific Data Mining, Integration, and Visualization| This report summarises the workshop on Scientific Data Mining, Integration and Visualization (SDMIV) held at the e-Science Institute, Edinburgh (eSI[1] ) on 24-25 October 2002, and presents a set of recommendations arising from the discussion that took place there.  The aims of the workshop were three-fold: (A) To inform researchers in the SDMIV communities of the infrastructural advances being made by computing initiatives, such as the Grid; (B) To feed back requirements from the SDMIV areas to those developing the computational infrastructure; and (C) To foster interaction among all these communities, since the coordinated efforts of all of them will be required to realise the potential for scientific knowledge extraction offered by e-science initiatives worldwide.  The workshop had about fifty participants, ranging from software engineers developing Grid infrastructure software, to computer scientists with expertise in data mining and visualization, to application specialists from a wide range of disciplines,
Truncated Covariance Matrices and Toeplitz Methods in Gaussian Processes| Abstract Gaussian processes are a limit extension of neural networks.  Standard Gaussian process techniques use a squared exponential covariance function.  Here, the use of truncated covariances is proposed.  Such covariances have compact support.  Their use speeds up matrix inversion and increases precision.  Furthermore they allow the use of speedy, memory efficient Toeplitz inversion for high dimensional grid based Gaussian process predictors. 
Renewal Strings for Cleaning Astronomical Databases| Abstract Large astronomical databases obtained from sky surveys such as the SuperCOSMOS Sky Surveys (SSS) invariably suffer from spurious records coming from artefactual effects of the telescope, satellites and junk objects in orbit around earth and physical defects on the photographic plate or CCD.  Though relatively small in number these spurious records present a significant problem in many situations where they can become a large proportion of the records potentially of interest to a given astronomer.  We have developed renewal strings, a probabilistic technique combining the Hough transform, renewal processes and hidden Markov models which has proven highly effective in this context.  The methods are applied to the SSS data to develop a dataset of spurious object detections, along with confidence measures, which can allow this unwanted data to be removed from consideration.  These methods are general and can be adapted to any other astronomical survey data. 
Cleaning Sky Survey Databases using Hough Transform and Renewal String Approaches| Received ---; in original form --ABSTRACT ABSTRACT Large astronomical databases obtained from sky surveys such as the SuperCOSMOS Sky Survey (SSS) invariably suffer from spurious records coming from artefactual effects of the telescope, satellites and junk objects in orbit around earth and physical defects on the photographic plate or CCD.  Though relatively small in number these spurious records present a significant problem in many situations where they can become a large proportion of the records potentially of interest to a given astronomer.  Accurate and robust techniques are needed for locating and flagging such spurious objects, and we are undertaking a programme investigating the use of machine learning techniques in this context.  In this paper we focus on the four most common causes of unwanted records in the SSS: satellite or aeroplane tracks, scratches, fibres and other linear phenomena introduced to the plate, circular halos around bright stars due to internal reflections within the telescope and diffraction spikes near to bright stars.  Appropriate techniques are developed for the detection of each of these.  The methods are applied to the SSS data to develop a dataset of spurious object detections, along with confidence measures, which can allow these unwanted data to be removed from consideration.  These methods are general and can be adapted to other astronomical survey data. 
Comparing Mean Field and Exact EM in Tree Structured Belief Networks| Abstract We make a thorough comparison between a variationally-based learning approach and exact EM using tractable fixed architecture tree-structured belief networks, and so gain valuable insights into learning with mean field methods.  We then introduce disconnections into the model showing how they can be folded into a single structure by viewing them as degeneracies in the conditional probability tables, and investigate learning with them.  The results suggest that mean field performs suciently well to be useful in learning in more complex models where standard approaches are intractable. 
Position Encoding Dynamic Trees for Image Sequence Analysis| Abstract Position Encoding Dynamic Trees have been demonstrated to be useful models for individual images.  In the theory and experiments presented here, we show that the PEDT models can be used for the analysis of ordered sequences of images.  We provide a theoretical framework in which to update the current prior model sequentially using the variational approximation to the posterior probability distribution from the previous time frame.  We present experimental results demonstrating improved segmentation performance using the new approach. 
NIPS 2002 Vancouver DRAFT Paper Please only refer to final paper Dynamic Structure Super-Resolution| Abstract The problem of super-resolution involves generating feasible higher resolution images, which are pleasing to the eye and realistic, from a given low resolution image.  This might be attempted by using simple filters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images.  In this paper we describe an approach which lies between the two extremes.  It is a generic unsupervised method which is usable in all domains, but goes beyond simple smoothing methods in what it achieves.  We use a dynamic tree-like architecture to model the high resolution data.  Approximate conditioning on the low resolution image is achieved through a mean field approach. 
Dynamic positional trees for structural image analysis|
Image Modeling with Position-Encoding Dynamic Trees|

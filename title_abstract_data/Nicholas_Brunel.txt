Optimal tuning curves for neurons spiking as a Poisson process| Abstract.  We calculate the information capacity of a neuron emitting as a Poisson process in response to a static stimulus, and the stimulus distribution required to reach the capacity, in the case of a constraint on the average frequency.  These optimal stimulus distributions (i. e.  the ones reaching the information capacity) are then reexpressed in terms of `tuning curves' for neurons with a continuous response to a scalar stimulus. 
COPULAS IN VECTORIAL HIDDEN MARKOV CHAINS FOR MULTICOMPONENT IMAGE SEGMENTATION| ABSTRACT Parametric estimation of non-Gaussian multidimensional probability density function (pdf) is a difficult problem that is required by many applications in signal and image processing.  A lot of efforts has been devoted to methods from multivariate analysis such as Principal or Independent Component Analysis (PCA and ICA).  In this work, we introduce an alternative solution based on a very general class of multivariate models called `copulas'.  Useful copulas models for image classification are used in the frame of multidimensional mixture estimation arising in the segmentation of multicomponent images, when using a vectorial Hidden Markov Chain (HMC). 
Dynamics of a recurrent network of spiking neurons before and following learning| Abstract Extensive simulations of large recurrent networks of
Hebbian learning of context in recurrent neural networks| Abstract Single electrode recordings in inferotemporal cortex of monkeys during delayed visual memory tasks provide evidence for attractor dynamics in the observed region.  The persistent elevated delay activities could be internal representations of features of the learned visual stimuli shown to the monkey during training.  When uncorrelated stimuli are presented during training in a fixed sequence, these experiments display significant correlations between the internal representations.  Recently a simple model of attractor neural network has reproduced quantitatively the measured correlations.  An underlying assumption of the model is that the synaptic matrix formed during the training phase contains in its efficacies information about the contiguity of persistent stimuli in the training sequence.  We present here a simple unsupervised learning dynamics which produces such a synaptic matrix if sequences of stimuli are repeatedly presented to the network at fixed order.  The resulting matrix is then shown to convert temporal correlations during training into spatial correlations between attractors.  The scenario is that, in presence of selective delay activity, at the presentation of each stimulus, the activity distribution in the neural assembly contains information both of the current stimulus as well as of the previous one (carried by the attractor).  Thus the recurrent synaptic matrix can code not only for each of the stimuli which were presented to the network, but also for their context.  We combine the idea that for learning to be effective synaptic modification should be stochastic, with the fact that attractors provide learnable information about two consecutive stimuli.  We calculate explicitly the probability distribution of synaptic efficacies as a function of training protocol, i. e.  the order in which stimuli are presented to the network.  We then solve for the dynamics of a network composed of integrate-and-fire excitatory and inhibitory neurons with a matrix of synaptic collaterals resulting from the learning dynamics.  The network has a stable spontaneous activity, and stable delay activity develops after a critical learning stage.  The availability of a learning dynamics makes possible a number of experimental predictions for the dependence of the delay activity distributions and the correlations between them, on the learning stage and the learning protocol.  In particular it makes specific predictions for pair-associates delay experiments. 
Global spontaneous activity and local structured (learned) delay activity in cortex| 1.  We investigate the conditions under which cortical activity alone makes spontaneous activity self-reproducing and stable against fluctuations of spike rates.  Invoking simple assumptions about properties of integrateand-fire neurons it is shown that the stochastic background activity, of 1-5 spikes/second, cannot be stabilized when all neurons are excitatory.  2.  On the other hand, spontaneous activity becomes self-stabilizing in presence of local inhibition: given reasonable values of the parameters of the network spontaneous activity reproduces itself and small fluctuations in the rate are suppressed.  a.  If the integration time constants of excitatory and inhibitory neurons at the soma are equal, local excitatory and inhibitory inputs to a neuron must balance to provide local stablility.  b.  If inhibition integrates faster its synaptic inputs, spontaneous activity is stable even when local recurrent excitation predominates.  3.  In a network sustaining spontaneous rates of 1-5 spikes/second, we study the effect of learning in a local module, expressed in synaptic modifications in specific populations of synapses.  We find: a.  Initially no stimulus specific delay activity manifests itself.  Instead, following the presentation and removal of any stimulus there is, in the local module, a delay activity in which all neurons selective (responding visually) to any of the stimuli presented for learning have rates which gradually increase with the amplitude of synaptic potentiation.  b.  When the average LTP increases beyond a critical value, specific local attractors appear abruptly against the background of the global uniform spontaneous attractor.  This happens with either gradual or discrete stochastic LTP.  4.  The above findings predict that in the process of learning unfamiliar stimuli, there is a stage in which all neurons selective to any of the learned stimuli enhance their spontaneous activity relative to the rest.  Then, abruptly, selective delay activity appears.  Both facts could be observed in single unit recordings in delayed match to sample experiments.  5.  Beyond this critical learning strength the local module has two types of collective activity.  It either participates in the global spontaneous activity, or it maintains a stimulus selective elevated activity distribution following the removal of the stimulus.  The particular mode of behavior depends on the stimulus: if it is unfamiliar, the activity is spontaneous; if similar to a learned stimulus, the delay activity is selective.  These new attractors (delay activities) reflect the synaptic structure developed during learning.  In each of them a small population of neurons have elevated rates, 20-30 spikes/second, depending on the strength of LTP.  The remaining neurons of the module have their activity at spontaneous rates. 
Mutual Information, Fisher Information and Population Coding| Abstract In the context of parameter estimation and model selection, it is only quite recently that a direct link between the Fisher information and information theoretic quantities has been exhibited.  We give an interpretation of this link within the standard framework of information theory.  We show that in the context of population coding, the mutual information between the activity of a large array of neurons and a stimulus to which the neurons are tuned is naturally related to the Fisher information.  In the light of this result we consider the optimization of the tuning curves parameters in the case of neurons responding to a stimulus represented by an angular variable. 
Adequate input for learning in attractor neural networks| Abstract In the context of learning in attractor neural networks (ANN) we discuss the issue of the constraints imposed by the requirement that the afferents arriving at the neurons in the attractor network from the stimulus, compete successfully with the afferents generated by the recurrent activity inside the network, in a situation in which the both sets of synaptic efficacies are weak and approximately equal.  We simulate and analyze a two component network: one representing the stimulus, the other an ANN.  We show that if stimuli are correlated with the receptive fields of neurons in the ANN, and are of sufficient contrast, the stimulus can provide the necessary information to the recurrent network to allow learning new stimuli, even in very disfavored situation of synaptic predominance in the recurrent part.  Stimuli which are insufficiently correlated with the receptive fields, or are of insufficient contrast, are submerged by the recurrent activity. 
Dynamics of an attractor neural network converting temporal into spatial correlations| Abstract The dynamics of a model attractor neural network, dominated by collateral
Learning internal representations in an attractor neural network with analogue neurons| Abstract A learning attractor neural network (LANN) with a double dynamics of neural activities and synaptic efficacies, operating on two different time scales is studied by simulations in preparation for an electronic implementation.  The present network includes several quasi-realistic features: neurons are represented by their afferent currents and output spike rates; excitatory and inhibitory neurons are separated; attractor spike rates as well as coding levels in arriving stimuli are low; learning takes place only between excitatory units.  Synaptic dynamics is an unsupervised, analog Hebbian process, but long term
Persistent activity and the single-cell frequency--current curve in a cortical network model| Abstract.  Neurophysiological experiments indicate that working memory of an object is maintained by the persistent activity of cells in the prefrontal cortex and infero-temporal cortex of the monkey.  This paper considers a cortical network model in which this persistent activity appears due to recurrent synaptic interactions.  The conditions under which the magnitude of spontaneous and persistent activity are close to one another (as is found empirically) are investigated using a simplified mean-field description in which firing rates in these states are given by the intersections of a straight line with the f--I curve of a single pyramidal cell.  The present analysis relates a network phenomenon---persistent activity in a `working memory' state---to single-cell data which are accessible to experiment.  It predicts that, in networks of the cerebral cortex in which persistent activity phenomena are observed, average synaptic inputs in both spontaneous and persistent activity should bring the cells close to firing threshold.  Cells should be slightly sub-threshold in spontaneous activity, and slightly supra-threshold in persistent activity.  The results are shown to be robust to the inclusion of inhomogeneities that produce wide distributions of firing rates, in both spontaneous and working memory states. 
Firing Rate of the Noisy Quadratic Integrate-and-Fire Neuron| We calculate the ring rate of the quadratic integrate-and-re neuron in response to a colored noise input current.  Such an input current is a good approximation to the noise due to the random bombardment of spikes, with the correlation time of the noise corresponding to the decay time of the synapses.  The key parameter that determines the ring rate is the ratio of the correlation time of the colored noise, s , to the neuronal time constant, m .  We calculate the ring rate exactly in two limits: when the ratio, s =m , goes to zero (white noise) and when it goes to innity.  The correction to the short correlation time limit is O. s =m /, which is qualitatively different from that of the leaky integrate-and-re neuron, where the correction is O.  p s =m /.  The difference is due to the different boundary conditions of the probability density function of the membrane potential of the neuron at ring threshold.  The correction to the long correlation time limit is O. m =s /.  By combining the short and long correlation time limits, we derive an expression that provides a good approximation to the ring rate over the whole range of s =m in the suprathreshold regime--that is, in a regime in which the average current is sufcient to make the cell re.  In the subthreshold regime, the expression breaks down somewhat when s becomes large compared to m . 
MODELING TEMPORAL DEPENDENCE OF SPHERICALLY INVARIANT RANDOM VECTORS WITH TRIPLET MARKOV CHAINS| ABSTRACT Our paper deals with multivariate hidden Markov chains (MHMC) with a view towards segmentation.  We propose a new model in which temporal dependencies are modelled using copulas and sensor dependencies are represented by Spherically Invariant Random Vector (SIRV).  Copulas are very useful and flexible tools, which have been little applied in signal processing problems until now.  In particular, for some desirable marginal distributions it is possible to obtain different kind of dependencies.  Using some recent results on Triplet Markov chains, the new model extends the case of MHMC when the observations are SIRV and independent conditionally on the states.  We propose algorithms for computing efficiently the posterior probabilities of the involved Triplet Markov Chain, in order to propose rapid segmentation and estimation procedures. 
Response Functions Improving Performance in Analogue Attractor Neural Networks| Abstract In the context of attractor neural networks, we study how the equilibrium analogue neural activities, reached by the network dynamics during memory retrieval, may improve storage performance by reducing the interferences between the recalled pattern and the other stored ones.  We determine a simple dynamics that stabilizes network states which are highly correlated with the retrieved pattern, for a number of stored memories that does not exceed ff ? N , where ff ? 2 [0; 0:41] depends on the global activity level in the network and N is the number of neurons.  87. 
Fast Global Oscillations in Networks of Integrate-and-Fire Neurons with Low Firing Rates|
Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons|
Effects of Neuromodulation in a Cortical Network Model of Object Working Memory Dominated by Recurrent Inhibition|
Dynamics of the Firing Probability of Noisy Integrate-and-Fire Neurons|
Firing Rate of the Noisy Quadratic Integrate-and-Fire Neuron|
An objective comparison of model output statistics and perfect prog systems in producing numerical weather element forecasts|
Development of the 35-km version of the operational regional forecast system|

Machine learning approaches for person identification and verification| ABSTRACT New machine learning strategies are proposed for person identification which can be used in several biometric modalities such as friction ridges, handwriting, signatures and speech.  The biometric or forensic performance task answers the question of whether or not a sample belongs to a known person.  Two different learning paradigms are discussed: person-independent (or general learning) and person-dependent (or person-specific learning).  In the first paradigm, learning is from a general population of ensemble of pairs, each of which is labelled as being from the same person or from different persons-- the learning process determines the range of variations for given persons and between different persons.  In the second paradigm the identity of a person is learnt when presented with multiple known samples of that person-- where the variation and similarities within a particular person are learnt.  The person-specific learning strategy is seen to perform better than general learning (5% higher performace with signatures).  Improvement of person-specific performance with increasing number of samples is also observed. 
A Bayesian approach to reconstructing genetic regulatory networks with hidden factors| ABSTRACT Motivation: We have used state-space models (
The Variational Bayesian EM Algorithm for Incomplete Data: with Application to Scoring Graphical Model Structures| SUMMARY We present an efficient procedure for estimating the marginal likelihood of probabilistic models with latent variables or incomplete data.  This method constructs and optimises a lower bound on the marginal likelihood using variational calculus, resulting in an iterative algorithm which generalises the EM algorithm by maintaining posterior distributions over both latent variables and parameters.  We define the family of conjugate-exponential models---which includes finite mixtures of exponential family models, factor analysis, hidden Markov models, linear state-space models, and other models of interest---for which this bound on the marginal likelihood can be computed very simply through a modification of the standard EM algorithm.  In particular, we focus on applying these bounds to the problem of scoring discrete directed graphical model structures (Bayesian networks).  Extensive simulations comparing the variational bounds to the usual approach based on the Bayesian Information Criterion (BIC) and to a sampling-based gold standard method known as Annealed Importance Sampling (AIS) show that variational bounds substantially outperform BIC in finding the correct model structure at relatively little computational cost, while approaching the performance of the much more costly AIS procedure.  Using AIS allows us to provide the first serious case study of the tightness of variational bounds.  We also analyse the perfomance of AIS through a variety of criteria, and outline directions in which this work can be extended. 
A Statistical Model For Writer Verification| Abstract A statistical model for determining whether a pair of documents, a known and a questioned, were written by the same individual is proposed.  The model has the following four components: (i) discriminating elements, e. g. , global features and characters, are extracted from each document, (ii) differences between corresponding elements from each document are computed, (iii) using conditional probability estimates of each difference, the log-likelihood ratio (LLR) is computed for the hypotheses that the documents were written by the same or different writers; the conditional probability estimates themselves are determined from labelled samples using either Gaussian or gamma estimates for the differences assuming their statistical independence, and (iv) distributions of the LLRs for same and different writer LLRs are analyzed to calibrate the strength of evidence into a standard nine-point scale used by questioned document examiners.  The model is illustrated with experimental results for a specific set of discriminating elements. 
Audio-Video Sensor Fusion with Probabilistic Graphical Models| Abstract.  We present a new approach to modeling and processing multimedia data.  This approach is based on graphical models that combine audio and video variables.  We demonstrate it by developing a new algorithm for tracking a moving object in a cluttered, noisy scene using two microphones and a camera.  Our model uses unobserved variables to describe the data in terms of the process that generates them.  It is therefore able to capture and exploit the statistical structure of the audio and video data separately, as well as their mutual dependencies.  Model parameters are learned from data via an EM algorithm, and automatic calibration is performed as part of this procedure.  Tracking is done by Bayesian inference of the object location from data.  We demonstrate successful performance on multimedia clips captured in real world scenarios using off-the-shelf equipment. 
Variational Inference for Bayesian Mixtures of Factor Analysers| Abstract We present an algorithm that infers the model structure of a mixture of factor analysers using an ecient and deterministic variational approximation to full Bayesian integration over model parameters.  This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i. e.  the number of factors in each factor analyser).  Alternatively it can be used to infer posterior distributions over number of components and dimensionalities.  Since all parameters are integrated out the method is not prone to overfitting.  Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima.  Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples.  By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exact predictive density, and the KL divergence between the variational posterior and the true posterior, not only in this model but for variational approximations in general. 
The Variational Kalman Smoother| Abstract In this note we outline the derivation of the variational Kalman smoother, in the context of Bayesian Linear Dynamical Systems.  The smoother is an efficient algorithm for the E-step in the ExpectationMaximisation (EM) algorithm for linear-Gaussian state-space models.  However, inference approximations are required if we hold distributions over parameters.  We derive the E-step updates for the hidden states (the variational smoother), and the M-step updates for the parameter distributions.  We show that inference of the hidden state is tractable for any distribution over parameters, provided the expectations of certain quantities are available, analytically or otherwise. 
The Infinite Hidden Markov Model| Abstract We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states.  By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data.  These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics.  The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence.  In this framework it is also natural to allow the alphabet of emitted symbols to be infinite--consider, for example, symbols being possible words appearing in English text. 
Signature verification using Kolmogorov-Smirnov statistic| Abstract Automatic signature verification of scanned documents are presented here.  The strategy used for verification is applicable in scenarios where there are multiple knowns(genuine signature samples) from a writer.  First the learning process invovles learning the variation and similarities from the known genuine samples from the given writer and then classification problem answers the question whether or not a given questioned sample belongs to the ensemble of known samples or not.  The learning strategy discussed, compares pairs of signature samples from amongst the knwon samples, to obtain a distribution in distance space, that represents the distribution of the variation amongst samples, for that particular writer.  The corresponding classification method involves comparing the questioned sample, with all the available knowns, to obtain another distribution in distance space.  The classification task is now to compare the two distributions to obtain a probability of similarity of the two distributions, that represents, the probability of the questioned sample belonging to the ensemble of the knowns.  The above strategies are applied to the problem of signature verification and performance results are presented. 
Hierarchical Dirichlet Processes| Abstract We consider problems involving groups of data, where each observation within a group is a draw from a mixture model, and where it is desirable to share mixture components between groups.  We assume that the number of mixture components is unknown a priori and is to be inferred from the data.  In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group.  Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process.  Such a base measure being discrete, the child Dirichlet processes necessarily share atoms.  Thus, as desired, the mixture models in the different groups necessarily share mixture components.  We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the "Chinese restaurant franchise. " We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures, and describe applications to problems in information retrieval and text modelling. 
Graphical models and variational methods| 1 Abstract We review the use of variational methods of approximating inference and learning in probabilistic graphical models.  In particular, we focus on variational approximations to the integrals required for Bayesian learning.  For models in the conjugate-exponential family, a generalisation of the EM algorithm is derived that iterates between optimising hyperparameters of the distribution over parameters, and inferring the hidden variable distributions.  These approximations make use of available propagation algorithms for probabilistic graphical models.  We give two case studies of how the variational Bayesian approach can be used to learn model structure: inferring the number of clusters and dimensionalities in a mixture of factor analysers, and inferring the dimension of the state space of a linear dynamical system.  Finally, importance sampling corrections to the variational approximations are discussed, along with their limitations. 
Propagation Algorithms for Variational Bayesian Learning| Abstract Variational approximations are becoming a widespread tool for Bayesian learning of graphical models.  We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models.  We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning.  Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters.  We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 
Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models| Abstract We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations.  This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM).  An update begins with the creation of "pools" of candidate states at each time.  We then define an embedded HMM whose states are indexes within these pools.  Using a forward-backward dynamic programming algorithm, we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools.  We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error.  We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 
Variational Algorithms for Approximate Bayesian Inference|
The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures|
Graphical Models and Variational Methods, Advanced Mean Field Method--Theory and Practice|
The variational Kalman smoother|
A Graphical Model for Audiovisual Object Tracking|
Learning model structure|

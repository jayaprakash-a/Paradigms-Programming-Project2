Hierarchical Optimal Control of MDPs| Abstract Fundamental to reinforcement learning, as well as to the
ON THE CURVATURE COLLINEATION IN FINSLER SPACE| Abstract.  We define # x i = x i + v i (x)t as the h-curvature collineation of a Finsler space, supposing that the Lie derivative of Bervald's curvature tensor is equal to zero.  Then we prove that every motion and every homothetic transformation admitted in a Finsler space are H-curvature collineations. 
Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes| Abstract Reinforcement learning (RL) has become a central
Analytical Mean Squared Error Curves in Temporal Difference Learning| Abstract Wehave calculated analytical expressions for how the bias and variance of the estimators provided byvarious temporal difference value estimation algorithms change with offline updates over trials in absorbing Markovchains using lookup table representations.  We illustrate classes of learning curvebehavior in various chains, and show the manner in which TD is sensitive to the choice of its stepsize and eligibility trace parameters. 
Planning with Closed-Loop Macro Actions| Abstract Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence.  In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning.  Conventional model-based reinforcement learning uses primitive actions that last one time step and that can be modeled independently of the learning agent.  These can be generalized to macro actions, multi-step actions specified by an arbitrary policy and a way of completing.  Macro actions generalize the classical notion of a macro operator in that they are closed loop, uncertain, and of variable duration.  Macro actions are needed to represent common-sense higher-level actions such as going to lunch, grasping an object, or traveling to a distant city.  This paper generalizes prior work on temporally abstract models (Sutton 1995) and extends it from the prediction setting to include actions, control, and planning.  We define a semantics of models of macro actions that guarantees the validity of planning using such models.  This paper present new results in the theory of planning with macro actions and illustrates its potential advantages in a gridworld task. 
An Upper Bound on the Loss from Approximate Optimal-Value Functions| Abstract Many reinforcement learning approaches can be formulated using the theory of Markov decision processes and the associated method of dynamic programming (DP).  The value of this theoretical understanding, however, is tempered by many practical concerns.  One important question is whether DP-based approaches that use function approximation rather than lookup tables can avoid catastrophic effects on performance.  This note presents a result of Bertsekas (1987) which guarantees that small errors in the approximation of a task's optimal value function cannot produce arbitrarily bad performance when actions are selected by a greedy policy. We derive an upper bound on performance loss that is slightly tighter than that in Bertsekas (1987), and we show the extension of the bound to Q-learning (Watkins, 1989).  These results provide a partial theoretical rationale for the approximation of value functions, an issue of great practical importance in reinforcement learning. 
An MDP-Based Approach to Online Mechanism Design| Abstract Online mechanism design (MD) considers the problem of providing incentives to implement desired system-wide outcomes in systems with self-interested agents that arrive and depart dynamically.  Agents can choose to misrepresent their arrival and departure times, in addition to information about their value for different outcomes.  We consider the problem of maximizing the total longterm value of the system despite the self-interest of agents.  The online MD problem induces a Markov Decision Process (MDP), which when solved can be used to implement optimal policies in a truth-revealing Bayesian-Nash equilibrium. 
Near-Optimal Reinforcement Learning in Polynominal Time| Abstract We present new algorithms for reinforcement learning, and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes.  After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states, for both the undiscounted and discounted cases.  An interesting aspect of our algorithms is their explicit handling of the ExplorationExploitation trade-off. 
ON APPROXIMATION OF INTEGRABLE FUNCTIONS BY MODIFIED BERNSTEIN POLYNOMIALS| Abstract.  We introduce a class of positive linear operators defined for functions integrable on the simplex # = f(x1 ; x2 ) : x1 # 0; x2 # 0; x1 + x2 # 1g and study some approximations theorems on it. 
Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning| Abstract Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI.  this paper we consider how these challenges can be addressed within the mathematical framework reinforcement learning and Markov decision processes (MDPs).  We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over period of time.  Examples of options include picking up an object, going to lunch, and traveling a distant city, as well as primitive actions such as muscle twitches and joint torques.  Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in natural and general way.  In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning.  Formally, a set of options defined over MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory options.  However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory.  We present results for three such cases: (1) we show that the results planning with options can be used during execution interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able learn about an option from fragments of execution, and (3) we propose notion subgoal that can be used to improve the options themselves.  All of these results have precursors in the existing literature; the contribution of this paper is to establish them in simpler and more general setting with fewer changes to the existing reinforcement learning framework.  In particular, we show that these results can be obtained without committing (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro Corresponding author. 
Policy Gradient Methods for Reinforcement Learning with Function Approximation| Abstract Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable.  In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters.  Williams's REINFORCE method and actor--critic methods are examples of this approach.  Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function.  Using this result, we prove for the first time that a version of policy iteration with arbitrary dierentiable function approximation is convergent to a locally optimal policy.  Large applications of reinforcement learning (RL) require the use of generalizing function approximators such neural networks, decision-trees, or instance-based methods.  The dominant approach for the last decade has been the value-function approach, in which all function approximation eort goes into estimating a value function, with the action-selection policy represented implicitly as the "greedy" policy with respect to the estimated values (e. g. , as the policy that selects in each state the action with highest estimated value).  The value-function approach has worked well in many applications, but has several limitations.  First, it is oriented toward finding deterministic policies, whereas the optimal policy is often stochastic, selecting dierent actions with specific probabilities (e. g. , see Singh, Jaakkola, and Jordan, 1994).  Second, an arbitrarily small change in the estimated value of an action can cause it to be, or not be, selected.  Such discontinuous changes have been identified as a key obstacle to establishing convergence assurances for algorithms following the value-function approach (Bertsekas and Tsitsiklis, 1996).  For example, Q-learning, Sarsa, and dynamic programming methods have all been shown unable to converge to any policy for simple MDPs and simple function approximators (
Nash Convergence of Gradient Dynamics in General-Sum Games| Abstract Multi-agent games are becoming an increasingly prevalent formalism for the study of electronic commerce and auctions.  The speed at which transactions can take place and the growing complexity of electronic marketplaces makes the study of computationally simple agents an appealing direction.  In this work, we analyze the behavior of agents that incrementally adapt their strategy through gradient ascent on expected payoff, in the simple setting of two-player, two-action, iterated general-sum games, and present a surprising result.  We show that either the agents will converge to a Nash equilibrium, or if the strategies themselves do not converge, then their average payoffs will nevertheless converge to the payoffs of a Nash equilibrium. 
Adaptive cognitive orthotics: combining reinforcement learning and constraint-based temporal reasoning| Abstract Reminder systems support people with impaired prospective memory and/or executive function, by providing them with reminders of their functional daily activities.  We integrate temporal constraint reasoning with reinforcement learning (RL) to build an adaptive reminder system and in a simulated environment demonstrate that it can personalize to a user and adapt to both short- and long-term changes.  In addition to advancing the application domain, our integrated algorithm contributes to research on temporal constraint reasoning by showing how RL can select an optimal policy from amongst a set of temporally consistent ones, and it contributes to the work on RL by showing how temporal constraint reasoning can be used to dramatically reduce the space of actions from which an RL agent needs to learn. 
A social reinforcement learning agent| ABSTRACT We report on our reinforcement learning work on Cobot, a software agent that resides in the well-known online chat community LambdaMOO.  Our initial work on Cobot [Isbell et al. 2000] provided him with the ability to collect social statistics and report them to users in a reactive manner.  Here we describe our application of reinforcement learning to allow Cobot to proactively take actions in this complex social environment, and adapt his behavior from multiple sources of human reward.  After 5 months of training, Cobot received 3171 reward and punishmentevents from 254 different LambdaMOO users, and learned nontrivial preferences for a number of users.  Cobot modifies his behavior based on his current state in an attempt to maximize reward.  Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment. 
Predictive Representations of State| Abstract We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations.  State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations.  Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models.  Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls).  We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model.  In predicting or controlling a sequence of observations, the concepts of state and state estimation inevitably arise.  There have been two dominant approaches.  The generative-model approach, typified by research on partially observable Markov decision processes (POMDPs), hypothesizes a structure for generating observations and estimates its state and state dynamics.  The history-based approach, typified by k-order Markov methods, uses simple functions of past observations as state, that is, as the immediate basis for prediction and control.  (The data flow in these two approaches are diagrammed in Figure 1. ) Of the two, the generative-model approach is more general.  The model's internal state gives it temporally unlimited memory| the ability to remember an event that happened arbitrarily long ago|whereas a history-based approach can only remember as far back as its history extends.  The bane of generative-model approaches is that they are often strongly dependent on a good model of the system's dynamics.  Most uses of POMDPs, for example, assume a perfect dynamics model and attempt only to estimate state.  There are algorithms for simultaneously estimating state and dynamics (e. g. , Chrisman, 1992), analogous to the Baum-Welch algorithm for the uncontrolled case (Baum et al. , 1970), but these are only effective at tuning parameters that are already approximately correct (e. g. , Shatkay & Kaelbling, 1997).  State Update observations (and actions) state rep'n observations (and actions) state rep'n 1-step delays (a) (b) Figure 1: Data flow in a) POMDP and other recursive updating of state representation, and b) history-based state representation.  In practice, history-based approaches are often much more effective.  Here, the state representation is a relatively simple record of the stream of past actions and observations.  It might record the occurrence of a specific subsequence or that one event has occurred more recently than another.  Such representations are far more closely linked to the data than are POMDP representations.  One way of saying this is that POMDP learning algorithms encounter many local minima and saddle points because all their states are equipotential.  History-based systems immediately break symmetry, and their direct learning procedure makes them comparably simple.  McCallum (1995) has shown in a number of examples that sophisticated history-based methods can be effective in large problems, and are often more practical than POMDP methods even in small ones.  The predictive state representation (PSR) approach, which we develop in this paper, is like the generative-model approach in that it updates the state representation recursively, as in Figure 1(a), rather than directly computing it from data.  We show that this enables it to attain generality and compactness at least equal to that of the generative-model approach.  However, the PSR approach is also like the history-based approach in that its representations are grounded in data.  Whereas a history-based representation looks to the past and records what did happen, a PSR looks to the future and represents what will happen.  In particular, a PSR is a vector of predictions for a specially selected set of action{observation sequences, called tests (after Rivest & Schapire, 1994).  For example, consider the test a 1 o 1 a 2 o 2 , where a 1 and a 2 are specific actions and o 1 and o 2 are specific observations.  The correct prediction for this test given the data stream up to time k is the probability of its observations occurring (in order) given that its actions are taken (in order) (i. e. , P r fO k = o 1 ; O k+1 = o 2 j A k = a 1 ; A k+1 = a 2 g).  Each test is a kind of experiment that could be performed to tell us something about the system.  If we knew the outcome of all possible tests, then we would know everything there is to know about the system.  A PSR is a set of tests that is sucient information to determine the prediction for all possible tests (a sucient statistic).  As an example of these points, consider the #oat/reset problem (Figure 2) consisting of a linear string of 5 states with a distinguished reset state on the far right.  One action, f (#oat), causes the system to move uniformly at random to the right or left by one state, bounded at the two ends.  The other action, r (reset), causes a jump to the reset state irrespective of the current state.  The observation is always 0 unless the r action is taken when the system is already in the reset state, in which case the observation is 1.  Thus, on an f action, the correct prediction is always 0, whereas on an r action, the correct prediction depends on how many fs there have been since the last r: for zero fs, it is 1; for one or two fs, it is 0. 5; for three or four fs, it is 0. 375; for five or six fs, it is 0. 3125, and so on decreasing after every second f, asymptotically bottoming out at 0. 2.  No k-order Markov method can model this system exactly, because no
Learning Predictive State Representations| Abstract We introduce the first algorithm for learning predictive state representations (PSRs), which are a way of representing the state of a controlled dynamical system.  The state representation in a PSR is a vector of predictions of tests, where tests are sequences of actions and observations said to be true if and only if all the observations occur given that all the actions are taken.  The problem of finding a good PSR|one that is a sucient statistic for the dynamical system|can be divided into two parts: 1) discovery of a good set of tests, and 2) learning to make accurate predictions for those tests.  In this paper, we present detailed empirical results using a gradient-based algorithm for addressing the second problem.  Our results demonstrate several sample systems in which the algorithm learns to make correct predictions and several situations in which the algorithm is less successful.  Our analysis reveals challenges that will need to be addressed in future PSR learning algorithms. 
The Efficient Learning of Multiple Task Sequences| Abstract I present a modular network architecture and a learning algorithm based on incremental dynamic programming that allows a single learning agent to learn to solve multiple Markovian decision tasks (MDTs) with significant transfer of learning across the tasks.  I consider a class of MDTs, called composite tasks, formed by temporally concatenating a number of simpler, elemental MDTs.  The architecture is trained on a set of composite and elemental MDTs.  The temporal structure of a composite task is assumed to be unknown and the architecture learns to produce a temporal decomposition.  It is shown that under certain conditions the solution of a composite MDT can be constructed by computationally inexpensive modifications of the solutions of its constituent elemental MDTs. 
Intrinsically Motivated Reinforcement Learning| Abstract Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems.  Psychologists call these intrinsically motivated behaviors.  What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise.  In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. 
A Boosting Approach to Topic Spotting on Subdialogues| Abstract We report the results of a study on topic spotting in conversational speech.  Using a machine learning approach, we build classifiers that accept an audio file of conversational human speech as input, and output an estimate of the topic being discussed.  Our methodology makes use of a wellknown corpus of transcribed and topic-labeled speech (the Switchboard corpus), and involves an interesting double use of the BOOSTEXTER learning algorithm.  Our work is distinguished from previous efforts in topic spotting by our explicit study of the effects of dialogue length on classifier performance, and by our use of off-theshelf speech recognition technology.  One of our main results is the identification of a single classifier with good performance (relative to our classifier space) across all subdialogue lengths. 
Journal of Inequalities in Pure and Applied Mathematics| ABSTRACT.  A norm inequality is proved for elements of a star algebra so that the algebra is noncommutative.  In particular, a relation between maximal and minimal extensions of regular norm on a C # -algebra is established.  Key words and phrases: W # -algebras,
Learning Without State-Estimation in Partially Observable Markovian Decision Processes| Abstract Reinforcement learning (RL) algorithms provide a sound theoretical basis for building learning control architectures for embedded agents.  Unfortunately all of the theory and much of the practice (see Barto et al. , 1983, for an exception) of RL is limited to Markovian decision processes (MDPs).  Many realworld decision tasks, however, are inherently non-Markovian, i. e. , the state of the environment is only incompletely known to the learning agent.  In this paper we consider only partially observable MDPs (POMDPs), a useful class of non-Markovian decision processes.  Most previous approaches to such problems have combined computationally expensive state-estimation techniques with learning control.  This paper investigates learning in POMDPs without resorting to any form of state estimation.  We present results about what TD(0) and Q-learning will do when applied to POMDPs.  It is shown that the conventional discounted RL framework is inadequate to deal with POMDPs.  Finally we develop a new framework for learning without state-estimation in POMDPs by including stochastic policies in the search space, and by defining the value or utility of a distribution over states. 
Off-policy Learning with Recognizers| Abstract We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods.  We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections.  We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance.  This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence.  Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators.  Off-policy learning is learning about one way of behaving while actually behaving in another way.  For example, Q-learning is an off- policy learning method because it learns about the optimal policy while taking actions in a more exploratory fashion, e. g. , according to an e-greedy policy.  Off-policy learning is of interest because only one way of selecting actions can be used at any time, but we would like to learn about many different ways of behaving from the single resultant stream of experience.  For example, the options framework for temporal abstraction involves considering a variety of different ways of selecting actions.  For each such option one would like to learn a model of its possible outcomes suitable for planning and other uses.  Such option models have been proposed as fundamental building blocks of grounded world knowledge (Sutton, Precup & Singh, 1999; Sutton, Rafols & Koop, 2005).  Using off-policy learning, one would be able to learn predictive models for many options at the same time from a single stream of experience.  Unfortunately, off-policy learning using temporal-difference methods has proven problematic when used in conjunction with function approximation.  Function approximation is essential in order to handle the large state spaces that are inherent in many problem domains.  Q-learning, for example, has been proven to converge to an optimal policy in the tabular case, but is unsound and may diverge in the case of linear function approximation (Baird, 1996).  Precup, Sutton, and Dasgupta (2001) introduced and proved convergence for the first off-policy learning algorithm with linear function approximation.  They addressed the problem of learning the expected value of a target policy based on experience generated using a different behavior policy.  They used importance sampling techniques to reduce the off-policy case to the on-policy case, where existing convergence theorems apply (Tsitsiklis & Van Roy, 1997; Tadic, 2000).  There are two important difficulties with that approach.  First, the behavior policy needs to be stationary and known, because it is needed to compute the importance sampling corrections.  Second, the importance sampling weights are often ill-conditioned.  In the worst case, the variance could be infinite and convergence would not occur.  The conditions required to prevent this were somewhat awkward and, even when they applied and asymptotic convergence was assured, the variance could still be high and convergence could be slow.  In this paper we address both of these problems in the context of off-policy learning for options.  We introduce the notion of a recognizer.  Rather than specifying an explicit target policy (for instance, the policy of an option), about which we want to make predictions, a recognizer specifies a condition on the actions that are selected.  For example, a recognizer for the temporally extended action of picking up a cup would not specify which hand is to be used, or what the motion should be at all different positions of the cup.  The recognizer would recognize a whole variety of directions of motion and poses as part of picking the cup.  The advantage of this strategy is not that one might prefer a multitude of different behaviors, but that the behavior may be based on a variety of different strategies, all of which are relevant, and we would like to learn from any of them.  In general, a recognizer is a function that recognizes or accepts a space of different ways of behaving and thus, can learn from a wider range of data.  Recognizers have two advantages over direct specification of a target policy: 1) they are a natural and easy way to specify a target policy for which importance sampling will be well conditioned, and 2) they do not require the behavior policy to be known.  The latter is important because in many cases we may have little knowledge of the behavior policy, or a stationary behavior policy may not even exist.  We show that for the case of state aggregation, even if the behavior policy is unknown, convergence to a good model is achieved.  1 Non-sequential example The benefits of using recognizers in off-policy learning can be most easily seen in a nonsequential context with a single continuous action.  Suppose you are given a sequence of sample actions a i 2 [0, 1], selected i. i. d.  according to probability density b : [0, 1] 7! + (the behavior density).  For example, suppose the behavior density is of the oscillatory form shown as a red line in Figure 1.  For each each action, a i , we observe a corresponding outcome, z i 2 , a random variable whose distribution depends only on a i .  Thus the behavior density induces an outcome density.  The on-policy problem is to estimate the mean m b of the outcome density.  This problem can be solved simply by averaging the sample outcomes: ^ m b = i z i .  The off-policy problem is to use this same data to learn what the mean would be if actions were selected in some way other than b.  For example, if the actions were restricted to a designated range, such as between 0. 7 and 0. 9.  There are two natural ways to pose this off-policy problem.  The most straightforward way is to be equally interested in all actions within the designated region.  One professes to be interested in actions selected according to a target density p : [0, 1] 7! + , which in the example would be 5. 0 between 0. 7 and 0. 9, and zero elsewhere, as in the dashed line in Figure 1 (left).  The importance- sampling estimate of the mean outcome is ^ m p = i p(a i ) b(a i ) z i .  (1)
Planning with Predictive State Representations| Abstract Predictive state representation (PSR) models for controlled dynamical systems have recently been proposed as an alternative to traditional models such as partially observable Markov decision processes (POMDPs).  In this paper we develop and evaluate two general planning algorithms for PSR models.  First, we show how planning algorithms for POMDPs that exploit the piecewise linear property of value functions for finite-horizon problems can be extended to PSRs.  This requires an interesting replacement of the role of hidden nominalstates in POMDPs with linearly independent predictions in PSRs.  Second, we show how traditional reinforcement learning algorithms such as Q-learning can be extended to PSR models.  We empirically evaluate both our algorithms on a standard set of test POMDP problems. 
Predictive State Representations: A New Theory for Modeling Dynamical Systems| Abstract Modeling dynamical systems, both for control purposes and to make predictions about their behavior, is ubiquitous in science and engineering.  Predictive state representations (PSRs) are a recently introduced class of models for discrete-time dynamical systems.  The key idea behind PSRs and the closely related OOMs (Jaeger's observable operator models) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system.  This makes PSRs rather different from history-based models such as nth-order Markov models and hidden-statebased models such as HMMs and POMDPs.  We introduce an interesting new construct, the system-dynamics matrix, and show how PSRs can be derived simply from it.  We also use this construct to show formally that PSRs are more general than both nth-order Markov models and HMMs/POMDPs.  Finally, we discuss the main difference between PSRs and OOMs and conclude with directions for future work. 
Distributed Feedback Control for Decision Making on Supply Chains| Abstract Decision makers on supply chains face an uncertain, dynamic, and strategic multiagent environment.  We report on Deep Maize, an agent we designed to participate in the 2003 Trading Agent Competition Supply Chain Management (TAC/SCM) game. 
PAMAS -- Power Aware Multi-Access protocol with Signalling for Ad Hoc Networks| Abstract In this paper we develop a new multiaccess protocol for ad hoc radio networks.  The protocol is based on the original MACA protocol with the adition of a separate signalling channel.  The unique feature of our protocol is that it conserves battery power at nodes by intelligently powering off nodes that are not actively transmitting or receiving packets.  The manner in which nodes power themselves off does not influence the delay or throughput characteristics of our protocol.  We illustrate the power conserving behavior of PAMAS via extensive simulations performed over ad hoc networks containing 10--20 nodes.  Our results indicate that power savings of between 10% and 70% are attainable in most systems.  Finally, we discuss how the idea of power awareness can be built into other multiaccess protocols as well. 
Bias-Variance Error Bounds for Temporal Difference Updates| Abstract We give the first rigorous upper bounds on the error of temporal difference (td) algorithms for policy evaluation as a function of the amount of experience.  These upper bounds prove exponentially fast convergence, with both the rate of convergence and the asymptote strongly dependent on the length of the backups k or the parameter #.  Our bounds give formal verification to the long-standing intuition that td methods are subject to a "bias-variance" trade-off, and they lead to schedules for k and # that are predicted to be better than any fixed values for these parameters.  We give preliminary experimental confirmation of our theory for a version of the random walk problem. 
MIT Press| Improved Switching among Temporally Abstract Actions.  Abstract In robotics and other control applications it is commonplace to have a
Scaling Reinforcement Learning Algorithms by Learning Variable Temporal Resolution Models| Abstract The close connection between reinforcement learning (RL) algorithms and dynamic programming algorithms has fueled research on RL within the machine learning community.  Yet, despite increased theoretical understanding, RL algorithms remain applicable to simple tasks only.  In this paper I use the abstract framework afforded by the connection to dynamic programming to discuss the scaling issues faced by RL researchers.  I focus on learning agents that have to learn to solve multiple structured RL tasks in the same environment.  I propose learning abstract environment models where the abstract actions represent "intentions" of achieving a particular state.  Such models are variable temporal resolution models because in different parts of the state space the abstract actions span different number of time steps.  The operational definitions of abstract actions can be learned incrementally using repeated experience at solving RL tasks.  I prove that under certain conditions solutions to new RL tasks can be found by using simulated experience with abstract actions alone. 
Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems| Abstract In cellular telephone systems, an important problem is to dynamically allocate the communication resource (channels) so as to maximize service in a stochastic caller environment.  This problem is naturally formulated as a dynamic programming problem and we use a reinforcement learning (RL) method to find dynamic channel allocation policies that are better than previous heuristic solutions.  The policies obtained perform well for a broad variety of call traffic patterns.  We present results on a large cellular system with approximately 49 49 states.  In cellular communication systems, an important problem is to allocate the communication resource (bandwidth) so as to maximize the service provided to a set of mobile callers whose demand for service changes stochastically.  A given geographical area is divided into mutually disjoint cells, and each cell serves the calls that are within its boundaries (see Figure 1a).  The total system bandwidth is divided into channels, with each channel centered around a frequency.  Each channel can be used simultaneously at different cells, provided these cells are sufficiently separated spatially, so that there is no interference between them.  The minimum separation distance between simultaneous reuse of the same channel is called the channel reuse constraint .  When a call requests service in a given cell either a free channel (one that does not violate the channel reuse constraint) may be assigned to the call, or else the call is blocked from the system; this will happen if no free channel can be found.  Also, when a mobile caller crosses from one cell to another, the call is "handed off" to the cell of entry; that is, a new free channel is provided to the call at the new cell.  If no such channel is available, the call must be dropped/disconnected from the system.  One objective of a channel allocation policy is to allocate the available channels to calls so that the number of blocked calls is minimized.  An additional objective is to minimize the number of calls that are dropped when they are handed off to a busy cell.  These two objectives must be weighted appropriately to reflect their relative importance, since dropping existing calls is generally more undesirable than blocking new calls.  To illustrate the qualitative nature of the channel assignment decisions, suppose that there are only two channels and three cells arranged in a line.  Assume a channel reuse constraint of 2, i. e. , a channel may be used simultaneously in cells 1 and 3, but may not be used in channel 2 if it is already used in cell 1 or in cell 3.  Suppose that the system is serving one call in cell 1 and another call in cell 3.  Then serving both calls on the same channel results in a better channel usage pattern than serving them on different channels, since in the former case the other channel is free to be used in cell 2.  The purpose of the channel assignment and channel rearrangement strategy is, roughly speaking, to create such favorable usage patterns that minimize the likelihood of calls being blocked.  We formulate the channel assignment problem as a dynamic programming problem, which, however, is too complex to be solved exactly.  We introduce approximations based on the methodology of reinforcement learning (RL) (e. g. , Barto, Bradtke and Singh, 1995, or the recent textbook by Bertsekas and Tsitsiklis, 1996).  Our method learns channel allocation policies that outperform not only the most commonly used policy in cellular systems, but also the best heuristic policy we could find in the literature.  1 CHANNEL ASSIGNMENT POLICIES Many cellular systems are based on a fixed assignment (FA) channel allocation; that is, the set of channels is partitioned, and the partitions are permanently assigned to cells so that all cells can use all the channels assigned to them simultaneously without interference (see Figure 1a).  When a call arrives in a cell, if any preassigned channel is unused; it is assigned, else the call is blocked.  No rearrangement is done when a call terminates.  Such a policy is static and cannot take advantage of temporary stochastic variations in demand for service.  More efficient are dynamic channel allocation policies, which assign channels to different cells, so that every channel is available to every cell on a need basis, unless the channel is used in a nearby cell and the reuse constraint is violated.  The best existing dynamic channel allocation policy we found in the literature is Borrowing with Directional Channel Locking (BDCL) of Zhang & Yum (1989).  It numbers the channels from 1 to N , partitions and assigns them to cells as in FA.  The channels assigned to a cell are its nominal channels.  If a nominal channel is available when a call arrives in a cell, the smallest numbered such channel is assigned to the call.  If no nominal channel is available, then the largest numbered free channel is borrowed from the neighbour with the most free channels.  When a channel is borrowed, careful accounting of the directional effect of which cells can no longer use that channel because of interference is done.  The call is blocked if there are no free channels at all.  When a call terminates in a cell and the channel so freed is a nominal channel, say numbered i, of that cell, then if there is a call in that cell on a borrowed channel, the call on the smallest numbered borrowed channel is reassigned to i and the borrowed channel is returned to the appropriate cell.  If there is no call on a borrowed channel, then if there is a call on a nominal channel numbered larger than i, the call on the highest numbered nominal channel is reassigned to i.  If the call just terminated was itself on a borrowed channel, the
Proposal and Demonstration of Link Connectivity Assessment based Enhancements to Routing in Mobile Ad-hoc Networks| Abstract Routing is a challenge in dynamically changing network of mobile devices.  Proliferating ad-hoc network based applications have greatly increased the need for a robust routing protocol.  Inter-vehicle communication, for instance, has a huge scope in terms of potential applications targeting improvement in driving safety and convenience.  However, because of associated challenges like mobility and hostile driving conditions, ad-hoc multi-hop communication needs to be supported by a robust routing protocol.  In this paper, the application of link-connectivity assessment to efficient ad-hoc routing is investigated and demonstrated. 
Using Eligibility Traces to Find the Best Memoryless Policy in Partially Observable Markov Decision Processes| Abstract Recent research on hidden-state reinforcement learning (RL) problems has concentrated on overcoming partial observability by using memory to estimate state.  However, such methods are computationally extremely expensive and thus have very limited applicability.  This emphasis on state estimation has come about because it has been widely observed that the presence of hidden state or partial observability renders popular RL methods such as Q-learning and Sarsa useless.  However, this observation is misleading in two ways: first, the theoretical results supporting it only apply to RL algorithms that do not use eligibility traces, and second these results are worst-case results, which leaves open the possibility that there may be large classes of hidden-state problems in which RL algorithms work well without any state estimation.  In this paper we show empirically that Sarsa(#), a well known family of RL algorithms that use eligibility traces, can work very well on hidden state problems that have good memoryless policies, i. e. , on RL problems in which there may well be very poor observability but there also exists a mapping from immediate observations to actions that yields near-optimal return.  We apply conventional Sarsa(#) to four test problems taken from the recent work of Littman, Littman Cassandra and Kaelbling, Parr and Russell, and Chrisman, and in each case we show that it is able to find the best, or a very good, memoryless policy without any of the computational expense of state estimation. 
Reinforcement Learning with Soft State Aggregation| Abstract It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems.  Unfortunately almost all of the theory of reinforcement learning assumes lookup table representations.  In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approximator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation.  Preliminary empirical results are also presented. 
A Nonlinear Predictive State Representation| Abstract Predictive state representations (PSRs) use predictions of a set of tests to represent the state of controlled dynamical systems.  One reason why this representation is exciting as an alternative to partially observable Markov decision processes (POMDPs) is that PSR models of dynamical systems may be much more compact than POMDP models.  Empirical work on PSRs to date has focused on linear PSRs, which have not allowed for compression relative to POMDPs.  We introduce a new notion of tests which allows us to define a new type of PSR that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems.  These new tests, called e-tests, are related to the tests used by Rivest and Schapire [1] in their work with the diversity representation, but our PSR avoids some of the pitfalls of their representation---in particular, its potential to be exponentially larger than the equivalent POMDP. 
Improving Policies without Measuring Merits| Abstract Performing policy iteration in dynamic programming should only require knowledge of relative rather than absolute measures of the utility of actions -- what Baird (1993) calls the advantages of actions at states.  Nevertheless, existing methods in dynamic programming (including Baird's) compute some form of absolute utility function.  For smooth problems, advantages satisfy two differential consistency conditions (including the requirement that they be free of curl), and we show that enforcing these can lead to appropriate policy improvement solely in terms of advantages. 
Approximately Efficient Online Mechanism Design| Abstract Online mechanism design (OMD) addresses the problem of sequential decision making in a stochastic environment with multiple self-interested agents.  The goal in OMD is to make value-maximizing decisions despite this self-interest.  In previous work we presented a Markov decision process (MDP)-based approach to OMD in large-scale problem domains.  In practice the underlying MDP needed to solve OMD is too large and hence the mechanism must consider approximations.  This raises the possibility that agents may be able to exploit the approximation for selfish gain.  We adopt sparse-sampling-based MDP algorithms to implement #efficient policies, and retain truth-revelation as an approximate BayesianNash equilibrium.  Our approach is empirically illustrated in the context of the dynamic allocation of WiFi connectivity to users in a coffeehouse. 
Improved Switching among Temporally Abstract Actions| Abstract In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps hand-crafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible.  In this paper we present a framework based on Markov decision processes and semi-Markov decision processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem.  In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers, with negligible additional cost and no re-planning.  In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.  In many applications, solutions to parts of a task are known, either because they were handcrafted by people or because they were previously learned or planned.  For example, in robotics applications, there may exist controllers for moving joints to positions, picking up objects, controlling eye movements, or navigating along hallways.  More generally, an intelligent system may have available to it several temporally extended courses of action to choose from.  In such cases, a key challenge is to take full advantage of the existing temporally extended actions, to choose or switch among them effectively, and to plan at their level rather than at the level of individual actions.  Recently, several researchers have begun to address these challenges within the framework of reinforcement learning and Markov decision processes (e. g. , Singh, 1992; Kaelbling, 1993; Dayan & Hinton, 1993; Thrun and Schwartz, 1995; Sutton, 1995; Dietterich, 1998; Parr & Russell, 1998; McGovern, Sutton & Fagg, 1997).  Common to much of this recent work is the modeling of a temporally extended action as a policy (controller) and a condition for terminating, which we together refer to as an option (Sutton, Precup & Singh, 1998).  In this paper we consider the problem of effectively combining given options into one overall policy, generalizing prior work by Kaelbling (1993).  Sections 1--3 introduce the framework; our new results are in Sections 4 and 5.  1 Reinforcement Learning (MDP) Framework In a Markov decision process (MDP), an agent interacts with an environment at some discrete, lowest-level time scale t = 0; 1; 2; : : : On each time step, the agent perceives the state of the environment, s t 2 S, and on that basis chooses a primitive action, a t 2 A.  In response to each action, a t , the environment produces one step later a numerical reward, r t+1 , and a next state, s t+1 .  The one-step model of the environment consists of the one-step statetransition probabilities and the one-step expected rewards, p a ss 0 = Prfs t+1 = s 0 j s t = s; a t = ag and r a s = Efr t+1 j s t = s; a t = ag; for all s; s 0 2 S and a 2 A.  The agent's objective is to learn an optimal Markov policy, a mapping from states to probabilities of taking each available primitive action, : S \Theta A ! [0; 1], that maximizes the expected discounted future reward from each state s: V (s) = E n r t+1 + flr t+2 + \Delta \Delta \Delta fi fi fi s t = s; o = X a2As (s; a)[r a s + fl X s 0 p a ss 0 V (s 0 )]; where (s; a) is the probability with which the policy chooses action a 2 A s in state s, and fl 2 [0; 1] is a discount-rate parameter.  V (s) is called the value of state s under policy , and V is called the state-value function for .  The optimal state-value function gives the value of a state under an optimal policy: V \Lambda (s) = max V (s) = max a2As [r a s + fl P s 0 p a ss 0 V \Lambda (s 0 )].  Given V \Lambda , an optimal policy is easily formed by choosing in each state s any action that achieves the maximum in this equation.  A parallel set of value functions, denoted Q and Q \Lambda , and Bellman equations can be defined for state-action pairs, rather than for states.  Planning in reinforcement learning refers to the use of models of the environment to compute value functions and thereby to optimize or improve policies.  2 Options We use the term options for our generalization of primitive actions to include temporally extended courses of action.  Let h t;T = s t ; a t ; r t+1 ; s t+1 ; a t+1 ; : : : ; r T ; s T be the history sequence from time t T to time T , and let \Omega denote the set of all possible histories in the given MDP.  Options consist of three components: an initiation set I ` S, a policy : \Omega \Theta A ! [0; 1], and a termination condition fi : \Omega ! [0; 1].  An option o = hI; ; fii can be taken in state s if and only if s 2 I.  If o is taken in state s t , the next action a t is selected according to (s t ; \Delta).  The environment then makes a transition to s t+1 , where o terminates with probability fi(h t;t+1 ), or else continues, determining a t+1 according to (h t;t+1 ; \Delta), and transitioning to state s t+2 , where o terminates with probability fi(h t;t+2 ) etc.  We call the general options defined above semi-Markov because and fi depend on the history sequence; in Markov options and fi depend only on the current state.  Semi-Markov options allow "timeouts", i. e. , termination after some period of time has elapsed, and other extensions which cannot be handled by Markov options.  The initiation set and termination condition of an option together limit the states over which the option's policy must be defined.  For example, a hand-crafted policy for a mobile robot to dock with its battery charger might be defined only for states I in which the battery charger is within sight.  The termination condition fi would be defined to be 1 outside of I and when the robot is successfully docked.  We can now define policies over options.  Let the set of options available in state s be denoted O s ; the set of all options is denoted O = S s2S O s .  When initiated in a state s t , the Markov policy over options : S \Theta O ! [0; 1] selects an option o 2 O s t according to the probability distribution (s t ; \Delta).  The option o is then taken in s t , determining actions until it terminates in s t+k , at which point a new option is selected, according to (s t+k ; \Delta), and so on.  In this way a policy over options, , determines a (non-stationary) policy over actions, or flat policy, = f().  We define the value of a state s under a general flat policy as the expected return
Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms| Abstract In this paper, we address two issues of long-standing interest in the reinforcement learning literature.  First, what kinds of performance guarantees can be made for Q-learning after only a finite number of actions? Second, what quantitative comparisons can be made between Q-learning and model-based (indirect) approaches, which use experience to estimate next-state distributions for off-line value iteration? We first show that both Q-learning and the indirect approach enjoy rather rapid convergence to the optimal policy as a function of the number of state transitions observed.  In particular, on the order of only (N log(1=#)=# 2 )(log(N) + log log(1=#)) transitions are sufficient for both algorithms to come within # of the optimal policy, in an idealized model that assumes the observed transitions are \well-mixed" throughout an N-state MDP. Thus, the two approaches have roughly the same sample complexity. Perhaps surprisingly, this sample complexity is far less than what is required for the model-based approach to actually construct a good approximation to the next-state distribution.  The result also shows that the amount of memory required by the model-based approach is closer to N than to N 2 .  For either approach, to remove the assumption that the observed transitions are well-mixed, we consider a model in which the transitions are determined by a fixed, arbitrary exploration policy.  Bounds on the number of transitions required in order to achieve a desired level of performance are then related to the stationary distribution and mixing time of this policy. 
Hybrid Sort-First and Sort-Last Parallel Rendering with a Cluster of PCs| Abstract We investigate a new hybrid of sort-first and sort-last approach for parallel polygon rendering, using as a target platform a cluster of PCs.  Unlike previous methods that statically partition the 3D model and/or the 2D image, our approach performs dynamic, viewdependent and coordinated partitioning of both the 3D model and the 2D image.  Using a specific algorithm that follows this approach, we show that it performs better than previous approaches and scales better with both processor count and screen resolution.  Overall, our algorithm is able to achieve interactive frame rates with efficiencies of 55. 0% to 70. 5% during simulations of a system with 64 PCs. 
Strategic Procurement in TAC/SCM: An Empirical Game-Theoretic Analysis| Abstract The TAC supply-chain game presented automated trading agents with a challenging strategic problem.  Embedded within a high-dimensional stochastic environment was a pivotal strategic decision about initial procurement of components.  Early evidence suggested that the entrant field was headed toward a self-destructive, mutually unprofitable equilibrium.  Our agent, Deep Maize, introduced a preemptive strategy designed to neutralize aggressive procurement, perturbing the field to a more profitable equilibrium.  It worked.  Not only did preemption improve Deep Maize's profitability, it improved profitability for the whole field.  Whereas it is perhaps counterintuitive that action designed to prevent others from achieving their goals actually helps them, strategic analysis employing an empirical game-theoretic methodology verifies and provides insight about this outcome. 
Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems| Abstract Increasing attention has been paid to reinforcement learning algorithms in recent years, partly due to successes in the theoretical analysis of their behavior in Markov environments.  If the Markov assumption is removed, however, neither generally the algorithms nor the analyses continue to be usable.  We propose and analyze a new learning algorithm to solve a certain class of non-Markov decision problems.  Our algorithm applies to problems in which the environment is Markov, but the learner has restricted access to state information.  The algorithm involves a Monte-Carlo policy evaluation combined with a policy improvement method that is similar to that of Markov decision problems and is guaranteed to converge to a local maximum.  The algorithm operates in the space of stochastic policies, a space which can yield a policy that performs considerably better than any deterministic policy.  Although the space of stochastic policies is continuous---even for a discrete action space---our algorithm is computationally tractable. 
Reinforcement Learning for 3 vs| 2 Keepaway.  Abstract.  As a sequential decision problem, robotic soccer can bene#t from research in reinforcement learning.  We introduce the 3 vs.  2 keepaway domain, a subproblem of robotic soccer implemented in the RoboCup soccer server.  We then explore reinforcement learning methods for policy evaluation and action selection in this distributed, real-time, partially observable, noisy domain.  We present empirical results demonstrating that a learned policy can dramatically outperform hand-coded policies. 
Optimizing Admission Control while Ensuring Quality of Service in Multimedia Networks via Reinforcement Learning| Abstract This paper examines the application of reinforcement learning to a telecommunications networking problem.  The problem requires that revenue be maximized while simultaneously meeting a quality of service constraint that forbids entry into certain states.  We present a general solution to this multi-criteria problem that is able to earn significantly higher revenues than alternatives. 
FAucS : An FCC Spectrum Auction Simulator for Autonomous Bidding Agents| Abstract.  We introduce FAucS, a software testbed for studying automated agent bidding strategies in simulated auctions, specifically the United States FCC wireless frequency spectrum auctions.  In addition to the complexity of these auctions, which provides ample opportunities for intelligent approaches to bidding, this type of auction has huge commercial importance, each bringing in billions of dollars to governments around the world.  We implement straightforward sample agents in FAucS and use them to replicate known beneficial bidding strategies in this type of auction.  We then discuss potential in-depth studies of autonomous bidding agent behaviors using FAucS.  The main contribution of this work is the implementation, description, and empirical validation of the FAucS testbed.  We present it as a challenging and promising AI research domain. 
Robust Reinforcement Learning in Motion Planning| Abstract While exploring to find better solutions, an agent performing online reinforcement learning (RL) can perform worse than is acceptable.  In some cases, exploration might have unsafe, or even catastrophic, results, often modeled in terms of reaching `failure' states of the agent's environment.  This paper presents a method that uses domain knowledge to reduce the number of failures during exploration.  This method formulates the set of actions from which the RL agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies.  The resulting action set has a more abstract relationship to the task being solved than is common in many applications of RL.  Although the cost of this added safety is that learning may result in a suboptimal solution, we argue that this is an appropriate tradeoff in many problems.  We illustrate this method in the domain of motion planning. 
Fast Planning in Stochastic Games| Abstract Stochastic games generalize Markov decision processes (MDPs) to a multiagent setting by allowing the state transitions to depend jointly on all player actions, and having rewards determined by multiplayer matrix games at each state.  We consider the problem of computing Nash equilibria in stochastic games, the analogue of planning in MDPs.  We begin by providing a generalization of finite-horizon value iteration that computes a Nash strategy for each player in generalsum stochastic games.  The algorithm takes an arbitrary Nash selection function as input, which allows the translation of local choices between multiple Nash equilibria into the selection of a single global Nash equilibrium.  Our main technical result is an algorithm for computing near-Nash equilibria in large or infinite state spaces.  This algorithm builds on our finite-horizon value iteration algorithm, and adapts the sparse sampling methods of Kearns, Mansour and Ng (1999) to stochastic games.  We conclude by describing a counterexample showing that infinite-horizon discounted value iteration, which was shown by Shapley to converge in the zero-sum case (a result we give extend slightly here), does not converge in the general-sum case. 
Quality of Service Guarantees in Mobile Computing| Abstract With rapid technological advances being made in the area of wireless communications it is expected that, in the near future, mobile users will be able to access a wide variety of services that will be made available over future high-speed networks.  The quality of these services in the high-speed network domain can be specified in terms of several QOS parameters.  In this paper we identify two new QOS parameters unique to the mobile environment -- guarantee of seamless service and ensuring graceful degradation of service in situations where user demands exceed the network's capacity to satisfy them.  A network architecture and a suite of transport level services are proposed that enables these QOS parameters to be satisfied. 
Cobot: A Social Reinforcement Learning Agent| Abstract We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO.  Our initial work on Cobot (Isbell et al. 2000) provided him with the ability to collect social statistics and report them to users.  Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward.  After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modifing his behavior based on his current state.  Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment. 
ON THE DEGRE OF L 1 -APROXIMATION BY MODIFIED BERNSTEIN POLYNOMIALS| Abstract.  Recently many researchers like Bojani#c & Shisha, and A.  Grundmann have obtained the degree of L 1 approximation to integrable functions by modified Bernstein polynomials.  The object of the present note is to improve their results. 
Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms| Abstract.  An important application of reinforcement learning (RL) is to
An Ecient Exact Algorithm for Singly Connected Graphical Games| Abstract We describe a new algorithm for computing a Nash equilibrium in graphical games , a compact representation for multi-agent systems that we introduced in previous work.  The algorithm is the first to compute equilibria both eciently and exactly for a non-trivial class of graphical games. 
Reinforcement Learning for Spoken Dialogue Systems| Abstract Recently, a number of authors have proposed treating dialogue systems as Markov decision processes (MDPs).  However, the practical application of MDP algorithms to dialogue systems faces a number of severe technical challenges.  We have built a general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework, and have applied it to dialogue corpora gathered from two dialogue systems built at AT&T Labs.  Our experiments demonstrate that RLDS holds promise as a tool for "browsing" and understanding correlations in complex, temporally dependent dialogue corpora. 
Learning to Act Using Real-Time Dynamic Programming| Abstract Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence.  Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known.  We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience.  RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty.  We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems.  We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm.  A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory. 
Automatic Optimization of Dialogue Management| Abstract Designing the dialogue strategy of a spoken dialogue system involves many nontrivial choices.  This paper presents a reinforcement learning approach for automatically optimizing dialogue strategy.  We first present a practical methodology that addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users.  We then demonstrate how we have used this methodology to measurably improve performance in a large-scale experimental system. 
On Step-Size and Bias in Temporal-Difference Learning| Abstract We present results for three new algorithms for setting the step-size parameters, ff and , of temporaldifference learning methods such as TD( ).  The overall task is that of learning to predict the outcome of an unknown Markov chain based on repeated observations of its state trajectories.  The new algorithms select step-size parameters online in such a way as to eliminate the bias normally inherent in temporaldifference methods.  We compare our algorithms with conventional Monte Carlo methods.  Monte Carlo methods have a natural way of setting the step size: for each state s they use a step size of 1=n s , where n s is the number of times state s has been visited.  We seek and come close to achieving comparable stepsize algorithms for TD( ).  One new algorithm uses a = 1=n s schedule to achieve the same effect as processing a state backwards with TD(0), but remains completely incremental.  Another algorithm uses a at each time equal to the estimated transition probability of the current transition.  We present empirical results showing improvement in convergence rate over Monte Carlo methods and conventional TD( ).  A limitation of our results at present is that they apply only to tasks whose state trajectories do not contain cycles. 
Intrinsically Motivated Learning of Hierarchical Collections of Skills| Abstract Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems.  Psychologists call these intrinsically motivated behaviors.  What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise.  In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.  At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 
ATTac-2000: an adaptive autonomous bidding agent| Abstract The First Trading Agent Competition (TAC) was held from June 22nd to July 8th, 2000.  TAC was designed to create a benchmark problem in the complex domain of emarketplaces and to motivate researchers to apply unique approaches to a common task.  This article describes ATTac-2000, the #rst-place finisher in TAC.  ATTac-2000 uses a principled bidding strategy that includes several elements of adaptivity .  In addition to the success at the competition, isolated empirical results are presented indicating the robustness and effectiveness of ATTac-2000's adaptive strategy. 
Category: Reinforcement Learning and Control; ORAL presentation Improved Switching among Temporally Abstract Actions| Abstract In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps handcrafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible.  In this paper we present a framework based on Markov Decision Processes and Semi-Markov Decision Processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem.  In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers with negligible additional cost and no replanning.  In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.  In many applications, possible solutions to parts of a task are known, either because they were handcrafted by people or because they were previously learned or planned.  For example, in robotics applications, there may exist controllers for moving joints to positions, picking up objects, controlling eye movements, or navigating along hallways.  More generally, an intelligent system may have available to it several temporally extended courses of action to choose from.  In such cases, a key challenge is to take full advantage of the existing temporally extended actions, to choose or switch among them effectively and to plan at their level rather than at the level of individual actions.  Recently, several researchers have begun to address these challenges within the framework of reinforcement learning and Markov decision processes (e. g. , Singh, 1992; Kaelbling, 1993; Dayan & Hinton, 1993; Thrun and Schwartz, 1995; Sutton, 1995; Dietterich, 1998; Parr & Russell, 1998; McGovern, Sutton & Fagg, 1997).  Common to much of this recent work is the modeling of a temporally extended action as a policy (controller) and a condition for terminating, which we together refer to as an option (Sutton, Precup & Singh, 1998, in preparation).  In this paper we consider the problem of effectively combining given options into one overall policy.  1 Reinforcement Learning (MDP) Framework In the Markov Decision Process (MDP) framework an agent interacts with an environment at some discrete, lowest-level time scale t = 0; 1;
Graphical Models for Game Theory| Abstract We introduce a compact graph-theoretic representation for multi-party game theory.  Our main result is a provably correct and efficient algorithm for computing approximate Nash equilibria in (one-stage) games represented by trees or sparse graphs. 
Value-Driven Procurement in the TAC Supply Chain Game| Abstract The TAC supply-chain game presents automated trading agents with challenging decision problems, including procurement of supplies across multiple periods using multiattribute negotiation.  The procurement process involves substantial uncertainty and competition among multiple agents.  Our agent, Deep Maize, generates requests for components based on deviations from a reference inventory trajectory defined by estimated market conditions.  It then selects among supplier offers by optimizing a value function over potential inventory profiles.  This approach offered strategic flexibility and achieved competitive performance in the TAC-03 tournament. 
Learning and discovery of predictive state representations in dynamical systems with reset| Abstract Predictive state representations (PSRs) are a recently proposed way of modeling controlled dynamical systems.  PSR-based models use predictions of observable outcomes of tests that could be done on the system as their state representation, and have model parameters that define how the predictive state representation changes over time as actions are taken and observations noted.  Learning PSR-based models requires solving two subproblems: 1) discovery of the tests whose predictions constitute state, and 2) learning the model parameters that define the dynamics.  So far, there have been no results available on the discovery subproblem while for the learning subproblem an approximategradient algorithm has been proposed (Singh et al. , 2003) with mixed results (it works on some domains and not on others).  In this paper, we provide the first discovery algorithm and a new learning algorithm for linear PSRs for the special class of controlled dynamical systems that have a reset operation.  We provide experimental verification of our algorithms.  Finally, we also distinguish our work from prior work by Jaeger (2000) on observable operator models (OOMs). 
Theoretical Results on Reinforcement Learning with Temporally Abstract Options| Abstract.  We present new theoretical results on planning within the framework of temporally abstract reinforcement learning (Precup & Sutton, 1997; Sutton, 1995).  Temporal abstraction is a key step in any decision making system that involves planning and prediction.  In temporally abstract reinforcement learning, the agent is allowed to choose among "options", whole courses of action that may be temporally extended, stochastic, and contingent on previous events.  Examples of options include closed-loop policies such as picking up an object, as well as primitive actions such as joint torques.  Knowledge about the consequences of options is represented by special structures called multi-time models.  In this paper we focus on the theory of planning with multi-time models.  We define new Bellman equations that are satisfied for sets of multi-time models.  As a consequence, multi-time models can be used interchangeably with models of primitive actions in a variety of well-known planning methods including value iteration, policy improvement and policy iteration. 
Intra-Option Learning about Temporally Abstract Actions| Abstract Several researchers have proposed modeling temporally abstract actions in reinforcement learning by the combination of a policy and a termination condition, which we refer to as an option.  Value functions over options and models of options can be learned using methods designed for semi-Markov decision processes (SMDPs).  However, all these methods require an option to be executed to termination.  In this paper we explore methods that learn about an option from small fragments of experience consistent with that option, even if the option itself is not executed.  We call these methods intra-option learning methods because they learn from experience within an option.  Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporaldifference mechanisms to learn simultaneously about all the options consistent with an experience, not just the few that were actually executed.  In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the consequences of options.  We present computational examples in which these new methods learn much faster than SMDP methods and learn effectively when SMDP methods cannot learn at all.  We also sketch a convergence proof for intraoption value learning. 
Draft: Please do not distribute A Nonlinear Predictive State Representation| Abstract Predictive state representations (PSRs) use predictions of a set of tests to represent the state of controlled dynamical systems.  One reason why this representation is exciting as an alternative to partially observable Markov decision processes (POMDPs) is that PSR models of dynamical systems may be much more compact than POMDP models.  Work on PSRs to date has focused on linear PSRs, which have not allowed for compression relative to POMDPs.  We introduce a new notion of tests which allows us to define a new type of PSR that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems.  These new tests, called e-tests, are related to the tests used by Rivest and Schapire [1] in their work with the diversity representation, but our PSR avoids some of the pitfalls of their representation---in particular, its potential to be exponentially larger than the equivalent POMDP. 
Game Theory with Restricted Strategies (DRAFT: Please do not circulate|).  Abstract We present a theory of multiplayer games in which each player chooses their strategy from a limited class that may be significantly weaker than the class of all mixed strategies.  As in much of AI and machine learning, such restrictions may be desirable for reasons of computational tractability.  We define and study the notions of restricted Nash equilibria, local restricted Nash equilibria, and restricted security strategies.  We illustrate a number of these concepts with two well-studied games from the literature, the War of Attrition and Pricebots games, in which both players are restricted to play mixed strategies that are mixtures of Gaussians. 
Eligibility Traces for Off-Policy Policy Evaluation| Abstract Eligibility traces have been shown to speed reinforcement learning, to make it more robust to hidden states, and to provide a link between Monte Carlo and temporal-difference methods.  Here we generalize eligibility traces to off-policy learning, in which one learns about a policy different from the policy that generates the data.  Off-policy methods can greatly multiply learning, as many policies can be learned about from the same data stream, and have been identified as particularly useful for learning about subgoals and temporally extended macro-actions.  In this paper we consider the off-policy version of the policy evaluation problem, for which only one eligibility trace algorithm is known, a Monte Carlo method.  We analyze and compare this and four new eligibility trace algorithms, emphasizing their relationships to the classical statistical technique known as importance sampling.  Our main results are 1) to establish the consistency and bias properties of the new methods and 2) to empirically rank the new methods, showing improvement over one-step and Monte Carlo methods.  Our results are restricted to model-free, table-lookup methods and to offline updating (at the end of each episode) although several of the algorithms could be applied more generally.  In reinforcement learning, we generally learn from experience, that is, from the sequence of states, actions, and rewards generated by the agent interacting with its environment.  This data is affected by the decision-making policy used by the agent to select its actions, and thus we often end up learning something that is a function of the agent's policy.  For example, the common subproblem of policy evaluation is to learn the value function for the agent's policy (the function giving the expected future reward available from each state--action pair).  In general, however, we might want to learn about policies other than that currently followed by the agent, a process known as off-policy learning.  For example, 1-step Q-learning is often used in an off-policy manner, learning about the greedy policy while the data is generated by a slightly randomized policy that ensures exploration.  Off-policy learning is especially important for research on the use of temporally extended actions in reinforcement learning (
Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System| Abstract Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices.  This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users.  We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey.  Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance. 
Performance of TCP over Different Routing Protocols in Mobile Ad-Hoc Networks| Abstract---TCP/IP is the standard networking protocol on the internet and is also the most widely used.  Due to these reasons, its use over Mobile Ad-Hoc networks is a certainty.  Ad-Hoc networks are prone to link failures due to mobility.  TCP is unable to distinguish between losses due to route failures and losses due to congestion.  As a result, throughput degrades significantly when nodes move.  It is therefore essential to study how TCP performs over ad-hoc networks.  We have used simulations in the cmu extension to ns to analyse the performance of TCP Tahoe over a set of routing protocols including the Signal Stability Adaptive routing protocol which we have implemented in ns-2.  We identify characteristics in each of these routing protocols that determine the behaviour of TCP over them. 
Cobot in LambdaMOO: A Social Statistics Agent| Abstract We describe our development of Cobot, a software agent who lives in LambdaMOO, a popular virtual world frequented by hundreds of users.  We present a detailed discussion of the functionality that has made him one of the objects most frequently interacted with in LambdaMOO, human or artificial. 
On the Complexity of Policy Iteration| Abstract Decision-making problems in uncertain or stochastic domains are often formulated as Markov decision processes (MDPs).  Policy iteration (PI) is a popular algorithm for searching over policy-space, the size of which is exponential in the number of states.  We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor.  In this paper we prove the first such non-trivial, worst-case, upper bounds on the number of iterations required by PI to converge to the optimal policy. 
An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games| Abstract We describe a new algorithm for computing a Nash equilibrium in graphical games , a compact representation for multi-agent systems that we introduced in previous work.  The algorithm is the first to compute equilibria both eciently and exactly for a non-trivial class of graphical games. 
Convergence of Stochastic Iterative Dynamic Programming Algorithms| Abstract Increasing attention has recently been paid to algorithms based on dynamic programming (DP) due to the suitability of DP for learning problems involving control.  In stochastic environments where the system being controlled is only incompletely known, however, a unifying theoretical account of the behavior of these methods has been missing.  In this paper we relate DP-based learning algorithms to powerful techniques of stochastic approximation via a new convergence theorem, enabling us to establish a class of convergent algorithms to which both TD(#) and Q-learning belong. 
On the Convergence of Stochastic Iterative Dynamic Programming Algorithms| Abstract Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments.  These algorithms, including the TD( ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP).  In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem.  The theorem establishes a general class of convergent algorithms to which both TD( ) and Q-learning belong.  An important component of many real world learning problems is the temporal credit assignment problem---the problem of assigning credit or blame to individual components of a temporally-extended plan of action, based on the success or failure of the plan as a whole.  To solve such a problem, the learner must be equipped with the ability to assess the long-term consequences of particular choices of action and must be willing to forego an immediate payoff for the prospect of a longer term gain.  Moreover, because most real world problems involving prediction of the future consequences of actions involve substantial uncertainty, the learner must be prepared to make use of a probability calculus for assessing and comparing actions.  There has been increasing interest in the temporal credit assignment problem, due principally to the development of learning algorithms based on the theory of dynamic programming (DP) (Barto, Sutton,
Reinforcement Learning with a Hierarchy of Abstract Models|
Distributed representations of limb motor programs in arrays of adjustable pattern generators|
Thermodynamics of nonrelativistic free quark gas //|
Power-Aware Routing in Mobile Ad Hoc Networks|
On approximation by modified Szasz operators|
On random approximations and a random fixed point theorem for set valued mappings,|
An adaptive sensorimotor network simulation inspired by the anatomy and physiology of the cerebellum|
Scalable Routing Protocol for Ad Hoc Networks|
Asymptotically optimal Gossiping in Radio Networks|
Total tree volume table for Tectona grandis (teak)|
On approximating fixed points,|
Route-Lifetime Assessment Based Routing (RABR) Protocol for Mobile Ad-Hoc Networks|
How to Dynamically Merge Markov Decision Processes|
On the Computational Economics of Reinforcement Learning|
Analytical Mean Squared Error Curves for Temporal Dierence|
Learning to Solve Markovian Decision Processes|
Transfer of learning across compositions of sequential tasks|
Recurrent Finsler space of second order,|
Proximity maps and fixed points,|
A theorem on best approximations,|
On recurrent Finsler spaces,|
Multicast scheduling algorithms in mobile networks|
Asynchronous modified policy iteration with single-sided updates|
Approximate Planning for Factored POMDPs using Belief State Simplification|
Reinforcement learning algorithms for average-payoff Markovain decision processes,|
Reinforcement learning with eligibility traces|
Collected Papers,|
To appear)| An upper bound on the loss from approximate optimalvalue functions. 
Learning Control in Dynamic Environments|
An empirical comparison of the KSR and DASH multiprocessors",|
Transfer of learning across sequential tasks|
Learning predictive state representations in dynamical systems without reset|
Asynchronous policy iteration with single-sided updates (Working Paper)|
Broadcasting on [0, L]|
Experimental Results on Learning Stochastic Memoryless Policies for Partially Observable Markov Decision Processes|
Stochastic convergence of iterative DP algorithms|
to appear)| Learning to act using real-time dynamic programming. 
Talk given at the|
Predicting Lifetimes in Dynamically Allocated Memory|
Construct design for efficient, effective and highthroughput gene silencing in plants|
this volume| Learning to act using real-time dynamic programming. 
KKM-maps and fixed point theorems,|
A new floating resistor for CMOS technology,|
Computing approximate bayes-nash equilibria in tree-games of incomplete information|
Analytical Mean Squared Error Curves for Temporal Difference Learning|
On best simultaneous approximation in Banach spaces,|
to appear) Reinforcement learning with replacing eligibility traces|
On best simultaneous Chebyshev approximation with additive weight functions and related results,|
dan| Reinforcement learning algorithm for partially observable markov decision problems. 
Reinforcement learning with relacing eligibility traces|
Symbolic and neural net learning algorithms: An experimental comparison|

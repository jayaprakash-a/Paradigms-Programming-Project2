Learning the higher-order structure of a natural sound| Abstract.  Unsupervised learning algorithms paying attention only to second-order statistics ignore the phase structure (higher-order statistics) of signals, which contains all the informative temporal and spatial coincidences which we think of as `features'.  Here we discuss how an Independent Component Analysis (ICA) algorithm may be used to elucidate the higher-order structure of natural signals, yielding their independent basis functions.  This is illustrated with the ICA transform of the sound of a fingernail tapping musically on a tooth.  The resulting independent basis functions look like the sounds themselves, having the same temporal envelopes and the same musical pitches.  Thus they reflect both the phase and frequency information inherent in the data.  Short title: Learning higher order structure.  February 2, 1996 1.  The poverty of second-order statistics.  Natural signals have characteristic statistical dependencies across space and time.  One view of sensory systems is that they must uncover these dependencies by processing them with filters whose form depends on the characteristic statistics of the ensemble of signals to which they are exposed (Barlow 1989, Atick & Redlich 1990).  A considerable effort has gone into finding unsupervised learning algorithms able to self-organise to produce such filters (Linsker 1988, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990, Intrator 1992, Atick & Redlich 1993 and many others).  These efforts have been criticised by David Field 1987, 1994.  A major component of Field's argument is that the above methods are sensitive only to second-order statistics, since they all use correlation-based learning rules (ie: Hebbian and/or antiHebbian rules. ) Most of the methods bear some relation to Principal Components Analysis (the Karhunen-Loeve Transform), a second-order statistical technique.  The most informative features of natural signals, however, require higher-order statistics for their characterisation.  An edge in an image, or the transient attack or decay of a sound waveform, are examples of `features' which involve relationships between not just two, but many tens or even hundreds of pixels or time-points.  The failure of correlation-based learning is most clearly shown by the filters they produce when trained on stationary ensembles of signals.  The filters are typically global (see Figure 2a-c), sensitive to different spatio- or temporal frequencies, but with non-zero weights extending throughout the filter.  They reflect only the amplitude spectrum of the signal and ignore the phase spectrum where most of the suspicious local coincidences in natural signals take place.  An edge in an image, for example, is a coincidence in the phase spectrum, since if we were to Fourier analyse it, we would see many sine waves of different frequencies, all aligned in phase where the edge occurred.  Correlation-based methods cannot learn edge-detectors, though they often may seem to be doing so by local-windowing of the learnt Fourier components, turning them into Gabor-like filters (see Daugman 1985 for an analysis of the pertinence of Gabor filters to vision. ).  To illustrate formally that second order statistics only carry information about the amplitude spectrum, consider the autocorrelation function of a signal, which contains all its second order structure.  The Fourier transform of this is the power spectrum, which is the square of the amplitude spectrum.  Thus the two carry identical information.  To demonstrate intuitively that what we consider as the informative part of a natural signal is captured in the phase spectrum, Fourier transform the signal, remove the phase information, and transform it back to the space or time domain.  It will then look or sound like noise, typically with a 1/f amplitude spectrum.  All the visual features or sounds that our perceptual system thinks of as of as `signal' will be lost.  On the other hand, if we remove the amplitude information, and preserve the phase, the signal will be distorted but remain recognisable.  This points to a curious paradox: correlation-based learning algorithms are sensitive to exactly the part of natural signals which we regard as least meaningful (amplitude), and ignore the part of the signal which we find most meaningful (phase).  To encode the phase of signals, we need an algorithm sensitive to higher-order staistics.  The problem with higher-order statistics is that there are an infinite number of them.  Deciding which to measure a priori would be a difficult task.  Looking for horizontal bars in an image, for example, we may decide to estimate the average
The `Independent Components' of Natural Scenes are Edge Filters| Abstract Field (1994) has suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representaton of natural scenes, and Barlow (1989) has reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features.  We show here that a new unsupervised learning algorithm that is based on information maximisation, a non-linear `infomax' network (Bell and Sejnowski, 1995) when applied to an ensemble of natural scenes, produces sets of visual filters that are localised and oriented.  Some of these filters are Gabor-like and resemble those produced by the sparseness-maximisation network of Olshausen & Field (1996).  In addition, the outputs of these filters are as independent as possible, since the infomax network is able to perform Independent Components Analysis (ICA).  We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA).  The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes.  They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form a natural, informationtheoretic co-ordinate system for natural images.  \Lambda Please send comments to tony@salk. edu.  This paper is submitted to Vision Research. 
An information-maximisation approach to blind separation and blind deconvolution| Abstract We derive a new self-organising learning algorithm which maximises the information transferred in a network of non-linear units.  The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit.  Under these conditions, information maximisation has extra properties not found in the linear case (Linsker 1989).  The non-linearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation.  This enables the network to separate statistically independent components in the inputs: a higher-order generalisation of Principal Components Analysis.  We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to ten speakers.  We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal).  Finally, we derive dependencies of information transfer on time delays.  We suggest that information maximisation provides a unifying framework for problems in `blind' signal processing. 
Fast blind separation based on information theory| ABSTRACT Blind separation is an information theoretic problem, and we have proposed an information theoretic `sigmoid-based' solution [2].  Here we elaborate on several aspects of that solution.  Firstly, we argue that the separation matrix may be exactly found by maximising the joint entropy of the random vector resulting from a linear transformation of the mixtures followed by sigmoidal non-linearities which are the cumulative density functions of the `unknown' sources.  Secondly, we present the learning rule for performing this maximisation.  Thirdly, we discuss the role of prior knowledge of the c. d. f. 's of the sources in customising the learning rule.  We argue that sigmoid-based methods are better able to make use of this prior knowledge than cumulant-based methods, because the optimal non-linearity they should use is just an estimate of the source c. d. f.  We also suggest that they may have the edge in terms of robustness and speed of convergence.  Improvements in convergence speed have been facilitated by the introduction of pre-whitening of the mixture data.  An example result demonstrating this is the perfect separation of ten artificially mixed audio signals in 10 seconds of workstation computing time (4 to prewhiten and 6 to separate).  I.  Blind signal processing .  Statistically independent sources propagating in a medium are subject to several forms of distortion and interference.  They may be (1) mixed with other sources (2) mixed with time delayed versions of themselves, and (3) time-delayed.  The mixing may be linear or non-linear.  The inversion of these three forms of scrambling without any knowledge of their form may be called blind signal processing, or blind identification.  When the mixing is linear, we usually refer to (1) as the problem of blind separation [4], (2) as the problem of blind deconvolution, and (3) as the problem of blind time alignment.  These problems are information theoretic problems in the sense that we are dealing with the removal of statistical dependencies introduced by the medium, and the correct measure of statistical dependency is mutual information (see below).  In the most general information theoretic formalism, no special status is given to noise introduced by the medium or the sensors.  It is regarded as another `source' to be separated out.  It cannot be assumed to be characterised only by secondorder statistics (gaussian).  In fact, if we are lucky (and we usually are), it will not be gaussian, for it is the higher-order statistics which characterise a signal as independent and enable it to be separated out from others.  In [2], an information theoretic approach was outlined to all three of the above problems.  This paper is really a series of footnotes to [2], and should be read in conjunction with it if fuller details, or material of an introductory or tutorial nature are needed.  Here we will concentrate on the blind separation problem in order to show more clearly how it is solved by information theory.  II.  Separation through information theory.  A vector of sources s(t) = [s 1 (t); : : : ; s N (t)] propagates in a medium and mixtures of them, x(t) = [x 1 (t); : : : ; xN (t)] = As(t) 1 , are picked up by sensors.  The mixing is linear and static, there are no time delays and there are the same number (N ) of sensors as sources so that the mixing matrix, A, is square.  The important fact that distinguishes a source, s i , from a mixture, x i , is that it is statistically indepen1 henceforth, for convenience, the time index will be considered as implicit.  dent from the other sources, s j .  Their joint probability density function (p. d. f. ), measured across the time ensemble, factorises: f s (s) = N Y i=1 f s i (s i ) (1) Another way of saying this is that the mutual information between any two sources, i and j, is zero: I(s i ; s j ) = E '' ln f s (s) Q N i=1 f s i (s i ) # = 0 (2) where E[:] denotes expected value across the time ensemble.  Mixtures of sources will be statistically dependent on each other and the mutual information between them, I(x i ; x j ) will in general be positive.  Blind separation then consists in finding a matrix, W, so that the linear transformation u = Wx = WAs reestablishes the condition I(u i ; u j ) = 0, for all i 6= j.  This is the problem of Independent Component Analysis (ICA) [4, 3] One solution to this problem is that W is the inverse of A so that WA=I, the identity matrix.  Any other solution matrix, W, can be shown to be a permutation and rescaling of this one.  See Comon [3] for a fuller discussion of these matters.  To make the u i independent, we need to operate on non-linearly transformed output variables, y i = g(u i ), g() being a sigmoidal function.  2 The sigmoidal function provides, through its Taylor series expansion, all the higher-order statistics necessary to establish independence.  This assertion is justified through the following theorem: Theorem.  3 Independent Component Analysis (blind separation) can be performed exactly, by finding the maximum, with respect to W, of the joint entropy, H(y), of an output vector, y, which is the vector u, except that each element is transformed by a sigmoidal function which is a c. d. f.  of a sources which we are looking for.  In practice, we will often assume that all the sources have the same c. d. f.  and use the same sigmoidal function for each element of u.  To prove this theorem, we develop the following six points: Point 1.  Independent variables cannot become dependent by passing each one through a sigmoid.  Thus 2 a sigmoidal function is defined somewhat generally here as an invertible twice-differentiable function mapping the real line into some interval, often the unit interval: R! [0; 1].  3 Subsequent to the publication of this paper in the NOLTA proceedings, it was pointed out to us by Prof.  Nadal that this theorem is subsumed in the argument given in reference [7].  We would like to apologise for the fact that it appears here as an apparently original result and refer the reader to [7] for a fuller development.  if I(u i ; u j ) = 0 and y=g(u), g() being invertible, then I(y i ; y j ) = 0.  Since g\Gamma 1 is also invertible, the converse also holds.  Point 2.  The entropy, H(y), of a sigmoidally transformed variable has its maximum value (of zero) when the sigmoid function is the cumulative density function (c. d. f. ) of the u-variable.  Proof: H(y) is maximum when f y (y)=1 (the uniform distribution).  Thus by the relation: f y (y) = f u (u) dy=du (3) we have dy=du = fu (u) which means y = Fu (u), the cumulative density.  Point 3.  The joint entropy, H(y 1 ; y 2 ), of two sigmoidally transformed variables has its maximum value (of zero) when y 1 and y 2 are independent and the sigmoid function in each is the c. d. f.  of u 1 and u 2 respectively.  This is a clear consequence of Point 2 and the relation: H(y 1 ; y 2 ) = H(y 1 ) + H(y 2 ) \Gamma I(y 1 ; y 2 ) (4) The N-variable joint entropy, H(y), is similarly maximal when each f y i (y i ) term is maximum and all the I(y i ; y j ) are zero.  Point 4.  When two independent non-gaussian variables, u i and u j are linearly combined, the p. d. f.  of the resulting variable has a different shape from either of f u i (u i ) or f u j (u j ).  In general, the p. d. f.  becomes more gaussian, a trend ultimately enshrined in the Central Limit Theorem.  Gaussian variables are the only ones which retain the form of their p. d. f.  under linear combination.  Point 5.  Consider the joint entropy, H(y), of N sigmoidally transformed variables, where the sigmoid functions are the c. d. f. 's of N independent nongaussian sources (ie: y i = F s i (u i )).  This has its maximal value when u i = s i , in other words when the sources are separated! Any mixing of sources, u i = P j s j , will both: ffl introduce statistical dependencies between the u's, moving I(u i ; u j ) away from zero (and hence also I(y i ; y j ) --- see Point 1), and ffl decrease the individual entropy terms, H(y i ), through deviation of f y i (y i ) from 1.  This latter fact is born out by Points 2 and 4 above.  Taken together, this shows that under the special condition that y i = F s i (u i ), the joint entropy H(y) is maximal when the individual entropies, H(y i ), are
An information-maximization approach to blind separation and blind deconvolution|
Blind separation of multiple speakers in a multipath environment,|
Balancing" of conductances may explain irregular cortical spiking (|
Edges are the Independent Components of Natural Scenes|
Blind separation and blind deconvolution: an information-theoretic approach,|
A Non-linear Information Maximisation Algorithm that Performs Blind Separation|

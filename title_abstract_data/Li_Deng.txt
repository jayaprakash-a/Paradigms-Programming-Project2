Distributed Speech Processing in MiPad's Multimodal User Interface| Abstract---This paper describes the main components of MiPad (Multimodal Interactive PAD) and especially its distributed speech processing aspects.  MiPad is a wireless mobile PDA prototype that enables users to accomplish many common tasks using a multimodal spoken language interface and wireless-data technologies.  It fully integrates continuous speech recognition and spoken language understanding, and provides a novel solution for data entry in PDAs or smart phones, often done by pecking with tiny styluses or typing on minuscule keyboards.  Our user study indicates that the throughput of MiPad is significantly superior to that of the existing pen-based PDA interface.  Acoustic modeling and noise robustness in distributed speech recognition are key components in MiPad's design and implementation.  In a typical scenario, the user speaks to the device at a distance so that he or she can see the screen.  The built-in microphone thus picks up a lot of background noise, which requires MiPad be noise robust.  For complex tasks, such as dictating e-mails, resource limitations demand the use of a client--server (peer-to-peer) architecture, where the PDA performs primitive feature extraction, feature quantization, and error protection, while the transmitted features to the server are subject to further speech feature enhancement, speech decoding and understanding before a dialog is carried out and actions rendered.  Noise robustness can be achieved at the client, at the server or both.  Various speech processing aspects of this type of distributed computation as related to MiPad's potential deployment are presented in this paper.  Recent user interface study results are also described.  Finally, we point out future research directions as related to several key MiPad functionalities. 
Speech Denoising and Dereverberation Using Probabilistic Models| Abstract This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals.  The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation.  The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech.  Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors.  The framework covers both single and multiple microphones.  We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods. 
A FINITE DIFFERENCE APPROXIMATION FOR A COUPLED SYSTEM OF NONLINEAR SIZE-STRUCTURED POPULATIONS| Abstract: We study a quasilinear nonlocal hyperbolic initial-boundary value problem that models the evolution of N size-structured subpopulations competing for common resources.  We develop an implicit finite difference scheme to approximate the solution of this model.  The convergence of this approximation to a unique bounded variation weak solution is obtained.  The numerical results for a special case of this model suggest that when subpopulations are closed under reproduction, one subpopulation survives and the others go to extinction.  Moreover, in the case of open reproduction, survival of more than one population is possible. 
AN EKF-BASED ALGORITHM FOR LEARNING STATISTICAL HIDDEN DYNAMIC MODEL PARAMETERS FOR PHONETIC RECOGNITION| ABSTRACT This paper presents a new parameter estimation algorithm based on the Extended Kalman Filter (EKF) for the recently proposed statistical coarticulatory Hidden Dynamic Model (HDM).  We show how the EKF parameter estimation algorithm unifies and simplifies the estimation of both the state and parameter vectors.  Experiments based on N-best rescoring demonstrate superior performance of the (contextindependent) HDM over a triphone baseline HMM in the TIMIT phonetic recognition task.  We also show that the HDM is capable of generating speech vectors close to those from the corresponding real data. 
MULTI-SENSORY SPEECH PROCESSING: INCORPORATING AUTOMATICALLY EXTRACTED HIDDEN DYNAMIC INFORMATION| Abstract We describe a novel technique for multi-sensory speech processing for enhancing noisy speech and for improved noiserobust speech recognition.  Both air- and bone-conductive microphones are used to capture speech data where the bone sensor contains virtually noise-free hidden dynamic information of clean speech in the form of formant trajectories.  The distortion in the bone-sensor signal such as teethclacking and noise leakage can be effectively removed by making use of the automatically extracted formant information from the bone-sensor signal.  This paper reports an improved technique for synthesizing speech waveforms based on the LPC cepstra computed analytically from the formant trajectories.  When this new signal stream is fused with the other available speech data streams, we achieved improved performance for noisy speech recognition. 
RECOVERING VOCAL TRACT SHAPES FROM MFCC PARAMETERS| ABSTRACT Recovering vocal tract shapes from the speech signal is a well known inversion problem of transformation from the articulatory system to speech acoustics.  Most of the studies on this problem in the past have been focused on vowels.  There have not been general methods effective for recovering the vocal tract shapes from the speech signal for all classes of speech sounds.  In this paper we describe our attempt towards speech inverse mapping by using the mel-frequency cepstrum coefficients to represent the acoustic parameters of the speech signal.  An inversion method is developed based on Kalman filtering and a dynamic-system model describing the articulatory motion.  This method uses an articulatory-acoustic codebook derived from Maeda's articulatory model. 
A Robust Speech Understanding System Using Conceptual Relational Grammar| ABSTRACT We describe a robust speech understanding system based on our newly developed approach to spoken language processing.  We show that a robust NLU system can be rapidly developed using a relatively simple speech recognizer to provide sufficient information for database retrieval by spoken language.  Our experimental system consists of three components: a speech recognizer based on HMM, a natural language parser based on conceptual relational grammar and a data retrieval system based on the ATIS database.  With the use of the robust parsing strategy, database query tasks can be successfully performed. 
Optimal Filtering and Smoothing for Speech Recognition Using a Stochastic Target Model| ABSTRACT This paper presents a stochastic target model of speech production, where articulator motion in the vocal tract is represented by the state of a Markov-modulated linear dynamical system, driven by a piecewise-deterministic control trajectory, and observed through a non-linear function representing the articulatory-acoustic mapping.  Optimal filtering and smoothing algorithms for estimating the hidden states of the model from acoustic measurements are derived using a measure-change technique, and require solution of recursive integral equations.  A sub-optimal approximation is developed, and illustrated using examples taken from real speech. 
An Investigation of Segmental Hidden Dynamic Models of Speech Coarticulation for Automatic Speech Recognition| Abstract Conversational speech recognition is a challenging problem primarily because speakers rarely fully articulate sounds.  A successful speech recognition approach must infer intended spectral targets from the speech data, or develop a method of dealing with large variances in the data.  The Workshop Project explored a new approach to acoustic-phonetic modelling, the Hidden Dynamic Model (HDM), which explicitly accounts for the coarticulation and transitions between neighbouring phones.  Inspired by the fact that speech is really produced by an underlying dynamic system, an HDM has a vector target per phone in a hidden dynamic space in which speech trajectories are produced by a simple dynamic system.  The hidden space is mapped to the surface acoustic representation via a non-linear mapping in the form of a multilayer perceptron (MLP).  HDMs are a radical departure from conventional hidden Markov models (HMMs), which simply account for variation in the observed data.  In this report we present details of two types of HDM and our experience with using them to model speech patterns.  Although the HDMs are still at an very early stage, we attempted an initial evaluation of such models on a conversational speech recognition task involving a subset of the SWITCHBOARD corpus.  We have indications that in an N-Best rescoring paradigm, HDMs are capable of supplying extra information, even when trained only on a limited amount of speech from a single speaker. 
Speech Trajectory Discrimination Using the Minimum Classification Error Learning| Abstract--- In this paper, we extend the maximum likelihood (ML) training algorithm to the minimum classification error (MCE) training algorithm for discriminatively estimating the state-dependent polynomial coefficients in the stochastic trajectory model or the trended hidden Markov model (HMM) originally proposed in [2].  The main motivation of this extension is the new model space for smoothness-constrained, state-bound speech trajectories associated with the trended HMM, contrasting the conventional, stationary-state HMM, which describes only the piecewise-constant "degraded trajectories" in the observation data.  The discriminative training implemented for the trended HMM has the potential to utilize this new, constrained model space, thereby providing stronger power to disambiguate the observational trajectories generated from nonstationary sources corresponding to different speech classes.  Phonetic classification results are reported which demonstrate consistent performance improvements with use of the MCE-trained trended HMM both over the regular ML-trained trended HMM and over the MCEtrained stationary-state HMM. 
Test-Cost Sensitive Naive Bayes Classification| Abstract Inductive learning techniques such as the naive Bayes and decision tree algorithms have been extended in the past to handle different types of costs mainly by distinguishing different costs of classification errors.  However, it is an equally important issue to consider how to handle the test costs associated with querying the missing values in a test case.  When the value of an attribute is missing in a test case, it may or may not be worthwhile to take the effort to obtain its missing value, depending on how much the value will result in a potential gain in the classification accuracy.  In this paper, we show how to obtain a test-cost sensitive naive Bayes classifier (csNB) by including a test strategy which determines how unknown attributes are selected to perform test on in order to minimize the sum of the misclassification costs and test costs.  We propose and evaluate several potential test strategies including one that allows several tests to be done at once.  We empirically evaluate the csNB method, and show that it compares favorably with its decision tree counterpart. 
TOPICS ON HIDDEN MARKOV MODELS AND THEIR APPLICATIONS IN SPEECH RECOGNITION| ABSTRACT We present an algorithm for estimating state-dependent polynomial coecients in the nonstationary-state hidden Markov model (or the trended HMM) which allows for the flexibility of linear time warping or scaling in individual model states.  The need for the state-dependent time warping arises from the consideration that due to speaking rate variation and other temporal factors in speech.  Multiple state-segmented speech data sequences used for training a single set of polynomial coecients often vary appreciably in their sequence lengths.  The algorithm is developed based on a general framework with the use of auxiliary parameters, which, of no interests in themselves, nevertheless provide an intermediate tool for achieving maximal accuracy for estimating the polynomial coecients in the trended HMM.  It is proven that the proposed estimation algorithm converges to a solution equivalent to the state-optimized maximum likelihood estimate.  Effectiveness of the algorithm is demonstrated in experiments designed to fit a single trended HMM simultaneously to multiple sequences of speech data (from the TIMIT database) which are different renditions of the same word yet vary over a wide range in the sequence length.  Speech recognition experiments have been performed based on the standard acoustic-phonetic TIMIT database.  The speech recognition results demonstrate the advantages of the time-warping trended HMMs over the regular trended HMMs about 10 to 15 % improvement in terms of the recognition rate. 
Estimation of Articulatory Parameters from Speech Acoustics by Kalman Filtering| Abstract This paper presents our research work of estimating articulatory states and model parameters from speech acoustics.  This work represents a prerequisite for a speech recognition system based on articulatory dynamical models.  The model parameter estimation was based on the vowels obtained from 590 sentences of 59 speakers from TIMIT speech database, whereas the state estimation experiments have been done using the vowels from 100 sentences of 10 speakers from TIMIT.  The 10 English vowels investigated have the following transcription: /AA/, /AE/, /AH/, /AO/, /EH/, /EY/, /IH/, /IY/, /UH/ and /UW/.  For each vowel, articulatory dynamical models were created using secondorder dynamical systems.  The states of these models represent the positions of articulators such as lips, tongue and pharynx.  We used 8 state parameters to represent these articulators and 3 formant frequencies to represent the acoustic observation vectors.  The nonlinear relationship between articulatory state vector and speech acoustics has been approximated using a piecewise linear approximation on small regions in the articulatory space.  This linearization was performed using a codebook of 610,000 pairs of articulatory and acoustic vectors created using the Metropolis algorithm.  The whole codebook was then linearized on about 7,500 regions using a vector quantization method.  The articulatory model parameters and states were estimated using the EM (expectation-maximization) algorithm and Kalman filtering techniques.  The results obtained in estimating the articulatory parameters encourage us to apply this technique to automatic speech recognition. 
ACOUSTIC-TO-ARTICULATORY INVERSION USING DYNAMICAL AND PHONOLOGICAL CONSTRAINTS| ABSTRACT A well-known difficulty in using the articulatory representation for applications in the areas of speech coding, synthesis and recognition is the poor accuracy in the estimation of the articulatory parameters from the acoustic signal of speech.  The difficulty is especially serious for most classes of consonantal sounds.  This paper presents a statistical method of estimating the articulatory trajectories from the speech signal based on training databases of articulatory-acoustic parameters obtained from continuous speech utterances.  The estimation of articulatory trajectories uses the extended Kalman filtering (EKF) technique and is based on new linguistic constraints imposed to acoustic-to-articulatory inversion.  These new constraints are mainly implemented by dividing the whole articulatory-acoustic function into a number of phonological sub-functions, each corresponding to a unit of speech defined as the patterns of the continuous transition between two consecutive phonemes.  The articulatory-acoustic sub-function is a part of the state-space model that represents each phonological unit of speech.  A method of segmenting the speech signal and recognizing the phonological units was developed based on likelihood computation from Kalman filtering with different models.  The final estimation of articulatory trajectories is obtained from Kalman smoother using the parameters of the recognized models.  Estimation results compared to articulographic and X-ray speech data are presented in this paper.  Average RMS errors of about 2 mm have been obtained between estimated and actual articulatory trajectories. 
Spying Out Real User Preferences for Metasearch Engine Personalization| Abstract Most current metasearch engines provide uniform service to users but do not cater for the specific needs of individual users.  To address this problem, research has been done on personalizing a metasearch engine.  An interesting and practical approach is to optimize its ranking function using clickthrough data.  However, it is still challenging to infer accurate user preferences from the clickthrough data.  In this paper, we propose a novel learning technique called "Spy Naive Bayes" (SpyNB) to identify the user preference pairs generated from clickthrough data.  We then employ ranking SVM to build a metasearch engine optimizer.  To evaluate the eectiveness of SpyNB on ranking quality, we develop a metasearch engine prototype that comprises three underlying search engines: MSNSearch 1 , WiseNut 2 and Overture 3 to conduct experimental evaluation.  The empirical results show that, compared with the original ranking, SpyNB can significantly improve the average ranks of users' click by 20%, while the performance of the existing methods are not satisfactory. 
MULTI-SENSORY MICROPHONES FOR ROBUST SPEECH DETECTION, ENHANCEMENT AND RECOGNITION| ABSTRACT In this paper, we present new hardware prototypes that integrate several heterogeneous sensors into a single headset and describe the underlying DSP techniques for robust speech detection, enhancement and recognition in highly non-stationary noisy environments.  We also speculate other business uses with this type of devices. 
Joint State and Parameter Estimation for a Target-Directed Nonlinear Dynamic System Model| Abstract In this paper, we present a new approach to joint state and parameter estimation for a target-directed, nonlinear dynamic system model with switching states.  The model is also called the hidden dynamic model (HDM) recently proposed for representing speech dynamics.  The model parameters subject to statistical estimation consist of the target vector and the system matrix (also called the "time-constants"), as well as the parameters characterizing the non-linear mapping from the hidden state to the observation.  These latter parameters are implemented in the current work as the weights of a three-layer feedforward multi-layer perceptron (MLP) network.  The new estimation approach presented in this paper is based on the extended Kalman filter (EKF), and its performance is compared with the more traditional approach based on the expectation-maximisation (EM) algorithm.  Extensive simulation experiment results are presented using the proposed EKF-based and the EM algorithms and under the typical conditions for employing the HDM for speech modeling.  The results demonstrate superior convergence performance of the EKF-based algorithm compared with the EM algorithm, but the former suffers from excessive computational loads when adopted for training the MLP weights.  In all cases, the simulation results show that the simulated model output converges to the given observation sequence.  However, only in the case where the MLP weights or the target vector are assumed known, do the time-constant parameters converge to their true values.  We also show that the MLP weights never converge to their true values, thus demonstrating the many-to-one mapping property of the feed-forward MLP.  We conclude from these simulation experiments that for the system to be identifiable, restrictions on the parameter space are needed. 
HIERARCHICAL PARTITION OF THE ARTICULATORY STATE SPACE FOR OVERLAPPING-FEATURE BASED SPEECH RECOGNITION| ABSTRACT We describe our recent work on improving an overlapping articulatory feature (sub-phonemic) based speech recognizer with robustness to the requirement of training data.  A new decision-tree algorithm is developed and applied to the recognizer design which results in hierarchical partitioning of the articulatory state space.  The articulatory states associated with common acoustic correlates, a phenomenon caused by the many-to-one articulation-to-acoustics mapping well known in speech production, are automatically clustered by the decision-tree algorithm.  This enables effective prediction of the unseen articulatory states in the training, thereby increasing the recognizer's robustness.  Some preliminary experimental results are provided. 
Classification of Articulatory and Acoustic Patterns of Speech in the Task Spaces| Abstract The performance of automatic speech recognition is limited at present by the large variability encountered in speech.  A potential area of research for improving the performance of automatic speech recognition is the articulatory representation of speech.  This involves the study of some articulatory invariant features of speech patterns which could be detected from speech acoustics.  Besides automatic speech recognition, the articulatory representations of speech could also have applications in speech synthesis, speech coding and training deaf people to speak.  This paper presents some preliminary results of our research in the fields of articulatory representation of speech and classification of articulatory and acoustic patterns of speech in some spaces called task spaces.  These task spaces are phoneme specific and are defined as some subspaces of the original articulatory and acoustic spaces in which the patterns are maximally concentrated. 
H1 FILTERING FOR SPEECH ENHANCEMENT| ABSTRACT In this paper, a new approach based on the H1 filtering is presented for speech enhancement.  This approach differs from the traditional modified Wiener/Kalman filtering approach in the following two aspects: 1) no aprioriknowledge of the noise statistics is required; instead the noise signals are only assumed to have finite energy; 2) the estimation criterion for the filter design is to minimize the worst possible amplification of the estimation error signal in terms of the modeling errors and additive noises.  Since most additive noises in speech are not Gaussian, this approach is highly robust and is more appropriate in practical speech enhancement.  The global signal-tonoise ratio (SNR), time domain speech representation and listening evaluations are used to verify the performance of the H1 filtering algorithm.  Experimented results show that the filtering performance is better than other speech enhancement approaches in the literature under similar experimental conditions. 
Submitted to Signal Processing, December 1999 Parameter Estimation of a Target-Directed Dynamic System Model with Switching States| Abstract In this paper, we describe an implementation of the extended Kalman filter (EKF) for joint state and parameter estimation for a target-directed, switching state-space nonlinear system model and compare its performance with a maximum-likelihood parameter estimation procedure based on the Expectation-Maximisation (EM) algorithm.  The model parameters consist of the target one and the time-constant one.  Simulation experimental results are presented for individual and joint estimation of all model parameters for both algorithms.  The results show that both algorithms are able to converge to the true target parameter in the model, with the EKF approach exhibiting faster convergence.  This is true even under the target-undershoot condition when the observation sequence is relatively short.  However, convergence to the true time-constant parameter is not evident, possibly due to the non-unique nature of the parameter estimation problem.  We also show empirically that in the case of joint estimation of the parameters, the EM algorithm diverges shortly after a small number of iterations whereas the EKF approach gives more desirable convergence properties. 
AIR- AND BONE-CONDUCTIVE INTEGRATED MICROPHONES FOR ROBUST SPEECH DETECTIONAND ENHANCEMENT| ABSTRACT We present a novel hardware device that combines a regular microphone with a bone-conductive microphone.  The device looks like a regular headset and it can be plugged into any machine with a USB port.  The bone-conductive microphone has an interesting property: it is insensitive to ambient noise and captures the low frequency portion of the speech signals.  Thanks to the signals from the boneconductive microphone, we are able to detect very robustly whether the speaker is talking, eliminating more than 90% of background speech.  Furthermore, by combining both channels, we are able to significantly remove background speech even when the background speaker speaks at the same time as the speaker wearing the headset. 
Geophysics in Object-Oriented Numerics (GOON): An informal conference| ABSTRACT We held an open and informal conference at SEP attempting to facilitate cooperation in introducing Object Oriented computational techniques among workers in Geophysics.  Java seems attractive for the next century.  For the remainder of this century a wide variety of language combination options were considered and many opinions were expressed.  A few who stayed longest at the conference (perhaps the most dedicated to cooperation) developed a linkage of the Gockenbach-Symes HCL (Hilbert Class Library) in C++ to Fortran 90.  PREFACE We held a "Workshop on object-oriented numerics and reproducible research in computational geophysics
SIMULATION OF DISORDERED SPEECH USING A FREQUENCY-DOMAIN VOCAL TRACT MODEL| ABSTRACT In this paper, we address the issue of how the perception of disorderness in selected types of speech disorders may be correlated with the abnormal articulatory structure and with the related acoustic properties.  As a first step towards this end we have developed an articulatory synthesizer based on frequency-domain simulation of vocal-tract wave propagation.  The synthesizer has been implemented by three numerical methods --- Runge-Kutta, ABCD matrix, and Finite difference, which provide frequency-domain solutions to the transmission-line equation characterizing a lossy vocal tract.  The synthesizer is applied in preliminary experiments where the synthesizer's outputs are used to match samples from a corpus of steady-state speech sound, obtained from a dysarthric speaker, uttered in the /hV/ context. 
ON CRITICAL EXPONENTS FOR A SYSTEM OF HEAT EQUATIONS COUPLED IN THE BOUNDARY CONDITIONS| Abstract.  In this paper, we consider the system u t = flu, v t = #v x 2 R N + , t } 0, - @u @x1 = v p , - @v @x1 = u q x1 = 0, t } 0,
Monotone method for nonlinear nonlocal hyperbolic problems| Abstract We present recent results concerning the application of the monotone method for studying existence and uniqueness of solutions to general firstorder nonlinear nonlocal hyperbolic problems.  The limitations of comparison principles for such nonlocal problems are discussed.  To overcome these limitations, we introduce new definitions for upper and lower solutions. 
State-dependent time warping in the trended hidden Markov model| Total number of pages: 24
A mixed-level switching dynamic system for continuous speech recognition|
Learning to recognize time series: Combining arma models with memory-based learning|
Phonetic Classification and Recognition Using HMM Representation of Overlapping Articulatory Features for all classes of English sounds,"|
Hidden markov model representation of quantized articulatory features for speech recognition|
A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features|
Parameter estimation of a target-directed dynamic system model with switching states",|
Spontaneous speech recognition using a statistical coarticulatory model for the hidden vocal-tract-resonance dynamics,|
A path-stack algorithm for optimizing dynamic regimes in a statistical hidden dynamic model of speech",|
Maximum Likelihood in statistical estimation of dynamic systems: Decomposition algorithm and simulation results",|
A statistical coarticulatory model for the hidden vocal-tract-resonance dynamics|
Initial evaluation of hidden dynamic models on conversational speech|
A dynamic, feature-based approach to the interface between phonology and phonetics for speech modeling and recognition",|
Maximum-Likelihood Estimation for Articulatory Speech Recognition Using a Stochastic Target Model|
"Speech Trajectory Discrimination Using the Minimum Classification Error Training,|
A well-collimated quasi-continuous atom laser|
Blow-up rates for parabolic systems,|
The role of critical exponents in blowup theorems:|
Discrete H1 Filter Design with Application to Speech Enhancement",|
On the blow up of u t at quenching|
Multiresolution Instance-Based Learning|
Analysis of acoustic-phonetic variations in fluent speech using TIMIT",|
Polynomial Regression Predictions|
Studies of surface coverage and orientation of DNA molecules immobilized onto preformed alkanethiol self-assembled monolayers,|
"Vocal-Tract Length Normalization for Acoustic-to-Articulatory Mapping Using Neural Networks"|
The Trended HMM with Discriminative Training for Phonetic Classification,|
Production models as a structural basis for automatic speech recognition|
Data-driven model construction for continuous speech recognition using overlapping acoustic features,|
Efficient Locally Weighted Polynomial Regression Predictions|
Acoustic RecognitionComponent of an 86,000-word Speech Recognizer,|
Integrated multilingual speech recognition using universal phonological features in a functional speech production model|
Context-dependent Markov model structured by locus equations|
A Locus model of coarticulation in an HMM speech recognizer|
Comparative performance of spectral subtraction and HMM-based speech enhancement with application to hearing aid design,"|
Evaluation of the SPLICE Algorithm on the Aurora2 Database,|
A new method for speech denoising and robust speech recognition using probabilistic models for clean speech and for noise|
Parameter estimation for Markov modulated Poisson processes via the EM algorithm with time discretization|
Speech recognition using hidden markov models with polynomial regression functions as nonstationary states|
Modeling acoustic transitions in speech by state-interpolation hidden Markov models|
HMM representation of quantized articulatory features for recognition of highly confusible words",|
Comparison principle for some nonlocal problems|
HMM-based speech recognition using state-dependent discriminatively derived transforms on Mel-warped DFT features|
ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition|
Large-vocabulary speech recognition under adverse acoustic environments|
Recursive estimation of nonstationary noise using iterative stochastic approximation for robust speech recognition|
HMM adaptation using vector taylor series for noisy speech recognition,|
Exploting variances in robust feature extraction based on a parametric model for speech distortion",|
Evaluation of the SPLICE on the Aurora2 and 3 Tasks",|
Prediction of kink band width in compressed ber composites|
Responses of auditory-nerve fibers to multipletone complexes|
Time domain computation of a nonlinear nonlocal cochlear model with applications to multitone interaction in hearing,|
Iterative approximation of Lipschitz strictly pseudocontractive mappings in uniformly smooth Banach spaces,|
A statistical model for formant-transition microsegments of speech incorporating locus equations|
Nonexistence of entire solutions of a coupled elliptic system,|
Phase-coherent amplification of matter waves|
A generalized hidden markov model with state-conditioned trend functions of time for the speech signal|
HMM-based strategies for enhancement of speech signals embedded in nonstationary noise,|
The CWP object-oriented optimization library:|
High-performance robust speech recognition using stereo training data|
Efficient decoding strategy for conversational speech recognition using state-space models for vocal-tract-resonance dynamics|
Variational inference and learning for segmental switching state space models of hidden speech dynamics|
Optimization of dynamic regimes in a statistical hidden dynamic model for conversational speech recognition|
A statistical approach to ASR using atomic units constructed from overlapping articulatory features|
Dynamic formant tracking of noisy speech using temporal analysis on outputs from a nonlinear cochlear model|
\Computational Models for speech production",|
Algonquin: Iterating laplace's method to remove multiple types of acoustic distortion for robust speech recognition,|
A Bayesian approach to speech feature enhancement using the dynamic cepstral prior|
Nonstationary-state hidden Markov model representation of speech signals for speech enhancement,|
Enhancement of nuclear factor-kappa B acetylation by coactivator p300 and HIV-1 Tat proteins,|
"A comparion of three nonlinear observation models for noisy speech features,|
A Dynamic Feature Based Approach to Speech Modeling and Recognition,|
Efficient locally weighted polynomial regression prediction,|
The CWP object-oriented optimization library|
A composite model of the auditory periphery for the processing of speech,|
A difference approximation for a coupled system of nonlinear size-structured populations,|
On a first order hyperbolic coagulation model,|
Existence-uniqueness of solutions for a nonlinear nonautonomous size-structured population model: an upper-lower solution approach,|
A monotone approximation for a nonlinear nonautonomous size-structured population model,|
On blowup of u t at quenching,|
An Overlapping-Feature Based Phonological Model Incorporating Linguistic Constraints: Applications to Speech Recognition",|
Speech analysis and recognition using interval statistics generated from a composite auditory model",|
Multiresolutioninstance-lutio learning|
Tracking vocal tract resonances using an analytical nonlinear predictor and a target-guided temporal constraint|
Evaluation of SPLICE on the aurora 2 and 3 tasks,|
"Speech recognition using dynamically defined speech units,"|
Analysis of the correlation structure for a neural predictive model with application to speech recognition|
"A Bayesian Approach to the Verification Problem: Application to Speaker Verification,|
Speaker independent phonetic classification using hidden Markov models with state conditioned mixtures of trend functions,|
Use of generalized dynamic feature parameters for speech recognition,|
Modeling microsegments of stop consonants in a hidden Markov model based word recognizer,|
"A mixture linear model with target-directed dynamics for spontaneous speech recognition",|
Processing of acoustic signals in a cochlear model incorporating laterally coupled suppresive elements,|
Use of generalized dynamic feature parameters for speech recognition: maximum likelihood and minimum classification error approaches|
An expectationmaximization approach for formant tracking using a parameter-free non-linear predictor,|
and XHuang|
Challenges in adopting speech recognition|
A structured speech model with continuous hidden dynamics and prediction-residual training for tracking vocal tract resonances,|
A stochastic framework for articulatory speech recognition|
Hmm adaptation using vector taylor series for noisy speech,"|
Waveform-based speech recognition using hidden filter models: Parameter selection and sensitivity to power normalization,|
"A Dynamic System Approach to Speech Enhancement Using the H # Filter Algorithm",|
Game Theory Approach to Discrete H # Filter Design",|
Microstructural speech units and their HMM representations for discrete utterance speech recognition|
"A nonlinear observation model for removing noise from corrupted speech log mel-spectral energies,"|
Recursive estimation of nonstationary noise using a nonlinear model with iterative stochastic approximation,|
Efficient on-line acoustic environment estimation for FCDCN in a continuous speech recognition system,|
Speech denoising and dereverberation using probabilistic models: Mathematical details|
A Monotone Approximation for the Nonautonomous SizeStructured Population Model,|
Coherent splitting of Bose-Einstein consensed atoms with optically induced Bragg diffraction,|
Transitional speech units and their representation by the regressive Markov states: Applications to speech recognition,|
Design of a feature-based speech recognizer aiming at integration of auditory processing, signal modeling, and phonological structure of speech,|

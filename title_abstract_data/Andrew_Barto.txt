Nonlinear Damping Dynamics and the Variability of Rapid Aimed Movements| Abstract This report describes a set of computational experiments aimed at studying movement variability and speed-accuracy trade-off relationships for dynamic models with nonlinear damping.  We analyzed ballistic movements with noisy control signals and noisy plant dynamics.  Most of the theories suggested so far to explain observed speed-accuracy relationships are purely kinematic and do not take into consideration the dynamic behavior of the system in question.  A dynamic model with fractional-power damping is motivated by biological evidence and allows a completely new view of the speed-accuracy trade-off phenomenon.  This model gives rise to a dynamic behavior called a stiction region.  Data obtained from our simulations make a good fit with the linear speed-accuracy relationship model.  The speed-accuracy relationship critically depends on the proportion of movements that hit the inner area of the stiction region versus the proportion of movements that undershoot or overshoot the region, thereby effectively stopping on an edge of the region.  These results provide a new perspective on possible mechanisms for both the linear and logarithmic speed-accuracy relationships observed in reaching. 
Elevator Group Control Using Multiple Reinforcement Learning Agents| Abstract.  Recent algorithmic and theoretical advances in reinforcement learning (RL) have attracted widespread interest. 
Combining Reinforcement Learning with a Local Control Algorithm| Abstract We explore combining reinforcement learning with a hand-crafted local controller in a manner suggested by the chaotic control algorithm of Vincent, Schmitt and Vincent (1994).  A closedloop controller is designed using conventional means that creates a domain of attraction about a target state.  Chaotic behavior is used or induced to bring the system into this region, at which time the local controller is turned on to bring the system to the target state and stabilize it there.  We describe experiments in which we use reinforcement learning instead of, and in addition to, chaotic behavior to learn an efficient policy for driving the system into the local controller's domain of attraction.  Using a simulated double pendulum, we illustrate how this method allows reinforcement learning to be effective in a problem that cannot be easily solved by reinforcement learning alone, and we show how reinforcement learning can improve upon the chaotic control algorithm when the domain of attraction can only be approximately determined.  Similar results are shown using the Hnon map.  This is a simple and effective way of extending reinforcement learning to more difficult problems. 
Supervised Learning Combined with an Actor-Critic Architecture| Abstract To address the shortcomings of reinforcement learning (RL) a number of researchers have focused recently on ways to take advantage of structure in RL problems and on ways to make domain knowledge part of RL algorithms.  In this paper we examine a supervised actor-critic architecture, whereby a supervisor adds structure to a learning problem and supervised learning makes that structure part of an actor-critic framework for reinforcement learning.  We provide a steepest descent algorithm for real-valued actions such that the actor adjusts its policy in accordance with gradient information from both supervisor and critic.  We also illustrate the approach with two kinds of supervisors: a feedback controller that is easily designed yet suboptimal, and a human operator providing intermittent control of a simulated robotic arm. 
Adaptive linear quadratic control using policy iteration| Abstract In this paper we present stability and convergence results for Dynamic Programmingbased reinforcement learning applied to Linear Quadratic Regulation (LQR).  The specific algorithm we analyze is based on Q-learning and it is proven to converge to the optimal controller provided that the underlying system is controllable and a particular signal vector is persistently excited.  The performance of the algorithm is illustrated by applying it to a model of a flexible beam. 
Accelerating Reinforcement Learning through the Discovery of Useful Subgoals| Abstract An ability to adjust to changing environments and unforeseen circumstances is likely to be an important component of a successful autonomous space robot.  This paper shows how to augment reinforcement learning algorithms with a method for automatically discovering certain types of subgoals online.  By creating useful new subgoals while learning, the agent is able to accelerate learning on a current task and to transfer its expertise to related tasks through the reuse of its ability to attain subgoals.  Subgoals are created based on commonalities across multiple paths to a solution.  We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions.  We introduced this approach in [10] and here we present additional results for a simulated mobile robot task. 
AUTONOMOUS DISCOVERY OF TEMPORAL ABSTRACTIONS FROM INTERACTION WITH AN ENVIRONMENT| The ability to create and to use abstractions in complex environments, that is, to systematically ignore irrelevant details, is a key reason that humans are effective problem solvers.  Although the utility of abstraction is commonly accepted, there has been relatively little research on autonomously discovering or creating useful abstractions.  A system that can create new abstractions autonomously can learn and plan in situations that its original designer was not able to anticipate.  This dissertation introduces two related methods that allow an agent to autonomously discover and create temporal abstractions from its accumulated experience with its environment.  A temporal abstraction is an encapsulation of a complex set of actions into a single higher-level action that allows an agent to learn and plan while ignoring details that appear at finer levels of temporal resolution.  The main idea of both methods is to search vii for patterns that occur frequently within an agent's accumulated successful experience and that do not occur in unsuccessful experiences.  These patterns are used to create the new temporal abstractions.  The two types of temporal abstractions that our methods create are 1) subgoals and closed-loop policies for achieving them, and 2) open-loop policies, or action sequences, that are useful "macros. " We demonstrate the utility of both types of temporal abstractions in several simulated tasks, including two simulated mobile robot tasks.  We use these tasks to demonstrate that the autonomously created temporal abstractions can both facilitate the learning of an agent within a task and can enable effective knowledge transfer to related tasks.  As a larger task, we focus on the difficult problem of scheduling the assembly instructions for computers with multiple pipelines in such a manner that the reordered instructions will execute as quickly as possible.  We demonstrate that the autonomously discovered action sequences can significantly improve performance of the scheduler and can enable effective knowledge transfer across similar processors.  Both methods can extract the temporal abstractions from collections of behavioral trajectories generated by different processes.  In particular, we demonstrate that the methods can be effective when applied to collections generated by reinforcement learning agents, heuristic searchers, and human tele-operators.  viii TABLE OF CONTENTS Page ACKNOWLEDGMENTS .  . 
Learning to Reach Via Corrective Movements| Abstract When infants begin to perform goal-directed reaching between approximately 4 and 5 months-of-age, the kinematics of their reaches show multiple accelerations and decelerations of the hand, which appear to reflect a correcting series of submovements.  Although each submovement is an inaccurate correction, the sequence of submovements is often successful in reaching the target.  By what process does the infant's reach controller come to eventually produce adult-like reaching behavior, characterized by bell-shaped velocity profiles and only occasional corrections? We suggest that motor command parameters are tuned in a supervised learning fashion using training information derived from proprioceptive feedback received during corrective movements.  Although this is not high-quality training information, it is sufficient to tune motor command parameters in order to bring a dynamic arm quickly and accurately to the target, as we show in a series of simulations.  We also suggest that additional parameters can be tuned to satisfy constraints other than end-point constraints through a reinforcement learning process. 
Using Crude Corrective Movements to Learn Accurate Motor Programs for Reaching| Abstract A computational model that uses crude corrective movements to learn accurate reaching programs is presented.  The model learns a feed-forward motor program in the form of pulse-step commands which drive a dynamic, 2 DOF, planar arm.  The arm is actuated by two pairs of opposing muscles, which drive the shoulder and elbow joints.  The model is able to learn motor programs that accurately bring the arm to the target, while producing in many cases bell-shaped tangential velocity profiles. 
A Model of Wrist Movement Representation in Primary Motor Cortex (DRAFT - please do not distribute)| Abstract In executing a voluntary movement, we are faced with the problem of translating a specification of the movement in task space (e. g.  a visual goal) into a muscle recruitment pattern.  Among many brain regions, the primary motor cortex (MI) plays a prominent role in the specification of arm and hand movements.  Early experiments suggested that individual MI cells contribute to movements in a visual-centered (or extrinsic) coordinate system.  Subsequent experimental and theoretical work have argued for a muscle-centered encoding of movement by MI cells.  However, in a recent two-dimensional wrist step tracking experiment, Kakei et al.  (1999) observed both extrinsic-like and muscle-like task-related cells in MI.  Thus result was interpreted as evidence for a cascade of transformations within MI from an extrinsic representation of movement to a muscle-like representation.  This paper presents a model which examines the complexity of the transformation from the extrinsic visual space to the intrinsic muscle space that implements the wrist movements on which Kakei et al.  (1999) focused.  We demonstrate that, given a realistic extrinsic-like representation of movement, a simple linear network is capable of representing the transformation from the extrinsic-like cells to the necessary muscle activation pattern.  This suggests that cells exhibiting extrinsic-like qualities can be involved in the direct recruitment of spinal motor neurons.  These results call into question models that presume a serial cascade of transformations that terminates with MI pyramidal tract neurons (PTNs) that vary their activity exclusively with muscle activity. 
Explaining Temporal Differences to Create Useful Concepts for Evaluating States| Abstract We describe a technique for improving problemsolving performance by creating concepts that allow problem states to be evaluated through an efficient recognition process.  A temporal-difference (TD) method is used to bootstrap a collection of useful concepts by backing up evaluations from recognized states to their predecessors.  This procedure is combined with explanation-based generalization (EBG) and goal regression to use knowledge of the problem domain to help generalize the new concept definitions.  This maintains the efficiency of using the concepts and accelerates the learning process in comparison to knowledge-free approaches.  Also, because the learned definitions may describe negative conditions, it becomes possible to use EBG to explain why some instance is not an example of a concept.  The learning technique has been elaborated for minimax gameplaying and tested on a Tic-Tac-Toe system, T2.  Given only concepts defining the end-game states and constrained to a two-ply search bound, experiments show that T2 learns concepts for achieving near-perfect play.  T2's total searching time, including concept recognition, is within acceptable performance limits while perfect play without the concepts requires searches taking well over 100 times longer than T2's. 
A Computational Model of Cerebellar Learning for Limb Control \Lambda| Abstract We present a model of cerebellar learning for control of limb movements.  The model learns to generate relatively smooth and direct arm movements, while not assuming (as some models do) a high-level input that specifies a preplanned trajectory.  The model controls a simulated two-joint arm, which is actuated by a set of six muscles.  Control of the arm is shared by a learning cerebellar module and a hard-wired extra-cerebellar (EC) system.  The cerebellar module consists of a group of independent submodules, referred to as Adjustable Pattern Generators (APGs).  Each APG is made up of a set of Purkinje cells and a single cerebellar nuclear cell.  Activation of an APG's nuclear cell drives a muscle synergy (a subset of the muscles).  The EC system is responsible for producing low-quality corrective movements in situations where the cerebellar module is unable to bring the arm to the specified target.  Via proprioceptive inputs, the modeled inferior olive assesses directional errors in cerebellar-generated movements by observing subsequent corrective movements produced by the EC system. 
SMDP Homomorphisms: An Algebraic Approach to Abstraction in Semi-Markov Decision Processes| Abstract To operate effectively in complex environments learning agents require the ability to selectively ignore irrelevant details and form useful abstractions.  In this article we consider the question of what constitutes a useful abstraction in a stochastic sequential decision problem modeled as a semi-Markov Decision Process (SMDPs).  We introduce the notion of SMDP homomorphism and argue that it provides a useful tool for a rigorous study of abstraction for SMDPs.  We present an SMDP minimization framework and an abstraction framework for factored MDPs based on SMDP homomorphisms.  We also model different classes of abstractions that arise in hierarchical systems.  Although we use the options framework for purposes of illustration, the ideas are more generally applicable.  We also show that the conditions for abstraction we employ are a generalization of earlier work by Dietterich as applied to the options framework. 
Intrinsically Motivated Reinforcement Learning| Abstract Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems.  Psychologists call these intrinsically motivated behaviors.  What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise.  In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. 
Heuristic Search in Infinite State Spaces Guided by Lyapunov Analysis| Abstract In infinite state spaces, many standard heuristic search algorithms do not terminate if the problem is unsolvable.  Under some conditions, they can fail to terminate even when there are solutions.  We show how techniques from control theory, in particular Lyapunov stability analysis, can be employed to prove the existence of solution paths and provide guarantees that search algorithms will find those solutions.  We study both optimal search algorithms, such as A*, and suboptimal/real-time search methods.  A Lyapunov framework is useful for analyzing infinite-state search problems, and provides guidance for formulating search problems so that they become tractable for heuristic search.  We illustrate these ideas with experiments using a simulated robot arm. 
Learning Instance-Independent Value Functions to Enhance Local Search| Abstract Reinforcement learning methods can be used to improve the performance of local search algorithms for combinatorial optimization by learning an evaluation function that predicts the outcome of search.  The evaluation function is therefore able to guide search to low-cost solutions better than can the original cost function.  We describe a reinforcement learning method for enhancing local search that combines aspects of previous work by Zhang and Dietterich (1995) and Boyan and Moore (1997, Boyan 1998).  In an off-line learning phase, a value function is learned that is useful for guiding search for multiple problem sizes and instances.  We illustrate our technique by developing several such functions for the Dial-A-Ride Problem.  Our learning-enhanced local search algorithm exhibits an improvement of more then 30% over a standard local search algorithm. 
Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density| Abstract This paper presents a method by which a reinforcement learning agent can automatically discover certain types of subgoals online.  By creating useful new subgoals while learning, the agent is able to accelerate learning on the current task and to transfer its expertise to other, related tasks through the reuse of its ability to attain subgoals.  The agent discovers subgoals based on commonalities across multiple paths to a solution.  We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions.  We illustrate this approach using several gridworld tasks. 
To Appear in Neural Computation 1 A Cerebellar Model of Timing and Prediction in the Control of Reaching| Abstract A simplified model of the cerebellum was developed to explore its potential for adaptive, predictive control based on delayed feedback information.  An abstract representation of a single Purkinje cell with multistable properties was interfaced, via a formalized premotor network, with a simulated single degree-of-freedom limb.  The limb actuator was a nonlinear spring-mass system based on the nonlinear velocity dependence of the stretch reflex.  By including realistic mossy fiber signals, as well as realistic conduction delays in afferent and efferent pathways, the model allowed the investigation of timing and predictive processes relevant to cerebellar involvement in the control of movement.  The model regulates movement by learning to react in an anticipatory fashion to sensory feedback.  Learning depends on training information generated from corrective movements and uses a temporally-asymmetric form of plasticity for the parallel fiber synapses on Purkinje cells. 
Cerebellar Learning for Control of a Two-Link Arm in Muscle Space \Lambda| Abstract Biological control systems have long been studied as possible inspiration for the construction of robotic controllers.  The cerebellum is known to be involved in the production and learning of smooth, coordinated movements.  In this paper, we present a model of cerebellar control of a muscle-actuated, two-link, planar arm.  The model learns in a trial-and-error fashion to produce bursts of muscle activity that accurately bring the arm to a specified target.  When the cerebellum fails to bring the arm to the target, an extra-cerebellar module performs low-quality corrective movements, from which the cerebellum may update its program.  In learning to perform the task, the cerebellum constructs an implicit inverse model of the plant.  This model uses a combination of delayed sensory signals and recentlygenerated motor commands to compute the new output motor signal. 
A Predictive Switching Model of Cerebellar Movement Control| Abstract We present a hypothesis about how the cerebellum could participate in regulating movement in the presence of significant feedback delays without resorting to a forward model of the motor plant.  We show how a simplified cerebellar model can learn to control endpoint positioning of a nonlinear spring--mass system with realistic delays in both afferent and efferent pathways.  The model's operation involves prediction, but instead of predicting sensory input, it directly regulates movement by reacting in an anticipatory fashion to input patterns that include delayed sensory feedback. 
PolicyBlocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning| Abstract We present PolicyBlocks, an algorithm by which a reinforcement learning agent can extract useful macro-actions from a set of related tasks.  The agent creates macroactions by finding commonalities in solutions to previous tasks.  Using these macro-actions, learning to do future related tasks is accelerated.  This increase in performance is illustrated in a \rooms" grid-world, in which the macro-actions found by PolicyBlocks outperform even hand designed macro-actions, and in a hydroelectric reservoir control task.  We provide empirical comparisons of PolicyBlocks with the Reuse options of Bernstein (1999) and the SKILLS algorithm of Thrun and Schwartz (1995), which elucidate conditions under which each algorithm performs well. 
Symmetries and Model Minimization in Markov Decision Processes| Abstract Current solution and modelling approaches to Markov Decision Processes (MDPs) scale poorly with the size of the MDP.  Model minimization methods address this issue by exploiting redundancy in problem specification to reduce the size of the MDP model.  Symmetries in a problem specification can give rise to special forms of redundancy that are not exploited by existing minimization methods.  In this work we extend the model minimization framework proposed by Dean and Givan to include symmetries.  We base our framework on concepts derived from finite state automata and group theory. 
Intrinsically Motivated Reinforcement Learning: A Promising Framework For Developmental Robot Learning| Abstract One of the primary challenges of developmental robotics is the question of how to learn and represent increasingly complex behavior in a self-motivated, open-ended way. 
Optimal Control Methods for Simulating the Perception of Causality in Young Infants| Abstract There is a growing debate among developmental theorists concerning the perception of causality in young infants.  Some
Convergence of Indirect Adaptive Asynchronous Value Iteration Algorithms| Abstract Reinforcement Learning methods based on approximating dynamic programming (DP) are receiving increased attention due to their utility in forming reactive control policies for systems embedded in dynamic environments.  Environments are usually modeled as controlled Markov processes, but when the environment model is not known a priori, adaptive methods are necessary.  Adaptive control methods are often classified as being direct or indirect.  Direct methods directly adapt the control policy from experience, whereas indirect methods adapt a model of the controlled process and compute control policies based on the latest model.  Our focus is on indirect adaptive DP-based methods in this paper.  We present a convergence result for indirect adaptive asynchronous value iteration algorithms for the case in which a look-up table is used to store the value function.  Our result implies convergence of several existing reinforcement learning algorithms such as adaptive real-time dynamic programming (ARTDP) (Barto, Bradtke, & Singh, 1993) and prioritized sweeping (Moore & Atkeson, 1993).  Although the emphasis of researchers studying DP-based reinforcement learning has been on direct adaptive methods such as Q-Learning (Watkins, 1989) and methods using TD algorithms (Sutton, 1988), it is not clear that these direct methods are preferable in practice to indirect methods such as those analyzed in this paper. 
LARGE-SCALE DYNAMIC OPTIMIZATION USING TEAMS OF REINFORCEMENT LEARNING AGENTS| Recent algorithmic and theoretical advances in reinforcement learning (RL) are attracting widespread interest.  RL algorithms have appeared that approximate dynamic programming (DP) on an incremental basis.  Unlike traditional DP algorithms, these algorithms do not require knowledge of the state transition probabilities or reward structure of a system.  This allows them to be trained using real or simulated experiences, focusing their computations on the areas of state space that are actually visited during control, making them computationally tractable on very large problems.  RL algorithms can be used as components of multi-agent algorithms.  If each member of a team of agents employs one of these algorithms, a new collective learning algorithm emerges for the team as a whole.  In this dissertation we demonstrate that such collective RL algorithms can be powerful heuristic methods for addressing large--scale control problems.  vi Elevator group control serves as our primary testbed.  The elevator domain poses a combination of challenges not seen in most RL research to date.  Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems.  Their states are not fully observable and they are non-stationary due to changing passenger arrival rates.  As a way of streamlining the search through policy space, we use a team of RL agents, each of which is responsible for controlling one elevator car.  The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state.  In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware.  These results demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility. 
Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning| Abstract We present a new method for automatically creating useful temporal abstractions in reinforcement learning.  We argue that states that allow the agent to transition to a different region of the state space are useful subgoals, and propose a method for identifying them using the concept of relative novelty.  When such a state is identified, a temporallyextended activity (e. g. , an option) is generated that takes the agent eciently to this state.  We illustrate the utility of the method in a number of tasks. 
Robot Weightlifting By Direct Policy Search| Abstract This paper describes a method for structuring a robot motor learning task.  By designing a suitably parameterized policy, we show that a simple search algorithm, along with biologically motivated constraints, offers an effective means for motor skill acquisition.  The framework makes use of the robot counterparts to several elements found in human motor learning: imitation, equilibrium-point control, motor programs, and synergies.  We demonstrate that through learning, coordinated behavior emerges from initial, crude knowledge about a difficult robot weightlifting task. 
Evaluating the Feasibility of Learning Student Models from Data| Abstract Recent work on intelligent tutoring systems has used Bayesian networks to model students' acquisition of skills.  In many cases, researchers have hand-coded the parameters of the networks, arguing that the conditional probabilities of models containing hidden variables are too difficult to learn from data.  We present a machine learning approach that uses Expectation-Maximization to learn the parameters of a dynamic Bayesian network with hidden variables.  We test our methodology on data that was simulated using a state-based model of skill acquisition.  Results indicate that it is possible to learn the parameters of hidden variables given enough sequential data of training sessions on similar problems. 
Relativized Options: Choosing the Right Transformation| Abstract Relativized options combine model minimization methods and a hierarchical reinforcement learning framework to derive compact reduced representations of a related family of tasks.  Relativized options are defined without an absolute frame of reference, and an option's policy is transformed suitably based on the circumstances under which the option is invoked.  In earlier work we addressed the issue of learning the option policy online.  In this article we develop an algorithm for choosing, from among a set of candidate transformations, the right transformation for each member of the family of tasks. 
Robust Reinforcement Learning in Motion Planning| Abstract While exploring to find better solutions, an agent performing online reinforcement learning (RL) can perform worse than is acceptable.  In some cases, exploration might have unsafe, or even catastrophic, results, often modeled in terms of reaching `failure' states of the agent's environment.  This paper presents a method that uses domain knowledge to reduce the number of failures during exploration.  This method formulates the set of actions from which the RL agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies.  The resulting action set has a more abstract relationship to the task being solved than is common in many applications of RL.  Although the cost of this added safety is that learning may result in a suboptimal solution, we argue that this is an appropriate tradeoff in many problems.  We illustrate this method in the domain of motion planning. 
Reinforcement Learning for Mixed Open-loop and Closed-loop Control| Abstract Closed-loop control relies on sensory feedback that is usually assumed to be free.  But if sensing incurs a cost, it may be costeffective to take sequences of actions in open-loop mode.  We describe a reinforcement learning algorithm that learns to combine open-loop and closed-loop control when sensing incurs a cost.  Although we assume reliable sensors, use of open-loop control means that actions must sometimes be taken when the current state of the controlled system is uncertain.  This is a special case of the hidden-state problem in reinforcement learning, and to cope, our algorithm relies on short-term memory.  The main result of the paper is a rule that significantly limits exploration of possible memory states by pruning memory states for which the estimated value of information is greater than its cost.  We prove that this rule allows convergence to an optimal policy. 
Machine Learning for Subproblem Selection| Abstract Subproblem generation, solution, and recombination is a standard approach to combinatorial optimization problems.  In many settings identifying suitable subproblems is itself a significant component of the technique.  Such subproblems are often identified using a heuristic rule.  Here we show how to use machine learning to make this identi#cation.  In particular we use a learned objective function to direct search in an appropriate space of subproblem decompositions.  We demonstrate the ecacy of our technique for problem decomposition on a particular wellknown combinatorial optimization problem, graph coloring for geometric graphs. 
Learning to Act Using Real-Time Dynamic Programming| Abstract Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence.  Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known.  We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience.  RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty.  We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems.  We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm.  A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory. 
A Model of Cerebellar Learning for Control of Arm Movements Using Muscle Synergies \Lambda| Abstract Biological control systems have long been studied as possible inspiration for the construction of robotic controllers.  The cerebellum is known to be involved in the production and learning of smooth, coordinated movements.  In this paper, we present a model of cerebellar control of a muscle-actuated, two-link, planar arm.  The model learns in a trial-and-error fashion to generate the appropriate sequence of motor signals that accurately bring the arm to a specified target.  The motor signals produced by the cerebellum are specified in muscle synergy space.  When the cerebellum fails to bring the arm to the target, an extra-cerebellar module performs low-quality corrective movements, from which the cerebellum updates its program.  In learning to perform the task, the cerebellum constructs an implicit inverse model of the plant.  This model uses a combination of delayed sensory signals and recentlygenerated motor commands to compute the new output motor signal. 
Task Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks| Abstract A novel modular connectionist architecture is presented in which the networks composing the architecture compete to learn the training patterns.  An outcome of the competition is that different networks learn different training patterns and, thus, learn to compute different functions.  The architecture performs task decomposition in the sense that it learns to partition a task into two or more functionally independent tasks and allocates distinct networks to learn each task.  In addition, the architecture tends to allocate to each task the network whose topology is most appropriate to that task.  The architecture's performance on "what" and "where" vision tasks is presented and compared with the performance of two multi--layer networks.  Finally, it is noted that function decomposition is an underconstrained problem and, thus, different modular architectures may decompose a function in different ways.  We argue that a desirable decomposition can be achieved if the architecture is suitably restricted in the types of functions that it can compute.  Appropriate restrictions can be found through the application of domain knowledge.  A strength of the modular architecture is that its structure is well--suited for incorporating domain knowledge.  Although many biologists and psychologists share the view that the brain has a modular architecture, there is no general agreement on the number of modules, the function of the modules, the nature of the interaction between modules, or the manner in which the modules develop.  One reason for this diversity of opinion is that answering questions about the modular nature of the brain involves the difficult task of reasoning about a system with a large number of interacting components.  Even systems of interacting components with a small fraction of the brain's complexity present formidable conceptual and analytical difficulties.  In many cases, mathematical and computer models provide essential tools for understanding aspects of these systems.  One class of models that has the potential for helping to answer questions about modular systems is the class of connectionist models, also known as artificial neural network models.  A hierarchical classification of the components of connectionist models may be defined in which a unit is the finest level of classification, a layer is a coarser level, and a network is a still coarser level.  Connectionist researchers typically design systems that are modular at the level of units or layers.  In this paper we argue that there are significant practical and theoretical advantages to be realized by considering modularity at the level of networks.  In other words, we argue that connectionist architectures should consist of multiple networks, and that connectionist learning algorithms should be designed to take advantage of this modular structure.  Although terms such as layer or network are imprecise, it is generally agreed that they provide a convenient language for discussing connectionist architectures.  An analogous situation occurs in the neurosciences where researchers debate whether nervous systems are
LEARNING TO EXPLOIT DYNAMICS FOR ROBOT MOTOR COORDINATION| occasionally on topics far astray from the latest technical challenge of my research.  I always learn something new from our time together, and Andy is a wonderful role model of how to study a problem from many different points of view.  To seek out or even to simply recognize the connections among diverse scientific disciplines is a valuable skill to instill in a young scientist. 
Learning Reactive Admittance Control| Abstract In this paper, a peg-in-hole insertion task is used as an example to illustrate the utility of direct associative reinforcement learning methods for learning control under real-world conditions of uncertainty and noise.  An associative reinforcement learning system has to learn appropriate actions in various situations through search guided by evaluative performance feedback.  We used such a learning system, implemented as a connectionist network, to learn active compliant control for peg-in-hole insertion.  Our results indicate that direct reinforcement learning can be used to learn a reactive control strategy that works well even in the presence of a high degree of noise and uncertainty. 
Intrinsically Motivated Learning of Hierarchical Collections of Skills| Abstract Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems.  Psychologists call these intrinsically motivated behaviors.  What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise.  In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.  At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 
Monte Carlo Matrix Inversion and Reinforcement Learning| Abstract We describe the relationship between certain reinforcement learning (RL) methods based on dynamic programming (DP) and a class of unorthodox Monte Carlo methods for solving systems of linear equations proposed in the 1950's.  These methods recast the solution of the linear system as the expected value of a statistic suitably defined over sample paths of a Markov chain.  The significance of our observations lies in arguments (Curtiss, 1954) that these Monte Carlo methods scale better with respect to state-space size than do standard, iterative techniques for solving systems of linear equations.  This analysis also establishes convergence rate estimates.  Because methods used in RL systems for approximating the evaluation function of a fixed control policy also approximate solutions to systems of linear equations, the connection to these Monte Carlo methods establishes that algorithms very similar to TD algorithms (Sutton, 1988) are asymptotically more efficient in a precise sense than other methods for evaluating policies.  Further, all DP-based RL methods have some of the properties of these Monte Carlo algorithms, which suggests that although RL is often perceived to be slow, for sufficiently large problems it may in fact be more efficient than other known classes of methods capable of producing the same results. 
An Algebraic Approach to Abstraction in Reinforcement Learning| Abstract To operate effectively in complex environments learning agents have to selectively ignore irrelevant details by forming useful abstractions.  In this article we outline a formulation of abstraction for reinforcement learning approaches to stochastic sequential decision problems modeled as semiMarkov Decision Processes (SMDPs).  Building on existing algebraic approaches, we propose the concept of SMDP homomorphism and argue that it provides a useful tool for a rigorous study of abstraction for SMDPs.  We apply this framework to different classes of abstractions that arise in hierarchical systems and discuss relativized options, a framework for compactly specifying a related family of temporally-extended actions.  Additional details of this work are described in refs.  [1, 2, 3]. 
A Computational Model of Muscle Recruitment for Wrist Movements| To execute a movement, the CNS must appropriately select and activate the set of muscles that will produce the desired movement.  This problem is particularly difficult because a variety of muscle subsets can usually be used to produce the same joint motion.  The motor system is therefore faced with a motor redundancy problem that must be resolved to produce the movement.  In this paper, we present a model of muscle recruitment in the wrist step-tracking task.  Muscle activation levels for five muscles are selected so as to satisfy task constraints (moving to the designated target) while also minimizing a measure of the total effort in producing the movement.  Imposing these constraints yields muscle activation patterns qualitatively similar to those observed experimentally.  In particular, the model reproduces the observed cosine-like recruitment of muscles as a function of movement direction and also appropriately predicts that certain muscles will be recruited most strongly in movement directions that differ significantly from their direction of action.  These results suggest that the observed recruitment behavior may not be an explicit strategy employed by the nervous system, but instead may result from a process of movement optimization. 
Cortical Involvement in the Recruitment of Wrist Muscles| Abstract In executing a voluntary movement, one is faced with the problem of translating a specification of the movement in task space (e. g.  a visual goal) into a muscle recruitment pattern.  Among many brain regions, the primary motor cortex (MI) plays a prominent role in the specification of movements.  In what coordinate frame MI represents movement has been a topic of considerable debate.  In a two-dimensional wrist step tracking experiment, Kakei et al.  (1999) described some MI cells as encoding movement in a muscle coordinate frame and other cells as encoding movement in an extrinsic coordinate frame.  This result was interpreted as evidence for a cascade of transformations within MI from an extrinsic representation of movement to a muscle-like representation.  However, we present a model that demonstrates that, given a realistic extrinsic-like representation of movement, a simple linear network is capable of representing the transformation from an extrinsic-space to the muscle recruitment patterns implementing the movements on which Kakei et al.  (1999) focused.  This suggests that cells exhibiting extrinsic-like qualities can be involved in the direct recruitment of spinal motor neurons.  These results call into question models that presume a serial cascade of transformations terminating with MI pyramidal tract neurons that vary their activation exclusively with muscle activity.  Further analysis of the model shows that the correlation between the activity of an MI neuron and a muscle does not predict the strength of the connection between the MI neuron and muscle.  This result cautions against the use of correlation methods as a measure of cellular connectivity. 
The emergence of movement units through learning with noisy efferent signals and delayed sensory feedback| Rapid human arm movements often have velocity profiles consisting of several bell-shaped acceleration-deceleration phases, sometimes overlapping in time and sometimes appearing separately.  We show how such sub-movement sequences can emerge naturally as an optimal control policy is approximated by a reinforcement learning system in the face of uncertainty and feedback delay.  The system learns to generate sequences of pulse-step commands, producing fast initial sub-movements followed by several slow corrective sub-movements that often begin before the initial sub-movement has completed.  These results suggest how the nervous system might efficiently control a stochastic motor plant under uncertainty and feedback delay. 
Automated State Abstraction for Options using the U-Tree Algorithm| Abstract Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks.  An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task.  In this paper, we study hierarchical learning using the framework of options.  We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated.  We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task.  Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective. 
Reinforcement Learning and Local Search: A Case Study| Abstract We describe a reinforcement learning-based variation to the combinatorial optimization technique known as local search.  The hillclimbing aspect of local search uses the problem's primary cost function to guide search via local neighborhoods to high quality solutions.  In complicated optimization problems, however, other problem characteristics can also help guide the search process.  In this report we present an approach to constructing more general, derived, cost functions for combinatorial optimization problems using reinforcement learning.  Such derived cost functions integrate a variety of problem characteristics into a single hillclimbing function.  We illustrate our technique by developing several such functions for the Dial-A-Ride Problem, a variant of the well-known Traveling Salesman Problem. 
Lyapunov-Constrained Action Sets for Reinforcement Learning| Abstract Lyapunov analysis is a standard approach to studying the stability of dynamical systems and to designing controllers.  We propose to design the actions of a reinforcement learning (RL) agent to be descending on a Lyapunov function.  For minimum cost-to-target problems, this has the theoretical benefit of guaranteeing that the agent will reach a goal state on every trial, regardless of the RL algorithm it uses.  In practice, Lyapunov-descent constraints can significantly shorten learning trials, improve initial and worst-case performance, and accelerate learning.  Although this method of constraining actions may limit the extent to which an RL agent can minimize cost, it allows one to construct robust RL systems for problems in which Lyapunov domain knowledge is available.  This includes many important individual problems as well as general classes of problems, such as the control of feedback linearizable systems (e. g. , industrial robots) and continuous-state path-planning problems.  We demonstrate the general approach on two simulated control problems: pendulum swing-up and robot arm control. 
Correspondence to: Jette Randlv Category: Reinforcement Learning and Control No part of this paper has been submitted before| Learning and control in a chaotic system.  Abstract Most real world problems can be solved at least partially by simple, nonadaptive means.  When we want to use an adaptive learning technique like reinforcement learning for practical purposes, we need to be able to cooperate with one or several of these partial solutions.  In this paper we consider the problem of making a reinforcement learning agent cooperate with a hand-crafted local controller and a global chaotic controller, and designing a regime that improves upon these controller. 
MODELS OF THE CEREBELLUM AND MOTOR LEARNING|
Identifying Useful Subgoals in Reinforcement Learning by Local Graph Partitioning| Abstract We present a new method for automatically creating useful temporallyextended actions in reinforcement learning.  Our method identifies states that lie between two densely-connected regions of the state space and generates temporally-extended actions that take the agent efficiently to these states.  We search for these states using a graph partitioning algorithm on local estimates of the transition graph---those that are constructed using only the most recent experiences of the agent.  This local perspective is a key property of our algorithm and one that differentiates it from most of the earlier work in this area. 
Local Bandit Approximation for Optimal Learning Problems| Abstract In general, procedures for determining Bayes-optimal adaptive controls for Markov decision processes (MDP's) require a prohibitive amount of computation---the optimal learning problem is intractable.  This paper proposes an approximate approach in which bandit processes are used to model, in a certain "local" sense, a given MDP.  Bandit processes constitute an important subclass of MDP's, and have optimal learning strategies (defined in terms of Gittins indices) that can be computed relatively efficiently.  Thus, one scheme for achieving approximately-optimal learning for general MDP's proceeds by taking actions suggested by strategies that are optimal with respect to local bandit models. 
Basic-block Instruction Scheduling Using Reinforcement Learning and Rollouts| Abstract The execution order of a block of computer instructions on a pipelined machine can make a difference in its running time by a factor of two or more.  In order to achieve the best possible speed, compilers use heuristic schedulers appropriate to each specific architecture implementation.  However, these heuristic schedulers are time-consuming and expensive to build.  We present empirical results using both rollouts and reinforcement learning to construct heuristics for scheduling basic blocks.  In simulation, both the rollout scheduler and the reinforcement learning scheduler outperformed a commercial scheduler on several applications.  1 Motivation Although high-level code is generally written as if it were going to be executed sequentially, most modern computers exhibit parallelism in instruction execution using techniques such as the simultaneous issue of multiple instructions.  In order to take the best advantage of multiple pipelines, when a compiler turns the high-level code into machine instructions, it employs an instruction scheduler to reorder the machine code.  The scheduler needs to reorder the instructions in such a way as to preserve the original in-order semantics of the high level code while having the reordered code execute as quickly as possible.  An efficient schedule can produce a speedup in execution of a factor of two or more.  Building an instruction scheduler can be an arduous process.  Schedulers are specific to the architecture of each machine and the general problem of instruction scheduling is NP-hard (Proebsting).  Because of these characteristics, schedulers are currently built using handcrafted heuristic algorithms.  However, this method is both labor and time intensive.  Building algorithms to select and combine heuristics automatically using machine learning techniques can save time and money.  As computer architects develop new machine designs, new schedulers would be built automatically to test design changes rather than requiring hand-built heuristics for each change.  This would allow architects to explore the design space more thoroughly and to use more accurate metrics in evaluating designs.  A second possible use of machine learning techniques in instruction scheduling is by the end user.  Instead of scheduling code using a static scheduler trained on benchmarks when the compiler was written, a user would employ a learning scheduler to discover important characteristics of that user's code.  The learning scheduler would exploit the user's coding characteristics to build schedules better tuned for that user.  Instruction scheduling is a large-scale optimization problem in several ways.  First, there can be millions of instructions and tens of thousands of basic blocks within a given program.  Scheduling each block optimally using exhaustive search is much too difficult and timeconsuming to work in practice.  Second, the problem of generalizing the scheduler from the training programs to all possible user programs is also an optimization problem.  With these motivations in mind, we formulated and tested two autonomous methods of building an instruction scheduler.  The first method used rollouts (Bertsekas, 1997; Bertsekas et al. , 1997; Tesauro and Galperin, 1996) and the second focused on reinforcement learning (RL) (Sutton and Barto, 1998).  Both methods were implemented for the Digital Alpha 21064.  The next section gives a brief overview of the domain.  For a complete description, see McGovern et al.  (1999). 
Neuronlike elements that can solve difficult learning control problems|
Learning and sequential decision making|
Distributed sensorimotor learning|
Distributed representations of limb motor programs in arrays of adjustable pattern generators|
Prediction of complex two-dimensional trajectories by a cerebellar model of smooth pursuit eye movement|
Time-derivative models of Pavlovian reinforcement|
Cooperativity in networks of pattern recognizing stochastic learning automata|
A model of how the basal ganglia generate and use neural signals that predict reinforcement|
Reinforcement learning and adaptive critic methods|
An Online Book of Reinforcement Learning,|
Training and Tracking in Robotics|
Improving Elevator Performance Using Reinforcement Learning|
Learning to act using real-time dynamic programming|
Pattern recognizing stochastic learning automata|
A temporaldifference model of classical conditioning| Proceedings of the ninth conference of the cognitive science society. 
SP| Real-Time Learning and Control using Asynchronous Dynamic Programming. 
RS and Watkins, CJCH| Learning and Sequential Decision Making. 
Recent advances in hierarchical reinforcement learning| Discrete Event Systems, Special issue on reinforcement learning,. 
An adaptive sensorimotor network simulation inspired by the anatomy and physiology of the cerebellum|
Linear Least-Squares Algorithms for Temporal Difference Learning|
Lyapunov Design for Safe Reinforcement Learning|
Connectionist approaches for control|
Connectionist learning for control: An overview|
Adaptive critics and the basal ganglia|
Forming control policies from simulation models using reinforcement learning|
Some learning tasks from a control perspective|
An Actor/Critic Algorithm that is Equivalent to Q-Learning|
Reinforcement Learning: An Introduction MIT Press|
Distributed motor commands in the limb premotor network|
Towards a modern theory of adaptive networks: Expectation and prediction|
Text-Based Information Retrieval Using Exponentiated Gradient Descent|
Associative search network: a reinforcement learning associative memory,|
Sequential decision problems and neural networks|
An adaptive network that constructs and uses an internal model of its environment|
Reinforcement learning is direct adaptive control|
Model Minimization in Hierarchical Reinforcement Learning|
Landmark learning: An illustration of associative search|
On the Computational Economics of Reinforcement Learning|
Synthesis of nonlinear control surfaces by a layered associative search network|
An approach to Learning Control Surfaces by Connectionist Systems",|
Simulation of the classically conditioned nictitating membrane response by a neuron-like adaptive element: Response topography, neuronal firing and interstimulus intervals|
Reinforcement learning",|
Reinforcement Learning|
Adaptive predictive control with a cerebellar model|
Models of cerebellum and motor learning|
Adaptation of learning rate parameters, Appendix C of Goal Seeking Components for Adaptive Intelligence: An Initial Assessment| Air Force Wright Aeronautical Laboratories/Avionics. 
Structural learning in connectionist systems|
Linear discriminant diverse density for automatic discovery of subgoals in reinforcement learning|
Reinforcement learning in motor ontrol|
A temporal-di#erence method of classical conditioning|
Task Decompostiion Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks|
Shaping as a method for accelerating reinforcement learning|
Neural problem solving|
Reinforcement learning: Course notes|
Tensor product variable binding and the representation of symbolic structures in connectionist networks|
A model of prediction in smooth eye movements|
Controlling a nonlinear spring-mass system with a cerebellar model|
Simulation of anticipatory responses in classical conditioning by a neuron-like adaptive element|
Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks|
Statistical machine learning for large-scale optimization,|
Symmetries and model minimization of Markov decision processes|
Automatic discogery of subgoals in reinforcement learning using diverse density|
Simulation of Anticipatory Responses in Classical Conditioning by a Neuron-like Adaptive Element",|
Linear systems analysis of the relationship between firing of deep cerebellar neurons and the classically conditioned nictitating membrane response in rabbits|
Connectionist learning for control| In Miller WT, Sutton RS and Werbos PJ (Eds). 
Pattern recognition and feedback via parallel distributed processing|
Neural Networks and Adaptive Control|
Reinforcement learning, final exam CMPSCI|
A pulse-step model of control for arm reaching movements|
If desired, the present analysis of the stochastic neuronal dynamics can be replaced by an analysis of this deterministic neuronal dynamics,|
Learning Admittance Mappings for Force-Guided Assembly|
Sutton R S and Anderson C W 1983 Neuronlike adaptive elements that solve dicult learning control problems|
to appear)| Learning to act using real-time dynamic programming. 
A computational model of muscle recruitment for wrist movements|
Improving elevator performace using reinforcement learning|
A Cerebellar Model of Timing and Prediction in the Control of Reaching|
Combining reinforcement learning with a local control algorithm|
Convergence to optimal cost of adaptive policy iteration|
this volume| Learning to act using real-time dynamic programming. 
Reinforcement learning and its relationship to supervised learning|
Toward a modern theory of adaptative network : Expectation and prediction|
Reinforcement Learning --- An Introd ction",|
"Reinforcement Learning is Direct Adaptice Optimal Control",|
Agents learning about agents: a Framework and Analysis",|
Neural networks for control",|
Adaptive Neural Networks for Learning Control: Some Computational Experiments,|

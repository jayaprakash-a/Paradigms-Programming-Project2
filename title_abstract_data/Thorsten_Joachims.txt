Eye-tracking analysis of user behavior in WWW search| ABSTRACT We investigate how users interact with the results page of a WWW search engine using eye-tracking.  The goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration.  Such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback (e. g.  clickthrough) for machine learning.  The following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as measures of how thoroughly searchers evaluate their results set. 
Support vector machine learning for interdependent and structured output spaces| Abstract Learning general functional dependencies is one of the main goals in machine learning.  Recent progress in kernel-based methods has focused on designing flexible and powerful input representations.  This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces.  We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs.  The resulting optimization problem is solved eciently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem.  We demonstrate the versatility and eectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment. 
The enigmatics of affect| ABSTRACT Affective computation generally focuses on the informatics of affect: structuring, formalizing, and representing emotion as informational units.  We propose instead an enigmatics of affect, a critical technical practice that respects the rich and undefinable complexities of human affective experience.  Our interactive installation, the Influencing Machine, allows users to explore a dynamic landscape of emotionally expressive sound and child-like drawings, using a tangible, intuitive input device that supports open-ended engagement.  The Influencing Machine bridges the subjective experience of the user and the necessary objective rationality of the underlying code.  It functions as a cultural probe, reflecting and challenging users to reflect on the cultural meaning of affective computation. 
Transductive Learning via Spectral Graph Partitioning| Abstract We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier.  Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods.  We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently.  A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits.  Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case. 
thematic issue on Knowledge-Based Information Management in Intensive Care|
WebWatcher: A Learning Apprentice for the World Wide Web| Abstract We describe an information seeking assistant for the World Wide Web.  This agent, called WebWatcher, interactively helps users locate desired information by employing learned knowledge about which hyperlinks are likely to lead to the target information.  Our primary focus to date has been on two issues: (1) organizing WebWatcher to provide interactive advice to Mosaic users while logging their successful and unsuccessful searches as training data, and (2) incorporating machine learning methods to automatically acquire knowledge for selecting an appropriate hyperlink given the current web page viewed by the user and the user's information goal.  We describe the initial design of WebWatcher, and the results of our preliminary learning experiments.  1 Overview Many have noted the need for software to assist people in locating information on the world wide web.  This paper 1 presents the initial design and implementation of an agent called WebWatcher that is intended to assist users both by interactively advising them as they traverse web links in search of information, and by searching autonomously on their behalf.  In interactive mode, WebWatcher acts as a learning apprentice [Mitchell et al. , 1985, Mitchell et.  al. , 1994], providing interactive advice to the Mosaic user regarding which hyperlinks to follow next, then learning by observing the user's reaction to this advice as well as the eventual success or failure of the user's actions.  The initial implementation of WebWatcher provides only this interactive mode, and it does not yet possess sufficient knowledge to give widely useful search advice.  In this paper we present WebWatcher as a case study in the design of web-based learning agents for information retrieval.  We focus in particular on the interface that enables WebWatcher to observe and advise any consenting user browsing any location on the web, and on results of initial experiments with its learning methods.  1 This paper originally appeared in the 1995 AAAI Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments.  March, 1995.  2 WebWatcher This section presents the design of WebWatcher through a scenario of its use.  WebWatcher is an information search agent that is "invoked" by following a web hyperlink to its web page, then filling out a Mosaic form to indicate what information is sought (e. g. , a publication by some author).  WebWatcher then returns the user to (a copy of) the web page from which he or she came, and assists the user as they follow hyperlinks forward through the web in search of the target information.  As the user traverses the web, WebWatcher uses its learned knowledge to recommend especially promising hyperlinks to the user by highlighting these links on the user's display.  At any point, the user may dismiss WebWatcher, by clicking one of two indicators on the WebWatcher icon, indicating either that the search has succeeded, or that the user wishes to give up on this search.  The sequence of web pages visited by the user in a typical scenario is illustrated in figures 1 through 5.  The first screen shows a typical web page, 2 providing information about Machine Learning.  Notice in the third paragraph, this page invites the user to try out WebWatcher.  If the user clicks on this link, he or she arrives at the front door WebWatcher page (Figure 2), which allows the user to identify the type of information he seeks.  In this scenario the user indicates that the goal is to locate a paper, so he is shown a new screen (Figure 3) with a form to elaborate this information request.  Once completed, the user is returned to the original page (Figure 4), with WebWatcher now "looking over his shoulder".  Notice the WebWatcher icon at the top of the screen, and the highlighted link (bracketed by the WebWatcher eyes icon) halfway down the screen.  This highlighted link indicates WebWatcher's advice that the user follow the link to the University of Illinois / Urbana (UIUC) AI / ML Page.  The user decides to select this recommended link, and arrives at the new web page shown in Figure 5, which contains new advice from WebWatcher.  The search continues in this way, with the user directing the search and WebWatcher highlighting recommended links, until the user dismisses WebWatcher by clicking on "I found it" or "I give up".  From the user's perspective WebWatcher is an agent with specialized knowledge about how to search outward from the page on which it was invoked.  While WebWatcher suggests which hyperlink the user should take, the user remains firmly in control, and may ignore the system's advice at any step.  We feel it is important for the user to remain in control, because WebWatcher's knowledge may provide imperfect advice, and because WebWatcher might not perfectly understand the user's information seeking goal.  From WebWatcher's perspective, the above scenario looks somewhat different.  When first invoked it accepts an argument, encoded in the URL that accesses it, which contains the user's "return address. " The return address is the URL of the web page from which the user came.  Once the user fills out the form specifying his or her information seeking goal, WebWatcher sends the user back to a copy of this original page, after making three changes.  First, the WebWatcher banner is added to the top of the page.  Second, each hyperlink URL in the original page is replaced by a new URL that points back to the WebWatcher.  Third, if the WebWatcher finds that any of the hyperlinks on this page are strongly recommended by its search control knowledge, then it highlights the most promising links in order to suggest them to the user.  It sends this modified copy of the return page to the user, and opens a file to begin logging this user's information search as training data.  While it waits for the user's next step, it prefetches any web pages it has just recommended to the user, and begins to process these pages to determine their most promising outgoing hyperlink.  When the user clicks on the next hyperlink, WebWatcher updates the log for this search, 2 This is a copy of the web page http:
A Statistical Learning Model of Text Classification for Support Vector Machines| ABSTRACT This paper develops a theoretical learning model of text classi#cation for Support Vector Machines (SVMs).  It connects the statistical properties of text-classi#cation tasks with the generalization performance of a SVM in a quantitative way.  Unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when SVMs perform well for text classi#cation.  In particular, it addresses the following questions: Why can support vector machines handle the large feature spaces in text classification effectively? How is this related to the statistical properties of text? What are sucient conditions for applying SVMs to text-classi#cation problems successfully?
Transductive Inference for Text Classification using Support Vector Machines| Abstract This paper introduces Transductive Support Vector Machines (TSVMs) for text classi#cation.  While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines takeinto account a particular test set and try to minimize misclassi#cations of just those particular examples.  The paper presents an analysis of why TSVMs are well suited for text classification.  These theoretical findings are supported by experiments on three test collections.  The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks.  This work also proposes an algorithm for training TSVMs e#ciently, handling 10,000 examples and more. 
UNIVERSIT AT DORTMUND Fachbereich Informatik Lehrstuhl VIII Kunstliche Intelligenz Estimating the Generalization Performance of a SVM Efficiently| VIII (KI) Research Reports of the unit no. 
A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization| Abstract The Rocchio relevance feedback algorithm is one of the most popular and widely applied learning methods from information retrieval.  Here, a probabilistic analysis of this algorithm is presented in a text categorization framework.  The analysis gives theoretical insightinto the heuristics used in the Rocchio algorithm, particularly the word weighting scheme and the similarity metric.  It also suggests improvements which lead to a probabilistic variant of the Rocchio classifier.  The Rocchio classifier, its probabilistic variant, and a naiveBayes classifier are compared on six text categorization tasks.  The results show that the probabilistic algorithms are preferable to the heuristic Rocchio classifier not only because they are more well-founded, but also because they achieve better performance. 
The Enigmatics of Affect| ABSTRACT Affective computation generally focuses on the informatics of affect: structuring, formalizing, and representing emotion as informational units.  We propose instead an enigmatics of affect, a critical technical practice that respects the rich and undefinable complexities of human affective experience.  Our interactive installation, the Influencing Machine, allows users to explore a dynamic landscape of emotionally expressive sound and child-like drawings, using a tangible, intuitive input device that supports open-ended engagement.  The Influencing Machine bridges the subjective experience of the user and the necessary objective rationality of the underlying code.  It functions as a cultural probe, reflecting and challenging users to reflect on the cultural meaning of affective computation. 
Evaluating Retrieval Performance Using Clickthrough Data| Abstract This abstract explores a method for evaluating the quality of retrieval functions based on clickthrough data.  Unlike manual relevance
Web Watcher: A Tour Guide for the World Wide Web| Abstract We explore the notion of a tour guide software agent for assisting users browsing the world wide web.  A web tour guide agent provides assistance similar to that provided by a human tour guide in a museum -- it guides the user along an appropriate path through the collection, based on its knowledge of the user's interests, of the location and relevance of various items in the collection, and of the way in which others have interacted with the collection in the past.  This paper describes a simple but operational tour guide, called WebWatcher, which has given over 5000 tours to people browsing CMU's School of Computer Science web pages.  WebWatcher accompanies users from page to page, suggests appropriate hyperlinks, and learns from experience to improve its advice-giving skills. 
Making Large-Scale SVM Learning Practical| VIII (KI) Research Reports of the unit no.  VIII (AI)
A Machine Learning Architecture for Optimizing Web Search Engines| documents.  In this paper, we describe a wide range of such heuristics---including a novel one inspired by reinforcement learning techniques for propagating rewards through a graph---which can be used to affect a search engine's rankings.  We then demonstrate a system which learns to combine these heuristics automatically, based on feedback collected unintrusively from users, resulting in much improved rankings. 
Combining Statistical Learning with a Knowledge-Based Approach - A Case Study in Intensive Care Monitoring| Abstract The paper describes a case study in combining different methods for acquiring medical knowledge.  Given a huge amount of noisy, high dimensional numerical time series data describing patients in intensive care, the support vector machine is used to learn when and how to change the dose of which drug.  Given medical knowledge about and expertise in clinical decision making, a first-order logic knowledge base about effects of therapeutical interventions has been built.  As a preprocessing mechanism it uses another statistical method.  The integration of numerical and knowledge-based procedures eases the task of validation in twoways.  On one hand, the knowledge base is validated with respect to past patients' records.  On the other hand, medical interventions that are recommended by learning results are justified by the knowledge base. 
Text Categorization with Suport Vector Machines: Learning with Many Relevant Features| VIII (KI) Research Reports of the unit no.  VIII (AI)
Learning a Distance Metric from Relative Comparisons| Abstract This paper presents a method for learning a distance metric from relative comparison such as "A is closer to B than A is to C".  Taking a Support Vector Machine (SVM) approach, we develop an algorithm that provides a flexible way of describing qualitative training data as a set of constraints.  We show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard methods for SVM training.  We empirically evaluate the performance and the modelling flexibility of the algorithm on a collection of text documents. 
Integrating Kernel Methods Into a Knowledge-based Approach to Evidence-based Medicine| Operational protocols are a valuable means for quality control.  However, developing operational protocols is a highly complex and costly task.  We present an integrated approach involving both intelligent data analysis and knowledge acquisition from experts that supports the development and validation of operational protocols.  The aim is to lower development cost through the use of machine learning and at the same time ensure high quality standards for the protocol through empirical validation.  We demonstrate our approach of integrating expert knowledge with data driven techniques based on our effort to develop an operational protocol for the hemodynamic system.  1 Introduction An abundance of information is generated during the process of critical care.  Much of this information can now be captured and stored using clinical information systems (CIS) that have become commercially available for use in intensive care over the last years.  These systems provide for a complete medical documentation at the bedside and their clinical usefulness and efficiency has been shown repeatedly [6, 7, 11].  While databases with more than 2,000 separate patient-related variables are now available for further analysis [8], the multitude of variables presented at the bedside even without a CIS precludes medical judgement by humans.  A physician may be confronted with more than 200 variables in the critically ill during a typical morning round [21].  We know, however, that even an experienced physician is often not able to develop a systematic response to any problem involving more than seven variables [18].  Moreover, humans are limited in their ability to estimate the degree of relatedness between only two variables [12].  This problem is most pronounced in the evaluation of the measurable effect of a therapeutic intervention.  Personal bias, experience, and a certain expectation toward the respective intervention may distort an objective judgement [4].  These arguments motivate the use of decision support systems.  Clinical decision support aims at providing health care professionals with therapy guidelines directly at the bed-side.  This should enhance the quality of clinical care, since the guidelines sort out high value practices from those that have little or no value.  The goal of decision support is to supply the best recommendation under all circumstances [22].  The computerized protocol of care can take into account more aspects of the patient than a physician can accommodate.  It is not disturbed by circumstances or hospital constraints.  It bridges the gap between low-level numerical measurements (the level of the equipment) and high-level qualitative principles (the level of medical reasoning).  While knowledge-based systems have mostly been applied for diagnosis and therapy planning (e. g.  [25], [16]), some systems also aim at on-line patient monitoring [5, 17, 22].  Methods that have proved their value in handling low-frequency patient data are not applicable for on-line monitoring [17].  Quantitative measurements and qualitative reasoning have to be integrated in a system that recommends interventions in real-time.  The numerical measurements of the patients' vital signs have to be abstracted into qualitative terms of high abstraction.  The aspect of time has to be handled both at the level of measurements and the level of expert knowledge [3, 14, 17, 25].  In the expert's reasoning, time becomes the relation between time intervals, abstracting from the exact duration of, e. g. , an increasing heart rate, and focusing on tendencies of other parameters (e. g. , cardiac output) within overlapping time intervals.  One of the big obstacles to the more frequent implementation of decision support systems is the tedious and time-consuming task of developing the knowledge base.  The decision support system for respiratory care at the LDS Hospital, Salt Lake City, USA [22], for instance, has been developed in about 25 person years.  The method of guideline development itself is not supported by a computer system.  Mechanisms of temporal abstraction and reasoning presuppose manually designed models or ontologies [3, 17, 25].  Why not use techniques of knowledge discovery and statistical time series analysis in order to ease the process of guideline generation? Machine learning and statistical analysis have been applied in building-up diagnostical systems successfully (e. g. , [15]).  We now want to exploit the huge amount of data for the development of guidelines for on-line monitoring.  Our task is to build a decision support system for on-line hemodynamic monitoring in the critically ill.  We do not aim at modeling the actual physician's behavior.  Imitating the actual interventions made by physicians is not the goal.  Actual behavior is influenced by the overall hospital situation, e. g. , how long is the physician on duty, how many patients require attention at the same time.  Machine learning from patients` data could lead to a knowledge base that mirrors such disturbing effects.  Therefore, the learned decision rules have to be checked by additional rules about effects of drug and fluid administration.  Our approach is to combine statistics, knowledge acquisition, and machine learning.  Our aim is to develop a method for guideline generation that is faster and more reliable than current methods. 
Learning to Align Sequences: A Maximum-Margin Approach| Abstract We propose a discriminative method for learning the parameters (e. g.  cost of substitutions, deletions, insertions) of linear sequence alignment models from training examples.  While the resulting training problem leads to an optimization problem with an exponential number of constraints, we present a simple algorithm that finds an arbitrarily close approximation after considering only a subset of the constraints that is linear in the number of training examples and polynomial in the length of the sequences.  We also evaluate empirically that the method effectively learns good parameter values while being computationally feasible. 
Composite Kernels for Hypertext Categorisation| Abstract Kernels are problem-specific functions that act as an interface between the learning system and the data.  While it is well-known when the combination of two kernels is again a valid kernel, it is an open question if the resulting kernel will perform well.  In particular, in which situations can a combination of kernel be expected to perform better than its components considered separately? We investigate this problem by looking at the task of designing kernels for hypertext classi#cation, where both words and links information can be exploited.  We provide sucient conditions that indicate when an improvement can be expected, highlighting and formalising the notion of \independent kernels".  Experimental results confirm the predictions of the theory in the hypertext domain. 
WebWatcher: Machine Learning and Hypertext| Abstract This paper describes the first implementation of WebWatcher, a Learning Apprentice for the World Wide Web.  We also explore the possibility of extracting information from the structure of hypertext.  We introduce an algorithm which identifies pages that are related to a given page using only hypertext structure.  We motivate the algorithm by using the Minimum Description Length principle. 
`Development and structure of an X|25 implementation',. 
Optimizing search engines using clickthrough data|
A Statistical Learning Model of Text Classification with Support Vector Machines|
Guest Editors' Introduction to the Special Issue on Automated Text Categorization|
Support vector machine|
Categorization with support vector machines: Learning with many relevant features|
Composite kernels for hypertext categorization,|
Svmlight support vector machine| In http://svmlight. joachims. org/,. 
A Machine Learning Architecture for Optimizing Web Searches|
Making Large-Scale Support Vector Machine Learning Practical",|
A Tour Guide for the|
Expected Error Analysis for Model Selection|
Advances in kernel methods support vector learning,|
Practical Advances in Kernel Methods --- Support Vector Learning,|
Browsing-Assistenten, Tour Guides und adaptive WWW-Server|
The Maximum-Margin Approach to Learning Text Classifiers|
Knowledge discovery and knowledge validation in intensive care|
SVM-Light Support Vector Machine 3|5. 
A learning apprentice for the World Wide Web|

Temporally Dependent Plasticity: An Information Theoretic Account| Abstract The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes.  This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity.  We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level.  Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs.  These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters
An Information Theoretic Approach to the Study of Auditory Coding Thesis submitted for the degree| Someone once said that every manuscript is just a graphomaniac appendix to the acknowledgment page.  The current page provides this hypothesis with hard empirical evidence.  This thesis summarizes a wonderful period I spent in the Hebrew University.  I was lucky to study and work with many gifted people who enriched me in numerous and often unexpected ways.  First of all, I am in great debt to Naftali Tishby and Israel Nelken, my supervisors.  I learned from them very different lessons, typical to the experimental and theoretical approaches to scientific work.  Tali instructed me how to seek principled approaches, and to base methodologies on fundamental first-principles.  Eli guided me how to explore experimental data in order to reveal its organization principles, and how to turn empirical findings into well established insightful observations.  They both taught me not only how deep and creative thinking can be combined with careful and detailed investigation, but also the importance of polishing to perfection the clear presentation of findings and ideas.  Their patient guidance, wide knowledge, generosity and friendship turned our joint work into pleasure, and made my PhD period an experience I will always be happy to recall.  My work was based on data collected by a series of dedicated people in several labs, all connected through a joint project under a
A needle in a haystack: local one-class optimization| Abstract This paper addresses the problem of finding a small and coherent subset of points in a given data.  This problem, sometimes referred to as one-class or set covering, requires to find a small-radius ball that covers as many data points as possible.  It rises naturally in a wide range of applications, from finding gene-modules to extracting documents' topics, where many data points are irrelevant to the task at hand, or in applications where only positive examples are available.  Most previous approaches to this problem focus on identifying and discarding a possible set of outliers.  In this paper we adopt an opposite approach which directly aims to find a small set of coherently structured regions, by using a loss function that focuses on local properties of the data.  We formalize the learning task as an optimization problem using the InformationBottleneck principle.  An algorithm to solve this optimization problem is then derived and analyzed.  Experiments on gene expression data and a text document corpus demonstrate the merits of our approach. 
Types, super-types and the mutual information distribution| Abstract Discovering stochastic dependencies in empirical data is a fundamental problem in data modeling.  Dependencies are usually measured using the mutual information or chi square statistics, and then have to be compared to their sampling distribution under the independence hypothesis.  However, for finite sample sizes the exact sampling distribution of these statistics is still unknown.  The current paper presents a novel approach for approximation of these distributions for any finite sample size.  This approach is based on the method of types, used to identify different counts that yield similar independence values.  It provides an approximation to the distribution using a mixture of simple components, thus allowing for ecient numerical calculation of the mutual information distribution.  For binary variables the analysis provides an approximation algorithm that is linear in the sample size and considerably improves on the chi square distribution approximation, especially for the tail of the distribution. 
Information Bottleneck for Gaussian Variables| Abstract The problem of extracting the relevant aspects of data was addressed through the information bottleneck (IB) method, by (soft) clustering one variable while preserving information about another - relevance - variable.  An interesting question addressed in the current work is the extension of these ideas to obtain continuous representations (embeddings) that preserve relevant information, rather than discrete clusters.  We give a formal definition of the general continuous IB problem and obtain an analytic solution for the optimal representation for the important case of multivariate Gaussian variables.  The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized correlation matrix # xjy # 1 x , which is also the basis obtained in Canonical Correlation Analysis.  However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector.  This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level.  Our analysis also provides analytic expression for the optimal tradeoff - the information curve - in terms of the eigenvalue spectrum. 
Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway| Abstract The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach.  Identifying the case of stimulus-conditioned independent neurons, we develop redundancy measures that allow enhanced information estimation for groups of neurons.  These measures are then applied to study the collaborative coding eciency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (A1).  Under two different coding paradigms we show differences in both information content and group redundancies between IC and cortical auditory neurons.  These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized by Barlow (1959).  The redundancy effects under the single-spikes coding paradigm are significant only for groups larger than ten cells, and cannot be revealed with the standard redundancy measures that use only pairs of cells.  Our results suggest that redundancy reduction transformations are not limited to low level sensory processing (aimed to reduce redundancy in input statistics) but are applied even at cortical sensory stations. 
Are There Representations in Embodied Evolved Agents? Taking Measures|
Suffificient dimensionality redcution with irrelevence statistics|
Distributional clustering of movements based on neural responses|
Information bottleneck and linear projections of gaussian processes|
Spike-Timing-Dependent Plasticity and Relevant Mutual Information Maximization|

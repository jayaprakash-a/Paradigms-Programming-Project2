Visualizing the embedding of objects in Euclidean space| ABSTRACT Matrices representing dissimilarities within a set of objects are familiar in mathematics, statistics and psychology.  In this paper we describe XGvis, a software system which accepts diverse input data, such as graphs and multivariate data, develops a dissimilarity matrix from the data, and then iteratively and interactively embeds objects in a Euclidean space of arbitrary dimension.  Using a technique called multidimensional scaling, objects are positioned so that their pairwise distances match the target dissimilarities as well as possible.  Users can interact with XGobi, a software system for visualizing highdimensional data, to browse the resulting embeddings.  Mathematicians and statisticians have found XGvis to be useful for discovering and exploring structure.  XGvis runs under the X Window System TM . 
Modeling Auction Price Uncertainty Using Boosting-based Conditional Density Estimation| Abstract In complicated, interacting auctions, a fundamental problem is the prediction of prices of goods in the auctions, and more broadly, the modeling of uncertainty regarding these prices.  In this paper, we present a machine-learning approach to this problem.  The technique is based on a new and general boosting-based algorithm for conditional density estimation problems of this kind, i. e. , supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label.  This algorithm, which we present in detail, is at the heart of ATTac2001, a top-scoring agent in the recent Trading Agent Competition (TAC-01).  We describe how ATTac-2001 works, the results of the competition, and controlled experiments evaluating the effectiveness of price prediction in auctions. 
Friend-or-Foe Q-learning in General-Sum Games| Abstract This paper describes an approach to reinforcement learning in multiagent general-sum games in which a learner is told to treat each other agent as either a \friend" or \foe".  This Q-learning-style algorithm provides strong convergence guarantees compared to an existing Nash-equilibrium-based learning rule. 
Algorithm Selection using Reinforcement Learning| Abstract Many computational problems can be solved by multiple algorithms, with different algorithms fastest for different problem sizes, input distributions, and hardware characteristics.  We consider the problem of algorithm selection: dynamically choose an algorithm to attack an instance of a problem with the goal of minimizing the overall execution time.  We formulate the problem as a kind of Markov decision process (MDP), and use ideas from reinforcement learning to solve it.  This paper introduces a kind of MDP that models the algorithm selection problem by allowing multiple state transitions.  The well known Q-learning algorithm is adapted for this case in a way that combines both Monte-Carlo and Temporal Difference methods.  Also, this work uses, and extends in a way to control problems, the Least-Squares Temporal Difference algorithm (LSTD(0)) of Boyan.  The experimental study focuses on the classic problems of order statistic selection and sorting.  The encouraging results reveal the potential of applying learning methods to traditional computational problems. 
Agent Mediated Electronic Commerce IV: Designing Mechanisms and#Systems, Springer Verlag| This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01).  ATTac-2001 uses boosting techniques to learn conditional distributions of auction clearing prices.  We present experiments demonstrating the effectiveness of this predictor relative to several reasonable alternatives. 
Representations and Algorithms for Exact Time-Dependent MDPs| Abstract We describe an extension of the Markov decision process model where a continuous time dimension is included in the state space.  This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time.  We examine problems based on route planning with public transportation and telescope observation scheduling. 
An Instance-Based State Representation for Network Repair| Abstract We describe a formal framework for diagnosis and repair problems that shares elements of the well known partially observable MDP and cost-sensitive classification models.  Our cost-sensitive fault remediation model is amenable to implementation as a reinforcementlearning system, and we describe an instance-based state representation that is compatible with learning and planning in this framework.  We demonstrate a system that uses these ideas to learn to efficiently restore network connectivity after a failure. 
Planning and Acting in Partially Observable Stochastic Domains| Abstract In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains.  We begin by introducing the theory of Markov decision processes (mdps) and partially observable mdps (pomdps).  We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a pomdp.  We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.  Consider the problem of a robot navigating in a large office building.  The robot can move from hallway intersection to intersection and can make local observations of its world.  Its actions are not completely reliable, however.  Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots.  It has similar problems with observation.  Sometimes a corridor looks like a corner; sometimes a T-junction looks like an L-junction.  How can such an error-plagued robot navigate, even given a map of the corridors?
Large-Scale Planning Under Uncertainty: A Survey| Abstract Our research area is planning under uncertainty, that is, making sequences of decisions in the face of imperfect information.  We are particularly concerned with developing planning algorithms that perform well in large, real-world domains.  This paper is a brief introduction to this area of research, which draws upon results from operations research (Markov decision processes), machine learning (reinforcement learning), and artificial intelligence (planning).  Although techniques for planning under uncertainty are extremely promising for tackling real-world problems, there is a real need at this stage to look at large-scale applications to provide direction to future development and analysis. 
Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems| Abstract Existing statistical approaches to natural
Planning and Acting in Partially Observable Stochastic Domains| Abstract In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains.  We begin by introducing the theory of Markov decision processes (mdps) and partially observable mdps (pomdps).  We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a pomdp.  We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions. 
Three-Dimensional Tutte Embedding| Abstract Conditions are given for a graph to have a convex representation in three dimensions.  This extends Tutte's barycentric embedding. 
Abstraction Methods for Game Theoretic Poker| Abstract.  Abstraction is a method often applied to keep the combinatorial explosion under control and to solve problems of large complexity.  Our work focuses on applying abstraction to solve large stochastic imperfect-information games, specifically variants of poker.  We examine several different medium-size poker variants and give encouraging results for abstraction-based methods on these games. 
Constraint Satisfaction with Probabilistic Preferences on Variable Values| Abstract Crossword puzzle solving is a classic constraint satisfaction problem, but, when solving a real puzzle, the mapping from clues to variable domains is not perfectly crisp.  At best, clues induce a probability distribution over viable targets, which must somehow be respected along with the constraints of the puzzle.  Motivated by this type of problem, we provide a formal model of constraint satisfaction with probabilistic preferences on variable values.  Two natural optimization problems are defined for this model: maximizing the probability of a correct solution, and maximizing the number of correct words (variable values) in the solution.  We provide an efficient iterative approximation for the latter based on dynamic programming and present very encouraging results on a collection of real and artificial crossword puzzles. 
Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach| Abstract This paper describes the Q-routing algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network.  Only local communication is used by each node to keep accurate statistics on which routing decisions lead to minimal delivery times.  In simple experiments involving a 36-node, irregularly connected network, Q-routing proves superior to a nonadaptive algorithm based on precomputed shortest paths and is able to route efficiently even when critical aspects of the simulation, such as the network load, are allowed to vary dynamically.  The paper concludes with a discussion of the tradeoff between discovering shortcuts and maintaining stable policies. 
On the Complexity of Solving Markov Decision Problems| Abstract Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning.  In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms.  We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly.  To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs. 
An optimization-based categorization of reinforcement learning environments| Abstract 1 This paper proposes a categorization of reinforcement learning environments based on the optimization of a reinforcement signal over time.  Environments are classified by the simplest agent that can possibly achieve optimal reinforcement.  Two parameters, h and fi, abstractly characterize the complexity of an agent: the ideal (h,fi)-agent uses the input information provided by the environment and at most h bits of local storage to choose an action that maximizes the discounted sum of the next fi reinforcements.  In an (h,fi)-environment, an ideal (h,fi)-agent achieves the maximum possible expected reinforcement for that environment.  The paper discusses the special cases when either h = 0 or fi = 1 in detail, describes some theoretical bounds on h and fi and reexplores a well-known reinforcement learning environment with this new notation. 
Probabilistic Propositional Planning: Representations and Complexity| Abstract Many representations for probabilistic propositional planning problems have been studied.  This paper
The Computational Complexity of Probabilistic Planning| Abstract We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both flat and propositional representations.  The complexity of plan evaluation and existence varies with the plan type sought; we examine totally ordered plans, acyclic plans, and looping plans, and partially ordered plans under three natural definitions of plan value.  We show that problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NP PP , co-NP PP , and PSPACE.  In the process of proving that certain planning problems are complete for NP PP , we introduce a new basic NP PP -complete problem, E-Majsat, which generalizes the standard Boolean satisfiability problem to computations involving probabilistic quantities; our results suggest that the development of good heuristics for E-Majsat could be important for the creation of efficient algorithms for a wide variety of problems. 
Approximate Dimension Reduction at NTCIR| Abstract We carried out a comparison of cross-language retrieval methods on the NTCIR-1 data based on dimension reduction (latent semantic indexing).  These methods all use a collection parallel documents (translations or approximate translations) and very little, if any, linguistic knowledge.  In NTCIR-1, we compared latent semantic indexing, local LSI, and approximate dimensional equalization (ADE).  We found that local LSI and ADE performed the best on this collection and were comparable to the best performing systems reported elsewhere.  We also ran ADE on the NTCIR-2 and found it fared considerably less well. 
Learning to Select Branching Rules in the DPLL Procedure for Satisfiability| Abstract The DPLL procedure is the most popular complete satisfiability (SAT) solver.  While its worst case complexity is exponential, the actual running time is greatly affected by the ordering of branch variables during the search.  Several branching rules have been proposed, but none is the best in all cases.  This work investigates the use of automated methods for choosing the most appropriate branching rule at each node in the search tree.  We consider a reinforcement-learning approach where a value function, which predicts the performance of each branching rule in each case, is learned through trial runs on a typical problem set of the target class of SAT problems.  Our results indicate that, provided sufficient training on a given class, the resulting strategy performs as well as (and, in some cases, better than) the best branching rule for that class. 
Tree-Based Batch Mode Reinforcement Learning| Abstract Reinforcement learning aims to determine an optimal control policy from interaction with a system or from observations gathered from a system.  In batch mode, it can be achieved by approximating the so-called Q-function based on a set of four-tuples (x
Predictive Representations of State| Abstract We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations.  State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations.  Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models.  Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls).  We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model.  In predicting or controlling a sequence of observations, the concepts of state and state estimation inevitably arise.  There have been two dominant approaches.  The generative-model approach, typified by research on partially observable Markov decision processes (POMDPs), hypothesizes a structure for generating observations and estimates its state and state dynamics.  The history-based approach, typified by k-order Markov methods, uses simple functions of past observations as state, that is, as the immediate basis for prediction and control.  (The data flow in these two approaches are diagrammed in Figure 1. ) Of the two, the generative-model approach is more general.  The model's internal state gives it temporally unlimited memory| the ability to remember an event that happened arbitrarily long ago|whereas a history-based approach can only remember as far back as its history extends.  The bane of generative-model approaches is that they are often strongly dependent on a good model of the system's dynamics.  Most uses of POMDPs, for example, assume a perfect dynamics model and attempt only to estimate state.  There are algorithms for simultaneously estimating state and dynamics (e. g. , Chrisman, 1992), analogous to the Baum-Welch algorithm for the uncontrolled case (Baum et al. , 1970), but these are only effective at tuning parameters that are already approximately correct (e. g. , Shatkay & Kaelbling, 1997).  State Update observations (and actions) state rep'n observations (and actions) state rep'n 1-step delays (a) (b) Figure 1: Data flow in a) POMDP and other recursive updating of state representation, and b) history-based state representation.  In practice, history-based approaches are often much more effective.  Here, the state representation is a relatively simple record of the stream of past actions and observations.  It might record the occurrence of a specific subsequence or that one event has occurred more recently than another.  Such representations are far more closely linked to the data than are POMDP representations.  One way of saying this is that POMDP learning algorithms encounter many local minima and saddle points because all their states are equipotential.  History-based systems immediately break symmetry, and their direct learning procedure makes them comparably simple.  McCallum (1995) has shown in a number of examples that sophisticated history-based methods can be effective in large problems, and are often more practical than POMDP methods even in small ones.  The predictive state representation (PSR) approach, which we develop in this paper, is like the generative-model approach in that it updates the state representation recursively, as in Figure 1(a), rather than directly computing it from data.  We show that this enables it to attain generality and compactness at least equal to that of the generative-model approach.  However, the PSR approach is also like the history-based approach in that its representations are grounded in data.  Whereas a history-based representation looks to the past and records what did happen, a PSR looks to the future and represents what will happen.  In particular, a PSR is a vector of predictions for a specially selected set of action{observation sequences, called tests (after Rivest & Schapire, 1994).  For example, consider the test a 1 o 1 a 2 o 2 , where a 1 and a 2 are specific actions and o 1 and o 2 are specific observations.  The correct prediction for this test given the data stream up to time k is the probability of its observations occurring (in order) given that its actions are taken (in order) (i. e. , P r fO k = o 1 ; O k+1 = o 2 j A k = a 1 ; A k+1 = a 2 g).  Each test is a kind of experiment that could be performed to tell us something about the system.  If we knew the outcome of all possible tests, then we would know everything there is to know about the system.  A PSR is a set of tests that is sucient information to determine the prediction for all possible tests (a sucient statistic).  As an example of these points, consider the #oat/reset problem (Figure 2) consisting of a linear string of 5 states with a distinguished reset state on the far right.  One action, f (#oat), causes the system to move uniformly at random to the right or left by one state, bounded at the two ends.  The other action, r (reset), causes a jump to the reset state irrespective of the current state.  The observation is always 0 unless the r action is taken when the system is already in the reset state, in which case the observation is 1.  Thus, on an f action, the correct prediction is always 0, whereas on an r action, the correct prediction depends on how many fs there have been since the last r: for zero fs, it is 1; for one or two fs, it is 0. 5; for three or four fs, it is 0. 375; for five or six fs, it is 0. 3125, and so on decreasing after every second f, asymptotically bottoming out at 0. 2.  No k-order Markov method can model this system exactly, because no
Learning Predictive State Representations| Abstract We introduce the first algorithm for learning predictive state representations (PSRs), which are a way of representing the state of a controlled dynamical system.  The state representation in a PSR is a vector of predictions of tests, where tests are sequences of actions and observations said to be true if and only if all the observations occur given that all the actions are taken.  The problem of finding a good PSR|one that is a sucient statistic for the dynamical system|can be divided into two parts: 1) discovery of a good set of tests, and 2) learning to make accurate predictions for those tests.  In this paper, we present detailed empirical results using a gradient-based algorithm for addressing the second problem.  Our results demonstrate several sample systems in which the algorithm learns to make correct predictions and several situations in which the algorithm is less successful.  Our analysis reveals challenges that will need to be addressed in future PSR learning algorithms. 
Contingent Planning Under Uncertainty via Stochastic Satisfiability| Abstract We describe two new probabilistic planning
Exact Solutions to Time-Dependent MDPs| Abstract We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space.  This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time.  We examine problems based on route planning with public transportation and telescope observation scheduling. 
Introduction to the Probabilistic Planning Track| Abstract The 2004 International Planning Competition, IPC-4, includes a probabilistic planning track for the first time.  We briefly summarize the design of the track. 
Adaptation in Constant Utility Non-Stationary Environments| Abstract Environments that vary over time present a fundamental problem to adaptive systems.  Although in the worst case there is no hope of effective adaptation, some forms environmental variability do provide adaptive opportunities.  We consider a broad class of non-stationary environments, those which combine a variable result function with an invariant utility function, and demonstrate via simulation that an adaptive strategy employing both evolution and learning can tolerate a much higher rate of environmental variation than an evolution-only strategy.  We suggest that in many cases where stability has previously been assumed, the constant utility non-stationary environment may in fact be a more powerful viewpoint.  1 Non-stationary environments An adaptive system within an environment performs two basic tasks.  First, there is the search for, and the representation of, regularities in the history of interactions with the environment.  Second, there is the attempt to gain some advantage from the constructed representation, by basing future actions on the assumption that those regularities will persist.  If that fundamental adaptive assumption fails utterly, and the environment totally lacks such regularities, adaptation can provide no benefit.  Of course, not all non-stationary environments are so pathological.  Even if the environment, when viewed as a monolithic function, may be observed to change over time in unpredictable ways, it may be possible to view that environment as being formed from interacting components some of which do possess persistent regularities.  If that is possible, an adaptive system possessing such a view can gain advantage --- even though the environment will always have some surprises in store --- by finding, representing, and exploiting those component regularities.  1. 1 Constant utility In this paper we demonstrate one way this can occur within the context of evolutionary adaptation via the genetic algorithm [11, 8].  Consider this scenario in which an organism is faced with a series of interactions with an environment: In each interaction, the environment presents the organism with a situation, and then the organism responds with an action, and then the environment responds with a result.  A utility function, hidden from the organism, reduces the result to a scalar value.  The average utility of the results created by the organism over some given lifespan determines the organism's fitness.  The nasty trick is this: At unpredictable times and in unpredictable ways, the environmental mapping from situation and action to result changes.  As a consequence, if averaged over many environmental changes, the fitness produced by any single mapping will be at the chance level.  The adaptive challenge is to produce organisms that yield high online fitness (i. e. , fitness averaged over successive generations of organisms, see [6]), in the face of the changing environment.  Although this scenario is familiar enough to genetic algorithm researchers in some ways, it also has certain unusual aspects that are central to the present endeavor.  To begin with, the environment is non-stationary: The fitness of a given situation-action map changes over time.  Also, the fitness function can readily be decomposed into two sub-functions: One function mapping from situation and action to result, and another function mapping from result to utility.  And finally, although the result function varies over time, the utility function does not change.  We call environments that possess these properties constant utility nonstationary environments.  Although adaptation may not be beneficial in worst-case non-stationary environments, we can exploit the persistent regularity of the utility function to build an effective adaptation algorithm for
Leading Best-Response Strategies in Repeated Games| Abstract In repeated general-sum games, an agent using a \best response" strategy maximizes its own payoff assuming its behavior has no effect on its opponent.  This notion of best response requires some degree of learning to determine the fixed opponent behavior.  Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a \follower," since it adapts to its opponent.  However, pairing two best-response agents in a repeated game can result in suboptimal behavior.  We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy.  We then examine two \leader" strategies that induce better performance from opponent followers via stubbornness and threats. 
Planning with Predictive State Representations| Abstract Predictive state representation (PSR) models for controlled dynamical systems have recently been proposed as an alternative to traditional models such as partially observable Markov decision processes (POMDPs).  In this paper we develop and evaluate two general planning algorithms for PSR models.  First, we show how planning algorithms for POMDPs that exploit the piecewise linear property of value functions for finite-horizon problems can be extended to PSRs.  This requires an interesting replacement of the role of hidden nominalstates in POMDPs with linearly independent predictions in PSRs.  Second, we show how traditional reinforcement learning algorithms such as Q-learning can be extended to PSR models.  We empirically evaluate both our algorithms on a standard set of test POMDP problems. 
IPC 2004 Probabilistic Planning Track: FAQ 0|1.  Abstract The 2004 International Planning Competition, IPC 2004, will include a probabilistic planning track for the first time.  This document provides some of the high level decisions that have been made concerning how the competition will be run.  The 2004 International Planning Competition, IPC 2004, will include a probabilistic planning track for the first time.  This document lays out some of the high level decisions that have been made concerning how the competition will be run.  The details are still in flux at the time of this writing, although we hope to have them spelled out more precisely by the beginning of Summer 2003.  The overriding goal of the probabilistic planning track is to bring together two communities converging on a similar set of research issues and aid them in creating comparable tools and approaches.  One community consists of Markov decision process (MDP) researchers interested in developing algorithms that apply to powerfully expressive representations of environments.  The other consists of planning researchers incorporating probabilistic and decision theoretic concepts into their planning algorithms.  Cross fertilization has begun, but the probabilistic planning track promises a set of shared benchmarks and evaluation metrics that could crystallize efforts in this domain of study.  This document represents a snapshot of the ongoing development of the IPC 2004 probabilistic track.  For the latest developments, please visit:
PROVERB: The Probabilistic Cruciverbalist| Abstract We attacked the problem of solving crossword puzzles by computer: given a set of clues and a crossword grid, try to maximize the number of words correctly filled in.  In our system, \expert modules" specialize in solving specific types of clues, drawing on ideas from information retrieval, database search, and machine learning.  Each expert module generates a (possibly empty) candidate list for each clue, and the lists are merged together and placed into the grid by a centralized solver.  We used a probabilistic representation throughout the system as a common interchange language between subsystems and to drive the search for an optimal solution.  Proverb, the complete system, averages 95. 3% words correct and 98. 1% letters correct in under 15 minutes per puzzle on a sample of 370 puzzles taken from the New York Times and several other puzzle sources.  This corresponds to missing roughly 3 words or 4 letters on a daily 15 # 15 puzzle, making Proverb a better-than-average cruciverbalist (crossword solver). 
Taggers for Parsers| Abstract We consider what tagging models are most appropriate as front ends for probabilistic context-free-grammar parsers.  In particular we ask if using a tagger that returns more than one tag, a "multple tagger," improves parsing performance.  Our conclusion is somewhat surprising: single tag Markov-model taggers are quite adequate for the task.  First of all, parsing accuracy, as measured by the correct assignment of parts of speech to words, does not increase significantly when parsers select the tags themselves.  In addition, the work required to parse a sentence goes up with increasing tag ambiguity, though not as much as one might expect.  Thus, for the moment, single taggers are the best taggers. 
Implicit Negotiation in Repeated Games| Abstract.  In business-related interactions such as the on-going highstakes FCC spectrum auctions, explicit communication among participants is regarded as collusion, and is therefore illegal.  In this paper, we consider the possibility of autonomous agents engaging in implicit negotiation via their tacit interactions.  In repeated general-sum games, our testbed for studying this type of interaction, an agent using a \best response" strategy maximizes its own payoff assuming its behavior has no effect on its opponent.  This notion of best response requires some degree of learning to determine the fixed opponent behavior.  Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a \follower," since it adapts to its opponent.  However, pairing two best-response agents in a repeated game can result in suboptimal behavior.  We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy.  We then examine two \leader" strategies that induce better performance from opponent followers via stubbornness and threats.  These tactics are forms of implicit negotiation in that they aim to achieve a mutually beneficial outcome without using explicit communication outside of the game. 
An Empirical Evaluation of Interval Estimation for Markov Decision Processes| Abstract This paper takes an empirical approach to evaluating three model-based reinforcement-learning methods.  All methods intend to speed the learning process by mixing exploitation of learned knowledge with exploration of possibly promising alternatives.  We consider #-greedy exploration, which is computationally cheap and popular, but unfocused in its exploration effort; R-Max exploration, a simplification of an exploration scheme that comes with a theoretical guarantee of efficiency; and a well-grounded approach, model-based interval estimation, that better integrates exploration and exploitation.  Our experiments indicate that effective exploration can result in dramatic improvements in the observed rate of learning. 
A polynomial-time nash equilibrium algorithm for repeated games| ABSTRACT With the increasing reliance on game theory as a foundation for auctions and electronic commerce, efficient algorithms for computing equilibria in multiplayer general-sum games are of great theoretical and practical interest.  The computational complexity of finding a Nash equilibrium for a one-shot bimatrix game is a well known open problem.  This paper treats a closely related problem, that of finding a Nash equilibrium for an average-payo# repeated bimatrix game, and presents a polynomial-time algorithm.  Our approach draws on the "folk theorem" from game theory and shows how finite-state equilibrium strategies can be found efficiently and expressed succinctly. 
A Unified Analysis of Value-Function-Based Reinforcement-Learning Algorithms| Abstract Reinforcement learning is the problem of generating optimal
Initial Experiments in Stochastic Satisfiability| Abstract This paper looks at the rich intersection between satisfiability problems and probabilistic models, opening the door for the use of satisfiability approaches in probabilistic domains.  A generic stochastic satisfiability problem is examined, which can function for probabilistic domains as Sat does for deterministic domains.  The paper defines a Davis-Putnam-Logemann-Loveland-style procedure for solving stochastic satisfiability problems, and reports on a preliminary empirical exploration of the complexity of the algorithm for a collection of randomly generated probabilistic problems.  The results exhibit the familiar easyhardest-hard pattern for the difficulty of random Sat formulae.  Special cases of the stochastic satisfiability problem lie in different complexity classes, and one counterintuitive result is that the computational complexity and the empirical complexity of the problems examined do not track each other exactly---problems in the hardest complexity class are not the hardest to solve. 
Self-Enforcing Strategic Demand Reduction| When all use PRSDR, bidders obtain significantly better results than when using a reasonable baseline approach.  The strategy automatically detects and punishes non-cooperating bidders to achieve robustness in the face of agent defection, and performs well under alternative conditions.  The PRSDR strategy is fully implemented and we present detailed empirical results. 
A statistical method for language-independent representation of the topical content of text segments| Abstract Where there are texts in more than one language,
Incremental Pruning: A Simple, Fast, Exact Method for Partially Observable Markov Decision Processes| Abstract Most exact algorithms for general partially observable Markov decision processes (pomdps) use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another.  We examine variations of the "incremental pruning" method for solving this problem and compare them to earlier algorithms from theoretical and empirical perspectives.  We find that incremental pruning is presently the most efficient exact method for solving pomdps. 
Efficient Exploration With Latent Structure| Abstract--- When interacting with a new environment, a robot can improve its online performance by efficiently exploring the effects of its actions.  The efficiency of exploration can be expanded significantly by modeling and using latent structure to generalize experiences.  We provide a theoretical development of the problem of exploration with latent structure, analyze several algorithms and prove matching lower bounds.  We demonstrate our algorithmic ideas on a simple robot car repeatedly traversing a path with two different surface properties. 
Cost-Sensitive Fault Remediation for Autonomic Computing| Abstract We introduce a formal model of cost-sensitive fault remediation, derive an exact algorithm for solving the special case of deterministic observations, and demonstrate it on two example problems.  This effort is part of two self-healing software projects that attempt to use collected data for better decision making in emerging autonomic systems. 
Higher Dimensional Representations of Graphs| Abstract Graphs are often used to model complex systems and to visualize relationships, and this often involves drawing a graph in the plane.  For this, a variety of algorithms and mathematical tools have been used with varying success.  We demonstrate why it is often more natural and more meaningful to view higher dimensional representations of graphs.  We present some of the theory and problems associated with constructing such representations, and we briefly describe some visualization tools which are now available for experimental research in this area. 
Reinforcement Learning: A Survey| Abstract This paper surveys the field of reinforcement learning from a computer-science
Markov Games as a Framework for Multi-Agent Reinforcement Learning| Abstract In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function.  In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior.  The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals.  This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment.  It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic. 
FAucS : An FCC Spectrum Auction Simulator for Autonomous Bidding Agents| Abstract.  We introduce FAucS, a software testbed for studying automated agent bidding strategies in simulated auctions, specifically the United States FCC wireless frequency spectrum auctions.  In addition to the complexity of these auctions, which provides ample opportunities for intelligent approaches to bidding, this type of auction has huge commercial importance, each bringing in billions of dollars to governments around the world.  We implement straightforward sample agents in FAucS and use them to replicate known beneficial bidding strategies in this type of auction.  We then discuss potential in-depth studies of autonomous bidding agent behaviors using FAucS.  The main contribution of this work is the implementation, description, and empirical validation of the FAucS testbed.  We present it as a challenging and promising AI research domain. 
Decision-Theoretic Bidding Based on Learned Density Models in Simultaneous, Interacting Auctions| Abstract Auctions are becoming an increasingly popular method for transacting business, especially over the Internet.  This article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods.  A core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate, to the greatest extent possible, optimal bids.  We introduce a new and general boosting-based algorithm for conditional density estimation problems of this kind, i. e. , supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label.  This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01).  We present experiments demonstrating the effectiveness of our boosting-based price predictor relative to several reasonable alternatives. 
Using Caching to Solve Larger Probabilistic Planning Problems| Abstract Probabilistic planning algorithms seek effective plans for large, stochastic domains.  maxplan is a recently developed algorithm that converts a planning problem into an E-Majsat problem, an NP PP -complete problem that is essentially a probabilistic version of Sat, and draws on techniques from Boolean satisfiability and dynamic programming to solve the E-Majsat problem.  This solution method is able to solve planning problems at state-of-the-art speeds, but it depends on the ability to store a value for each CNF subformula encountered in the solution process and is therefore quite memory intensive; searching for moderate-size plans even on simple problems can exhaust memory.  This paper presents two techniques, based on caching, that overcome this problem without significant performance degradation.  The first technique uses an LRU cache to store a fixed number of subformula values.  The second technique uses a heuristic based on a measure of subformula difficulty to selectively save the values of only those subformulas whose values are sufficiently difficult to compute and are likely to be reused later in the solution process.  We report results for both techniques on a stochastic test problem. 
Reinforcement Learning for Selfish Load Balancing in a Distributed Memory Environment| Abstract Load balancing is a difficult problem whose solution can greatly increase the speedup one achieves in a parallel distributed memory environment.  The necessity for load balancing can arise not only from the structure or dynamics of one's problem, but from the need to compete for processor time with other users.  Given a lengthy computation, the ability to exploit changes in processor loads when allocating work or deciding whether to reallocate work is critical in making the computation time-feasible.  We show how this aspect of the load balancing problem can be formulated as a Markov decision process (MDP), and describe some preliminary attempts to solve this MDP using guided off-line Qlearning and a linear value-function approximator.  In particular, we describe difficulties with value-function approximator divergence and techniques we applied to correct this problem. 
Algorithms for Informed Cows| Abstract We extend the classic on-line search problem known as the cow-path problem to the case in which goal locations are selected according to one of a set of possible known probability distributions.  We present a polynomial-time linear-programming algorithm for this problem. 
Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms| Abstract.  An important application of reinforcement learning (RL) is to
An Ecient Exact Algorithm for Singly Connected Graphical Games| Abstract We describe a new algorithm for computing a Nash equilibrium in graphical games , a compact representation for multi-agent systems that we introduced in previous work.  The algorithm is the first to compute equilibria both eciently and exactly for a non-trivial class of graphical games. 
PPDDL1|0: An Extension to PDDL for Expressing Planning Domains with Probabilistic Effects.  Abstract We desribe a variation of the planning domain definition language, PDDL, that permits the
Memoryless policies: theoretical limitations and practical results| Abstract One form of adaptive behavior is "goal-seeking" in which an agent acts so as to minimize the time it takes to reach a goal state.  This paper presents some theoretical and empirical findings on algorithms that devise goal-seeking behaviors for ``memoryless" agents who base their behavioral decisions solely on current sensations.  The basic results are that (1) the general problem of finding good deterministic memoryless policies is intractable, however, (2) simple branch-and-bound heuristics can be used to find optimal memoryless policies extremely quickly for some established example environments. 
Approximate Dimension Equalization in Vector-based Information Retrieval| Abstract Vector-based information retrieval methods such as the vector space model (VSM), latent semantic indexing (LSI), and the generalized vector space model (GVSM) represent both queries and documents by high-dimensional vectors learned from analyzing a training collection of text.  VSM scales well to large collections, but cannot represent term{term correlations, which prevents it from being used in cross-language retrieval.  GVSM and LSI can represent term{term correlations, but do not scale well to large collections.  We point out a deep mathematical similaritybetween VSM, LSI, and GVSM, and use this to derive anovel method we call approximate dimension equalization (ADE) that performs well on large collections, scales well computationally, and can represent term{term correlations.  We compare the performance of ADE to the other methods on both large and small collections with both monolingual and crosslanguage queries.  ADE outperforms all other methods on large cross-language collections, and is close to the best in all other cases. 
A Theoretical Analysis of Model-Based Interval Estimation: Proofs| Abstract Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient.  Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation.  This paper presents the first theoretical analysis of MBIE, proving its efficiency even under worst-case conditions.  The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature. 
A Generalized Reinforcement-Learning Model: Convergence and Applications| Abstract Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior.  The Markov decision process (mdp) model is a popular way of formalizing the reinforcement-learning problem, but it is by no means the only way.  In this paper, we show how many of the important theoretical results concerning reinforcement learning in mdps extend to a generalized mdp model that includes mdps, two-player games and mdps under a worst-case optimality criterion as special cases.  The basis of this extension is a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence. 
Efficient dynamic-programming updates in partially observable Markov decision processes| Abstract We examine the problem of performing exact dynamic-programming updates in partially observable Markov decision processes (pomdps) from a computational complexity viewpoint.  Dynamic-programming updates are a crucial operation in a wide range of pomdp solution methods and we find that it is intractable to perform these updates on piecewise-linear convex value functions for general pomdps.  We offer a new algorithm, called the witness algorithm, which can compute updated value functions efficiently on a restricted class of pomdps in which the number of linear facets is not too great.  We compare the witness algorithm to existing algorithms analytically and empirically and find that it is the fastest algorithm over a wide range of pomdp sizes. 
ATTac-2000: an adaptive autonomous bidding agent| Abstract The First Trading Agent Competition (TAC) was held from June 22nd to July 8th, 2000.  TAC was designed to create a benchmark problem in the complex domain of emarketplaces and to motivate researchers to apply unique approaches to a common task.  This article describes ATTac-2000, the #rst-place finisher in TAC.  ATTac-2000 uses a principled bidding strategy that includes several elements of adaptivity .  In addition to the success at the competition, isolated empirical results are presented indicating the robustness and effectiveness of ATTac-2000's adaptive strategy. 
Graphical Models for Game Theory| Abstract We introduce a compact graph-theoretic representation for multi-party game theory.  Our main result is a provably correct and efficient algorithm for computing approximate Nash equilibria in (one-stage) games represented by trees or sparse graphs. 
Solving Crossword Puzzles as Probabilistic Constraint Satisfaction| Abstract Crossword puzzle solving is a classic constraint satisfaction problem, but, when solving a real puzzle, the mapping from clues to variable domains is not perfectly crisp.  At best, clues induce a probability distribution over viable targets, which must somehow be respected along with the constraints of the puzzle.  Motivated by this type of problem, we describe a formal model of constraint satisfaction with probabilistic preferences on variable values.  Two natural optimization problems are defined for this model: maximizing the probability of a correct solution, and maximizing the number of correct words (variable values) in the solution.  To the latter, we apply an efficient iterative approximation equivalent to turbo decoding and present results on a collection of real and artificial crossword puzzles. 
Efficient Singular Value Decomposition via Improved Document Sampling| Abstract Singular value decomposition (SVD) is a general-purpose mathematical analysis tool that has been used in a variety of information-retrieval applications.  As the size and complexity of retrieval collections increase, it is crucial for our analysis tools to scale accordingly.  To this end, we have studied the application of a new theoretically justified SVD approximation algorithm to the problem of text retrieval.  We show that, in the case of latent semantic indexing, we can achieve near optimal approximations of the exact SVD using considerably less computation by using an appropriate distribution to sample the documents we include in our SVD analysis. 
Cyclic Equilibria in Markov Games| Abstract Although variants of value iteration have been proposed for finding Nash or correlated equilibria in general-sum Markov games, these variants have not been shown to be effective in general.  In this paper, we demonstrate by construction that existing variants of value iteration cannot find stationary equilibrium policies in arbitrary general-sum Markov games.  Instead, we propose an alternative interpretation of the output of value iteration based on a new (non-stationary) equilibrium concept that we call "cyclic equilibria. " We prove that value iteration identifies cyclic equilibria in a class of games in which it fails to find stationary equilibria.  We also demonstrate empirically that value iteration finds cyclic equilibria in nearly all examples drawn from a random distribution of Markov games. 
The Complexity of Plan Existence and Evaluation in Probabilistic Domains| Abstract We examine the computational complexity of testing and finding small plans in probabilistic planning domains (both flat and succinct).  We show that many problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NP PP , co-NP PP , and PSPACE.  Of these, the probabilistic classes PP and NP PP are likely to be of special interest in the field of uncertainty in artificial intelligence and are deserving of additional study. 
Phase and Amplitude Control Ability using Spatial Light Modulators and Zero Path Length Difference Michelson Interferometer| ABSTRACT A new method is presented for amplitude and phase control using two liquid crystal spatial light modulators in conjunction with a white light Michelson interferometer.  Preliminary proof-of-concept measurements are given showing the prospect of using this method for correction of amplitude errors in telescopes. 
Measuring Praise and Criticism: Inference of Semantic Orientation from Association| The evaluative character of a word is called its semantic orientation.  Positive semantic orientation indicates praise (e. g. , "honest", "intrepid") and negative semantic orientation indicates criticism (e. g. , "disturbing", "superfluous").  Semantic orientation varies in both direction (positive or negative) and degree (mild to strong).  An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots).  This article introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words.  Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA).  The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words).  The method attains an accuracy of 82. 8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words. 
Markov Indecision Processes: A Formal Model of Decision-Making Under Extreme Confusion| Abstract We present a mathematical model of indecisive agents faced with a sequence of dicult decisions, extending Adams' bistromathics to the multistage case.  This is almost the first work on modeling stochastic processes for which the probabilities are fundamentally unknowable.  This paper describes a novel algorithm, complexity results, and a model-free learning algorithm for Markov indecision processes.  Two applications are discussed based on real-world domains: presidential elections and the stock market. 
Acting Optimally in Partially Observable Stochastic Domains| Abstract In this paper, we describe the partially observable Markov decision process (pomdp) approach to finding optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment.  The pomdp approach was originally developed in the operations research community and provides a formal basis for planning problems that have been of interest to the AI community.  We found the existing algorithms for computing optimal control strategies to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient.  We sketch this algorithm and present preliminary results on several small problems that illustrate important properties of the pomdp approach. 
A Distributed Reinforcement Learning Scheme for Network Routing| Abstract In this paper we describe a self-adjusting algorithm for packet routing in which a reinforcement learning method is embedded into each node of a network.  Only local information is used at each node to keep accurate statistics on which routing policies lead to minimal routing times.  In simple experiments involving a 36-node irregularlyconnected network, this learning approach proves superior to routing based on precomputed shortest paths. 
An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games| Abstract We describe a new algorithm for computing a Nash equilibrium in graphical games , a compact representation for multi-agent systems that we introduced in previous work.  The algorithm is the first to compute equilibria both eciently and exactly for a non-trivial class of graphical games. 
The Witness Algorithm: Solving Partially Observable Markov Decision Processes, Department of Computer Science,|
Planning and acting in partially observable stochastic domains|
Fully automatic crosslanguage document retrieval using latent semantic indexing,|
"A Gaussian Pupil Coronagraph: A New Approach to Detecting Terrestrial Planets",|
Algorithms for Sequential Decision Making|
Generalized Markov decision processes: Dynamic-programming and reinforcement-learning algorithms|
Interactions between learning and evolution|
Combining independent modules in lexical multiple-choice problems|
Automatic Cross-Language Information Retrieval using Latent Semantic Indexing",|
Ppddl 1|0: An extension to pddl for expressing planning domains with probabilistic effects. 
An interface for navigating clustered document sets returned by queries|
Supporting Informal Communication via Ephemeral Interest Groups|
Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus|
The Witness algorithm: Solving partially observable Markov decision processes|
Automatic cross-language retrieval using latent semantic indexing|
Solving large POMDPs: Lessons from complexity theory| Talk presented at the DARPA AI Workshop in Providence, RI. 
Forthcoming| Taggers for parsers. 
Value-function reinforcement learning in markov games|
A Case for Lamarckian Evolution, Artificial Life III, Santa Fe Institute Studies in the Sciences of Complexity,|
CPS130 course notes - sorting(5), Fall|
Stochastic Boolean Satisfiability|
Altruism in the evolution of communication|
ATTac-2001: A Learning, Autonomous Bidding Agent|
PAC Generalization Bounds for Co-training|
Learning Analogies and Semantic Relations|
A probabilistic approach to solving crossword puzzles|
Probabilistic strips planning is exptime-complete|
The computational complexity of probabilistic plan existence and evaluation|
Initial experiments in probabilistic satisfiability|
A polynomial-time algorithm for computing Nash equilibria in repeated bimatrix games|
Randomized strategic demand reduction: getting more by asking for less|
On constructiong a transfer dictionary for man and machine|
On the complexity of counting satisfying assignments|
Computerized cross-language document retrieval using latent semantic indexing|
\Large Scale Turbulence Simulations on the NavierStokes Computer"|
Solving Crosswords with PROVERB|
Reinforcement Learning for Algorithm Selection|
Spectrally narrow pulsed dye laser without beam expander,|
Automatic 3-Language Cross-Language Information Retrieval with Latent Semantic Indexing|
An adaptive autonomous bidding agent|
A learning, autonomous bidding agent|
Language games and other meaningful pursuits|
Learning a Language-Independent Representation for Terms from a Partially Aligned Corpus|
Interactions between evolution and learning|
eds|,. 
An efficient algorithm for dynamic programming in partially observable Markov decision processes|
Markov games as a framework for a1|
Representations and complexity|
Partially Observable Markov Decision Processes for Artificial Intelligence|
InfoSearch: A program for iterative information retrieval using LSI|
Comparison of exact algorithms for partially observable Markov decision processes,|
Cross-language text retrieval with three languages|
Corpus-based learning of analogies and semantic relations|
Learning Analogies and Semantic Relations, National Research Council, Institute for Information Technology,|
Hypertext for the Electronic Library? CORE Sample Results|
Dynamical eBects at avoided level crossing: A study of the Landau-Zener eBect using rydberg atoms|
Proverb:|
A case for distributed Lamarckian evolution| Working Paper, Cognitive Science Research Group,. 
in press)| Memoryless policies: Theoretical limitations and practical results. 
Implicit negotiation in repeated games|
Reinforcement Laerning: A Survey'|
Leading best-response stratgies in repeated games|
Efficient exploration via model-based interval estimation|
Markov Decision Processes and Reinforcement learning|
Selecting the right algorithm|
Least-Squares Methods in Reinforcement Learning for Control|

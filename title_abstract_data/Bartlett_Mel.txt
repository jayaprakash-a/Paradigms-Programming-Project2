Memory Capacity of Linear vs| Nonlinear Models of Dendritic Integration.  Abstract Previous biophysical modeling work showed that nonlinear interactions among nearby synapses located on active dendritic trees can provide a large boost in the memory capacity of a cell (Mel, 1992a, 1992b).  The aim of our present work is to quantify this boost by estimating the capacity of (1) a neuron model with passive dendritic integration where inputs are combined linearly across the entire cell followed by a single global threshold, and (2) an active dendrite model in which a threshold is applied separately to the output of each branch, and the branch subtotals are combined linearly.  We focus here on the limiting case of binary-valued synaptic weights, and derive expressions which measure model capacity by estimating the number of distinct input-output functions available to both neuron types.  We show that (1) the application of a fixed nonlinearity to each dendritic compartment substantially increases the model's flexibility, (2) for a neuron of realistic size, the capacity of the nonlinear cell can exceed that of the same-sized linear cell by more than an order of magnitude, and (3) the largest capacity boost occurs for cells with a relatively large number of dendritic subunits of relatively small size.  We validated the analysis by empirically measuring memory capacity with randomized two-class classi#cation problems, where a stochastic delta rule was used to train both linear and nonlinear models.  We found that large capacity boosts predicted for the nonlinear dendritic model were readily achieved in practice. 
Choice and Value Flexibility Jointly Contribute to the Capacity of a Subsampled Quadratic Classifier| Abstract Biophysical modeling studies have suggested that neurons with active dendrites can be viewed as linear units augmented by product terms which arise from interactions between synaptic inputs within the same dendritic subregions.  However, the degree to which local nonlinear synaptic interactions augment the memory capacity of a neuron is not known in a quantitative sense.  To approach this question, we have studied the family of subsampled quadratic classifiers, i. e.  linear classifiers augmented by the best k terms from the set of K = (d 2 + d)=2 second-order product terms available in d dimensions.  We developed an expression for the total parameter entropy of an SQ classifier, whose form shows that the capacity of an SQ classifier does not reside solely in its conventional weight values|i. e.  the explicit memory used to store constant, linear, and higher-order coecients.  Rather, we identify a second type of parameter flexibility which jointly contributes to an SQ classi#er's capacity: the choice as to which product terms are included in the model, and which are not.  We validate the form of the entropy expression using empirical studies of relative capacity within families of geometrically isomorphic SQ classifiers.  Our results have direct implications for neurobiological (and other hardware) learning systems, where in the limit of high-dimensional input spaces and low-resolution synaptic weight values, this relatively little explored form of \choice flexibility" could constitute a major source of trainable model capacity.  A B C A+B A+C hypothetical response at soma soma Figure 1: Conceptual illustration of location-dependent nonlinear interactions in an active dendritic tree.  Two inputs A and B activated at high frequency on a dendritic branch containing a thresholding nonlinearity could be sucient to cause the branch (and the cell) to \#re" repeatedly, while coactivation of more distantly separated inputs A and C might generate only a rare suprathreshold event at any location.  This type of nonlinear interaction between A and B is sometimes loosely termed \multiplicative". 
Minimizing Binding Errors Using Learned Conjunctive Features| We have studied some of the design trade-offs governing visual representations based on spatially invariant conjunctive feature detectors, with an emphasis on the susceptibility of such systems to false-positive recognition errors---Malsburg's classical binding problem.  We begin by deriving an analytical model that makes explicit how recognition performance is affected by the number of objects that must be distinguished, the number of features included in the representation, the complexity of individual objects, and the clutter load, that is, the amount of visual material in the field of view in which multiple objects must be simultaneously recognized, independent of pose, and without explicit segmentation.  Using the domain of text to model object recognition in cluttered scenes, we show that with corrections for the nonuniform probability and nonindependence of text features, the analytical model achieves good fits to measured recognition rates in simulations involving a wide range of clutter loads, word sizes, and feature counts.  We then introduce a greedy algorithm for feature learning, derived from the analytical model, which grows a representation by choosing those conjunctive features that are most likely to distinguish objects from the cluttered backgrounds in which they are embedded.  We show that the representations produced by this algorithm are compact, decorrelated, and heavily weighted toward features of low conjunctive order.  Our results provide a more quantitative basis for understanding when spatially invariant conjunctive features can support unambiguous perception in multiobject scenes, and lead to several insights regarding the properties of visual representations optimized for specific recognition tasks. 
How Receptive Field Parameters Affect Neural Learning| Abstract We identify the three principle factors affecting the performance of learning by networks with localized units: unit noise, sample density, and the structure of the target function.  We then analyze the effect of unit receptive field parameters on these factors and use this analysis to propose a new learning algorithm which dynamically alters receptive field properties during learning. 
SEEMORE: A View-Based Approach to 3-D Object Recognition Using Multiple Visual Cues| Abstract A neurally-inspired visual object recognition system is described called seemore, whose goal is to identify common objects from a large known set---independent of 3-D viewing angle, distance, and non-rigid distortion.  seemore's database consists of 100 objects that are rigid (shovel), non-rigid (telephone cord), articulated (book), statistical (shrubbery), and complex (photographs of scenes).  Recognition results were obtained using a set of 102 color and shape feature channels within a simple feedforward network architecture.  In response to a test set of 600 novel test views (6 of each object) presented individually in color video images, seemore identified the object correctly 97% of the time (chance is 1%) using a nearest neighbor classifier.  Similar levels of performance were obtained for the subset of 15 non-rigid objects.  Generalization behavior reveals emergence of striking natural category structure not explicit in the input feature dimensions. 
by Cell Press Impact of Active Dendrites and Structural Plasticity on the Memory Capacity of Neural Tissue| abstract synaptic weights of connectionist theory and the physical substrate for long-term learning and memory in the brain.  First, a spate of recent experiments indicates that the efficacy of synaptic transmission at cortical synapses can undergo substantial fluctuations
Section Editor: David Van Essen Translation-Invariant Orientation Tuning in Visual `Complex' Cells Could Derive from Intradendritic Computations| Abstract Hubel and Wiesel (1962) first distinguished "simple" from "complex" cells in visual cortex, and proposed a processing hierarchy in which rows of LGN cells are pooled to drive oriented simple cell subunits, which are pooled in turn to drive complex cells.  Though parsimonious and highly influential, the pure hierarchical model has since been challenged by results indicating many complex cells receive excitatory monosynaptic input from LGN cells, or do not depend on simple cell input.  Alternative accounts for complex cell orientation tuning remain scant, however, and the function of monosynaptic LGN contacts onto complex cell dendrites remains unknown.  We have used a biophysically detailed compartmental model to investigate whether nonlinear integration of LGN synaptic inputs within the dendrites of individual pyramidal cells could contribute to complex-cell receptive field structure.  We show that an isolated cortical neuron with "active" dendrites, driven only by excitatory inputs from overlapping ON- and OFF-center LGN subfields, can produce clear phase-invariant orientation tuning---a hallmark response characteristic of a complex cell.  The tuning is shown to depend critically upon both the spatial arrangement of LGN synaptic contacts across the complex cell dendritic tree, established by a Hebbian developmental principle, and on the physiological efficacy of excitatory voltagedependent dendritic ion channels.  We conclude that unoriented LGN inputs to a complex cell could contribute in a significant way to its orientation tuning, acting in concert with oriented inputs to the same cell provided by simple cells or other complex cells.  As such, our model provides a novel, experimentally testable hypothesis regarding the basis of orientation tuning in the complex cell population, and more generally, underscores the potential importance of nonlinear intradendritic subunit processing in cortical neurophysiology. 
The Memory Capacity of Subsampled Quadratic Classifiers: Why Active Dendrites May Remember More| Abstract Previous biophysical modeling studies have suggested that neurons with active dendritic trees may be viewed as linear classifiers augmented by a few second-order product terms which arise from multiplicative interactions among synaptic inputs (Mel, 1992
To appear in Neural Computation Minimizing Binding Errors Using Learned Conjunctive Features| Abstract We have studied some of the design tradeoffs governing visual representations based on spatially-invariant conjunctive feature detectors, with an emphasis on the susceptibility of such systems to false-positive recognition errors|Malsburg's classical \binding" problem.  We begin by deriving an analytical model that makes explicit how recognition performance is affected by the number of objects that must be distinguished, the number of features included in the representation, the complexity of individual objects, and the clutter load, i. e.  the amount of visual material in the field of view in which multiple objects must be simultaneously recognized, independent of pose, and without explicit segmentation.  Using the domain of text to model object recognition in cluttered scenes, we show that, with corrections for the non-uniform probability and non-independence of text features, the analytical model achieves good fits to measured recognition rates in simulations involving a wide range of clutter loads, word sizes, and feature counts.  We then introduce a greedy algorithm for feature learning, derived from the analytical model, which grows a representation by choosing those conjunctive features that are most likely to distinguish objects from the cluttered backgrounds in which they are embedded.  We show that the representations produced by this algorithm are compact, decorrelated, and heavily weighted toward features of low conjunctive order.  Our results provide a more quantitative basis for understanding when spatially-invariant conjunctive features can support unambiguous perception in multiobject scenes, and lead to several insights regarding the properties of visual representations optimized for specific recognition tasks. 
LETTER Communicated by Anthony Zador Choice and Value Flexibility Jointly Contribute to the Capacity of a Subsampled Quadratic Classifier| Biophysical modeling studies have suggested that neurons with active dendrites can be viewed as linear units augmented by product terms that arise from interactions between synaptic inputs within the same dendritic subregions.  However, the degree to which local nonlinear synaptic interactions could augment the memory capacity of a neuron is not known in a quantitative sense.  To approach this question, we have studied the family of subsampled quadratic classifiers: linear classifiers augmented by the best k terms from the set of K = (d 2 + d)/2 second-order product terms available in d dimensions.  We developed an expression for the total parameter entropy, whose form shows that the capacity of an SQ classifier does not reside solely in its conventional weight values---the explicit memory used to store constant, linear, and higher-order coefficients.  Rather, we identify a second type of parameter flexibility that jointly contributes to an SQ classifier's capacity: the choice as to which product terms are included in the model and which are not.  We validate the form of the entropy expression using empirical studies of relative capacity within families of geometrically isomorphic SQ classifiers.  Our results have direct implications for neurobiological (and other hardware) learning systems, where in the limit of high-dimensional input spaces and low-resolution synaptic weight values, this relatively little explored form of choice flexibility could constitute a major source of trainable model capacity. 
SEEMORE: Combining Color, Shape and Texture Histogramming in a Neurally-Inspired Approach to Visual Object Recognition| Abstract Severe architectural and timing constraints within the primate visual system support the conjecture that the early phase of object recognition in the brain is based on a feedforward feature-extraction hierarchy.  To assess the plausibility of this conjecture in an engineering context, a difficult 3-D object recognition domain was developed to challenge a pure feedforward, receptive-field-based recognition model called seemore.  seemore is based on 102 viewpoint-invariant nonlinear filters which are as a group sensitive to contour, texture, and color cues.  The visual domain consists of 100 real objects of many different types, including rigid (shovel), non-rigid (telephone cord), and statistical (maple leaf cluster) objects, and photographs of complex scenes.  Objects were individually-presented in color video images under normal room lighting conditions. 
Toward a Single-Cell Account of Binocular Disparity Tuning: An Energy Model May be Hiding in Your Dendrites| Abstract Hubel and Wiesel (1962) proposed that complex cells in visual cortex are driven by a pool of simple cells with the same preferred orientation but different spatial phases.  However, a wide variety of experimental results over the past two decades have challenged the pure hierarchical model, primarily by demonstrating that many complex cells receive monosynaptic input from unoriented LGN cells, or do not depend on simple cell input.  We recently showed using a detailed biophysical model that nonlinear interactions among synaptic inputs to an excitable dendritic tree could provide the nonlinear subunit computations that underlie complex cell responses (Mel, Ruderman, & Archie, 1997).  Our present work extends this result to the case of complex cell binocular disparity tuning, by demonstrating in an isolated model pyramidal cell (1) disparity tuning at a resolution much finer than the the overall dimensions of the cell's receptive field, and (2) systematically shifted optimal disparity values for rivalrous pairs of light and dark bars---both in good agreement with published reports (Ohzawa, DeAngelis, \Lambda Section: Neurobiology.  Desired presentation format: oral.  A description of similar work has been submitted in abstract form to the CNS*97 conference.  & Freeman, 1997).  Our results reemphasize the potential importance of intradendritic computation for binocular visual processing in particular, and for cortical neurophysiology in general. 
Information processing in dendritic trees|
Combining color, shape, and texture histogramming in a neurally inspired approach to visual object recognition|
Translation-invariant orientation tuning in visual ''complex" cells could derive from intradendritic computations|
Minimizing Binding Errors Using Learned Conjunctive Features|
Further Explorations in Visually-Guided Reaching: Making MURPHY Smarter|
Think positive to find parts|
Vision-based robot motion planning|
The Clusteron: Toward a Simple Abstraction for a Complex Neuron|
Why have dendrites? A computational perspective|
NMDA-based pattern discrimination in a modeled cortical neuron|
Synaptic integration in an excitable dendritic tree|
Impact of active dendrites and structural plasticity on the memory capacity of neural tissue|
Sigma-Pi Learning: On Radial Basis Functions and Cortical Associative Learning|
Complex-Cell Responses Derived from Center-Surround Inputs: The Surprising Power of Intradendritic Computation|
A robot that learns by doing|
distinct potassium channel sybtypes in CA1 pyramidal neurons|
MURPHY: A Connectionist Approach to Vision-Based Robot Motion Planning|
Connectionist Robot Motion Planning: A Neurally-Inspired Approach to Visually-Guided Reaching|
MURPHY: A Robot that Learns by Doing|

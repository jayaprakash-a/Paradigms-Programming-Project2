Hierarchical Optimal Control of MDPs| Abstract Fundamental to reinforcement learning, as well as to the
Planning with Closed-Loop Macro Actions| Abstract Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence.  In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning.  Conventional model-based reinforcement learning uses primitive actions that last one time step and that can be modeled independently of the learning agent.  These can be generalized to macro actions, multi-step actions specified by an arbitrary policy and a way of completing.  Macro actions generalize the classical notion of a macro operator in that they are closed loop, uncertain, and of variable duration.  Macro actions are needed to represent common-sense higher-level actions such as going to lunch, grasping an object, or traveling to a distant city.  This paper generalizes prior work on temporally abstract models (Sutton 1995) and extends it from the prediction setting to include actions, control, and planning.  We define a semantics of models of macro actions that guarantees the validity of planning using such models.  This paper present new results in the theory of planning with macro actions and illustrates its potential advantages in a gridworld task. 
Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning| Abstract Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI.  this paper we consider how these challenges can be addressed within the mathematical framework reinforcement learning and Markov decision processes (MDPs).  We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over period of time.  Examples of options include picking up an object, going to lunch, and traveling a distant city, as well as primitive actions such as muscle twitches and joint torques.  Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in natural and general way.  In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning.  Formally, a set of options defined over MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory options.  However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory.  We present results for three such cases: (1) we show that the results planning with options can be used during execution interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able learn about an option from fragments of execution, and (3) we propose notion subgoal that can be used to improve the options themselves.  All of these results have precursors in the existing literature; the contribution of this paper is to establish them in simpler and more general setting with fewer changes to the existing reinforcement learning framework.  In particular, we show that these results can be obtained without committing (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro Corresponding author. 
Policy Gradient Methods for Reinforcement Learning with Function Approximation| Abstract Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable.  In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters.  Williams's REINFORCE method and actor--critic methods are examples of this approach.  Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function.  Using this result, we prove for the first time that a version of policy iteration with arbitrary dierentiable function approximation is convergent to a locally optimal policy.  Large applications of reinforcement learning (RL) require the use of generalizing function approximators such neural networks, decision-trees, or instance-based methods.  The dominant approach for the last decade has been the value-function approach, in which all function approximation eort goes into estimating a value function, with the action-selection policy represented implicitly as the "greedy" policy with respect to the estimated values (e. g. , as the policy that selects in each state the action with highest estimated value).  The value-function approach has worked well in many applications, but has several limitations.  First, it is oriented toward finding deterministic policies, whereas the optimal policy is often stochastic, selecting dierent actions with specific probabilities (e. g. , see Singh, Jaakkola, and Jordan, 1994).  Second, an arbitrarily small change in the estimated value of an action can cause it to be, or not be, selected.  Such discontinuous changes have been identified as a key obstacle to establishing convergence assurances for algorithms following the value-function approach (Bertsekas and Tsitsiklis, 1996).  For example, Q-learning, Sarsa, and dynamic programming methods have all been shown unable to converge to any policy for simple MDPs and simple function approximators (
TD Models: Modeling the World at a Mixture of Time Scales| Abstract Temporal-difference (TD) learning can be used not just to predict rewards, as is commonly done in reinforcement learning, but also to predict states, i. e. , to learn a model of the world's dynamics.  We present theory and algorithms for intermixing TD models of the world at different levels of temporal abstraction within a single structure.  Such multi-scale TD models can be used in model-based reinforcement-learning architectures and dynamic programming methods in place of conventional Markov models.  This enables planning at higher and varied levels of abstraction, and, as such, may prove useful in formulating methods for hierarchical or multi-level planning and reinforcement learning.  In this paper we treat only the prediction problem---that of learning a model and value function for the case of fixed agent behavior.  Within this context, we establish the theoretical foundations of multi-scale models and derive TD algorithms for learning them.  Two small computational experiments are presented to test and illustrate the theory.  This work is an extension and generalization of the work of Singh (1992), Dayan (1993), and Sutton & Pinette (1985). 
Predictive Representations of State| Abstract We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations.  State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations.  Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models.  Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls).  We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model.  In predicting or controlling a sequence of observations, the concepts of state and state estimation inevitably arise.  There have been two dominant approaches.  The generative-model approach, typified by research on partially observable Markov decision processes (POMDPs), hypothesizes a structure for generating observations and estimates its state and state dynamics.  The history-based approach, typified by k-order Markov methods, uses simple functions of past observations as state, that is, as the immediate basis for prediction and control.  (The data flow in these two approaches are diagrammed in Figure 1. ) Of the two, the generative-model approach is more general.  The model's internal state gives it temporally unlimited memory| the ability to remember an event that happened arbitrarily long ago|whereas a history-based approach can only remember as far back as its history extends.  The bane of generative-model approaches is that they are often strongly dependent on a good model of the system's dynamics.  Most uses of POMDPs, for example, assume a perfect dynamics model and attempt only to estimate state.  There are algorithms for simultaneously estimating state and dynamics (e. g. , Chrisman, 1992), analogous to the Baum-Welch algorithm for the uncontrolled case (Baum et al. , 1970), but these are only effective at tuning parameters that are already approximately correct (e. g. , Shatkay & Kaelbling, 1997).  State Update observations (and actions) state rep'n observations (and actions) state rep'n 1-step delays (a) (b) Figure 1: Data flow in a) POMDP and other recursive updating of state representation, and b) history-based state representation.  In practice, history-based approaches are often much more effective.  Here, the state representation is a relatively simple record of the stream of past actions and observations.  It might record the occurrence of a specific subsequence or that one event has occurred more recently than another.  Such representations are far more closely linked to the data than are POMDP representations.  One way of saying this is that POMDP learning algorithms encounter many local minima and saddle points because all their states are equipotential.  History-based systems immediately break symmetry, and their direct learning procedure makes them comparably simple.  McCallum (1995) has shown in a number of examples that sophisticated history-based methods can be effective in large problems, and are often more practical than POMDP methods even in small ones.  The predictive state representation (PSR) approach, which we develop in this paper, is like the generative-model approach in that it updates the state representation recursively, as in Figure 1(a), rather than directly computing it from data.  We show that this enables it to attain generality and compactness at least equal to that of the generative-model approach.  However, the PSR approach is also like the history-based approach in that its representations are grounded in data.  Whereas a history-based representation looks to the past and records what did happen, a PSR looks to the future and represents what will happen.  In particular, a PSR is a vector of predictions for a specially selected set of action{observation sequences, called tests (after Rivest & Schapire, 1994).  For example, consider the test a 1 o 1 a 2 o 2 , where a 1 and a 2 are specific actions and o 1 and o 2 are specific observations.  The correct prediction for this test given the data stream up to time k is the probability of its observations occurring (in order) given that its actions are taken (in order) (i. e. , P r fO k = o 1 ; O k+1 = o 2 j A k = a 1 ; A k+1 = a 2 g).  Each test is a kind of experiment that could be performed to tell us something about the system.  If we knew the outcome of all possible tests, then we would know everything there is to know about the system.  A PSR is a set of tests that is sucient information to determine the prediction for all possible tests (a sucient statistic).  As an example of these points, consider the #oat/reset problem (Figure 2) consisting of a linear string of 5 states with a distinguished reset state on the far right.  One action, f (#oat), causes the system to move uniformly at random to the right or left by one state, bounded at the two ends.  The other action, r (reset), causes a jump to the reset state irrespective of the current state.  The observation is always 0 unless the r action is taken when the system is already in the reset state, in which case the observation is 1.  Thus, on an f action, the correct prediction is always 0, whereas on an r action, the correct prediction depends on how many fs there have been since the last r: for zero fs, it is 1; for one or two fs, it is 0. 5; for three or four fs, it is 0. 375; for five or six fs, it is 0. 3125, and so on decreasing after every second f, asymptotically bottoming out at 0. 2.  No k-order Markov method can model this system exactly, because no
Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming| Abstract This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods.  Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world.  In this paper, I present and show results for two Dyna architectures.  The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems).  Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model.  The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning.  Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use.  We show that Dyna-Q architectures are easy to adapt for use in changing environments. 
Learning Instance-Independent Value Functions to Enhance Local Search| Abstract Reinforcement learning methods can be used to improve the performance of local search algorithms for combinatorial optimization by learning an evaluation function that predicts the outcome of search.  The evaluation function is therefore able to guide search to low-cost solutions better than can the original cost function.  We describe a reinforcement learning method for enhancing local search that combines aspects of previous work by Zhang and Dietterich (1995) and Boyan and Moore (1997, Boyan 1998).  In an off-line learning phase, a value function is learned that is useful for guiding search for multiple problem sizes and instances.  We illustrate our technique by developing several such functions for the Dial-A-Ride Problem.  Our learning-enhanced local search algorithm exhibits an improvement of more then 30% over a standard local search algorithm. 
Off-policy Learning with Recognizers| Abstract We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods.  We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections.  We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance.  This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence.  Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators.  Off-policy learning is learning about one way of behaving while actually behaving in another way.  For example, Q-learning is an off- policy learning method because it learns about the optimal policy while taking actions in a more exploratory fashion, e. g. , according to an e-greedy policy.  Off-policy learning is of interest because only one way of selecting actions can be used at any time, but we would like to learn about many different ways of behaving from the single resultant stream of experience.  For example, the options framework for temporal abstraction involves considering a variety of different ways of selecting actions.  For each such option one would like to learn a model of its possible outcomes suitable for planning and other uses.  Such option models have been proposed as fundamental building blocks of grounded world knowledge (Sutton, Precup & Singh, 1999; Sutton, Rafols & Koop, 2005).  Using off-policy learning, one would be able to learn predictive models for many options at the same time from a single stream of experience.  Unfortunately, off-policy learning using temporal-difference methods has proven problematic when used in conjunction with function approximation.  Function approximation is essential in order to handle the large state spaces that are inherent in many problem domains.  Q-learning, for example, has been proven to converge to an optimal policy in the tabular case, but is unsound and may diverge in the case of linear function approximation (Baird, 1996).  Precup, Sutton, and Dasgupta (2001) introduced and proved convergence for the first off-policy learning algorithm with linear function approximation.  They addressed the problem of learning the expected value of a target policy based on experience generated using a different behavior policy.  They used importance sampling techniques to reduce the off-policy case to the on-policy case, where existing convergence theorems apply (Tsitsiklis & Van Roy, 1997; Tadic, 2000).  There are two important difficulties with that approach.  First, the behavior policy needs to be stationary and known, because it is needed to compute the importance sampling corrections.  Second, the importance sampling weights are often ill-conditioned.  In the worst case, the variance could be infinite and convergence would not occur.  The conditions required to prevent this were somewhat awkward and, even when they applied and asymptotic convergence was assured, the variance could still be high and convergence could be slow.  In this paper we address both of these problems in the context of off-policy learning for options.  We introduce the notion of a recognizer.  Rather than specifying an explicit target policy (for instance, the policy of an option), about which we want to make predictions, a recognizer specifies a condition on the actions that are selected.  For example, a recognizer for the temporally extended action of picking up a cup would not specify which hand is to be used, or what the motion should be at all different positions of the cup.  The recognizer would recognize a whole variety of directions of motion and poses as part of picking the cup.  The advantage of this strategy is not that one might prefer a multitude of different behaviors, but that the behavior may be based on a variety of different strategies, all of which are relevant, and we would like to learn from any of them.  In general, a recognizer is a function that recognizes or accepts a space of different ways of behaving and thus, can learn from a wider range of data.  Recognizers have two advantages over direct specification of a target policy: 1) they are a natural and easy way to specify a target policy for which importance sampling will be well conditioned, and 2) they do not require the behavior policy to be known.  The latter is important because in many cases we may have little knowledge of the behavior policy, or a stationary behavior policy may not even exist.  We show that for the case of state aggregation, even if the behavior policy is unknown, convergence to a good model is achieved.  1 Non-sequential example The benefits of using recognizers in off-policy learning can be most easily seen in a nonsequential context with a single continuous action.  Suppose you are given a sequence of sample actions a i 2 [0, 1], selected i. i. d.  according to probability density b : [0, 1] 7! + (the behavior density).  For example, suppose the behavior density is of the oscillatory form shown as a red line in Figure 1.  For each each action, a i , we observe a corresponding outcome, z i 2 , a random variable whose distribution depends only on a i .  Thus the behavior density induces an outcome density.  The on-policy problem is to estimate the mean m b of the outcome density.  This problem can be solved simply by averaging the sample outcomes: ^ m b = i z i .  The off-policy problem is to use this same data to learn what the mean would be if actions were selected in some way other than b.  For example, if the actions were restricted to a designated range, such as between 0. 7 and 0. 9.  There are two natural ways to pose this off-policy problem.  The most straightforward way is to be equally interested in all actions within the designated region.  One professes to be interested in actions selected according to a target density p : [0, 1] 7! + , which in the example would be 5. 0 between 0. 7 and 0. 9, and zero elsewhere, as in the dashed line in Figure 1 (left).  The importance- sampling estimate of the mean outcome is ^ m p = i p(a i ) b(a i ) z i .  (1)
Learning Polynomial Functions by Feature Construction| Abstract We present a method for learning higherorder polynomial functions from examples using linear regression and feature construction.  Regression is used on a set of training instances to produce a weight vector for a linear function over the feature set.  If this hypothesis is imperfect, a new feature is constructed by forming the product of the two features that most effectively predict the squared error of the current hypothesis.  The algorithm is then repeated.  In an extension to this method, the specific pair of features to combine is selected by measuring their joint ability to predict the hypothesis' error. 
Using Predictive Representations to Improve Generalization in Reinforcement Learning| Abstract The predictive representations hypothesis holds that particularly good generalization will result from representing the state of the world in terms of predictions about possible future experience.  This hypothesis has been a central motivation behind recent research in, for example, PSRs and TD networks.  In this paper we present the first explicit investigation of this hypothesis.  We show in a reinforcement-learning example (a grid-world navigation task) that a predictive representation in tabular form can learn much faster than both the tabular explicit-state representation and a tabular history-based method. 
Multi-Time Models for Reinforcement Learning| Abstract Reinforcement learning can be used not only to predict rewards, but also to predict states, i. e.  to learn a model of the world's dynamics.  Models can be defined at different levels of temporal abstraction.  Multi-time models are models that focus on predicting what will happen, rather than when a certain event will take place.  Based on multi-time models, we can define abstract actions, which enable planning (presumably in a more efficient way) at various levels of abstraction.  1 Action models Model-based reinforcement learning offers a possible solution to the problem of integrating planning in a real-time learning agent.  Models are used to make predictions about the environment.  The input of a model is a state and an action.  The model should output a distribution of possible future states, as well as the expected value of the reward along the way.  Since models provide action-dependent predictions, they also define policies for achieving certain states, or for achieving maximum expected rewards.  Reinforcement learning algorithms have traditionally been concerned with 1-step models, which assume that the agent interacts with its environment at some discrete, lowest-level time scale.  We extend this framework by defining multi-time models, which describe the environment at different time scales.  Multi-time models are a formalism for describing abstract actions. 
MIT Press| Improved Switching among Temporally Abstract Actions.  Abstract In robotics and other control applications it is commonplace to have a
Macro-Actions in Reinforcement Learning: An Empirical Analysis| Abstract Several researchers have proposed reinforcement learning methods that obtain advantages in learning by using temporally extended actions, or macro-actions, but none has carefully analyzed what these advantages are.  In this paper, we separate and analyze two advantages of using macro-actions in reinforcement learning: the effect on exploratory behavior, independent of learning, and the effect on the speed with which the learning process propagates accurate value information.  We empirically measure the separate contributions of these two effects in gridworld and simulated robotic environments.  In these environments, both effects were significant, but the effect of value propagation was larger.  We also compare the accelerations of value propagation due to macro-actions and eligibility traces in the gridworld environment.  Although eligibility traces increased the rate of convergence to the optimal value function compared to learning with macro-actions but without eligibility traces, eligibility traces did not permit the optimal policy to be learned as quickly as it was using macro-actions. 
Exponentiated Gradient Methods for Reinforcement Learning| Abstract This paper introduces and evaluates a natural extension of linear exponentiated gradient methods that makes them applicable to reinforcement learning problems.  Just as these methods speed up supervised learning, we find that they can also increase the efficiency of reinforcement learning.  Comparisons are made with conventional reinforcement learning methods on two test problems using CMAC function approximators and replacing traces.  On a small prediction task, exponentiated gradient methods showed no improvement, but on a larger control task (Mountain Car) they improved the learning speed by approximately 25%.  A more detailed analysis suggests that the difference may be due to the distribution of irrelevant features. 
Gain Adaptation Beats Least Squares?| Abstract I present computational results suggesting that gainadaptation algorithms based in part on connectionist learning methods may improve over least squares and other classical parameter-estimation methods for stochastic time-varying linear systems.  The new algorithms are evaluated with respect to classical methods along three dimensions: asymptotic error, computational complexity, and required prior knowledge about the system.  The new algorithms are all of the same order of complexity as LMS methods, O(n), where n is the dimensionality of the system, whereas least-squares methods and the Kalman filter are O(n 2 ).  The new methods also improve over the Kalman filter in that they do not require a complete statistical model of how the system varies over time.  In a simple computational experiment, the new methods are shown to produce asymptotic error levels near that of the optimal Kalman filter and significantly below those of least-squares and LMS methods.  The new methods may perform better even than the Kalman filter if there is any error in the filter's model of how the system varies over time. 
Approximation in Model-Based Learning| Abstract Model-based reinforcement learning, in which a model of the environment's dynamics is learned and used to supplement direct learning from experience, has been proposed as a general approach to learning and planning.  We present experiments with this idea in which the model of the environment's dynamics is both approximate and learned online.  These experiments involve the Mountain Car task, which requires approximation of both value function and model because it has continuous state variables.  Naive model use is susceptible to modelling errors and can impair the value function.  We show that excessive model use performs worse than using no model at all.  Hybrid methods can mitigate learning with inherently incorrect models. 
Improved Switching among Temporally Abstract Actions| Abstract In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps hand-crafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible.  In this paper we present a framework based on Markov decision processes and semi-Markov decision processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem.  In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers, with negligible additional cost and no re-planning.  In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.  In many applications, solutions to parts of a task are known, either because they were handcrafted by people or because they were previously learned or planned.  For example, in robotics applications, there may exist controllers for moving joints to positions, picking up objects, controlling eye movements, or navigating along hallways.  More generally, an intelligent system may have available to it several temporally extended courses of action to choose from.  In such cases, a key challenge is to take full advantage of the existing temporally extended actions, to choose or switch among them effectively, and to plan at their level rather than at the level of individual actions.  Recently, several researchers have begun to address these challenges within the framework of reinforcement learning and Markov decision processes (e. g. , Singh, 1992; Kaelbling, 1993; Dayan & Hinton, 1993; Thrun and Schwartz, 1995; Sutton, 1995; Dietterich, 1998; Parr & Russell, 1998; McGovern, Sutton & Fagg, 1997).  Common to much of this recent work is the modeling of a temporally extended action as a policy (controller) and a condition for terminating, which we together refer to as an option (Sutton, Precup & Singh, 1998).  In this paper we consider the problem of effectively combining given options into one overall policy, generalizing prior work by Kaelbling (1993).  Sections 1--3 introduce the framework; our new results are in Sections 4 and 5.  1 Reinforcement Learning (MDP) Framework In a Markov decision process (MDP), an agent interacts with an environment at some discrete, lowest-level time scale t = 0; 1; 2; : : : On each time step, the agent perceives the state of the environment, s t 2 S, and on that basis chooses a primitive action, a t 2 A.  In response to each action, a t , the environment produces one step later a numerical reward, r t+1 , and a next state, s t+1 .  The one-step model of the environment consists of the one-step statetransition probabilities and the one-step expected rewards, p a ss 0 = Prfs t+1 = s 0 j s t = s; a t = ag and r a s = Efr t+1 j s t = s; a t = ag; for all s; s 0 2 S and a 2 A.  The agent's objective is to learn an optimal Markov policy, a mapping from states to probabilities of taking each available primitive action, : S \Theta A ! [0; 1], that maximizes the expected discounted future reward from each state s: V (s) = E n r t+1 + flr t+2 + \Delta \Delta \Delta fi fi fi s t = s; o = X a2As (s; a)[r a s + fl X s 0 p a ss 0 V (s 0 )]; where (s; a) is the probability with which the policy chooses action a 2 A s in state s, and fl 2 [0; 1] is a discount-rate parameter.  V (s) is called the value of state s under policy , and V is called the state-value function for .  The optimal state-value function gives the value of a state under an optimal policy: V \Lambda (s) = max V (s) = max a2As [r a s + fl P s 0 p a ss 0 V \Lambda (s 0 )].  Given V \Lambda , an optimal policy is easily formed by choosing in each state s any action that achieves the maximum in this equation.  A parallel set of value functions, denoted Q and Q \Lambda , and Bellman equations can be defined for state-action pairs, rather than for states.  Planning in reinforcement learning refers to the use of models of the environment to compute value functions and thereby to optimize or improve policies.  2 Options We use the term options for our generalization of primitive actions to include temporally extended courses of action.  Let h t;T = s t ; a t ; r t+1 ; s t+1 ; a t+1 ; : : : ; r T ; s T be the history sequence from time t T to time T , and let \Omega denote the set of all possible histories in the given MDP.  Options consist of three components: an initiation set I ` S, a policy : \Omega \Theta A ! [0; 1], and a termination condition fi : \Omega ! [0; 1].  An option o = hI; ; fii can be taken in state s if and only if s 2 I.  If o is taken in state s t , the next action a t is selected according to (s t ; \Delta).  The environment then makes a transition to s t+1 , where o terminates with probability fi(h t;t+1 ), or else continues, determining a t+1 according to (h t;t+1 ; \Delta), and transitioning to state s t+2 , where o terminates with probability fi(h t;t+2 ) etc.  We call the general options defined above semi-Markov because and fi depend on the history sequence; in Markov options and fi depend only on the current state.  Semi-Markov options allow "timeouts", i. e. , termination after some period of time has elapsed, and other extensions which cannot be handled by Markov options.  The initiation set and termination condition of an option together limit the states over which the option's policy must be defined.  For example, a hand-crafted policy for a mobile robot to dock with its battery charger might be defined only for states I in which the battery charger is within sight.  The termination condition fi would be defined to be 1 outside of I and when the robot is successfully docked.  We can now define policies over options.  Let the set of options available in state s be denoted O s ; the set of all options is denoted O = S s2S O s .  When initiated in a state s t , the Markov policy over options : S \Theta O ! [0; 1] selects an option o 2 O s t according to the probability distribution (s t ; \Delta).  The option o is then taken in s t , determining actions until it terminates in s t+k , at which point a new option is selected, according to (s t+k ; \Delta), and so on.  In this way a policy over options, , determines a (non-stationary) policy over actions, or flat policy, = f().  We define the value of a state s under a general flat policy as the expected return
On the Significance of Markov Decision Processes| Abstract.  Formulating the problem facing an intelligent agent as a Markov decision process (MDP) is increasingly common in artificial
Temporal-Difference Networks with History| Abstract Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form [Sutton and Tanner, 2005] .  However, not all partially observable Markov decision processes can be efficiently learned with TD networks.  In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation.  We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by historybased methods alone.  In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered.  We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.  Temporal-difference (TD) networks are a formalism for expressing and learning grounded knowledge about dynamical systems [Sutton and Tanner, 2005] .  TD networks represent the state of the dynamical system as a vector of predictions about future action--observation sequences.  Each prediction is an estimate of the probability or expected value of some future event.  For example, a prediction might estimate the probability of seeing a particular observation at the next time step.  The predictions generated at each time step are thought of as "answers" to a set of "questions" asked by the TD network.  Representations that encode the state of a dynamical system as a vector of predictions are known as predictive representations [Littman et al. , 2002; Jaeger, 1998; Rosencrantz et al. , 2004] .  Predictive representations are a relatively new area of research; as a community we are answering fundamental questions about their possibilities and limitations.  So far, the results have been encouraging.  Singh et al.  have shown that one particular representation known as linear predictive state representations (or linear PSRs) can represent any n th -order Markov model or partially observable Markov decision process (POMDP) [Singh et al. , 2004; Littman et al. , 2002] .  Further, they showed that the size of the PSR scales at least as well as the existing approaches; a linear PSR model is at least as compact as the equivalent POMDP or n th -order Markov model.  TD networks are a generalization of linear PSRs and therefore inherit their representational power [Singh, 2004] .  TD networks have been applied successfully to simple environments, both fully and partially observable.  Although TD networks have the expressive power to accurately model complex partially-observable environments, we show that the existing learning algorithm is insufficient for learning some of these models.  In this paper, we explore the classes of environment whose model could not previously be learned and we improve TD networks so that they can learn models of these environments.  In Section 1 we review the TD networks specification.  In Sections 2 and 3 we examine the class of problems that we have been unable to learn with the existing specification of TD networks, augment the specification, and present new results.  We present the results of applying these augmented TD networks to a more complex grid-world domain in Section 5.  Finally we conclude and discuss the direction of our future research in Section 6.  1 TD Networks without History In the following text we describe a specific instantiation of the original TD networks specification that is instructive for understanding the details of our work.  For information on the general specification of TD networks we direct the reader to the original work [Sutton and Tanner, 2005] .  The problem addressed by TD networks is a general one of learning to predict aspects of the interaction between a decision making agent and its environment (a dynamical system).  At each of a series of discrete time steps t, and agent takes an action a t 2 A and the environment responds by generating an observation o t 2 O.  In this work, we will consider TD networks with two observations, 0 and 1.  The action and observation events occur in sequence, a t- 1 , o t , a t , o t+1 , a t+1 , o t+2 .  This sequence will be called experience.  We are interested in predicting not just each next observation, but more general, action-conditional functions of future experience.  The focus of this work current is on partially observable environments---environments where the observation o t is not a sufficient statistic to make optimal predictions about future experience (o t does not uniquely identify the state of the environment).  We will be using TD networks to learn a model of the environment that is accurate and can be maintained over time.  A TD network is a network of nodes, each representing a single scalar prediction.  The nodes are interconnected by links representing target relationships between predictions, observations, and actions.  These nodes and links determine a set of questions being asked about the data and predictions, and accordingly are called the question network.  Each node on the TD network is a function approximator that outputs a prediction using inputs such as the current observation, the previous action, and the predictions made at the previous time step.  This computation part of the TD network is thought of as providing the answers to the questions, and accordingly is called the answer network.  Figure 1 shows a typical question network.  The question of node y 0 at time t is `If the next action is a1, what is the probability that the next observation o t+1 will be 1?'.  Similarly, node y 2 asks `If the next action is a1, what will node y 0 predict at time t + 1?'.  This is a desired relationship between predictions, but also a question about the data.  We can unroll this interpredictive (or TD) relationship to look at the extensive relationship between y 2 and the data, which yields the question `If the next two actions are a1, what is the probability that o t+2 will be 1?'.  In a fully observable (Markov) environment, it is natural for the question network to be a set of questions that are in some way interesting to the experimenter.  In partiallyobservable environments the structure of the question network has additional constraints; the answers should be a sufficient statistic that accurately represents the state and can be updated as new data becomes available.  Although the question of how to discover a question network that expresses a minimal sufficient statistic is important, it is tangental to the focus of this work.  The question networks we use are not minimal, and a network like that in Figure 1 likely contains O t+1 y 1 a2 y 0 a1 y 2 a1 y 3 a2 y 6 a1 y 7 a2 y 8 a1 y 9 a2 y 4 y 5 a1 a2 a1 a2 Figure 1: Symmetric action-conditional question network.  The network forms a symmetric tree, with a branching factor of |A|.  This example has depth d = 4.  Some of the labels have been left out of this diagram for clarity, each of these nodes should have a label y i and each is conditioned on some action.  extraneous questions.  Formally, y i t 2 [0, 1], i = 0, .  .  .  , n - 1 denotes the prediction for node i at time step t.  The column vector of predictions y t = (y 0 t , .  .  .  , y n- 1 t ) T is updated according to a vectorvalued prediction function with modifiable parameter W: y t = #(W t x t ) (1) This prediction function corresponds to the answer network, where x t 2 < m is a feature vector, W t is a n m matrix of weights, and # is the S-shaped logistic function #(s) = 1 1+e- s .  The feature vector is a function of the preceding action, observation, and node values.  In previous work on TD networks, x t had a binary component for each unique combination of the current observation and previous action (one of which is 1, the rest 0), and n more for the previous node values y t- 1 .  Additionally, x t has a bias term (constant value of 1).  In our experiments we also included real valued features with the complement of the predictions from the last time step (1- y t- 1 ), although we later found this was not necessary.  The learning algorithm for each component w ij t of W t is of the form #w ij t = #(z i t - y i t )(y i t )(1- y i t )x j t c i t , (2) where # is a step-size parameter, z i t is a target for y i defined by a question network, and c i t 2 {0, 1} corresponds to whether the action condition for y i was met at time t.  2 Cycle-World Counterexamples In our experimentation we found that TD networks are able to solve certain, but not all of our small testing problems.  The success of learning does not seem to be directly correlated with the complexity of the environment.  In previous work, TD networks were able to learn an accurate model of a partially-observable seven-state deterministic random walk as well as the probabilities in a fully observable stochastic random walk [Sutton and Tanner, 2005] .  Since that time, we have discovered that TD networks are unable to learn an accurate model of certain small partially-observable deterministic environments.  Careful analysis has determined that the question networks that were used were sufficient to represent the appropriate model, so the issue must lie somewhere in the TD network specification.  Figure 2 presents a simple example of a task and question network for which the solution is representable but not learnable by TD networks without history.  The cycle world consists of the four states shown on the left.  The current state of the system cycles clockwise through the states.  There is a single observation bit that is 1 at the top of the cycle, and 0 at all other times.  On the right is a question network which asks what the observation bit will be one, two, and three time steps in the future.  Recall that at time t, y t is calculated as a function of (y t- 1 , a t- 1 , o t , W t ).  If we assume that the y t- 1 is correct, there is a solution for the weights that will keep y t correct at each successive time step.  Unfortunately, y t- 1 will never be correct; the solution exists but will not be found.  To illustrate this, we must look at the question network, and remember that it specifies target values for the answer
Reinforcement Learning for 3 vs| 2 Keepaway.  Abstract.  As a sequential decision problem, robotic soccer can bene#t from research in reinforcement learning.  We introduce the 3 vs.  2 keepaway domain, a subproblem of robotic soccer implemented in the RoboCup soccer server.  We then explore reinforcement learning methods for policy evaluation and action selection in this distributed, real-time, partially observable, noisy domain.  We present empirical results demonstrating that a learned policy can dramatically outperform hand-coded policies. 
Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta| Abstract Appropriate bias is widely viewed as the key to efficient learning and generalization.  I present a new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience.  The IDBD algorithm is developed for the case of a simple, linear learning system---the LMS or delta rule with a separate learning-rate parameter for each input.  The IDBD algorithm adjusts the learning-rate parameters, which are an important form of bias for this system.  Because bias in this approach is adapted based on previous learning experience, the appropriate testbeds are drifting or non-stationary learning tasks.  For particular tasks of this type, I show that the IDBD algorithm performs better than ordinary LMS and in fact finds the optimal learning rates.  The IDBD algorithm extends and improves over prior work by Jacobs and by me in that it is fully incremental and has only a single free parameter.  This paper also extends previous work by presenting a derivation of the IDBD algorithm as gradient descent in the space of learning-rate parameters.  Finally, I offer a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation. 
Dyna, an Integrated Architecture for Learning, Planning, and Reacting| Abstract Dyna is an AI architecture that integrates learning, planning, and reactive execution.  Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world.  Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes.  Execution is fully reactive in the sense that no planning intervenes between perception and action.  Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method.  This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures. 
Reinforcement Learning Architectures| Abstract Reinforcement learning is the learning of a mapping from situations to actions so as to maximize a scalar reward or reinforcement signal.  The learner is not told which action to take, as in most forms of learning, but instead must discover which actions yield the highest reward by trying them.  In the most interesting and challenging cases, actions aect not only the immediate reward, but also the next situation, and through that all subsequent rewards.  These two characteristics---trial-and-error search and delayed reward---are the two most important distinguishing features of reinforcement learning.  In this paper I present a brief overview of the development of reinforcement learning architectures over the past decade, including reinforcement-comparison, actor-critic, and Q-learning architectures.  Finally, I present Dyna, a class of architectures based on reinforcement learning but which go beyond trial-and-error learning to include a learned internal model of the world.  By intermixing conventional trial and error with hypothetical trial and error using the world model, Dyna systems can plan and learn optimal behavior very rapidly.  1 Reinforcement Learning The reinforcement learning problem is summarized in Figure 1.  On some short time cycle, a learning agent receives sensory information from its environment and chooses an action to send to the environment.  In addition, the learning agent receives a special signal from the environment called the reward.  Unlike the sensory information, which may be a large feature vector, or the action, which may also have many components, the reward is a single realvalued scalar, a number.  The goal of learning is the maximization of the cumulative reward received over time.  Reinforcement learning systems can be defined as learning systems designed for and that perform well on this problem.  Informally, we define reinforcement learning as learning by trial and error from performance feedback---i. e. , from feedback that evaluates the behavior generated by the learning agent but does not indicate correct behavior.  The term "reinforcement learning" appears to have been coined by Minsky (1961), and independently in control theory by Waltz and Fu (1965).  The idea of course comes originally from animal-learning psychology. 
Model-Based Reinforcement Learning with an Approximate, Learned Model| Abstract: Model-based reinforcement learning, in which a model of the environment's dynamics is learned and used to supplement direct learning from experience, has been proposed as a general approach to learning and planning.  We present the first experiments with this idea in which the model of the environment's dynamics is both approximate and learned online.  These experiments involve the Mountain Car task, which requires approximation of both value function and model because it has continuous state variables.  We used models of the simplest possible form, state-aggregation or \grid" models, and CMACs to represent the value function.  We find that model-based methods do indeed perform better than model-free reinforcement learning. 
forLearning, Planning, and Reacting Based on Approximating Dynamic Programming| Abstract This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods.  Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world.  In this pap er, I present and show results for twoDynaarchitectures.  The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas suchasevaluation functions and universal plans (reactive systems).  Using a navigationtask, results are shown for a simple Dyna-PI system that simultaneously learns by trial anderror, learns a world model, and plans optimal routes using the evolving world model.  The Dyna-Q architecture is based on Watkins'sQ-learning, a new kind of reinforcement learning.  Dyna-Q uses a less familiar set of data structures than doesDyna-PI, but is arguably simpler to implement and use.  Weshow that Dyna-Q architectures are easy to adapt for use in changing environments. 
Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding| Abstract On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions.  In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed.  In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces.  In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger.  The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline.  Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes ("rollouts"), as in classical Monte Carlo methods, and as in the TD( ) algorithm when = 1.  However, in our experiments this always resulted in substantially poorer performance.  We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general . 
Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales| Abstract Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key challenges for AI.  In this paper we develop an approach to these problems based on the mathematical framework of reinforcement learning and Markov decision processes (MDPs).  We extend the usual notion of action to include options---whole courses of behavior that may be temporally extended, stochastic, and contingent on events.  Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques.  Options may be given a priori, learned by experience, or both.  They may be used interchangeably with actions in a variety of planning and learning methods.  The theory of semi-Markov decision processes (SMDPs) can be applied to model the consequences of options and as a basis for planning and learning methods using them.  In this paper we develop these connections, building on prior work by Bradtke and Duff (1995), Parr (1998) and others.  Our main novel results concern the interface between the MDP and SMDP levels of analysis.  We show how a set of options can be altered by changing only their termination conditions to improve over SMDP methods with no additional cost.  We also introduce intra-option temporal-difference methods that are able to learn from fragments of an option's execution.  Finally, we propose a notion of subgoal which can be used to improve the options themselves.  Overall, we argue that options and their models provide hitherto missing aspects of a powerful, clear, and expressive framework for representing and organizing knowledge.  1.  Temporal Abstraction To make everyday decisions, people must foresee the consequences of their possible courses of action at multiple levels of temporal abstraction.  Consider a traveler deciding to undertake a journey to a distant city.  To decide whether or not to go, the benefits of the trip must be weighed against the expense.  Having decided to go, choices must be made at each leg, e. g. , whether to fly or to drive, whether to take a taxi or to arrange a ride.  Each of these steps involves foresight and decision, all the way down to the smallest of actions.  For example, just to call a taxi may involve finding a telephone, dialing each digit, and the individual muscle contractions to lift the receiver to the ear.  Human decision making routinely
On Step-Size and Bias in Temporal-Difference Learning| Abstract We present results for three new algorithms for setting the step-size parameters, ff and , of temporaldifference learning methods such as TD( ).  The overall task is that of learning to predict the outcome of an unknown Markov chain based on repeated observations of its state trajectories.  The new algorithms select step-size parameters online in such a way as to eliminate the bias normally inherent in temporaldifference methods.  We compare our algorithms with conventional Monte Carlo methods.  Monte Carlo methods have a natural way of setting the step size: for each state s they use a step size of 1=n s , where n s is the number of times state s has been visited.  We seek and come close to achieving comparable stepsize algorithms for TD( ).  One new algorithm uses a = 1=n s schedule to achieve the same effect as processing a state backwards with TD(0), but remains completely incremental.  Another algorithm uses a at each time equal to the estimated transition probability of the current transition.  We present empirical results showing improvement in convergence rate over Monte Carlo methods and conventional TD( ).  A limitation of our results at present is that they apply only to tasks whose state trajectories do not contain cycles. 
NADALINE: A Normalized Adaptive Linear Element that Learns Eciently| Abstract This paper introduces a variant of the ADALINE in which the input signals are normalized to have zero mean and unit variance, and in which the bias or "threshold weight" is learned slightly dierently.  These changes result in a linear learning element that learns much more eciently and rapidly, and that is much less dependent on the choice of the step-size parameter.  Using simulation experiments, learning time improvements of from 30% to hundreds of times are shown.  The memory and computational complexity of the new element remains O(N) , where N is the number of input signals, and the added computations are entirely local.  Theoretical analysis indicates that the new element learns optimally fast in a certain sense and to the extent that the input signals are statistically independent. 
Category: Reinforcement Learning and Control; ORAL presentation Improved Switching among Temporally Abstract Actions| Abstract In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps handcrafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible.  In this paper we present a framework based on Markov Decision Processes and Semi-Markov Decision Processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem.  In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers with negligible additional cost and no replanning.  In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.  In many applications, possible solutions to parts of a task are known, either because they were handcrafted by people or because they were previously learned or planned.  For example, in robotics applications, there may exist controllers for moving joints to positions, picking up objects, controlling eye movements, or navigating along hallways.  More generally, an intelligent system may have available to it several temporally extended courses of action to choose from.  In such cases, a key challenge is to take full advantage of the existing temporally extended actions, to choose or switch among them effectively and to plan at their level rather than at the level of individual actions.  Recently, several researchers have begun to address these challenges within the framework of reinforcement learning and Markov decision processes (e. g. , Singh, 1992; Kaelbling, 1993; Dayan & Hinton, 1993; Thrun and Schwartz, 1995; Sutton, 1995; Dietterich, 1998; Parr & Russell, 1998; McGovern, Sutton & Fagg, 1997).  Common to much of this recent work is the modeling of a temporally extended action as a policy (controller) and a condition for terminating, which we together refer to as an option (Sutton, Precup & Singh, 1998, in preparation).  In this paper we consider the problem of effectively combining given options into one overall policy.  1 Reinforcement Learning (MDP) Framework In the Markov Decision Process (MDP) framework an agent interacts with an environment at some discrete, lowest-level time scale t = 0; 1;
Reinforcement Learning and Local Search: A Case Study| Abstract We describe a reinforcement learning-based variation to the combinatorial optimization technique known as local search.  The hillclimbing aspect of local search uses the problem's primary cost function to guide search via local neighborhoods to high quality solutions.  In complicated optimization problems, however, other problem characteristics can also help guide the search process.  In this report we present an approach to constructing more general, derived, cost functions for combinatorial optimization problems using reinforcement learning.  Such derived cost functions integrate a variety of problem characteristics into a single hillclimbing function.  We illustrate our technique by developing several such functions for the Dial-A-Ride Problem, a variant of the well-known Traveling Salesman Problem. 
Theoretical Results on Reinforcement Learning with Temporally Abstract Options| Abstract.  We present new theoretical results on planning within the framework of temporally abstract reinforcement learning (Precup & Sutton, 1997; Sutton, 1995).  Temporal abstraction is a key step in any decision making system that involves planning and prediction.  In temporally abstract reinforcement learning, the agent is allowed to choose among "options", whole courses of action that may be temporally extended, stochastic, and contingent on previous events.  Examples of options include closed-loop policies such as picking up an object, as well as primitive actions such as joint torques.  Knowledge about the consequences of options is represented by special structures called multi-time models.  In this paper we focus on the theory of planning with multi-time models.  We define new Bellman equations that are satisfied for sets of multi-time models.  As a consequence, multi-time models can be used interchangeably with models of primitive actions in a variety of well-known planning methods including value iteration, policy improvement and policy iteration. 
Intra-Option Learning about Temporally Abstract Actions| Abstract Several researchers have proposed modeling temporally abstract actions in reinforcement learning by the combination of a policy and a termination condition, which we refer to as an option.  Value functions over options and models of options can be learned using methods designed for semi-Markov decision processes (SMDPs).  However, all these methods require an option to be executed to termination.  In this paper we explore methods that learn about an option from small fragments of experience consistent with that option, even if the option itself is not executed.  We call these methods intra-option learning methods because they learn from experience within an option.  Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporaldifference mechanisms to learn simultaneously about all the options consistent with an experience, not just the few that were actually executed.  In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the consequences of options.  We present computational examples in which these new methods learn much faster than SMDP methods and learn effectively when SMDP methods cannot learn at all.  We also sketch a convergence proof for intraoption value learning. 
Eligibility Traces for Off-Policy Policy Evaluation| Abstract Eligibility traces have been shown to speed reinforcement learning, to make it more robust to hidden states, and to provide a link between Monte Carlo and temporal-difference methods.  Here we generalize eligibility traces to off-policy learning, in which one learns about a policy different from the policy that generates the data.  Off-policy methods can greatly multiply learning, as many policies can be learned about from the same data stream, and have been identified as particularly useful for learning about subgoals and temporally extended macro-actions.  In this paper we consider the off-policy version of the policy evaluation problem, for which only one eligibility trace algorithm is known, a Monte Carlo method.  We analyze and compare this and four new eligibility trace algorithms, emphasizing their relationships to the classical statistical technique known as importance sampling.  Our main results are 1) to establish the consistency and bias properties of the new methods and 2) to empirically rank the new methods, showing improvement over one-step and Monte Carlo methods.  Our results are restricted to model-free, table-lookup methods and to offline updating (at the end of each episode) although several of the algorithms could be applied more generally.  In reinforcement learning, we generally learn from experience, that is, from the sequence of states, actions, and rewards generated by the agent interacting with its environment.  This data is affected by the decision-making policy used by the agent to select its actions, and thus we often end up learning something that is a function of the agent's policy.  For example, the common subproblem of policy evaluation is to learn the value function for the agent's policy (the function giving the expected future reward available from each state--action pair).  In general, however, we might want to learn about policies other than that currently followed by the agent, a process known as off-policy learning.  For example, 1-step Q-learning is often used in an off-policy manner, learning about the greedy policy while the data is generated by a slightly randomized policy that ensures exploration.  Off-policy learning is especially important for research on the use of temporally extended actions in reinforcement learning (
Multi-time Models for Temporally Abstract Planning| Abstract Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence.  In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning.  Current model-based reinforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver.  This paper generalizes prior work on temporally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning.  We introduce a more general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship to the Bellman equations.  This paper summarizes the theoretical framework of multi-time models and illustrates their potential advantages in a gridworld planning task.  The need for hierarchical and abstract planning is a fundamental problem in AI (see, e. g. , Sacerdoti, 1977;
Planning by Incremental Dynamic Programming| Abstract This paper presents the basic results and ideas of dynamic programming as they relate most directly to the concerns of planning in AI.  These form the theoretical basis for the incremental planning methods used in the integrated architecture Dyna.  These incremental planning methods are based on continually updating an evaluation function and the situation-action mapping of a reactive system.  Actions are generated by the reactive system and thus involve minimal delay, while the incremental planning process guarantees that the actions and evaluation function will eventually be optimal---no matter how extensive a search is required.  These methods are well suited to stochastic tasks and to tasks in which a complete and accurate model is not available.  For tasks too large to implement the situation-action mapping as a table, supervised-learning methods must be used, and their capabilities remain a significant limitation of the approach. 
Learning to Predict by the Methods of Temporal Differences|
Neuronlike elements that can solve difficult learning control problems|
Learning and sequential decision making|
Learning to predict by the method of temporal difference|
Temporal credit assignment in reinforcement learning|
Time-derivative models of Pavlovian reinforcement|
Iterative Construction of Sparse Polynomial Approximations|
Adaptation of cue-specific learning rates in adaptive networks: Computational and psychological perspectives|
An Online Book of Reinforcement Learning,|
Training and Tracking in Robotics|
Two problems with backpropagation and other steepestdescent learning procedures for networks|
A temporaldifference model of classical conditioning| Proceedings of the ninth conference of the cognitive science society. 
Temporal abstraction in reinforcement learning|
Roles of macro-actions in accelerating reinforcement learning|
Off-Policy Temporal Difference Learning with Function Approximation|
Eligibility traces for o8-polcy policy evaluation|
Reinforcement learning for RoboCup-soccer keepaway|
Reinforcement Learning: An Introduction MIT Press|
Open Theoretical Questions in Reinforcement Learning|
Towards a modern theory of adaptive networks: Expectation and prediction|
Associative search network: a reinforcement learning associative memory,|
Integrated Modeling and Control Based on Reinforcement Learning|
On the Virtues of Linear Learning and Trajectory Distributions,|
Sequential decision problems and neural networks|
The learning of world models by connectionist networks|
An adaptive network that constructs and uses an internal model of its environment|
Reinforcement learning is direct adaptive control|
Landmark learning: An illustration of associative search|
Synthesis of nonlinear control surfaces by a layered associative search network|
Online Learning with Random Representations|
Keepaway Soccer: A Machine Learning Testbed|
First results with Dyna, an integrated architecture for learning, planning and reacting|
Introduction: The challenge of reinforcement learning|
Simulation of the classically conditioned nictitating membrane response by a neuron-like adaptive element: Response topography, neuronal firing and interstimulus intervals|
Learning a Nonlinear Model of a Manufacturing Process Using Multilayer Connectionist Networks,|
in preparation| Between MDPs and Semi-MDPs: learning, planning, and representing knowledge at multiple temporal scales. 
Reinforcement Learning|
Empirical comparison of gradient descent and exponentiated gradient descent in supervised and reinforcement learning,|
Adaptation of learning rate parameters, Appendix C of Goal Seeking Components for Adaptive Intelligence: An Initial Assessment| Air Force Wright Aeronautical Laboratories/Avionics. 
in preparation) Dynamically modifiable stimulus associability in network models of category learning|
A theory of salience change dependent on the relationship between discrepancies on successive trials on which the stimulus is present|
Special issue on reinforcement learning|
A temporal-di#erence method of classical conditioning|
Eligibilty trace methods for o#-policy evaluation|
Neural problem solving|
Reinforcement learning with eligibility traces|
Reinforcement learning: Course notes|
Tensor product variable binding and the representation of symbolic structures in connectionist networks|
in press)| Policy gradient methods for reinforcement learning with function approximation. 
editor|
\Policy gradient mehtods for reinforcement lenaring with function approximation,|
Simulation of anticipatory responses in classical conditioning by a neuron-like adaptive element|
Simulation of Anticipatory Responses in Classical Conditioning by a Neuron-like Adaptive Element",|
Guest Editor: Special issue on reinforcement learning|
Implementation details for the TD(#) procedure for the case of vector predictions and backpropagation (|
Open theoretical questions in reinforcement learning, Extended abstract of an invited talk at EuroCOLT'99|
Implementation details of the TD(#) procedure for the case of vector predictions and backpropagation,|
If desired, the present analysis of the stochastic neuronal dynamics can be replaced by an analysis of this deterministic neuronal dynamics,|
Barto A G 1985 Training and tracking in robotics|
eds|,. 
Temporal abstraction in temporal-difference networks"|
planning, and representing knowledge at multiple temporal scales|
editors|
Personal communication at the Seventh International Conference on Artificial Neural Networks (|
Temporal Credit Assignmenment in Reinforcement Learning|
Barto A G, "Reinforcement Learning:|
to appear) Reinforcement learning with replacing eligibility traces|
\fINeural networks for control|\fP. 
Reinforcement learning with relacing eligibility traces|
Multi-Time Models for Reinforcement Learning, University of Massachusetts Amherst|
Toward a modern theory of adaptative network : Expectation and prediction|
"Reinforcement Learning is Direct Adaptice Optimal Control",|
Constructive Induction Needs a Methodology based on Continuing Learning,|
Agents learning about agents: a Framework and Analysis",|
On theVirtues of Linear LearningandTrajectory Distributions," Proceedings of theWorkshoponValue Function Approximation, Machine Learning Conference 1995,|
Learning to predict by Temporal|

A Note on Support Vector Machine Degeneracy| Abstract When training Support Vector Machines (SVMs) over non-separable data sets, one sets the threshold b using any dual cost coefficient that is strictly between the bounds of 0 and C.  We show that there exist SVM training problems with dual optimal solutions with all coefficients at bounds, but that all such problems are degenerate in the sense that the "optimal separating hyperplane" is given by w = 0, and the resulting (degenerate) SVM will classify all future points identically (to the class that supplies more training data).  We also derive necessary and sufficient conditions on the input data for this to occur.  Finally, we show that an SVM training problem can always be made degenerate by the addition of a single data point belonging to a certain unbounded polyhedron, which we characterize in terms of its extreme points and rays. 
From Regression to Classification in Support Vector Machines| Abstract We study the relation between support vector machines (SVMs) for regression (SVMR) and SVM for classification (SVMC).  We show that for a given SVMC solution there exists a SVMR solution which is equivalent for a certain choice of the parameters.  In particular our result is that for # sufficiently close to one, the optimal hyperplane and threshold for the SVMC problem with regularization parameter C c are equal to 1 1- # times the optimal hyperplane and threshold for SVMR with regularization parameter C r = (1- #)C c .  A direct consequence of this result is that SVMC can be seen as a special case of SVMR. 
Comparing Convolution Kernels and Recursive Neural Networks for Learning Preferences on Structured Data| Abstract Convolution kernels and recursive neural networks (RNN) are both suitable approaches for supervised learning when the input portion of an instance is a discrete structure like a tree or a graph.  We report about an empirical comparison between the two architectures in a large scale preference learning problem related to natural language processing, where instances are candidate incremental parse trees.  We found that kernels never outperform RNNs, even when a limited number of examples is employed for learning.  We argue that convolution kernels may lead to feature space representations that are too sparse and too general because not focused on the specific learning task.  The adaptive encoding mechanism in RNNs in this case allows us to obtain better prediction accuracy at smaller computational cost. 
Feature Selection for SVMs| Abstract We introduce a method of feature selection for Support Vector Machines.  The method is based upon finding those features which minimize bounds on the leave-one-out error.  This search can be efficiently performed via gradient descent.  The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA microarray data. 
Fingerprint Classification with Combinations of Support Vector Machines| Abstract.  e report about some experiments on the fingerprint database NIST-4 using di(erent combinations of Support Vector Machine (SVM) classifiers.  Images have been preprocessed using the feature extraction technique as in [10].  Our best classification accuracy is 89. 3
New Results on Error Correcting Output Codes of Kernel Machines| Abstract---We study the problem of multiclass classification within the framework of error correcting output codes (ECOC) using margin-based binary classifiers.  Specifically, we address two important open problems in this context: decoding and model selection.  The decoding problem concerns how to map the outputs of the classifiers into class codewords.  In this paper we introduce a new decoding function that combines the margins through an estimate of their class conditional probabilities.  Concerning model selection, we present new theoretical results bounding the leave-one-out (LOO) error of ECOC of kernel machines, which can be used to tune kernel hyperparameters.  We report experiments using support vector machines as the base binary classifiers, showing the advantage of the proposed decoding function over other functions of the margin commonly used in practice.  Moreover, our empirical evaluations on model selection indicate that the bound leads to good estimates of kernel parameters. 
On Tuning Hyper-Parameters of Multiclass Margin Classifiers| Abstract.  The choice of hyper-parameters (e. g.  kernel parameters) can significantly affect generalization performance of large margin classifiers.  In this paper we are concerned with the problem of tuning these values in the case of multi-class problems that have been recast into a set of binary problems.  We report several experimental results comparing independent and joint tuning of the hyper-parameters of the binary classifiers.  Several different encoding strategies are explored, including error correcting output codes.  Tuning was carried out by using a validation set and a newly introduced bound on the leave-one-out error. 
On the Noise Model of Support Vector Machines Regression| Abstract.  Support Vector Machines Regression (SVMR) is a learning technique where the goodness of fit is measured not by the usual quadratic loss function (the mean square error), but by a different loss function called the #-Insensitive Loss Function (ILF), which is similar to loss functions used in the field of robust statistics.  The quadratic loss function is well justified under the assumption of Gaussian additive noise.  However, the noise model underlying the choice of the ILF is not clear.  In this paper the use of the ILF is justified under the assumption that the noise is additive and Gaussian, where the variance and mean of the Gaussian are random variables.  The probability distributions for the variance and mean will be stated explicitly.  While this work is presented in the framework of SVMR,
Wide Coverage Natural Language Processing using Kernel Methods and Neural Networks for Structured Data| Convolution kernels and recursive neural networks are both suitable approaches for supervised learning when the input is a discrete structure like a labeled tree or graph.  We compare these techniques in two natural language problems.  In both problems, the learning task consists in choosing the best alternative tree in a set of candidates.  We report about an empirical evaluation between the two methods on a large corpus of parsed sentences and speculate on the role played by the representation and the loss function. 
Combining flat and structured representations for fingerprint classification with recursive neural networks and support vector machines| Our best classification accuracy is of 95. 6 percent at 20 percent rejection rate and is obtained by training SVM on both FingerCode and RNN-extracted features.  This result indicates the benefit of integrating global and structured representations and suggests that SVM are a promising approach for fingerprint classification. 
Support Vector Machines for 3D Object Recognition| Abstract.  Support Vector Machines (SVMs) have been recently proposed as a new technique for pattern recognition.  Intuitively, given a set of points which belong to either of two classes, a linear SVM finds the hyperplane leaving the largest possible fraction of points of the same class on the same side, while maximizing the distance of either class from the hyperplane.  The hyperplane is determined by a subset of the points of the two classes, named support vectors, and has a number of interesting theoretical properties.  In this paper we use linear SVMs for 3-D object recognition.  We illustrate the potential of SVMs on a database of 7200 images of 100 different objects.  The proposed system does not require feature extraction and performs recognition on images regarded as points of a space of high dimension without estimating pose.  The excellent recognition rates achieved in all the performed experiments indicate that SVMs are well-suited for aspectbased recognition. 
Face Detection in Still Gray Images| Abstract We present a trainable system for detecting frontal and near-frontal views of faces in still gray images using Support Vector Machines (SVMs).  We first consider the problem of detecting the whole face pattern by a single SVM classifier.  In this context we compare different types of image features, present and evaluate a new method for reducing the number features and discuss practical issues concerning the parameterization of SVMs and the selection of training data.  The second part of the paper describes a component-based method for face detection consisting of a two-level hierarchy of SVM classifiers.  On the first level, component classi#ers independently detect components of a face, such as the eyes, the nose, and the mouth.  On the second level, a single classifier checks if the geometrical configuration of the detected components in the image matches a geometrical model of a face. 
Categorization by Learning and Combining Object Parts| Abstract We describe an algorithm for automatically learning discriminative components of objects with SVM classifiers.  It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM.  Component-based face classifiers are then combined in a second stage to yield a hierarchical SVM classifier.  Experimental results in face classification show considerable robustness against rotations in depth and suggest performance at significantly better level than other face detection systems.  Novel aspects of our approach are: a) an algorithm to learn component-based classification experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classifier which may be relevant for biological models of visual recognition. 
From Margins to Probabilities in Multiclass Learning Problems| Abstract.  We study the problem of multiclass classification within the framework of error correcting output codes (ECOC) using margin-based binary classifiers.  An important open problem in this context is how to measure the distance between class codewords and the outputs of the classifiers.  In this paper we propose a new decoding function that combines the margins through an estimate of their class conditional probabilities.  We report experiments using support vector machines as the base binary classifiers, showing the advantage of the proposed decoding function over other functions of the margin commonly used in practice.  We also present new theoretical results bounding the leave-one-out error of ECOC of kernel machines, which can be used to tune kernel parameters.  An empirical validation indicates that the bound leads to good estimates of kernel parameters and the corresponding classifiers attain high accuracy. 
Kernels for Multi--task Learning| Abstract This paper provides a foundation for multi--task learning using reproducing kernel Hilbert spaces of
Properties of Support Vector Machines|
Bounds on the Generalization Performance of Kernel Machine Ensembles|
Recognizing 3-D Objects with Linear Support Vector Machines|
Component-based Face Detection|
A Function Representation for Learning in Banach Spaces|
Regularized multi--task learning|
People Recognition and Pose Estimation in Image Sequences|
Full-body person recognition system|
Eects of heavy metals in soil microbial diversity and activity as shown by the sensitivity-resistance index an ecologically relevant parameter|

Large Scale Online Learning| Abstract We consider situations where training data is abundant and computing resources are comparatively scarce.  We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm.  Both theoretical and experimental evidences are presented. 
Geometric Clustering Using the Information Bottleneck Method| Abstract We argue that K--means and deterministic annealing algorithms for geometric clustering can be derived from the more general Information Bottleneck approach.  If we cluster the identities of data points to preserve information about their location, the set of optimal solutions is massively degenerate.  But if we treat the equations that define the optimal solution as an iterative algorithm, then a set of "smooth" initial conditions selects solutions with the desired geometrical properties.  In addition to conceptual unification, we argue that this approach can be more efficient and robust than classic algorithms. 
High Quality Document Image Compression with DjVu| Abstract We present a new image compression technique called "DjVu " that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color.  This enables fast transmission of document images over low-speed connections, while faithfully reproducing the visual aspect of the document, including color, fonts, pictures, and paper texture.  The DjVu compressor separates the text and drawings, which needs a high spatial resolution, from the pictures and backgrounds, which are smoother and can be coded at a lower spatial resolution.  Then, several novel techniques are used to maximize the compression ratio: the bi-level foreground image is encoded with AT&T's proposal to the new JBIG2 fax standard, and a new wavelet-based compression method is used for the backgrounds and pictures.  Both techniques use a new adaptive binary arithmetic coder called the Z-coder.  A typical magazine page in color at 300dpi can be compressed down to between40to60KB,approximately 5 to 10 times better than JPEG for a similar level of subjective quality.  A real-time, memory efficientversion of the decoder was implemented, and is available as a plug-in for popular web browsers. 
Max-Planck-Institut fr biologische Kybernetik| Abstract We describe a new method for performing a nonlinear form of Principal Component Analysis.  By the use of integral operator kernel functions, we can efficiently compute principal components in high--dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible 5--pixel products in 16\Theta16 images.  We give the derivation of the method, along with a discussion of other techniques which can be made nonlinear with the kernel approach; and present first experimental results on nonlinear feature extraction for pattern recognition.  AS and KRM are with GMD First (Forschungszentrum Informationstechnik)
Convergence Properties of the K-Means Algorithms| Abstract This paper studies the convergence properties of the well known K-Means clustering algorithm.  The K-Means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the EM algorithm to this hard threshold case.  We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm. 
Structural Risk Minimization for Character Recognition| Abstract The method of Structural Risk Minimization refers to tuning the capacity of the classifier to the available amount of training data.  This capacity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) learning algorithm.  Actions based on these three factors are combined here to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition.  1 RISK MINIMIZATION AND CAPACITY 1. 1 EMPIRICAL RISK MINIMIZATION A common way of training a given classifier is to adjust the parameters w in the classification function F (x; w) to minimize the training error E
A GENERAL SEGMENTATION SCHEME FOR DJVU DOCUMENT COMPRESSION| Abstract We describe the "DjVu" (Dej
Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks| Abstract Signal processing and pattern recognition algorithms make extensive use of convolution.  In many cases, computational accuracy is not as important as computational speed.  In feature extraction, for instance, the features of interest in a signal are usually quite distorted.  This form of noise justifies some level of quantization in order to achieve faster feature extraction.  Our approach consists of approximating regions of the signal with low degree polynomials, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions).  With this representation, convolution becomes extremely simple and can be implemented quite effectively.  The true convolution can be recovered byintegrating the result of the convolution.  This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks. 
Gradient-Based Learning Applied to Document Recognition| Abstract--Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique.  Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing.  This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task.  Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. 
Breaking SVM Complexity with Cross-Training| Abstract We propose to selectively remove examples from the training set using probabilistic estimates related to editing algorithms (Devijver and Kittler, 1982).  This heuristic procedure aims at creating a separable distribution of training examples with minimal impact on the position of the decision boundary.  It breaks the linear dependency between the number of SVs and the number of training examples, and sharply reduces the complexity of SVMs during both the training and prediction stages. 
COMPARISON OF LEARNING ALGORITHMS FOR HANDWRITTEN DIGIT RECOGNITION| Abstract This paper compares the performance of several classifier algorithms on a standard database of handwritten digits.  We consider not only raw accuracy, but also rejection, training time, recognition time, and memory requirements. 
Browsing through High Quality Document Images with DjVu| Abstract We present a new image compression technique called "DjVu " that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color.  With DjVu , any screen connected to the Internet can access and display images of scannedpages while faithfully reproducing the font, color, drawing, pictures, and paper texture.  A typical magazine page in color at 300dpi can becompressed down to between 40 to 60 KB, approximately 5 to 10 times better than JPEG for a similar level of subjective quality.  B&W documents are typically 15 to 30 KBytes at 300dpi, or 4 to 8 times better than CCITT-G4.  Areal-time, memory efficient version of the decoder was implemented, and is available as a plug-in for popular web browsers. 
A Framework for the Cooperation of Learning Algorithms|
Une approche theorique de l'apprentissage connexioniste: Applications ` alareconnaissance de la parole,|
Support vector machine reference manual|
Computeraided cleaning of large databases for character recognition|
Local Learning algorithms|
Online learning and stochastic approximations,|
Comparison of classifier methods: A case study in handwriting digit recognition|
Comparison of classifier methods: a case study in handwritten digit recognition|
Reading checks with graph transformer networks|
`Structural risk minimization for character recognition', this volume|
DjVu: Analyzing and Compressing Scanned Documents for Internet Distribution|
Local algorithms for pattern recognition and dependencies estimation|
Learning vector quantization, multi-layer perceptron and dynamic programming: Comparison and cooperation,|
Reading checks with multilayer graph transformer networks|
The Z-Coder Adaptive Binary Coder|
Global training of document processing systems using graph transformer networks,"|
Ef cient backprop|
Stochastic Approximations and Efficient Learning|
Online Learning for Very Large Datasets|
Efficient Back-prop|
Lossy Compression of Partially Masked Still Images|
Reconnaissance de la parole par r'eseaux multi-couches|
Online algorithms and stochastic approximations|
Online on an even tighter budget|
DCT-based scalable video coding with drift, ICIP,|
Managing Drift in DCT-Based Scalable Video Coding|
Comparision of learning algorithms for handwritten digit recognition|
'e| Galatea: a C-Library for Connectionist Applications. 
Local leaning algorithms|
Reconnaissance de la Parole par Reseaux Multi-Couches|
Online (and offline) on an even tighter budget|
MLP, LVQ and DP: Comparison & cooperation,"|
Speaker independent isolated digit recognition: multilayer perceptrons vs dynamic time warping,|
Signature verification with a siamese time delay neural network|
Diet and food preference of the adult horseshoe crab|
Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting|
Gradient-based learning applied to document recognition|
About direct regression and density estimation|
"A new approach to estimating density functions with artificial neural networks,"|
Color Documents on the Web with DJVU|
Conversion of digital documents to multilayer raster formats|
Electronic conversion of documents to DjVu using a Windows virtual printer driver|
International Conference on Artificial Neural Networks (|

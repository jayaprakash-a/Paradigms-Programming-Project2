Regularizing Phase-Based Stereo| Abstract Wavelet-based techniques proved to be a promising approach for estimating the disparity between two stereo images.  The complex-valued Gabor f ilter responses reduce the ambiguity of raw image intensities, and their phase differences between left and right image provide a direct measure for the disparities.  Experience shows that such phasebased measurements are reliable near edges but yield poor results between them.  To improve these phase-based disparity estimations, a regularization scheme is proposed which directly compares possible matching pairs.  Unreliable regions are f illed by using a simple smoothness constraint.  In the spirit of Markov random f ields, we propose a probabilistic lattice model which describes the complete disparity distribution instead of representing only a single conf iguration.  Experimental results are presented for an artif icial image pair generated by computer graphics. 
Visualizing Group Structure| Abstract Cluster analysis is a fundamental principle in exploratory data analysis, providing the user with a description of the group structure of given data.  A key problem in this context is the interpretation and visualization of clustering solutions in high{dimensional or abstract data spaces.  In particular, probabilistic descriptions of the group structure, essential to capture inter{cluster relationships, are hardly assessable by simple inspection of the probabilistic assignmentvariables.  We presentanovel approach to the visualization of group structure.  It is based on a statistical model of the object assignments whichhave been observed or estimated bya probabilistic clustering procedure.  The objects or data points are embedded in a low dimensional Euclidean space by approximating the observed data statistics with a Gaussian mixture model.  The algorithm provides a new approach to the visualization of the inherent structure for a broad variety of data types, e. g.  histogram data, proximity data and co{occurrence data.  To demonstrate the power of the approach, histograms of textured images are visualized as an example of a large{scale data mining application. 
A theory of proximity based clustering: structure detection by optimization| Abstract In this paper, a systematic optimization approach for clustering proximity or similarity data is developed.  Starting from fundamental invariance and robustness properties, a set of axioms is proposed and discussed to distinguish different cluster compactness and separation criteria.  The approach covers the case of sparse proximity matrices, and is extended to nested partitionings for hierarchical data clustering.  To solve the associated optimization problems, a rigorous mathematical framework for deterministic annealing and mean--field approximation is presented.  Efficient optimization heuristics are derived in a canonical way, which also clarifies the relation to stochastic optimization by Gibbs sampling.  Similarity-based clustering techniques have a broad range of possible applications in computer vision, pattern recognition, and data analysis.  As a major practical application we present a novel approach to the problem of unsupervised texture segmentation, which relies on statistical tests as a measure of homogeneity.  The quality of the algorithms is empirically evaluated on a large collection of Brodatz--like micro-texture Mondrians and on a set of real--word images.  To demonstrate the broad usefulness of the theory of proximity based clustering the performances of different criteria and algorithms are compared on an information retrieval task for a document database.  The superiority of optimization algorithms for clustering is supported by extensive experiments. 
ROBUST VECTOR QUANTIZATION BY COMPETITIVE LEARNING| ABSTRACT Competitive neural networks can be used to efficiently quantize image and video data.  We discuss a novel class of vector quantizers which perform noise robust data compression.  The vector quantizers are trained to simultaneously compensate channel noise and code vector elimination noise.  The training algorithm to estimate code vectors is derived by the maximum entropy principle in the spirit of deterministic annealing.  We demonstrate the performance of noise robust codebooks with compression results for a teleconferencing system on the basis of a wavelet image representation. 
Stability-Based Model Selection| Abstract Model selection is linked to model assessment, which is the problem of comparing different models, or model parameters, for a specific learning task.  For supervised learning, the standard practical technique is crossvalidation, which is not applicable for semi-supervised and unsupervised settings.  In this paper, a new model assessment scheme is introduced which is based on a notion of stability.  The stability measure yields an upper bound to cross-validation in the supervised case, but extends to semi-supervised and unsupervised problems.  In the experimental part, the performance of the stability measure is studied for model order selection in comparison to standard techniques in this area. 
A Resampling Approach to Cluster Validation| Abstract.  The concept of cluster stability is introduced as a means for assessing the validity of data partitionings found by clustering algorithms.  It allows us to explicitly quantify the quality of a clustering solution, without being dependent on external information.  The principle of maximizing the cluster stability can be interpreted as choosing the most self-consistent data partitioning.  We present an empirical estimator for the theoretically derived stability index, based on imitating independent sample-sets by way of resampling.  Experiments on both toy-examples and real-world problems effectively demonstrate that the proposed validation principle is highly suited for model selection. 
Clustering Principles and Empirical Risk Approximation| Abstract.  Data Clustering is one of the fundamental techniques in pattern recognition to extract structure from data.  We discuss a maximum entropy approach to clustering for different clustering cost functions and relate it to the estimation principle of Empirical Risk Approximation.  Large deviation techniques from statistical learning theory provide guarantees for the stability of clustering solutions. 
Unsupervised On-line Learning of Decision Trees for Hierarchical Data Analysis| Abstract An adaptive on--line algorithm is proposed to estimate hierarchical data structures for non--stationary data sources.  The approach is based on the principle of minimum cross entropy to derive a decision tree for data clustering and it employs a metalearning idea (learning to learn) to adapt to changes in data characteristics.  Its efficiency is demonstrated by grouping non--stationary artifical data and by hierarchical segmentation of LANDSAT images. 
Going Metric: Denoising Pairwise Data| Abstract Pairwise data in empirical sciences typically violate metricity, either due to noise or due to fallible estimates, and therefore are hard to analyze by conventional machine learning technology.  In this paper we therefore study ways to work around this problem.  First, we present an alternative embedding to multi-dimensional scaling (MDS) that allows us to apply the zoo of machine learning and signal processing algorithms.  The class of pairwise grouping algorithms which share the shift-invariance property is statistically invariant under this embedding procedure, leading to identical assignments of objects to clusters.  Based on this new vectorial representation, denoising methods are applied in a second step.  Both steps provide a theoretically well controlled setup to translate from pairwise data to the respective denoised metric representation.  We demonstrate the practical usefulness of our theoretical reasoning by discovering structure in protein sequence data bases, visibly improving performance upon existing automatic methods. 
Histogram Clustering for Unsupervised Image Segmentation| Abstract This paper introduces a novel statistical mixture model for probabilistic grouping of distributional (histogram) data.  Adopting the Bayesian framework, we propose to perform annealed maximum a posteriori estimation to compute optimal clustering solutions.  In order to accelerate the optimization process, an efficient multiscale formulation is developed.  We present a prototypical application of this method for the unsupervised segmentation of textured images based on local distributions of Gabor coefficients.  Benchmark results indicate superior performance compared to K--means clustering and proximity-based algorithms. 
A Hidden Markov Model for de Novo Peptide Sequencing| Abstract De novo Sequencing of peptides is a challenging task in proteome research.  While there exist reliable DNA-sequencing methods, the highthroughput de novo sequencing of proteins by mass spectrometry is still an open problem.  Current approaches suffer from a lack in precision to detect mass peaks in the spectrograms.  In this paper we present a novel method for de novo peptide sequencing based on a hidden Markov model.  Experiments effectively demonstrate that this new method significantly outperforms standard approaches in matching quality. 
Topology Free Hidden Markov Models: Application to Background Modeling| Abstract Hidden Markov Models (HMMs) are increasingly being used in computer vision for applications such as: gesture analysis, action recognition from video, and illumination modeling.  Their use involves an off-line learning step that is used as a basis for on-line decision making (i. e.  a stationarity assumption on the model parameters).  But, realworld applications are often non-stationary in nature.  This leads to the need for a dynamic mechanism to learn and update the model topology as well as its parameters.  This paper presents a new framework for HMM topology and parameter estimation in an online, dynamic fashion.  The topology and parameter estimation is posed as a model selection problem with an MDL prior.  Online modifications to the topology are made possible by incorporating a state splitting criterion.  To demonstrate the potential of the algorithm, the background modeling problem is considered.  Theoretical validation and real experiments are presented. 
Model selection in clustering by uniform convergence bounds| Abstract Unsupervised learning algorithms are designed to extract structure from data samples.  Reliable and robust inference requires a guarantee that extracted structures are typical for the data source, i. e. , similar structures have to be infered from a second sample set of the same data source.  The overfitting phenomenon in maximum entropy based annealing algorithms is exemplarily studied for a class of histogram clustering models.  Bernstein's inequality for large deviations is used to determine the maximally achievable approximation quality parameterized by a minimal temperature.  Monte Carlo simulations support the proposed model selection criterion by finite temperature annealing. 
Real-Time Phase-Based Stereo for a Mobile Robot| Abstract The performance of a mobile robot crucially depends on the accuracy, duration and reliability of its sensor interpretation.  A major source of information are CCD-cameras which provide a detailed view of the robot's environment.  This paper presents a real-time stereo algorithm implemented on the mobile robot RHINO of the University of Bonn.  The algorithm exploit the phases of wavelet-f iltered image pairs to localize edges and to estimate their disparities with subpixel accuracy.  The disparities are computed by an initial search for corresponding points within a given interval and a subsequent measurement of phasedif ferences.  The real-time constraints of autonomous object detection and navigation are fulf illed by partially implementing the stereo algorithm on a pipeline computer Datacube.  Experimental results on real world scenes under real world conditions demonstrate the stereo algorithm's robustness and suitability for autonomous robot applications. 
Clustering with the Connectivity Kernel| Abstract Clustering aims at extracting hidden structure in dataset.  While the problem of finding compact clusters has been widely studied in the literature, extracting arbitrarily formed elongated structures is considered a much harder problem.  In this paper we present a novel clustering algorithm which tackles the problem by a two step procedure: first the data are transformed in such a way that elongated structures become compact ones.  In a second step, these new objects are clustered by optimizing a compactness-based criterion.  The advantages of the method over related approaches are threefold: (i) robustness properties of compactness-based criteria naturally transfer to the problem of extracting elongated structures, leading to a model which is highly robust against outlier objects; (ii) the transformed distances induce a Mercer kernel which allows us to formulate a polynomial approximation scheme to the generally NPhard clustering problem; (iii) the new method does not contain free kernel parameters in contrast to methods like spectral clustering or mean-shift clustering. 
Landscape of Clustering Algorithms| Abstract Numerous clustering algorithms, their taxonomies and evaluation studies are available in the literature.  Despite the diversity of different clustering algorithms, solutions delivered by these algorithms exhibit many commonalities.  An analysis of the similarity and properties of clustering objective functions is necessary from the operational/user perspective.  We revisit conventional categorization of clustering algorithms and attempt to relate them according to the partitions they produce.  We empirically study the similarity of clustering solutions obtained by many traditional as well as relatively recent clustering algorithms on a number of real-world data sets.  Sammon's mapping and a complete-link clustering of the inter-clustering dissimilarity values are performed to detect a meaningful grouping of the objective functions.  We find that only a small number of clustering algorithms are sufficient to represent a large spectrum of clustering criteria.  For example, interesting groups of clustering algorithms are centered around the graph partitioning, linkage-based and Gaussian mixture model based algorithms. 
Deterministic Annealing: Fast Physical Heuristics for Real--Time Optimization of Large Systems| ABSTRACT This paper systematically investigates the heuristical optimization technique known as deterministic annealing.  This method is applicable to a large class of assignment and partitioning problems.  Moreover, the established theoretical results, as well as the general algorithmic solution scheme, are largely independent of the objective functions under consideration.  Deterministic annealing is derived from strict minimization principles, including a rigorous convergence analysis.  We stress the close relation to homotopy methods, and discuss some of the most important strengths and weaknesses in this framework.  Optimization results for unsupervised texture segmentation are presented for an autonomous robotics application. 
Multiscale Annealing for Real-Time Unsupervised Texture Segmentation| Abstract We derive real--time global optimization methods for several clustering optimization problems commonly used in unsupervised texture segmentation.  Speed is achieved by exploiting the image neighborhood relation of features to design a multiscale optimization technique, while accuracy and global optimization properties are gained using annealing techniques.  Coarse grained cost functions are derived for central and histogram--based clustering as well as several sparse proximity--based clustering methods.  The problem of coarsening sparse random graphs is solved by the concept of structured randomization.  For optimization deterministic annealing algorithms are applied.  Annealing schedule, coarse--to--fine optimization and the estimated number of segments are tightly coupled by a statistical convergence criterion derived from computational learning theory.  The notion of optimization scale introduced by a computational temperature is thus unified with the scales defined by image and object resolution.  The algorithms are benchmarked on Brodatz-like micro--texture mixtures.  Results are presented for an autonomous robotics application.  Extensions are discussed in the context of prestructuring large image databases which is necessary for fast and reliable image retrieval. 
Combined Color And Texture Segmentation by Parametric Distributional Clustering| Abstract Unsupervised image segmentation can be formulated as a clustering problem in which pixels or small image patches are grouped together based on local feature information.  In this contribution, parametric distributional clustering (PDC) is presented as a novel approach to image segmentation based on color and texture clues.  The objective function of the PDC model is derived from the recently proposed Information Bottleneck framework (Tishby et al.  [8]), but it can equivalently be formulated in terms of a maximum likelihood solution.  Its optimization is performed by deterministic annealing.  Segmentation results are shown for natural wildlife imagery. 
Non-parametric Similarity Measures for Unsupervised Texture Segmentation and Image Retrieval| Abstract In this paper we propose and examine non--parametric statistical tests to define similarity and homogeneity measures for textures.  The statistical tests are applied to the coefficients of images filtered by a multi--scale Gabor filter bank.  We will demonstrate that these similarity measures are useful for both, texture based image retrieval and for unsupervised texture segmentation, and hence offer an unified approach to these closely related tasks.  We present results on Brodatz--like micro--textures and a collection of real--word images. 
Multidimensional Scaling by Deterministic Annealing| Abstract.  Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidian space.  The quality of a data embedding is measured by a cost function called stress which compares proximity values with Euclidian distances of the respective points.  We present a novel deterministic annealing algorithm to efficiently determine embedding coordinates for this continuous optimization problem.  Experimental results demonstrate the superiority of the optimization technique compared to conventional gradient descent methods.  Furthermore, we propose a transformation of dissimilarities to reduce the mismatch between a high-dimensional data space and a low-dimensional embedding space.  1
with Applications to Gene Expression Data| Abstract.  The concept of cluster stability is introduced to assess the validity of data partitionings found by clustering algorithms.  It allows us to explicitly quantify the quality of a clustering solution, without being dependent on external information.  The principle of maximizing the cluster stability can be interpreted as choosing the most self-consistent data partitioning.  We present an empirical estimator for the theoretically derived stability index, based on resampling.  Experiments are conducted on well known gene expression data sets, re-analyzing the work by Alon et al.  [1] and by Spellman et al.  [8]. 
c #Springer-Verlag Semi-Supervised Image Segmentation by Parametric Distributional Clustering| We extend the Parametric Distributional Clustering (PDC) algorithm to fit into this learning framework.  The resulting optimization problem is solved by constrained Deterministic Annealing.  The approach is illustrated for both artificial data and real-world synthetic aperture radar (SAR) imagery. 
Optimal Cluster Preserving Embedding of Non-Metric Proximity Data| Abstract A new embedding method is introduced for clustering of pairwise data.  Pairwise data often violates metricity and, therefore, cannot be naturally embedded in a vector space.  Pairwise clustering methods take into account non metricity to yield a grouping solution based on all pairwise object relations.  We show that all cost-based pairwise methods sharing a certain shift invariance property can be reformulated as grouping problems in a Euclidian vector space, yielding identical assignments of objects to clusters.  This optimal embedding transforms the data in such a way that many vector based data mining algorithms become applicable, in particular denoising. 
Active Learning for Hierarchical Pairwise Data Clustering| Abstract Pairwise data clustering is a well founded grouping technique based on relational data of objects which has a widespread application domain.  However, its applicability suffers from the disadvantageous fact that N objects give rise to N(N 1)=2 relations.  To cure this unfavorable scaling, techniques to sparsely sample the relations have been developed.  Yet a randomly chosen subset of the data might not grasp the structure of the complete data set.  To overcome this deficit, we use active learning methods from the field of Statistical Decision Theory.  Extending on existing approaches we present a novel algorithm for actively learning hierarchical group structures based on mean field annealing optimization. 
The Noisy Euclidean Traveling Salesman Problem and Learning| Abstract We consider noisy Euclidean traveling salesman problems in the plane, which are random combinatorial problems with underlying structure.  Gibbs sampling is used to compute average trajectories, which estimate the underlying structure common to all instances.  This procedure requires identifying the exact relationship between permutations and tours.  In a learning setting, the average trajectory is used as a model to construct solutions to new instances sampled from the same source.  Experimental results show that the average trajectory can in fact estimate the underlying structure and that overfitting effects occur if the trajectory adapts too closely to a single instance. 
On Spatial Quantization of Color Images| Abstract Image quantization and digital halftoning are fundamental image processing problems in computer vision and graphics.  Both steps are generally performed sequentially and, in most cases, independent of each other.  Color reduction with a pixel-wise defined distortion measure and the halftoning process with its local averaging neighborhood typically optimize different quality criteria or, frequently, follow a heuristic approach without reference to any quality measure.  In this paper we propose a new model to simultaneously quantize and halftone color images.  The method is based on a rigorous cost--function approach which optimizes a quality criterion derived from a simplified model of human perception.  It incorporates spatial and contextual information into the quantization and thus overcomes the artificial separation of quantization and halftoning.  Optimization is performed by an efficient multiscale procedure which substantially alleviates the computational burden.  The quality criterion and the optimization algorithms are evaluated on a representative set of artificial and real--world images showing a significant image quality improvement compared to standard color reduction approaches.  Applying the developed cost function we also suggest a new distortion measure for evaluating the quality of color reduction schemes. 
STOCHASTIC ALGORITHMS FOR EXPLORATORY DATA ANALYSIS: DATA CLUSTERING AND DATA VISUALIZATION| Abstract.  Iterative, EM-type algorithms for data clustering and data visualization are derived on the basis of the maximum entropy principle.  These algorithms allow the data analyst to detect structure in vectorial or relational data.  Conceptually, the clustering and visualization procedures are formulated as combinatorial or continuous optimization problems which are solved by stochastic optimization. 
Illumination-Invariant Face Recognition with a Contrast Sensitive Silicon Retina| Abstract Changes in lighting conditions strongly effect the performance and reliability of computer vision systems.  We report face recognition results under drastically changing lighting conditions for a computer vision system which concurrently uses a contrast sensitive silicon retina and a conventional, gain controlled CCD camera.  For both input devices the face recognition system employs an elastic matching algorithm with wavelet based features to classify unknown faces.  To assess the effect of analog on-chip preprocessing by the silicon retina the CCD images have been "digitally preprocessed" with a bandpass filter to adjust the power spectrum.  The silicon retina with its ability to adjust sensitivity increases the recognition rate up to 50 percent.  These comparative experiments demonstrate that preprocessing with an analog VLSI silicon retina generates image data enriched with object-constant features. 
Contextual Classification by Entropy-Based Polygonization| Abstract To improve the performance of pixel-wise classification results for remotely sensed imagery, several contextual classification schemes have been proposed that aim at avoiding classification noise by local averaging.  These algorithms, however, bear the serious disadvantage of smoothing the segment boundaries and producing rounded segments that hardly match the true shapes.  In this contribution, we present a novel contextual classification algorithm that overcomes these shortcomings.  Using a hierarchical approach for generating a triangular mesh, it decomposes the image into a set of polygons that, in our application, represent individual land-cover types.  Compared to classical contextual classification approaches, this method has the advantage of generating output that matches the intuitively expected type of segmentation.  Besides, it achieves excellent classification results. 
Unsupervised Texture Segmentation in a Deterministic Annealing Framework| Abstract We present a novel optimization framework for unsupervised texture segmentation that relies on statistical tests as a measure of homogeneity.  Texture segmentation is formulated as a data clustering problem based on sparse proximity data.  Dissimilarities of pairs of textured regions are computed from a multi{scale Gabor filter image representation.  We discuss and compare a class of clustering objective functions which is systematically derived from invariance principles.  As a general optimization framework we propose deterministic annealing based on a mean{#eld approximation.  The canonical way to derive clustering algorithms within this framework as well as an efficient implementation of mean{#eld annealing and the closely related Gibbs sampler are presented.  We apply both annealing variants to Brodatz{like micro{texture mixtures and real{word images. 
Pairwise Data Clustering by Deterministic Annealing| Abstract---Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing.  Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data.  We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference.  The resulting Gibbs probability distributions are estimated by mean--field approximation.  A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization.  The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics.  The algorithm for pairwise data clustering is used to segment textured images. 
GROSSER SYSTEME ECHTZEITOPTIMIERUNG SCHWERPUNKTPROGRAMM DER DEUTSCHEN FORSCHUNGSGEMEINSCHAFT| Abstract Unsupervised learning algorithms are designed to extract structure from data without reference to explicit teacher information.  The quality of the infered structure is determined by a quality function which guides the search and validation process of structure in data.  This paper proposes EMPIRICAL RISK APPROXIMATION as a new induction principle for unsupervised learning.  ffl-covers are used to coarsen the hypothesis class of possible structures.  The complexity of the unsupervised learning models are automatically controlled by large deviation bounds.  The maximum entropy principle with deterministic annealing as an efficient search strategy arises from the Empirical Risk Approximation principle as the optimal inference strategy for large learning problems.  Parameter selection of learnable data structures is demonstrated for the case of k-means clustering.  1 What is unsupervised learning? Learning algorithms are designed with the goal in mind that they should extract structure from data.  Two classes of algorithms have been widely discussed in the literature -- supervised and unsupervised learning.  The distinction between the two classes relates to supervision or teacher information which is either available to the learning algorithm or missing in the learning process.  This paper presents a theory of unsupervised learning based on Empirical Risk Approximation (ERA) which generalizes the Empirical Risk Minimization induction principle to describe the approximation of structure in data.  The ERA principle has been developed in analogy to the highly successful statistical learning theory of classification and regression [Vapnik, 1982, Vapnik, 1998].  In supervised learning of classification boundaries or of regression functions the learning algorithm is provided with example points and selects the best candidate function from a set of functions, called the hypothesis class.  Statistical learning theory, developed by Vapnik and Chervonenkis in a series of seminal papers (VC--theory, see [Vapnik, 1982, Vapnik, 1998]), measures the amount of information in a data set which can be used to determine the parameters of the classification or regression models.  Computational learning theory [Valiant, 1984] addresses computational problems of supervised learning in addition to the statistical constraints.  1 In this paper I propose a theoretical framework for unsupervised learning based on optimization of a quality functional R(ff) for structures in the data set (indexed by ff).  The learning algorithm selects the structure which scores best on the sample set X and it averages this structure of minimal empirical risk ^ R(ff; X ) with structures of statistically indistinguishable quality.  This induction principle is refered to as Empirical Risk Approximation and it is summarized by the following inference steps: 1.  Define a hypothesis class H of structures in the data.  The quality of each candidate structure is measured by a loss function h(x; ff).  2.  Find the minimum of the empirical risk ^ R ? := inf ff ^ R(ff; X ).  3.  Determine an approximation accuracy R \Lambda such that induction becomes reliable (in the sense of statistical learning theory).  4.  Average over all structures whose empirical risk does not exceed the minimum of the empirical risk ^ R ? by more than 2R app .  It is important to note that the ERA principle averages over all structures within the approximation accuracy of the minimal empirical risk rather than selecting the loss function with minimal empirical risk.  This averaging prevents the induction algorithm from extracting a structure which is not typical at the given precision level R \Lambda .  Averaging over loss functions is similar in spirit to the Bayesian inference procedure and resembles the averaging of classifiers in bagging or boosting [
ON LEARNING TEXTURE EDGE DETECTORS| ABSTRACT Texture is an inherently non--local image property.  All common texture descriptors, therefore, have a significant spatial support which renders classical edge detection schemes inadequate for the detection of texture boundaries.  In this paper we propose a novel scheme to learn filters for texture edge detection.  Textures are defined by the statistical distribution of Gabor filter responses.  Optimality criteria for detection reliability and localization accuracy are suggested in the spirit of Canny's edge detector.  Texture edges are determined as zero crossings of the difference of the two a posteriori class distributions.  An optimization algorithm is designed to determine the best filter kernel according to the underlying quality measure.  The effectiveness of the approach is demonstrated on texture mondrians composed from the Brodatz album and a series of synthetic aperture radar (SAR) imagery.  Moreover, we indicate how the proposed scheme can be combined with snake--type algorithms for prior--knowledge driven boundary refinement and interactive annotation. 
Inferring Hierarchical Clustering Structures by Deterministic Annealing| Abstract The unsupervised detection of hierarchical structures is a major topic in unsupervised learning and one of the key questions in data analysis and representation.  We propose a novel algorithm for the problem of learning decision trees for data clustering and related problems.  In contrast to many other methods based on successive tree growing and pruning, we propose an objective function for tree evaluation and we derive a non--greedy technique for tree growing.  Applying the principles of maximum entropy and minimum cross entropy, a deterministic annealing algorithm is derived in a meanfield approximation.  This technique allows us to canonically superimpose tree structures and to fit parameters to averaged or `fuzzified' trees. 
Unsupervised Texture Segmentation on the Basis of Scale Space Features| Abstract A novel approach to unsupervised texture segmentation is presented, which is formulated as a combinatorial optimization problem known as sparse pairwise data clustering.  Pairwise dissimilarities between texture blocks are measured by scale space features, i. e. , multi-resolution edges.  These scale space features are computed by a Gabor filter bank tuned to spatial frequencies.  To solve the data clustering problem a deterministic annealing technique is applied.  This approach is examined from the viewpoint of scale space theory.  Based on a meanfield approximation an efficient algorithm is derived.  We present an application of the proposed algorithm to Brodatz-like microtexture collages. 
The Mobile Robot RHINO| Abstract--- Rhinowas the University of Bonn's entry in the 1994 AAAI mobile robot competition.  Rhino is a mobile robot designed for indoor navigation and manipulation tasks.  The general scientific goal of the Rhino project is the development and the analysis of autonomous and complex learning systems.  This paper briefly describes the major components of the Rhino control software, as they were exhibited at the competition.  It also sketches the basic philosophy of the Rhino architecture, and discusses some of the lessons that we learned during the competition.  I.  GENERAL OVERVIEW Rhino, shown in Figure 1, is a B21 mobile robot platform manufactured by Real World Interface Inc.  It is equipped with 24 sonar proximity sensors, a dual color camera system mounted on a pan/tilt unit, and two on-board i486 computers.  Sonar information is obtained at a rate of 1. 3 Hertz, and camera images are processed at a rate of 0. 7 Hertz.  Rhino communicates with external computers (two SUN Sparc stations) via a tetherless Ethernet link.  The Rhino project is generally concerned with the design of autonomous and complex learning systems [8].  The AAAI competition ended an initial six month period of software design.  Key features of Rhino's control software, as exhibited at the competition, are: ffl Autonomy.  Rhino operates completely autonomously.  It has been repeatedly operated for durations of up to one hour in populated office environments without human intervention.  ffl Learning.  To increase the flexibility of the software, learning mechanisms support the adaptation of the robot to its sensors and the environment.  For example, neural network learning is employed to interpret sonar measurements.  ffl Real-time operation.  In order to act continuously in real-time, any-time solutions [2] are employed wherever possible.  Any-time algorithms are able to make decisions regardless of the time spent for computation.  The more time is available, however, the better are the results.  ffl Reactive control and deliberation.  Rhino's navigation system integrates a fast, reactive on-board obstacle avoidance routine with knowledge- and computationintense map building and planning algorithms.  Rhino's software consists of a dozen different modules.  The interface modules (a base/sonar sensor interface, a camera interface and a speech interface) control the basic communication to and from the hardware components of the robot.  On top of these, a fast obstacle avoidance routine analyzes sonar measurements to avoid collisions with obstacles and walls at a speed of up to 90cm per second.  Global metric and topological maps are constructed on-the-fly using a neural network-based approach, combined with a database of maps showing typical rooms, doors and hallways.  Rhino employs a dynamic programming planner to explore unknown terrain and to navigate to arbitrary target locations.  It locates itself by continuously analyzing sonar information.  In addition, a fast vision module segments images from two color cameras, in order to find target objects and obstacles that block the path of the robot.  Rhino's control flow is monitored by an integrated task planner and a central user interface.  The integration of a dozen different software modules, which all exhibit different timing and response characteristics, requires a flexible scheme for the flow and synchronization of information.  The key principles for the design of Rhino's software are: Figure 1: The Rhino robot of
Empirical Evaluation of Dissimilarity Measures for Color and Texture| This paper empirically compares nine families of image dissimilarity measures that are based on distributions of color and texture features summarizing over 1000 CPU hours of computational experiments.  Ground truth is collected via a novel random sampling scheme for color, and by an image partitioning method for texture.  Quantitative performance evaluations are given for classification, image retrieval, and segmentation tasks, and for a wide variety of dissimilarity measure parameters.  It is demonstrated how the selection of a measure, based on large scale evaluation, substantially improves the quality of classification, retrieval, and unsupervised segmentation of color and texture images.  c # 2001 Elsevier Science (USA)
How Many Clusters? An Information-Theoretic Perspective| Clustering provides a common means of identifying structure in complex data, and there is renewed interest in clustering as a tool for the analysis of large data sets in many fields.  A natural question is howmany clusters are appropriate for the description of a given system.  Traditional approaches to this problem are based on either a framework in which clusters of a particular shape are assumed as a model of the system or on a two-step procedure in which a clustering criterion determines the optimal assignments for a given number of clusters and a separate criterion measures the goodness of the classification to determine the number of clusters.  In a statistical mechanics approach, clustering can be seen as a trade-off between energy- and entropy-like terms, with lower temperature driving the proliferation of clusters to provide a more detailed description of the data.  For finite data sets, we expect that there is a limit to the meaningful structure that can be resolved and therefore a minimum temperature beyond which we will capture sampling noise.  This suggests that correcting the clustering criterion for the bias that arises due to sampling errors will allow us to find a clustering solution at a temperature that is optimal in the sense that we capture maximal meaningful structure---without having to define an external criterion for the goodness or stability of the clustering.  We show that in a general information-theoretic framework, the finite size of a data set determines an optimal temperature, and we introduce a method for finding the maximal number of clusters that can be resolved from the data in the hard clustering limit. 
Self-organized Clustering of Mixture Models for Combined Color And Texture Segmentation| Abstract The segmentation of images based on color and texture cues is formulated as a clustering problem in the joint color and texture space.  Small image patches are grouped together on the basis of local color and spatial frequency statistics which is captured by Gaussian mixture models in feature space.  The locality of segments in feature space is taken into account by the topological organization of the cluster structure which corresponds to self-organizing maps.  The clusters define a one dimensional chain in the space of mixture models, which favors an ordering of the groups with similar color and texture distributions.  The probabilistic model is tested by segmentation experiments on images from the Corel data base. 
Learning With Constrained and Unlabelled Data| Abstract Classification problems abundantly arise in many computer vision tasks -- being of supervised, semi-supervised or unsupervised nature.  Even when class labels are not available, a user still might favor certain grouping solutions over others.  This bias can be expressed either by providing a clustering criterion or cost function and, in addition to that, by specifying pairwise constraints on the assignment of objects to classes.  In this work, we discuss a unifying formulation for labelled and unlabelled data that can incorporate constrained data for model fitting.  Our approach models the constraint information by the maximum entropy principle.  This modeling strategy allows us (i) to handle constraint violations and soft constraints, and, at the same time, (ii) to speed up the optimization process.  Experimental results on face classification and image segmentation indicates that the proposed algorithm is computationally efficient and generates superior groupings when compared with alternative techniques. 
Distortion Invariant Object Recognition in the Dynamic Link Architecture|
Deterministic Annealing for Unsupervised Texture Segmentation|
Vector quantization with complexity costs|
Empirical Evaluation of Dissimilarity Measures for Color and Texture|
Stability-Based Validation of Clustering Solutions|
Vector quantisation with complexity costs|
Empirical Risk Approximation: An induction principle for unsupervised learning|
Central and Pairwise Data Clustering by Competitive Neural Networks|
Data Resampling for Path Based Clustering|
Support vector machines for land usage classification in Landsat TM imagery|
Competitive learning algorithms for robust vector quantization|
Active Data Clustering|
Data visualization by multidimensional scaling: a deterministic annealing approach|
Multidimensional Scaling and Data Clustering|
Path Based Pairwise Data Clustering with Application to Texture Segmentation|
Coupled Clustering: a Method for Detecting Structural Correspondence|
Region-Based Motion Compensated 3D-Wavelet Transform Coding of Video|
Going metric: Denoising pariwise data|
NIPS 10|
Bagging for Path-Based Clustering|
Dithered Color Quantization|
Unsupervised Learning for Robust Texture Segmentation|
Discrete Mixture Models for Unsupervised Image Segmentation|
Unsupervised Segmentation of Textured Images by Pairwise Data Clustering,|
Path-Based Clustering for Grouping of Smooth Curves and Texture Segmentation|
Christoph von der Malsburg,|
Complexity Optimized Vector Quantization: A Neutral Network Approach|
Semi-supervised Image Segmentation by Parametric Distributional Clustering|
Real-Time Phase-Based Stereo for a Mobile Robot",|
A Minimum Entropy Approach to Adaptive Image Polygonization|
Model Selection in Clustering by Uniform Convergence Bounds|
An Optimization Approach to Unsupervised Hierarchical Texture Segmentation|
Parametric Distributional Clustering for Image Segmentation|
Parametric distributional clustering for color image segmentation|
Special Issue on Neural Networks for Data Mining and Knowledge Discovery,|
Image recognition: Visual grouping, recognition and learning|
An Annealed ``Neural Gas'' Network for Robust Vector Quantization|
Empirical risk approximation: A statistical learning theory of data clustering|
Oscillatory associative memories,|
Feature Selection for Support Vector Machines|

Receptive Fields for Vision: from Hyperacuity to Object Recognition| Abstract Many of the lower-level areas in the mammalian visual system are organized retinotopically,
Similarity-based Word Sense Disambiguation| We describe a method for automatic word sense disambiguation using a text corpus and a machinereadable dictionary (MRD).  The method is based on word similarity and context similarity measures.  Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words.  The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD.  A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context.  Experiments show that this method performs well, and can learn even from very sparse training data. 
Unsupervised language acquisition: syntax from plain corpus| 2004 We describe results of a novel algorithm for grammar induction from a large corpus.  The ADIOS (Automatic DIstillation of Structure) algorithm searches for significant patterns, chosen according to context dependent statistical criteria, and builds a hierarchy of such patterns according to a set of rules leading to structured generalization.  The corpus is thus generalized into a context free grammar (CFG), composed of patterns, equivalence classes and words of the initial lexicon.  We have evaluated our method both on corpora generated by CFG and on natural language ones.  The performance of ADIOS is judged by searching for both good recall (acceptance of correct novel sentences) and good precision (production of correct novel sentences).  The results are very encouraging. 
Representation, Similarity, and the Chorus of Prototypes| Abstract It is proposed to conceive of representation as an emergent phenomenon that is
Minimizing Binding Errors Using Learned Conjunctive Features| We have studied some of the design trade-offs governing visual representations based on spatially invariant conjunctive feature detectors, with an emphasis on the susceptibility of such systems to false-positive recognition errors---Malsburg's classical binding problem.  We begin by deriving an analytical model that makes explicit how recognition performance is affected by the number of objects that must be distinguished, the number of features included in the representation, the complexity of individual objects, and the clutter load, that is, the amount of visual material in the field of view in which multiple objects must be simultaneously recognized, independent of pose, and without explicit segmentation.  Using the domain of text to model object recognition in cluttered scenes, we show that with corrections for the nonuniform probability and nonindependence of text features, the analytical model achieves good fits to measured recognition rates in simulations involving a wide range of clutter loads, word sizes, and feature counts.  We then introduce a greedy algorithm for feature learning, derived from the analytical model, which grows a representation by choosing those conjunctive features that are most likely to distinguish objects from the cluttered backgrounds in which they are embedded.  We show that the representations produced by this algorithm are compact, decorrelated, and heavily weighted toward features of low conjunctive order.  Our results provide a more quantitative basis for understanding when spatially invariant conjunctive features can support unambiguous perception in multiobject scenes, and lead to several insights regarding the properties of visual representations optimized for specific recognition tasks. 
Is human object recognition better described by geon-structural-descriptions or by multiple-views?| ABSTRACT Is human object recognition viewpoint dependent or viewpointinvariant under \everyday" conditions? Biederman and Gerhardstein (1993) argue that viewpoint-invariant mechanisms are used almost exclusively. However, our analysis indicates that: 1) their conditions for immediate viewpointinvariance lack the generalitytocharacterize a wide range of recognition phenomena; 2) the extensive body of viewpoint-dependent results cannot be dismissed as processing \by-products" or \experimental artifacts"; 3) geon structural descriptions cannot coherently account for category recognition, the domain they are intended to explain.  We conclude that the weight of current evidence supports an exemplar-based multiple-
Representation of similarity in 3D object discrimination| Abstract How does the brain represent visual objects? In simple perceptual generalization tasks, the human visual system performs as if it represents the stimuli in a low-dimensional metric psychological space [1].  In theories of 3D shape recognition, the role of feature-space representations (as opposed to structural [2] or pictorial [
Canonical views in object representation and recognition| Abstract Human performance in the recognition of 3D objects, as measured by response times and error rates, frequently depends on the orientation of the object with respect to the observer.  We investigated the dependence of response time (RT) and error rate (ER) on stimulus orientation for a class of random wire-like objects.  First, we found no evidence for universally valid canonical views: the best view according to one subject's data was often hardly recognized by other subjects.  Second, a subject by subject analysis showed that the RT/ER scores were not linearly dependent on the shortest angular distance in 3D to the best view, as predicted by the mental rotation theories of recognition.  Rather, the performance was significantly correlated with an image-plane feature by feature deformation distance between the presented view and the best (shortest-RT and lowest-ER) view.  Our results suggest that measurement of image-plane similarity to a few (subject-specific) feature patterns is a better model than mental rotation for the mechanism used by the human visual system to recognize objects across changes in their 3D orientation.  1 Understanding the effects of viewpoint change in object recognition nition 1. 1 General background The appearance of a three-dimensional object (that is, the pattern formed by its projection onto the retina of an eye or onto the imaging plane of a camera) depends on the point of view of the observer.  The ability of the human visual system to recognize a familiar object viewed from an unfamiliar perspective is impressive, and has been termed object constancy, by analogy with other perceptual constancies.  However, this constancy is far from perfect.  Perceiving the shape of an object irrespective of the viewing conditions such as its orientation in space and its distance from the observer frequently incurs a certain information-processing cost, over and above what it takes to recognize the same object in its most familiar appearance.  This additional processing cost is reflected in longer response times and in higher error rates evoked by randomly chosen views of the object, as compared to certain so-called canonical views (Palmer, Rosch and Chase, 1981). 
Unsupervised Context Sensitive Language Acquisition from a Large Corpus| Abstract We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus.  This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text.  The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words).  Significant patterns, determined by recursive context-sensitive statistical inference, form new vertices.  Linguistic constructions are represented by trees composed of significant patterns and their associated equivalence classes.  An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proficiency.  The results are encouraging: the model attains a level of performance considered to be "intermediate" for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 
A Similarity-Based Method for the Generalization of Face Recognition over Pose and Expression| Abstract Human observers are capable of recognizing a face seen only once before when confronted with it subsequently under different viewing conditions.  We constructed a working computational model of such generalization from a single view, and tested it on a homogeneous database of face images obtained under tightly controlled viewing conditions.  The model effectively constructs a view space for novel faces by interpolating view spaces of familiar ones [5].  Its performance -- 30% error rate in one out of 18 recognition, and 8% in one out of three discrimination -- is encouraging, given that it reflects generalization from a single view/expression to a range of \Sigma34 ffi rotation in depth and to two additional expressions.  For comparison, human subjects in the one out of three task involving only viewpoint changes exhibit a 3% error rate [11]. 
Towards structural systematicity in distributed, statically bound visual representations| Abstract The problem of representing the spatial structure of images, which arises in visual object processing,
Generation of Natural-Looking 3D Shapes by Simulated Evolution| Abstract Diverse complex simulated objects are widely used in psychophysical study of visual recognition, in computer graphics, and in animation.  This paper describes a method to generate artificially objects belonging to a given subjectively characterized class.  The method uses artificial evolution and is based on the observation that humans may classify objects by comparing them with previously encountered structural prototypes.  An efficient artificial evolution algorithm was obtained using a thermodynamical point of view of the object universe, which regarded objects as populations of generic parts characterized by an entropy-like function used as fitness.  The object's evolution is driven by the prototypes, is locally controlled by the external conditions, and tends to maximize a measure of fitness which is analogous to entropy in the second law of thermodynamics. 
Rich Syntax from a Raw Corpus: Unsupervised Does It| Abstract We compare our model of unsupervised learning of linguistic structures, ADIOS [1], to some recent work in computational linguistics and in grammar theory.  Our approach resembles the Construction Grammar in its general philosophy (e. g. , in its reliance on structural generalizations rather than on syntax projected by the lexicon, as in the current generative theories), and the Tree Adjoining Grammar in its computational characteristics (e. g. , in its apparent affinity with Mildly Context Sensitive Languages).  The representations learned by our algorithm are truly emergent from the (unannotated) corpus data, whereas those found in published works on cognitive and construction grammars and on TAGs are hand-tailored.  Thus, our results complement and extend both the computational and the more linguistically oriented research into language acquisition.  We conclude by suggesting how empirical and formal study of language can be best integrated.  1 Unsupervised learning through redundancy reduction Reduction of redundancy is a general (and arguably the only conceivable) approach to unsupervised learning [2, 3].  Written natural language (or transcribed speech) is trivially redundant to the extent it relies on a fixed lexicon.  This property of language makes possible the unsupervised recovery of words from a text corpus with all the spaces omitted, through a straightforward minimization of per-letter entropy [4].  Pushing entropy minimization to the limit would lead to an absurd situation in which the agglomeration of words into successively longer "primitive" sequences renders the resulting representation useless for dealing with novel texts (that is, incapable of generalization; cf.  [5], p. 188).  We observe, however, that a word-based representation is still redundant to the extent that different sentences share the same word sequences.  Such sequences need not be contiguous; indeed, the detection of paradigmatic variation within a slot in a set of otherwise identical aligned sequences (syntagms) is the basis for the classical distributional theory of language [6], as well as for some modern NLP methods [7].  The pattern --- the syntagm and the equivalence class of complementary-distribution symbols that may appear in its open slot --- is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure) [1]. 
3D Object Recognition Using Unsupervised Feature Extraction| Abstract Intrator (1990) proposed a feature extraction method that is related to recent statistical theory (Huber, 1985; Friedman, 1987), and is based on a biologically motivated model of neuronal plasticity (Bienenstock et al. , 1982).  This method has been recently applied to feature extraction in the context of recognizing 3D objects from single 2D views (Intrator and Gold, 1991).  Here we describe experiments designed to analyze the nature of the extracted features, and their relevance to the theory and psychophysics of object recognition. 
Computational models of perceptual learning| Abstract In visual perception, learning is a pervasive phenomenon, which, when properly studied, may offer valuable insights into the inner workings of the brain.  We outline a theoretical framework for the computational study of perceptual learning, aiming to make the relationships among the existing models more readily apparent, and to identify promising directions for future research. 
Similarity, Connectionism, and the Problem of Representation in Vision| Abstract A representational scheme under which the ranking between represented similarities is
The interaction of shape- and location-based priming in object categorisation: Evidence for a hybrid "what + where" representation stage| Abstract The relationship between part shape and location is not well elucidated in current theories of object recognition.  Here we investigated the role of shape and location of object parts on recognition,using a classification priming paradigm with novel 3D objects.  In Experiment 1,the relative displacement of two parts comprising the prime gradually reduced the priming e5ect.  In Experiment 2, presenting single-part primes in locations progressively di5erent from those in the composite target had no e5ect on priming.  In Experiment 3,manipulating the relative position of composite prime and target strongly a5ected priming.  Finally,in Experiment 4 the relative displacement of single-part primes and composite targets did influence response time.  Together,these findings are best interpreted in terms of a hybrid theory,according to which conjunctions of shape and location are explicitly represented at some stage of visual object processing. 
On Similarity to Prototypes in 3D Object Representation| Abstract A representational scheme under which the ranking between represented dissimilarities is isomorphic to the ranking between the corresponding shape dissimilarities can support perfect shape classification, because it preserves the clustering of shapes according to the natural kinds prevailing in the external world.  We discuss the computational requirements of rank-preserving representation, and examine its plausibility within a prototype-based framework of shape vision. 
Learning as extraction of low-dimensional representations| Abstract Psychophysical findings accumulated over the past several decades indicate that perceptual
Evolution of language diversity: the survival of the fitness| Abstract We examined the role of fitness, commonly assumed without proof to be conferred by the mastery of language, in shaping the dynamics of language evolution.  To that end, we introduced island migration (a concept borrowed from population genetics) into the shared lexicon model of communication (Nowak et al. , 1999).  The effect of fitness linear in language coherence was compared to a control condition of neutral drift.  We found that in the neutral condition (no coherence-dependent fitness) even a small migration rate -- less than 1% -- suffices for one language to become dominant, albeit after a long time.  In comparison, when fitness-based selection is introduced, the subpopulations stabilize quite rapidly to form several distinct languages.  Our findings support the notion that language confers increased fitness.  The possibility that a shared language evolved as a result of neutral drift appears less likely, unless migration rates over evolutionary times were extremely small. 
Some Tests of an Unsupervised Model of Language Acquisition| Abstract We outline an unsupervised language acquisition algorithm and offer some psycholinguistic support for a model based on it.  Our approach resembles the Construction Grammar in its general philosophy, and the Tree Adjoining Grammar in its computational characteristics.  The model is trained on a corpus of transcribed child-directed speech (CHILDES).  The model's ability to process novel inputs makes it capable of taking various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability.  We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli.  1 The empirical problem of language acquisition The largely unsupervised, amazingly fast and almost invariably successful learning stint that is language acquisition by children has long been the envy of computer scientists (Bod, 1998; Clark, 2001; Roberts and Atwell, 2002) and a daunting enigma for linguists (Chomsky, 1986; Elman et al. , 1996).  Computational models of language acquisition or "grammar induction" are usually divided into two categories, depending on whether they subscribe to the classical generative theory of syntax, or invoke "general-purpose" statistical learning mechanisms.  We believe that polarization between classical and statistical approaches to syntax hampers the integration of the stronger aspects of each method into a common powerful framework.  On the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation and probabilistic learning, yet generic "connectionist" architectures are ill-suited to the abstraction and processing of symbolic information.  On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difficult to train.  We are developing an approach to the acquisition of distributional information from raw input (e. g. , transcribed speech corpora) that also supports the distillation of structural regularities comparable to those captured by Context Sensitive Grammars out of the accrued statistical knowledge.  In thinking about such regularities, we adopt Langacker's notion of grammar as "simply an inventory of linguistic units" ((Langacker, 1987), p. 63).  To detect potentially useful units, we identify and process partially redundant sentences that share the same word sequences.  We note that the detection of paradigmatic variation within a slot in a set of otherwise identical aligned sequences (syntagms) is the basis for the classical distributional theory of language (Harris, 1954), as well as for some modern work (van Zaanen, 2000).  Likewise, the pattern --- the syntagm and the equivalence class of complementary-distribution symbols that may appear in its open slot --- is the main representational building block of our system, ADIOS (for Automatic DIstillation Of Structure).  Our goal in the present short paper is to illustrate some of the capabilities of the representations learned by our method vis a vis standard tests used by developmental psychologists, by secondlanguage instructors, and by linguists.  Thus, the main computational principles behind the ADIOS model are outlined here only briefly.  The algorithmic details of our approach and accounts of its learning from CHILDES corpora appear elsewhere (Solan et al. , 2003a; Solan et al. , 2003b; Solan et al. , 2004; Edelman et al. , 2004).  2 The principles behind the ADIOS algorithm The representational power of ADIOS and its capacity for unsupervised learning rest on three principles: (1) probabilistic inference of pattern significance, (2) context-sensitive generalization, and (3) recursive construction of complex patterns.  Each of these is described briefly below.  P84 that P58 P63 E63 E64 P48 E64 Beth | Cindy | George | Jim | Joe | Pam | P49 | P51 P48 , doesn't it P51 the E50 P49 a E50 E50 bird | cat | cow | dog | horse | rabbit P61 who E62 E62 adores | loves | scolds | worships E53 Beth | Cindy | George | Jim | Joe | Pam E85 annoyes | bothers | disturbes | worries P58 E60 E64 E60
Viewpoint generalization in face recognition: The role of category-specific processes| Abstract The statistical structure of a class of objects such as human faces can be exploited to recognize familiar faces from novel viewpoints and under variable illumination conditions.  We present computational and psychophysical data concerning the extent to which class-based learning transfers or generalizes within the class of faces.  We first examine the computational prerequisite for generalization across views of novel faces, namely, the similarity of different faces to each other.  We next describe two computational models which exploit the similarity structure of the class of faces.  The performance of these models constrains hypotheses about the nature of face representation in human vision, and supports the notion that human face processing operates in a class-based fashion.  Finally, we relate the computational data to well-established findings in the human memory literature concerning the relationship between the typicality and recognizability of faces.  Viewpoint generalization in face recognition: The role of category-specific processes Computational theories of visual recognition that postulate image-based, view-dependent object representations have been gaining both computational and psychophysical support in recent years.  These provide an alternative to theoretical perspectives on object representation that assume three-dimensional shape reconstructions (Ullman, 1996; Edelman, 1997).  This theoretical development has created a certain tension within the psychological literature on object and face recognition.  Specifically, although psychologists consistently find that the human visual system is capable of making sense of an image of an object, even when the object is encountered under novel viewing conditions, it is not immediately clear how a view-based representation can support this ability: ``What can possibly be left if no parts can be found, if no depth relations can be determined .  .  .  ? Quite likely, the only remaining process is the simplest of all pattern operations: viewpoint-specific recognition.  This is, undeniably, a modern code word for 2-D templates.  As would be expected for such a topdown process, it only works for familiar objects, seen from familiar viewpoints. " (Cavanagh, 1995).  We attempt to ease the tensions between theory and experiments by showing (1) that given prior experience with objects of similar shape, multiple-view models can be made to exhibit a considerable degree of generalization, both to novel views and to novel objects, and (2) that such models are relevant for understanding human generalization performance on novel stimuli, which, likewise, depends on prior exposure to objects that belong to same shape
Similarity-based viewspace interpolation and the categorization of 3D objects| Abstract Visual objects can be represented by their similarities to a small number of reference shapes or prototypes.  This method yields low-dimensional (and therefore computationally tractable) representations, which support both the recognition of familiar shapes and the categorization of novel ones.  In this note, we show how such representations can be used in a variety of tasks involving novel objects: viewpoint-invariant recognition, recovery of a canonical view, estimation of pose, and prediction of an arbitrary view.  The unifying principle in all these cases is the representation of the view space of the novel object as an interpolation of the view spaces of the reference shapes. 
Motif Extraction and Protein Classification| Abstract We present a novel unsupervised method for extracting meaningful motifs from biological sequence data.  This de novo motif extraction (MEX) algorithm is data driven, finding motifs that are not necessarily over-represented in the data.  Applying MEX to the oxidoreductases class of enzymes a relatively small set of motifs is obtained.  This set spans a motif-space that is used for functional classification of the enzymes by an SVM classifier.  The classification based on MEX motifs surpasses that of two other SVM based methods: SVMProt, a method based on the analysis of physical-chemical properties of a protein generated from its sequence of amino acids, and SVM applied to a SmithWaterman distances matrix.  Our findings demonstrate that the MEX algorithm extracts relevant motifs, supporting a successful sequence-to-function classification. 
Running head: FACE RECOGNITION OVER VIEW CHANGE Stimulus-specific effects in face recognition over changes in viewpoint| Abstract Individual faces vary considerably in both the quality and quantity of the information they contain for recognition and for viewpoint generalization.  In the present study, we assessed the typicality, recognizability, and viewpoint generalizability of individual faces using data from both human observers and from a computational model of face recognition across viewpoint change.  The two-stage computational model incorporated a viewpoint alignment operation and a recognition-by-interpolation operation.  An interesting aspect of this particular model is that the effects of typicality it predicts at the alignment and recognition stages dissociate, such that face typicality is beneficial for the success of the alignment process, but is adverse for the success of the recognition process.  We applied a factor analysis to the covariance data for the human- and model-derived face measures across the different viewpoints and found two axes that appeared consistently across all viewpoints.  Projection scores for individual faces on these axes (i. e. , the extent to which a face's "performance profile" matched the pattern of human- and model-derived scores on that axis), correlated across viewpoint changes to a much higher degree than did the raw recognizability scores of the faces.  These results suggest that the stimulus information captured in the model measures may underlie distinct and dissociable aspects of the recognizability of individual faces across viewpoint change.  Stimulus-specific effects in face recognition over changes in viewpoint To recognize a face from a novel view, we must be able to encode something unique about the face that distinguishes it from all other faces in the world and further must be able to access this unique information from the novel view.  Studying the representations and processes that humans use to accomplish this task is difficult due to the complexity of the visual information observers experience in viewing faces from different viewpoints and due to the multitude of ways that such information can be encoded and represented.  We believe that to study human representations of faces, we must first study human faces, both as individuals and as exemplars of a category of objects that share a common physical structure.  Our approach here is to combine human data on the recognizability and viewpoint generalizbality of individual faces with a computational model of the representations and processes required to perform this task.  The rationale behind this approach is that individual faces vary both in the quality and quantity of the "uniqueness" information they provide a human observer for the task of recognition.  Some faces are highly unique and distinct, while others are rather less so.  Not suprisingly, there is good evidence to indicate that faces judged by human observers to be unusual are more accurately recognized than faces judged to be typical (e. g. , Light, Kayra-Stuart, & Hollander, 1979).  Despite the well-known findings relating the typicality and recognizability of faces, less is known about the ways in which faces can be typical or unusual.  It seems reasonable, however, to suppose that when faces are considered distinctive, the information that makes them so can vary
Generalization to Novel Images in Upright and Inverted Faces| Abstract An image of a face depends not only on its shape, but also on the viewpoint, illumination conditions, and facial expression.  A face recognition system must overcome the changes in face appearance induced by these factors.  This paper investigate two related questions: the capacity of the human visual system to generalize the recognition of faces to novel images, and the level at which this generalization occurs.  We approach this problems by comparing the identification and generalization capacity for upright and inverted faces.  For upright faces, we found remarkably good generalization to novel conditions.  For inverted faces, the generalization to novel views was significantly worse for both new illumination and viewpoint, although the performance on the training images was similar to the upright condition.  Our results indicate that at least some of the processes that support generalization across viewpoint and illumination are neither universal (because subjects did not generalize as easily for inverted faces as for upright ones), nor strictly objectspecific (because in upright faces nearly perfect generalization was possible from a single view, by itself insufficient for building a complete object-specific model).  We propose that generalization in face recognition occurs at an intermediate level that is applicable to a class of objects, and that at this level upright and inverted faces initially constitute distinct object classes. 
A Productive, Systematic Framework for the Representation of Visual Structure| Abstract We describe a unified framework for the understanding of structure representation in primate vision.  A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common "middle-scale" parts, represented as image fragments.  The model addresses the same concerns as previous work on compositional representation through the use of what+where receptive fields and attentional gain modulation.  It does not require prior exposure to the individual parts, and avoids the need for abstract symbolic binding.  1 The problem of structure representation The focus of theoretical discussion in visual object processing has recently started to shift from problems of recognition and categorization to the representation of object structure.  Although view- or appearance-based solutions for these problems proved effective on a variety of object classes [1], the "holistic" nature of this approach -- the lack of explicit representation of relational structure -- limits its appeal as a general framework for visual representation [2].  The main challenges in the processing of structure are productivity and systematicity, two traits commonly attributed to human cognition.  A visual system is productive if it is openended, that is, if it can deal effectively with a potentially infinite set of objects.  A visual representation is systematic if a well-defined change in the spatial configuration of the object (e. g. , swapping top and bottom parts) causes a principled change in the representation (e. g. , the interchange of the representations of top and bottom parts [3, 2]).  A solution commonly offered to the twin problems of productivity and systematicity is compositional representation, in which symbols standing for generic parts drawn from a small repertoire are bound together by categorical symbolically coded relations [4].  2 The Chorus of Fragments In visual representation, the need for symbolic binding may be alleviated by using location in the visual field in lieu of the abstract frame that encodes object structure.  Intuitively, the constituents of the object are then bound to each other by virtue of residing in their proper places in the visual field; this can be thought of as a pegboard, whose spatial structure supports the arrangement of parts suspended from its pegs.  This scheme exhibits shallow compositionality, which can be enhanced by allowing the "pegboard" mechanism to operate at different spatial scales, yielding effective systematicity across levels of resolution.  Coarse coding the constituents (e. g. , representing each object fragment in terms of its similarities to some basis shapes) will render the scheme productive.  We call this approach to the representation of structure the Chorus of Fragments (CoF; [5]). 
Towards direct visualization of the internal shape representation space by fMRI| Abstract The reports by Tanaka and others (
Receptive field spaces and class-based generalization from a single view in face recognition| Abstract We describe a computational model of face recognition, which generalizes from single views of faces by taking advantage of prior experience with other faces, seen under a wider range of viewing conditions.  The model represents face images by vectors of activities of graded overlapping receptive fields (RFs).  It relies on high spatial frequency information to estimate the viewing conditions, which are then used to normalize (via a transformation specific for faces), and identify, the low spatial frequency representation of the input.  The class-specific transformation approach allows the model to replicate a series of psychophysical findings on face recognition, and constitutes an advance over current face recognition methods, which are incapable of generalization from a single example. 
Unsupervised efficient learning and representation of language structure| Abstract We describe a linguistic pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of corpus data.  This is achieved by compactly coding recursively structured constituent patterns, and by placing strings that have an identical backbone and similar context structure into the same equivalence class.  The resulting representations constitute an efficient encoding of linguistic knowledge and support systematic generalization to unseen sentences.  Motivation Considerations of representational parsimony dictate that the explanation for the pattern of acceptable sentences in a language be as concise as possible.  A reduced representation of linguistic knowledge need not, however, take the form of a meta-language such as a prescriptive ruleset or grammar [1].  Instead, syntax may constitute an abstraction, emerging from a corpus of language [2], yet coexisting within the same representational mechanism that embodies the data.  The process of abstraction can be guided by principles such as complementarity of distributions: tokens that function similarly in some sense (phonological, morphological, syntactic or semantic) but represent systematic rather than free variation will form complementary distributions or classes (e. g. , [3, 4]).  In thinking about emergent regularities [2], or syntactic-semantic constructions [5], we adopt
Bridging language with the rest of cognition: computational, algorithmic and neurobiological issues and methods| Abstract The computational program for theoretical neuroscience initiated by Marr and Poggio (1977) calls for a study of biological information processing on several distinct levels of abstraction.  At each of these levels --- computational (defining the problems and considering possible solutions), algorithmic (specifying the sequence of operations leading to a solution) and implementational --- significant progress has been made in the understanding of cognition.  In the past three decades, computational principles have been discovered that are common to a wide range of functions in perception (vision, hearing, olfaction) and action (motor control).  More recently, these principles have been applied to the analysis of cognitive tasks that require dealing with structured information, such as visual scene understanding and analogical reasoning.  Insofar as language relies on cognition-general principles and mechanisms, it should be possible to capitalize on the recent advances in the computational study of cognition by extending its methods to linguistics.  The possibility of integrating linguistics into a unified science of cognition --- a desideratum put forward in many of the relevant disciplines --- depends on the degree to which common computational principles (Marr and Poggio, 1977) and brain mechanisms are shared by language and by the other cognitive functions.  To explore this possibility, we need to bring together ideas from several fields, which as yet have seen little intellectual cross-fertilization.  The first of these is cognitive linguistics (Langacker, 1987; Bernardez, 1999) --- a natural home discipline for the integration project, which consistently produces valuable insights into the psychology of language, yet is little concerned with algorithmic or implementational issues.  The second is computational linguistics (Jurafsky and Martin, 2000), including statistical natural language processing (Manning and Schutze, 1999) --- a field that examines the mathematical nature of language-related tasks and generates important applications, yet pays little attention to behavioral or neurobiological issues.  Lastly, there is the Marr-Poggio computational framework (Marr and Poggio, 1977), which is used across cognition and which spans all the relevant levels of analysis, but has not yet been extended to the study of language.  This chapter discusses some of the general computational principles that emerge as useful for understanding cognition, focusing on those that are likely to be especially relevant in dealing with structured knowledge.  It then brings these principles to bear on a theory of language that is rooted both in cognitive and in computational linguistics, and that views language as an incrementally learnable system of redundant, distributed representations akin to those found by neurobiologists in olfaction, audition, vision, and motor control.  1 Common principles of cognitive representation and processing The view that cognition hinges on the representation of knowledge by the brain is widely accepted in linguistics, psychology, neuroscience, and the philosophy of mind (Chomsky, 1957; Miller, 1962; Shepard, 1975; Marr, 1982; Gallistel, 1990; Cummins, 1996).  Most importantly, representations play a central role in those theories of mind/brain that construe cognition as computation defined over representational states (Baum, 2004).  1 A representational state in a cognitive system is characterized by its covariation with certain aspects of the relevant state of affairs in the world, and, crucially, by having counterfactually supported observable effects.  2 1. 1 How to garner empirical support for posited representations Whereas thirty years ago linguists were expected to prove that the representations they posit are psychologically real (Fodor et al. , 1974) by predicting and then demonstrating such effects, contemporary formal linguistics has, lamentably, given up on this requirement (Edelman and Christiansen, 2003).  Consider, for example, the following passage from an online introduction to a course in neurolinguistics: "We know already what isn't the right question: What is the psychological reality of linguistic entities and operations?" 3 As 1 Such states need not, and probably cannot, be wholly internal to the brain; cf.  "The primary function of perception is to keep our internal framework in good registration with that vast external memory, the external environment itself" (Reitman et al. 
A model of visual recognition and categorization| Abstract To recognize a previously seen object,
by Cell Press Differential Processing of Objects under Various Viewing Conditions in the Human Lateral Occipital Complex| size (Malach et al. , 1995), as well as changes in stimulus location within the visual field (Grill-Spector et al. , 1998b; Tootell et al. , 1998c).  These response properties are in line with the presumed role of the LOC in object recognition (Malach et al. 
Anatomical origin and computational role of diversity in the response properties of cortical neurons \Lambda| Abstract The maximization of diversity of neuronal response properties has been recently suggested as an organizing principle for the formation of such prominent features of the functional architecture of the brain as the cortical columns and the associated patchy projection patterns (Malach, 1994).  We report a computational study of two aspects of this hypothesis.  First, we show that maximal diversity is attained when the ratio of dendritic and axonal arbor sizes is equal to one, as it has been found in many cortical areas and across species (Lund et al. , 1993; Malach, 1994).  Second, we show that maximization of diversity leads to better performance in two case studies: in systems of receptive fields implementing steerable/shiftable filters, and in matching spatially distributed signals, a problem that arises in visual tasks such as stereopsis, motion processing, and recognition. 
A new approach to qualitative stereo| measurements surements A growing body of work in computer vision and visual perception is aimed at exploiting the advantages of the qualitative approach to the representation and processing of visual information [1].  According to the proponents of qualitative vision, trading precision for robustness can yield stable and noise-free representations without sacrificing utility for real-world tasks.  A typical qualitative method starts with a set of measurements (which themselves are, most frequently, quantitative), and derives from them a representation in which only certain qualitative features pertinent to the task are retained.  In binocular stereopsis, for example, one may process disparity data to compute qualitative characteristics of the surface such as the sign of its Gaussian curvature [16], instead of trying to recover surface depth.  In the present work, we show that it is possible to derive a quantitative representation from an intermediate set of qualitative data which satisfy certain simple constraints.  By relying, on the one hand, on ordinal intermediate representations, and by supporting, on the other hand, the extraction of metric information, the proposed method combines the advantages of both qualitative and classical approaches.  The method is illustrated on the example of stereopsis, although it is applicable also to the processing of other visual cues, as well as to the integration of several cues within a common computational framework.  \Lambda To whom correspondence should be addressed.  a 2 B x z y f. p.  l l L R R' P Figure 1: The viewing geometry assumed in the derivation of eq.  4.  2 Depth from pairwise disparity differences 2. 1 Binocular viewing geometry Consider two images of the same object, taken from viewpoints L and R, with the two cameras or eyes aimed at a common fixation point fp (see Figure 1).  Assuming that the baseline B is small compared to the distance to the object (B !! l), we can treat the right image as if it is taken from R 0 instead of R.  We place the origin of the coordinate system in which the depth will be reconstructed at the fixation point fp, so that the X-axis is parallel to the baseline LR 0 and the Z-axis coincides with the normal to LR 0 through fp.  In this coordinate system, a point P = (P x ; P y ; P z ) T is projected onto the left image as: P l x = P x cos ff + P z sin ff \Gamma P x sin ff + P z cos ff + l (1) and to the right image as: P r x = P x cos ff \Gamma P z sin ff P x sin ff + P z cos ff + l (2) where the angle ff is as depicted in Figure 1.  Consider now two points, P and Q, and define disparity differences (\Deltal \Gamma \Deltar) P;Q : = i P l x \Gamma Q l x j \Gamma (P r x \Gamma Q r x ) = i P l x \Gamma P r x j \Gamma i Q l x \Gamma Q r x j = ` P x cos ff + P z sin ff \Gamma P x sin ff + P z cos ff + l \Gamma Q x cos ff + Q z sin ff \Gamma Q x sin ff +Q z cos ff + l ' \Gamma
Unsupervised Learning of Visual Structure| acquisition of spatially localized features that can support systematic treatment of structured objects [1].  1 A paradox and some ways of resolving it It is logically impossible to form a principled structural description of a visual scene without prior knowledge of related scenes.  Adapting an observation made by R.  A.  Fisher, such knowledge must, in the first instance, be statistical.  Several recent studies indeed showed that subjects are capable of unsupervised acquisition of statistical regularities (e. g. , conditional probabilities of constituents) that can support structural interpretation of novel scenes composed of a few simple objects [2, 3].  Theoretical understanding of unsupervised statistical learning is, however, hindered by a paradox perceived as "monstrous and unmeaning" already in the Socratic epistemology: statistics can only be computed over a set of candidate primitive descriptors if these are identified in advance, yet the identification of the candidates requires prior statistical data (cf.  [4]).  1 Figure 1 illustrates the paradox at hand in the context of scene interpretation.  To decide whether the image on the left is better seen as containing horses (and riders) rather than centaurs requires tracking the representational utility of horse over a sequence of images.  But for that one must have already acquired the notion of horse --- an undertaking that we aimed to alleviate in the first place, by running statistics over multiple stimuli.  In what follows, we describe a way of breaking out of this vicious circle, suggested by computational and neurobiological considerations.  Fig.  1.  An intuitive illustration of the fundamental problem of unsupervised discovery of the structural units best suited for describing a visual scene (cf.  Left).  Is the being in the forefront of this picture integral or composite? The visual system of the Native Americans, who in their first encounter reportedly perceived mounted Spaniards as centaur-like creatures (cf.  [5], p. 127), presumably acted on a principle that prescribes an integral interpretation, in the absence of evidence to the contrary.  A sophisticated visual system should perceive such evidence in the appearance of certain candidate units in multiple contexts (cf.  Middle, where the conquistadors are seen dismounted).  Units should not have to appear in isolation (Right) to be seen as independent.  1. 1 Computational considerations The choice of primitives or features in terms of which composite objects and their structure are to be described is the central issue at the intersection of high-level vision and computational learning theory.  Studies of unsupervised feature extraction (see e. g.  [6] for a review) typically concentrate on the need for supporting recognition, that is, telling objects apart.  Here, we are concerned with the complementary need --- seeking to capture commonalities between objects --which stems from the coupled constraints of making explicit object structure, as per the principle of systematicity [1], and maintaining representational economy, as per the Minimum Description Length (MDL) principle [7].  One biologically relevant representational framework that aims for systematicity while observing parsimony is the Chorus of Fragments (CoF [8, 1]).  In the CoF model, the graded responses of "what+where" cells [9, 10] coarsely tuned both to shape and to location form a distributed representation of stimulus structure.  In this paper, we describe a method for unsupervised acquisition of "what+where" receptive fields from examples. 
Unsupervised statistical learning in vision: computational principles, biological evidence| Abstract Unsupervised statistical learning is the standard setting for the development of the only advanced visual system that is both highly sophisticated and versatile, and extensively studied: that of monkeys and humans.  In this extended abstract, we invoke philosophical observations, computational arguments, behavioral data and neurobiological findings to explain why computer vision researchers should care about (1) unsupervised learning, (2) statistical inference, and (3) the visual brain.  We then outline a neuromorphic approach to structural primitive learning motivated by these considerations, survey a range of neurobiological findings and behavioral data consistent with it, and conclude by mentioning some of the more challenging directions for future research.  1 Why computer vision should care about unsupervised learning As the goals of computer vision grow more ambitious, the importance of learning becomes more difficult to deny: nobody wants to have to enter object and scene representations into his or her system by hand.  But why should we insist that such learning be, in the first instance, unsupervised? Because we should not trust our analytical intuitions about the ontology of visual objects.  Although the increasing availability of annotated image databases encourages the development of highly sophisticated supervised learning methods that combine linguistic and visual information (Duygulu et al. , 2002), the success of such methods is limited by the poverty of the annotations, usually lexical labels.  Indeed, a label (such as {cat, forest, grass, tiger}, shown in the work just cited) attached to a picture is both ontologically deficient in that it leaves out a host of possible complementary or alternative labels (Akins, 1996; Smith, 2001), and descriptively deficient in that it falls far short of providing a listener with a clear notion of the scene depicted (Kitcher and Varzi, 2000; Edelman, 2002). 
Explorations of shape space| Abstract Using a small number of prototypical reference objects to span the internal shape representation space has been suggested as a general approach to the problem of object representation in vision (Edelman, 1995c).  We have investigated the ability of human subjects to form the low-dimensional metric shape representation space predicted by this approach.  In each of a series of experiments, which involved pairwise similarity judgment, and delayed match to sample, subjects were confronted with several classes of computer-rendered 3D animal-like shapes, arranged in a complex pattern in a common high-dimensional parameter space.  We combined response time and error rate data into a measure of view similarity, and submitted the resulting proximity matrix to nonmetric multidimensional scaling (MDS).  In the two-dimensional MDS solution, views of the same shape were invariably clustered together, and, in each experiment, the relative geometrical arrangement of the view clusters of the different objects reflected the true low-dimensional structure in parameter space (star, triangle, square, line) that defined the relationships between the stimuli classes.  These findings are now used used to guide the development of a detailed computational theory of shape vision based on similarity to prototypes. 
On the representation of object structure in human vision: evidence from differential priming of shape and location| Abstract Theories of object representation can be classified as structural, holistic or hybrid, depending on their approach to the mereology and compositionality of shapes.  We tested the predictions of some of the current theories in three experiments, by quantifying the effects of various priming cues on response times to 3D objects.  In experiment 1, there were two possible locations for the stimulus components: left-right and top-bottom.  The prime could be identical to the stimulus, identical in location but with different parts, identical in the complement of differently located parts, or altogether different.  Both location and part identity effects were significant.  In experiment 2 we added a part-neutral (empty frame) prime condition; the effect of location, but not of part, remained significant.  In experiment 3, which included an additional location-neutral prime condition, only the location effect, again, was significant.  These findings are not entirely compatible either with the structural description theories of representation (which predict priming by "disembodied" parts or geons) or with the holistic theories (which do not predict priming by ``shapeless" location on its own).  They may be interpreted in terms of a hybrid theory, according to which conjunctions of shape and location are explicitly represented, and therefore amenable to priming.  1 The Problem of Representation The nature of the memory trace left by perceived objects in the human visual system is a fascinating problem, whose solution would lead both to a better understanding of vision in the brain, and to the development of better artificial visual systems.  From a computational standpoint, the Problem of Representation of object shapes has several distinct aspects.  For example, any computational model of object recognition must explain how to represent objects internally in such a manner that the variability of their appearance caused by changing viewing conditions (such as viewpoint or illumination) will not disrupt recognition (Ullman, 1996).  In theorizing about human vision, this consideration led to the emergence of two classes of models.  On the one hand, there are models that postulate essentially viewpoint-invariant representations, claiming that human performance in recognition does not, by and large, depend on viewpoint (Biederman, 1987; Biederman and Gerhardstein, 1993; Biederman and Gerhardstein, 1995).  On the other hand, there are models that posit viewpoint-dependent representations, motivated by the increasingly extensive psychophysical evidence in favor of viewpoint-dependent performance in a variety of cases (Bulthoff et al. , 1995; Newell and Findlay, 1997; Newell, 1998; Jolicoeur and Humphrey, 1998).  In the present psychophysical study, we examine another issue concerning representation: how is object structure --- in particular, familiar shapes in new configurations --- represented and processed by the human visual system? Our decision to consider the problem of novel objects rather than new views is motivated by two considerations.  First, due to the recent advances in the theory of recognition (Ullman, 1996), the computational problem of compensating for viewpoint-related changes seems now tractable.  Dealing with new shapes (rather than new views of familiar shapes) is, therefore, the next challenge to be taken on now (Edelman, 1997).  Second, comparing the predictions of current models of recognition with observer performance on novel objects should help us distinguish between the various theories, including those that vie for offering the best model of recognition across viewpoints.  Thus, the results of this study can be fed back into the main theoretical debate about the Problem of Representation.  2 The representation of novel objects --- candidate models The difficulties facing any attempt to settle major issues concerning cognitive representations empirically have been highlighted repeatedly in the past; see, e. g. , (Anderson, 1978; Barsalou, 1990).  In view of the caveats mentioned by these authors (e. g. , the inseparability of the effects of representation and of processing), we attempt to distinguish among concrete, algorithm- or mechanism-level models, rather than among abstract, computational-level theories.  To that end, we proceed to formulate the models that are to be compared, filling in the algorithmic details where these are not available in the original formulation of the model in the literature.  2. 1 The Standard Structural Model (SSM) The first family of models we consider is based on the notion of structural decomposition: object shapes are described in terms of relatively few generic components, joined by spatial relationships chosen from an equally small fixed set.  The representation of novel objects is made possible through the standardization of the primitives (components and their relationships): if these are sufficiently varied, a great many shapes can be described, just as tens of thousands of spoken words can be generated using a small number of phonemes as components (Biederman, 1987).  A typical structural theory, Biederman's (1987) Recognition By Components (RBC), 1 postulates a set of 30 or so primitive shapes (geons), claimed to be easily detected in images due to their nonaccidental properties.  The latter are 3D features that are almost always (that is, barring an accident of viewpoint) preserved by the imaging (projection) process (Lowe and Binford, 1985).  A representative example of such a feature is a pair of parallel lines; because a chance image alignment of two segments that are in fact not parallel in 3D is unlikely (Richards and Jepson, 1992), two parallel lines in the image are a good indicator of the presence of a 3D geon such as a cylinder "out there" in the scene.  To be able to deal with novel objects, a model based on structural descriptions must form the representation of the whole in terms of its parts dynamically ("on the fly"), for each shape it encounters.  The implementation of the RBC theory described by Hummel and Biederman (1992) is an example of a model that binds the parts to each other dynamically.  It is important to note that this implementation includes special relational units dedicated to the binding operation, over 1 This, in fact, is a variant of a structural decomposition theory usually attributed to (Marr and Nishihara, 1978) and popularized as a psychological model by Biederman. 
How representation works is more important than what representations are| Abstract A theory of representation is incomplete if it states "representations are X" where X can be symbols, cell assemblies, functional states, or the flock of birds from Theaetetus, without explaining the nature of the link between the universe of X's and the world.  Amit's thesis, equating representations with reverberations in Hebbian cell assemblies, will only be considered a solution to the problem of representation when it is complemented by a theory of how a reverberation in the brain can be a representation of anything.  It is possible (and, according to some, necessary) to distinguish two problems about mental representation (Cummins, 1989).  The first of these, the Problem of Representations, is basically empirical, and has to do with the nature of the representations employed by a given cognitive system (such as the brain of a monkey).  Amit's thesis, equating long-term memory representations in the monkey visual system with reverberations in certain cell assemblies in the inferotemporal (IT) cortex, outlines a possible answer to the Problem of Representations, mobilizing in support of the proposal an impressive battery of data from neurophysiology and from the mathematics of attractor systems.  The target article does not, however, take a stand on the second problem, called by Cummins the Problem of Representation (singular).  Here, the central question is how, in principle, can a mental state, as realized, e. g. , by a reverberation in IT, refer to anything at all in the world (that is, what makes a given brain event the representation of, say, apple).  Let us consider the comparative value of theories addressing one but not the other of the two aspects of the problem of representation mentioned above.  Note first that one can formulate a \Lambda A commentary on D.  Amit, "The Hebbian paradigm reintegrated: Local reverberations as internal representations", Behavioral and Brain Sciences 18(4), 630-631, December 1995.  useful (i. e. , predictive) theory of mental processes while treating the individual representations as black-box entities causally related to events in the world (and to each other).  In the philosophy of mind, for instance, some of the doctrines that account for behavior in terms of the agent's beliefs and desires treat internal representations as such unanalyzable entities.  The great practical value of folk psychology as an explanatory and predictive tool attests to the possibility of a methodological separation between the questions of what representations are and what they are good for.  In other words, a solution to the psychophysical problem --- getting meanings into and out of the head --can stand on its own, no matter what is, precisely, the meaning of `meaning. ' 1 Whereas solving the psychophysical problem would constitute a most significant advance on the way to understanding Representation, unraveling the mechanism (such as dynamics of cell assemblies) whereby individual representations may be realized is, at best, a small step (albeit in the right direction).  To mix a Fodoresque metaphor with a simile borrowed from Harnad's paper on symbol grounding, seeking out and cataloging representations whose causal relations with the world are unknown or ill-understood amounts to trying to learn Mentalese from a Mentalese/Mentalese dictionary alone.  Granted, a dictionary of representations is better than nothing, but without grounding at least some of the entries in reality it is difficult to make progress (Harnad, 1990).  2 It is interesting to note that experimental neurobiologists appear to take the psychophysical aspect of the problem of representation much more seriously than some theoreticians.  One example in support of this observation can be found in (Sakai et al. , 1994), where the researchers describe an ingenious method of making sure that the stimulus associated with the response of a certain unit in monkey IT cortex is indeed the optimal one for that unit.  In one of the experiments there, the stimulus shape was varied parametrically, leading invariably to a decrease in the unit's response.  Under certain conditions on the shape space, this may be interpreted as evidence in support of the original stimulus being the optimal one (note that such evidence, if corroborated, would constitute a solution to the psychophysical problem for that particular neuron).  Another example of progress in this direction is the ground-breaking technique of stimulus reduction, developed by Tanaka and his group (Fujita et al. , 1992; Tanaka, 1992).  Once identified as effective for a given neuron in IT, a stimulus shape in a typical experiment undergoes successive simplification, until it no longer elicits a response from that neuron.  The last stimulus in the reduction sequence that is still effective is the elementary feature that lies at the intersection between the set of features present in the original stimulus and the shape-space "receptive field" of the neuron.  The representation inherent in the firing of this neuron is thus grounded in the external world.  Note that by narrowing down the range of stimuli to which a particular representational event may refer, the reduction technique offers a partial relief, at a reductionist neurophysiological 1 In fact, meaning does not have to be in the head at all (Putnam 1988, p. 73), provided that whatever is in the head obeys a well-defined causal relationship with what is "out there" in the world (Locke, 1690; Edelman, 1995).  2 As an edifying example, consider Ijon Tichy's attempt to learn the meaning of the Ardrite word "scrupt," and its consequences, as recounted in the fourteenth voyage of the "Star Diaries" (Lem 1985, p. 103). 
Generalization from a single view in face recognition| Abstract We describe a computational model of face recognition, which generalizes from single views of faces, by taking advantage of prior experience with other faces, seen under a wider range of viewing conditions.  The model represents face images by vectors of activities of graded overlapping receptive fields (RFs) [10].  It relies on high spatial frequency information to estimate the viewing conditions, which are then used to normalize (via a transformation specific for faces), and identify, the low spatial frequency representation of the input.  The class-specific transformation approach allows the model to replicate a series of psychophysical findings on face recognition [7], and constitutes an advance over current face recognition methods, which are incapable of generalization from a single example. 
Face distinctiveness in recognition across viewpoint: An analysis of the statistical structure of face spaces| Abstract We present an analysis of the effects of face distinctiveness on the performance of a computational model of recognition over viewpoint change.  In the first stage of the model, the face stimulus is normalized by being mapped to an arbitrary standard view.  In the second stage, the normalized stimulus is mapped into a "face space" spanned by a number of reference faces, and is classified as familiar or unfamiliar.  We carried out experiments employing a parametrically generated family of face stimuli that vary in distinctiveness.  The experiments show that while the "view-mapping" process operates more accurately for typical versus distinctive faces, the base level distinctiveness of the faces is preserved in the face space coding.  These data provide insight into how the psychophysically well-established inverse relationship between the typicality and recognizability of faces might operate for recognition across changes in viewpoint.  1 Psychophysical background The recognition of familiar faces is something that people do very well.  This is true even with relatively dramatic changes in faces that can occur daily (e. g. , hair style), and over longer periods of time as faces age.  Somewhat distinct from the problem of recognizing faces that have changed in various ways is the problem of generalizing this recognition ability across varying viewpoints.  In this case, the 3D structure of a face/head is relatively constant.  The problem is to determine whether or not we know a face when we see it from different, even completely novel, viewpoints.  Notably, the problem of recognition requires the ability to represent the information in individual faces that makes them unique.  The problem of generalization across views entails the additional requirement that this unique information be accessible across viewpoint variations.  It is evident that individual faces vary in the quality of the uniqueness information they provide for a face recognizer --- either human or computational.  More simply stated, individual faces vary in how "distinctive" or unusal they are, and hence in how likely they are to be mistaken for other faces.  The relationship between the distinctiveness of a face (as rated by human subjects) and the accuracy with which human observers recognize the face has been well established in the psychological literature: not surprisingly, distinctive or unusual faces are more accurately recognized than are typical faces [9, 10, 15].  This finding has implications both for theoretical accounts of human memory for faces and for more applied issues concerning the factors that affect, e. g. , the accuracy of eyewitness identification.  From a theoretical perspective, many psychological and computational models of face processing have posited a representation of faces in a "face space," with a prototype/average face at the center e. g. , [2, 14, 15].  By this account, individual faces are encoded in terms of their deviation from the prototype face --- typical faces are harder to recognize than unusual faces, because the face space is more "crowded" close to the prototype, making it easier to confuse typical faces with other (un)familiar faces.  While these data are well-established, they have been collected and applied almost exclusively to the problem of face recognition from a single viewpoint (though see [12], for an exception).  These data suggest that the human performance depends on the statistical structure of the set of faces to which the observer has been exposed.  This observation serves as the main guiding principle behind the model we describe next.  This computational model builds on the basic psychological findings and extends them to consider the effects of face distinctiveness for recognition over viewpoint change.  2 Computational background The central role of the statistics of the stimuli in our model is motivated both by the psychological considerations surveyed above, and by the growing importance attributed to the statistical structure of the visual world in current theories of visual processing.  A number of researchers have attempted to derive the shapes of the receptive fields found at the early stages of the visual system from the statistics of natural images ([6]; see [13] for a review).  More recently, it has been suggested that a similar approach may be productive at the higher levels of vision, which should be tuned to the statistics of natural objects (such as faces), rather than random scenes [16].  Our model relies on the statistics of a collection of face shapes in two ways.  First, the common manner in which images of faces change with viewpoint (due to the common 3D structure of faces) is exploited at the initial stage of the model, which performs normalization of the input image to a "standard" view of the face.  The normalized image is then compared to a number of reference faces, which span our version of the face space.  At this second stage, the statistics of the collection of faces with respect to a set of reference faces constitutes the system's internal representation of the face space.  The rest of this section describes the two stages of the model in some detail.  2. 1 The view space input image view-map (RBF) subsample train output vector equivalent image Figure 1: The view-mapper.  The way in which known faces change across viewpoint is exploited in deriving a normalized representation of a novel face seen from a familiar orientation.  As noted frequently in the vision literature, the human visual system is usually able to make sense of a 2D image, even when the object to which it corresponds was never before encountered under that particular combination of viewing conditions.  A possible solution to this difficult computationl problem is via class-based processing: assuming that the stimulus belongs to a familiar class, the visual system can take advantage of its prior experience with other members of that class in processing the image of a new member.  For example, a normalizing transformation that brings familiar members of the class of faces into a normal form, can be used to estimate the appearance of a less familiar face from some standard viewpoint, facilitating subsequent recognition of that face [8].  2. 2 The shape space recognize (threshold) identify (RBF) face space reference-face modules (RBF) stimulus familiar/unfamiliar? who? view-map (RBF) Figure 2: The entire model.  Following normalization of the stimulus image by the view-mapper [7], it is projected into a view-specific face space spanned by a set of reference faces [5].  At the recognition stage, the system must deal with a stimulus that may have been normalized (e. g. , by class-based processing), but may still turn out to be unfamiliar, i. e. , may not match any of the stimuli for which internal representations are available in long-term memory.  Just as the problem of making sense of an unfamiliar viewpoint can be dealt with by exploiting the similarity of the view space of a given face to those of other members of the class of faces, we treat the problem of making sense of
Similarity-based Word Sense Disambiguation \Lambda| Abstract We describe a method for automatic word sense disambiguation using a text corpus and a machine-readable dictionary (MRD).  The method is based on word similarity and context similarity measures.  Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words.  The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD.  A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context.  Experiments show that this method performs well, and can learn even from very sparse training data. 
Clustering Irregular Shapes Using High-Order Neurons| This article introduces a method for clustering irregularly shaped data arrangements using high-order neurons.  Complex analytical shapes are modeled by replacing the classic synaptic weight of the neuron by highorder tensors in homogeneous coordinates.  In the rst- and second-order cases, this neuron corresponds to a classic neuron and to an ellipsoidalmetric neuron.  We show how high-order shapes can be formulated to follow the maximum-correlation activation principle and permit simple local Hebbian learning.  We also demonstrate decomposition of spatial arrangements of data clusters, including very close and partially overlapping clusters, which are dif cult to distinguish using classic neurons.  Superior results are obtained for the Iris data. 
Bridging computational, formal and psycholinguistic approaches to language| Abstract We compare our model of unsupervised learning of linguistic structures,
Constraints on the nature of the neural representation of the visual world| Abstract Understanding the perception of all but the most impoverished and artificial scenes presents a
Probabilistic principles in unsupervised learning of visual structure: human data and a model| Abstract To find out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow's criterion of "suspicious coincidence" (the ratio of joint probability to the product of marginals).  We then compared the part verification response times for various probe/target combinations before and after the exposure.  For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not.  This effect was modulated by the significance of their co-occurrence as estimated by Barlow's criterion.  For lone-fragment probes, the speedup in all conditions was generally lower than for composites.  These results shed light on the brain's strategies for unsupervised acquisition of structural information in vision.  1 Motivation How does the human visual system decide for which objects it should maintain distinct and persistent internal representations of the kind typically postulated by theories of object recognition? Consider, for example, the image shown in Figure 1, left.  This image can be represented as a monolithic hieroglyph, a pair of Chinese characters (which we shall refer to as A and B), a set of strokes, or, trivially, as a collection of pixels.  Note that the second option is only available to a system previously exposed to various combinations of Chinese characters.  Indeed, a principled decision whether to represent this image as fABg, fA; Bg or otherwise can only be made on the basis of prior exposure to related images.  According to Barlow's [1] insight, one useful principle is tallying suspicious coincidences: two candidate fragments A and B should be combined into a composite object AB if the probability of their joint appearance P (A; B) is much higher than P (A)P (B), which is the probability expected in the case of their statistical independence.  This criterion may be compared to the Minimum Description Length (MDL) principle, which has been previously discussed in the context of object representation [2, 3].  In a simplified form [4], MDL calls for representing AB explicitly as a whole if P (A; B) # P (A)P (B), just as the principle of suspicious coincidences does.  While the Barlow/MDL criterion r : = P (A; B)= (P (A)P (B)) certainly indicates a suspicious coincidence, there are additional probabilistic considerations that may be used in setting the degree of association between A and B.  One example is the possible perfect predictability of A from B and vice versa, as measured by minCP : = min fP (AjB); P (BjA)g.  If minCP = 1, then A and B are perfectly predictive of each other and should really be coded by a single symbol, whereas the MDL criterion may suggest merely that some association between the representation of A and that of B be established.  In comparison, if A and B are not perfectly predictive of each other (minCP < 1), there is a case to be made in favor of coding them separately to allow for a maximally expressive representation, whereas MDL may actually suggest a high degree of association (if r = P (A; B)= (P (A)P (B)) # 1).  In this study we investigated whether the human visual system uses a criterion based on minCP alongside MDL while learning (in an unsupervised manner) to represent composite objects.  AB Figure 1: Left: how many objects are contained in image AB? Without prior knowledge, a reasonable answer, which embodies a holistic bias, should be "one" (Gestalt effects, which would suggest two convex "blobs" [5], are beyond the scope of the present discussion).  Right: in this set of ten images, AB appears five times as a whole; the other five times a fragment wholly contained in AB appears in isolation.  This statistical fact provides grounds for considering AB to be composite, consisting of two fragments (call the upper one A and the lower one B), because P (AjB) = 1, but P (BjA) = 0:5 < 1.  To date, psychophysical explorations of the sensitivity of human subjects to stimulus statistics tended to concentrate on means (and sometimes variances) of the frequency of various stimuli (e. g. , [6].  One recent and notable exception is the work of Saffran et al.  [7], who showed that infants (and adults) can distinguish between "words" (stable pairs of syllables that recur in a continuous auditory stimulus stream) and non-words (syllables accidentally paired with each other, the first of which comes from one "word" and the second -- from the following one).  Thus, subjects can sense (and act upon) differences in transition probabilities between successive auditory stimuli.  This finding has been recently replicated, with infants as young as 2 months, in the visual sequence domain, using successive presentation of simple geometric shapes with controlled transition probabilities [8].  Also in the visual domain, Fiser and Aslin [9] presented subjects with geometrical shapes in various spatial configurations, and found effects of conditional probabilities of shape co-occurrences, in a task that required the subjects to decide in each trial which of two simultaneously presented shapes was more familiar.  The present study was undertaken to investigate the relevance of the various notions of statistical independence to the unsupervised learning of complex visual stimuli by human subjects.  Our experimental approach differs from that of [9] in several respects.  First, instead of explicitly judging shape familiarity, our subjects had to verify the presence of a probe shape embedded in a target.  This objective task, which produces a pattern of response times, is arguably better suited to the investigation of internal representations involved in object recognition than subjective judgment.  Second, the estimation of familiarity requires the subject to access in each trial the representations of all the objects seen in the
Representation of similarity as a goal of early visual processing| Abstract We consider the representational capabilities of systems of receptive fields found in early mammalian vision, under the assumption that the successive stages of processing remap the retinal representation space in a manner that makes objectively similar stimuli (such as different views of the same 3D object) closer to each other, and dissimilar stimuli farther apart.  We present theoretical analysis and computational experiments that compare the similarity between stimuli as they are represented at the successive levels of the processing hierarchy, from the retina to the nonlinear cortical units.  Our results indicate that the representations at the higher levels of the hierarchy are indeed more useful for the classification of natural objects such as human faces.  1 Motivation Systems of receptive fields (RFs) are probably the most prominent and ubiquitous computational mechanism employed in biological information processing, and, in particular, in vision.  A natural question suggested by the hierarchy of RF types found in the visual pathway is, what is it good for? It may seem that the answer is to be found, jointly, in the many models of visual function based on population coding of various stimulus qualities, especially as some of these models draw explicit parallels between the representations they employ and the RFs found in biological vision.  However, mere invocation of the idea of population coding, if not accompanied by a computational (in the sense of Marr, 1982) statement of what it is that the visual system does with its representations, simply begs the question: At all levels of the visual system, complex objects appear to be coded by the activity of populations, or networks, of cells, and the representation of a particular object may be widely distributed throughout one or more visual areas.  That said, the goal of the anatomical pathway for object recognition becomes less obvious.  The photoreceptors are a population of cells, for example, and they are necessarily capable of coding, by their population response, any conceivable stimulus.  Why are subsequent populations needed? (Desimone and Ungerleider, 1989, p. 268).  A number of recent works that do address the computational problem of representation tend to employ information-theoretic terms such as redundancy reduction and efficient coding (Field, 1994; Daugman, 1988; Atick, 1992).  In this paper we suggest an alternative approach, based on the observation that object classification (which may be considered an ultimate goal of vision) requires faithful representation of true similarity between shapes (Edelman, 1993).  This observation leads to the hypothesis that successive stages of early visual processing remap the retinal space in a manner that makes objectively similar shapes closer to each other.  The paper is organized as follows.  In section 2, we consider a general formulation of the issue of similarity under different representations.  Section 3 contains an experimental evaluation of several similarity measures on a database of face images, and, in particular, a comparison of the similarity induced by realistic RFs with that of two control cases.  Section 4 contains a discussion of the results and lists directions for future research.  2 The effect of the choice of representation on similarity between images Recent theories of object recognition based on view interpolation (Poggio and Edelman, 1990) or on linear combination of views (Ullman and Basri, 1991) have underscored the possibility of representing 3D shapes by collections of their views, or images.  However, in models of biological vision, the notion of an image is ill-defined at any stage past the projection of the world onto the photoreceptor sheet in retina.  At all the subsequent levels, the visual system has at its disposal only the activities evoked by this input image in the units of the preceding level, and it can compare two images only by comparing these population activity vectors.  Thus, a representation scheme together with a metric in the representation space naturally induce a measure of similarity among (input) images.  For a recognition scheme such as view interpolation to succeed, this induced or proximal similarity between image representations must correspond in a principled manner to the objective or distal similarity between objects that give rise to the images.  Consequently, we propose to compare various representation schemes according to the similarity measures they induce. 
Learning to Grasp Using Visual Information| Abstract A scheme for learning to grasp objects using visual information is presented.  A system is considered that coordinates a parallel-jaw gripper (hand) and a camera (eye).  Given an object, and considering its geometry, the system chooses grasping points, and performs the grasp.  The system learns while performing grasping trials.  For each grasp we store location parameters that code the locations of the grasping points, quality parameters that are relevant features for the assessment of grasp quality, and the grade.  We learn two separate subproblems: (1) to choose grasping points, and (2) to predict the quality of a given grasp.  The location parameters are used to locate grasping points on new target objects.  We consider a function from the quality parameters to the grade, learn the function from examples, and later use it to estimate grasp quality.  In this way grasp quality for novel situations can be generalized and estimated.  An experimental setup using an AdeptOne manipulator to test this scheme was developed.  Given an object, the system takes one image of it with a stationary topview camera, uses the image to choose two grasping points on the boundary, performs a grasping trial with a parallel-jaw gripper, and assigns a grade to the trial using an additional side-mounted camera.  The system has demonstrated an ability to grasp a relatively wide variety of objects, and its performance improves with experience appreciably after a small number of trials. 
A Sequence of Object-Processing Stages Revealed by fMRI in the Human Occipital Lobe| Abstract: Functional magnetic resonance imaging was used in combined functional selectivity and retinotopic mapping tests to reveal object-related visual areas in the human occpital lobe.  Subjects were tested with right, left, up, or down hemivisual field stimuli which were composed of images of natural objects (faces, animals, man-made objects) or highly scrambled (1,024 elements) versions of the same images.  In a similar fashion, the horizontal and vertical meridians were mapped to define the borders of these areas.  Concurrently, the same cortical sites were tested for their sensitivity to image-scrambling by varying the number of scrambled picture fragments (from 16--1,024) while controlling for the Fourier power spectrum of the pictures and their order of presentation.  Our results reveal a stagewise decrease in retinotopy and an increase in sensitivity to image-scrambling.  Three main distinct foci were found in the human visual object recognition pathway (Ungerleider and Haxby [1994]: Curr Opin Neurobiol 4:157--165): 1) Retinotopic primary areas V1--3 did not exhibit significant reduction in activation to scrambled images.  2) Areas V4v (Sereno et al. , [1995]: Science 268:889--893) and V3A (DeYoe et al. , [1996]: Proc Natl Acad Sci USA 93:2382--2386; Tootell et al. , [1997]: J Neurosci 71:7060--7078) manifested both retinotopy and decreased activation to highly scrambled images.  3) The essentially nonretinotopic lateral occipital complex (LO) (Malach et al. , [1995]: Proc Natl Acad Sci USA 92:8135--8139; Tootell et al. , [1996]: Trends Neurosci 19:481--489) exhibited the highest sensitivity to image scrambling, and appears to be homologous to macaque the infero-temporal (IT) cortex (Tanaka [1996]: Curr Opin Neurobiol 523--529).  Breaking the images into 64, 256, or 1,024 randomly scrambled blocks reduced activation in LO voxels.  However, many LO voxels remained significantly activated by mildly scrambled images (16 blocks).  These results suggest the existence of object-fragment representation in LO. 
A Self-Organizing Multiple-View Representations of 3D Objects|
Imperfect invariance to object translation in the discrimination of complex shapes|
Learning visually guided grasping: a test case in sensorimotor learning|
Bringing the Grandmother back into the Picture: A Memory-Based View of Object Recognition|
How are threedimensional objects represented in the brain?|
Learning similarity-based word sense disambiguation from sparse data|
On Learning to Recognize 3-D Objects from Examples|
Viewpoint-specific representations in three dimensional object recognition|
A hierarchical model for 3D object recognition based on 2D visual representation|
Quadtrees in Concurrent PROLOG|
Low-angle neutron scattering analysis of Na/K-ATPase in detergent solution|

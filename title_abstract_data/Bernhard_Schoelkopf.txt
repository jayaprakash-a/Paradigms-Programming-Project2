A Generalized Representer Theorem| Abstract Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples.  We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a support vector kernel.  The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space. 
View--based cognitive map learning by an autonomous robot| Proceedings ICANN '95 / Neuro--Nmes '95, Vol. 
Efficient Face Detection by a Cascaded Support Vector Machine Expansion| We describe a fast system for the detection and localization of human faces in images using a nonlinear Support Vector Machine.  We approximate the decision surface in terms of a reduced set of expansion vectors and propose a cascaded evaluation which has the property that the full support vectors expansion is only evaluated on the face-like parts of the image, while the largest part of typical images is classified using a single expansion vector (simpler and more efficient classifier).  The cascaded evaluation offers a thirty-fold speed-up over an evaluation using the full set of reduced set vectors which itself already is thirty times faster than classification using all the support vectors. 
Statistical Learning and Kernel Methods| Abstract We briefly describe the main ideas of statistical learning theory, support vector machines, and kernel feature spaces. 
Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra| Abstract A system has been developed to extract diagnostic information from jet engine carcass vibration data.  Support Vector Machines applied to novelty detection provide a measure of how unusual the shape of a vibration signature is, by learning a representation of normality.  We describe a novel method for Support Vector Machines of including information from a second class for novelty detection and give results from the application to Jet Engine vibration analysis. 
Classification of Faces in Man and Machine| Abstract We attempt to shed light on the algorithms used by humans to classify images of human faces according to their gender.  For this, a novel methodology combining human psychophysics and machine learning is introduced.  We proceed as follows.  In a first stage, we apply Principal Component Analysis (PCA) on the pixel information of the face stimuli.  We then obtain a dataset composed of these PCA-eigenvectors combined with the subjects' gender estimates of the corresponding stimuli.  In a second stage we model the gender classification process on this dataset using a separating hyperplane (SH)
Generalization Bounds for Convex Combinations of Kernel Functions| Abstract We derive new bounds on covering numbers for hypothesis classes generated by convex combinations of basis functions.  These are useful in bounding the generalization performance of algorithms such as RBF-networks, boosting and a new class of linear programming machines similar to SV machines.  We show that p-convex combinations with p ? 1 lead to diverging bounds, whereas for p = 1 good bounds in terms of entropy numbers can be obtained.  In the case of kernel expansions, significantly better bounds can be obtained depending on the eigenvalues of the corresponding integral operators. 
Prior Knowledge in Support Vector Kernels| Abstract We explore methods for
Generalization Bounds and Learning Rates for Regularized Principal Manifolds| Abstract We derive uniform convergence bounds and learning rates for regularized principal manifolds.  This builds on previous work of Kegl et al. , however we are able to obtain stronger bounds taking advantage of the decomposition of the principal manifold in terms of kernel functions.  In particular, we are able to give bounds on the covering numbers which are independent of the number of basis functions (line elements) used.  Finally we are able to obtain a nearly optimal learning rate of order O(m\Gamma 1 2 +ff ) for certain types of regularization operators, where m is the sample size and ff an arbitrary positive constant.  A companion paper [4] describes the basic algorithm, details of the implementation and experimental results. 
Max-Planck-Institut fr biologische Kybernetik| Abstract We describe a new method for performing a nonlinear form of Principal Component Analysis.  By the use of integral operator kernel functions, we can efficiently compute principal components in high--dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible 5--pixel products in 16\Theta16 images.  We give the derivation of the method, along with a discussion of other techniques which can be made nonlinear with the kernel approach; and present first experimental results on nonlinear feature extraction for pattern recognition.  AS and KRM are with GMD First (Forschungszentrum Informationstechnik)
Sampling Techniques for Kernel Methods| Abstract We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself.  In all three cases, we give sharp bounds on the accuracy of the obtained approximations.  Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function k by a "randomized kernel" which behaves like k in expectation. 
The View-Graph Approach to Visual Navigation and Spatial Memory| Abstract.  This paper describes a purely visual navigation scheme based on two elementary mechanisms (piloting and guidance) and a graph structure combining individual navigation steps controlled by these mechanisms.  In robot experiments in real environments, both mechanisms have been tested, piloting in an open environment and guidance in a maze with restricted movement opportunities.  The results indicate that navigation and path planning can be brought about with these simple mechanisms.  We argue that the graph of local views (snapshots) is a general and biologically plausible means of representing space and integrating the various mechanisms of map behaviour. 
Kernel PCA and De-Noising in Feature Spaces| Abstract Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms.  But it can also be considered as a natural generalization of linear principal component analysis.  This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA.  This is a nontrivial task, as the results provided by kernel PCA live in some high dimensional feature space and need not have pre-images in input space.  This work presents ideas for finding approximate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data. 
Entropy Numbers of Linear Function Classes| Abstract This paper collects together a miscellany of results originally motivated by the analysis of the generalization performance of the "maximum-margin" algorithm due to Vapnik and others.  The key feature of the paper is its operator-theoretic viewpoint.  New bounds on covering numbers for classes related to Maximum Margin classes are derived directly without making use of a combinatorial dimension such as the VC-dimension.  Specific contents of the paper include: # a new and self-contained proof of Maurey's theorem and some generalizations with small explicit values of constants; # bounds on the covering numbers of maximum margin classes suitable for the analysis of their generalization performance; # the extension of such classes to those induced by balls in quasi-Banach spaces (such as ` p norms with 0 < p < 1).  # extension of results on the covering numbers of convex hulls of basis functions to p-convex hulls (0 < p # 1); # an appendix containing the tightest known bounds on the entropy numbers of the identity operator between ` n p1 and ` n p2 (0 < p 1 < p 2 # 1). 
Prediction on Spike Data Using Kernel Algorithms| Abstract We report and compare the performance of different learning algorithms based on data from cortical recordings.  The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons.  We compare several ways of improving the coding of the input (i. e. , the spike data) as well as of the output (i. e. , the orientation), and report the results obtained using different kernel algorithms. 
Training Invariant Support Vector Machines| Abstract.  Practical experience has shown that in order to obtain the best possible
Entropy Numbers, Operators and Support Vector Kernels| Abstract We derive new bounds for the generalization error of feature space machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers.  The proofs are based on a viewpoint that is apparently novel in the field of statistical learning theory.  The hypothesis class is described in terms of a linear operator mapping from a possibly infinite dimensional unit ball in feature space into a finite dimensional space.  The covering numbers of the class are then determined via the entropy numbers of the operator.  These numbers, which characterize the degree of compactness of the operator, can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine.  As a consequence we are able to theoretically explain the effect of the choice of kernel functions on the generalization performance of support vector machines. 
Improving the Accuracy and Speed of Support Vector Machines| Abstract Support Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inversion for ill-posed problems.  Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing interest.  In this paper we combine two such techniques on a pattern recognition problem.  The method for improving generalization performance (the "virtual support vector" method) does so by incorporating known invariances of the problem.  This method achieves a drop in the error rate on 10,000 NIST test digit images of 1. 4% to 1. 0%.  The method for improving the speed (the "reduced set" method) does so by approximating the support vector decision surface.  We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine.  The combined approach yields a machine which is both 22 times faster than the original machine, and which has better generalization performance, achieving 1. 1% error.  The virtual support vector method is applicable to any SVM problem with known invariances.  The reduced set method is applicable to any support vector machine. 
Machine Learning Applied to Perception: Decision-Images for Gender Classification| Abstract We study gender discrimination of human faces using a combination of psychophysical classification and discrimination experiments together with methods from machine learning.  We reduce the dimensionality of a set of face images using principal component analysis, and then train a set of linear classifiers on this reduced representation (linear support vector machines (SVMs), relevance vector machines (RVMs), Fisher linear discriminant (FLD), and prototype (prot) classifiers) using human classification data.  Because we combine a linear preprocessor with linear classifiers, the entire system acts as a linear classifier, allowing us to visualise the decision-image corresponding to the normal vector of the separating hyperplanes (SH) of each classifier.  We predict that the female-tomaleness transition along the normal vector for classifiers closely mimicking human classification (SVM and RVM [1]) should be faster than the transition along any other direction.  A psychophysical discrimination experiment using the decision images as stimuli is consistent with this prediction. 
An Improved Training Algorithm for Kernel Fisher Discriminants| Abstract We present a fast training algorithm for the kernel Fisher discriminant classifier.  It uses a greedy approximation technique and has an empirical scaling behavior which improves upon the state of the art by more than an order of magnitude, thus rendering the kernel Fisher algorithm a viable option also for large datasets. 
Classification on Proximity Data with LP--Machines| Abstract We provide a new linear program to deal with classification of data in the case of data given in terms of pairwise proximities.  This allows to avoid the problems inherent in using feature spaces with indefinite metric in Support Vector Machines, since the notion of a margin is purely needed in input space where the classification actually occurs.  Moreover in our approach we can enforce sparsity in the proximity representation by sacrificing training error.  This turns out to be favorable for proximity data.  Similar to --SV methods, the only parameter needed in the algorithm is the (asymptotical) number of data points being classified with a margin.  Finally, the algorithm is successfully compared with --SV learning in proximity space and K--nearest-neighbors on real world data from Neuroscience and molecular biology. 
Semiparametric Support Vector and Linear Programming Machines| Abstract Semiparametric models are useful tools in the case where domain knowledge exists about the function to be estimated or emphasis is put onto understandability of the model.  We extend two learning algorithms - Support Vector machines and Linear Programming machines to this case and give experimental results for SV machines. 
Estimating a Kernel Fisher Discriminant in the Presence of Label Noise| Abstract Data noise is present in many machine learning problems domains, some of these are well studied but others have received less attention.  In this paper we propose an algorithm for constructing a kernel Fisher discriminant (KFD) from training examples with noisy labels.  The approach allows to associate with each example a probability of the label being flipped.  We utilise an expectation maximization (EM) algorithm for updating the probabilities.  The E-step uses class conditional probabilities estimated as a by-product of the KFD algorithm.  The M-step updates the flip probabilities and determines the parameters of the discriminant.  We demonstrate the feasibility of the approach on two real-world data-sets. 
SVMs---a practical consequence of learning theory| Is there anything worthwhile to learn about the new SVM algorithm, or does it fall into the category of "yet-another-algorithm," in which case readers should stop here and save their time for something more useful? In this short overview, I will try to argue that studying support-vector learning is very useful in two respects.  First, it is quite satisfying from a theoretical point of view: SV learning is based on some beautifully simple ideas and provides a clear intuition of what learning from examples is about.  Second, it can lead to high performances in practical applications.  In the following sense can the SV algorithm be considered as lying at the intersection of learning theory and practice: for certain simple types of algorithms, statistical learning theory can identify rather precisely the factors that need to be taken into account to learn successfully.  Real-world applications, however, often mandate the use of more complex models and algorithms---such as neural networks---that are much harder to analyze theoretically.  The SV algorithm achieves both.  It constructs models that are complex enough: it contains a large class of neural nets, radial basis function (RBF) nets, and polynomial classifiers as special cases.  Yet it is simple enough to be analyzed mathematically, because it can be shown to correspond to a linear method in a high-dimensional feature space nonlinearly related to input space.  Moreover, even though we can think of it as a linear algorithm in a high-dimensional space, in practice, it does not involve any computations in that high-dimensional space.  By the use of kernels, all necessary computations are performed directly in input space.  This is the characteristic twist of SV methods---we are dealing with complex algorithms for nonlinear pattern recognition, 1 regression, 2 or feature extraction, 3 but for the sake of analysis and algorithmics, we can pretend that we are working with a simple linear algorithm.  I will explain the gist of SV methods by describing their roots in learning theory, the optimal hyperplane algorithm, the kernel trick, and SV function estimation.  For details and further references, see Vladimir Vapnik's authoritative treatment, 2 the collection my colleagues and I have put together, 4 and the SV Web page at
Ranking on Data Manifolds| Abstract The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks.  Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data.  The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data.  Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method. 
Methods Towards Invasive Human Brain Computer Interfaces| Abstract During the last ten years there has been growing interest in the development of Brain Computer Interfaces (BCIs).  The field has mainly been driven by the needs of completely paralyzed patients to communicate.  With a few exceptions, most human BCIs are based on extracranial electroencephalography (EEG).  However, reported bit rates are still low.  One reason for this is the low signal-to-noise ratio of the EEG [14].  We are currently investigating if BCIs based on electrocorticography (ECoG) are a viable alternative.  In this paper we present the method and examples of intracranial EEG recordings of three epilepsy patients with electrode grids placed on the motor cortex.  The patients were asked to repeatedly imagine movements of two kinds, e. g. , tongue or finger movements.  We analyze the classifiability of the data using Support Vector Machines (SVMs) [16, 19] and Recursive Channel Elimination (RCE) [10]. 
Making Large-Scale SVM Learning Practical| VIII (KI) Research Reports of the unit no.  VIII (AI)
Quantization Functionals and Regularized Principal Manifolds| Abstract Many settings of unsupervised learning can be viewed as quantization problems, namely of minimizing the expected quantization error subject to some restrictions.  This has the advantage that tools known from the theory of (supervised) risk minimization like regularization can be readily applied to unsupervised settings.  Moreover, one may show that this setting is very closely related to both, principal curves with a length constraint and the generative topographic map.  Experimental results demonstrate the feasibility of the proposed method.  In a companion paper we show that uniform convergence bounds can be given for algorithms such as a modified variant of the principal curves problem. 
Entropy Numbers of Compact Operators| 4 Received 07-JUL-98, Revised 21-OCT-98 Abstract We derive new bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers.  The proofs make use of a viewpoint that is apparently novel in the field of statistical learning theory.  The hypothesis class is described in terms of a linear operator mapping from a possibly infinite dimensional unit ball in feature space into a finite dimensional space.  The covering numbers of the class are then determined via the entropy numbers of the operator.  These numbers, which characterize the degree of compactness of the operator, can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine.  As a consequence we are able to theoretically explain the effect of the choice of kernel function on the generalization performance of support vector machines. 
Shrinking the Tube: A New Support Vector Regression Algorithm| Abstract A new algorithm for Support Vector regression is described.  For a priori chosen , it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction of the data points lie outside.  Moreover, it is shown how to use parametric tube shapes with non-constant radius.  The algorithm is analysed theoretically and experimentally. 
Sample Based Generalization Bounds| Abstract It is known that the covering numbers of a function class on a double sample (length 2m, where m is the number of points in the sample) can be used to bound the generalization performance of a classifier by using a margin based analysis.  Traditionally this has been done using a ``Sauer-like" relationship involving a combinatorial dimension such as the fat-shattering dimension.  In this paper we show that one can utilize an analogous argument in terms of the observed covering numbers on a single m-sample (being the actual observed data points).  The significance of this is that for certain interesting classes of functions, such as support vector machines, one can readily estimate the empirical covering numbers quite well.  We show how to do so in terms of the eigenvalues of the Gram matrix created from the data.  These covering numbers can be much less than a priori bounds indicate in situations where the particular data received is ``easy".  The work can be considered an extension of previous results which provided generalization performance bounds in terms of the VC-dimension of the class of hypotheses restricted to the sample, with the considerable advantage that the covering numbers can be readily computed, and they often are small. 
Dealing with Large Diagonals in Kernel Matrices| Abstract In kernel methods, all the information about the training data is contained in the Gram matrix.  If this matrix has large diagonal values, which arises for many types of kernels, then kernel methods do not perform well.  We propose and test several methods for dealing with this problem by reducing the dynamic range of the matrix while preserving the positive definiteness of the Hessian of the quadratic programming problem that one has to solve when training a Support Vector Machine, which is a common kernel approach for pattern recognition. 
SUPPORT VECTOR REGRESSION FOR BLACK-BOX SYSTEM IDENTIFICATION| ABSTRACT In this paper, we demonstrate the use of support vector regression (SVR) techniques for black-box system identification.  These methods derive from statistical learning theory, and are of great theoretical and practical interest.  We briefly describe the theory underpinning SVR, and compare support vector methods with other approaches using radial basis networks.  Finally, we apply SVR to modeling the behaviour of a hydraulic robot arm, and show that SVR improves on previously published results. 
Implicit Wiener Series for Higher-Order Image Analysis| Abstract The computation of classical higher-order statistics such as higher-order moments or spectra is difficult for images due to the huge number of terms to be estimated and interpreted.  We propose an alternative approach in which multiplicative pixel interactions are described by a series of Wiener functionals.  Since the functionals are estimated implicitly via polynomial kernels, the combinatorial explosion associated with the classical higher-order statistics is avoided.  First results show that image structures such as lines or corners can be predicted correctly, and that pixel interactions up to the order of five play an important role in natural images.  Most of the interesting structure in a natural image is characterized by its higher-order statistics.  Arbitrarily oriented lines and edges, for instance, cannot be described by the usual pairwise statistics such as the power spectrum or the autocorrelation function: From knowing the intensity of one point on a line alone, we cannot predict its neighbouring intensities.  This would require knowledge of a second point on the line, i. e. , we have to consider some third-order statistics which describe the interactions between triplets of points.  Analogously, the prediction of a corner neighbourhood needs at least fourth-order statistics, and so on.  In terms of Fourier analysis, higher-order image structures such as edges or corners are described by phase alignments, i. e.  phase correlations between several Fourier components of the image.  Classically, harmonic phase interactions are measured by higher-order spectra [4].  Unfortunately, the estimation of these spectra for high-dimensional signals such as images involves the estimation and interpretation of a huge number of terms.  For instance, a sixth-order spectrum of a 16#16 sized image contains roughly 10 12 coefficients, about 10 10 of which would have to be estimated independently if all symmetries in the spectrum are considered.  First attempts at estimating the higher-order structure of natural images were therefore restricted to global measures such as skewness or kurtosis [8], or to submanifolds of fourth-order spectra [9].  Here, we propose an alternative approach that models the interactions of image points in a series of Wiener functionals.  A Wiener functional of order n captures those image components that can be predicted from the multiplicative interaction of n image points.  In contrast to higher-order spectra or moments, the estimation of a Wiener model does not require the estimation of an excessive number of terms since it can be computed implicitly via polynomial kernels.  This allows us to decompose an image into components that are characterized by interactions of a given order.  In the next section, we introduce the Wiener expansion and discuss its capability of modeling higher-order pixel interactions.  The implicit estimation method is described in Sect.  2, followed by some examples of use in Sect.  3.  We conclude in Sect.  4 by briefly discussing the results and possible improvements. 
Incorporating Invariances in Non-Linear Support Vector Machines| Abstract The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance, it should therefore incorporate prior knowledge such as known transformation invariances.  We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels.  We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 
Natural Regularization in SVMs| Abstract We provide a regularization-theoretic analysis of a class of SV kernels {called natural kernels{ based on generative models with density p(xj#), such as the Fisher kernel proposed in [5].  In particular, we show that the latter corresponds to a regularization operator (prior) penalizing the L 2 (p)-norm of the estimated function.  Moreover, we compute the corresponding eigendecompositions and show that they isotropically map the input space into feature space. 
An Introduction to Kernel-Based Learning Algorithms| Abstract---This paper provides an introduction to support vector machines (SVMs), kernel Fisher discriminant analysis, and kernel principal component analysis (PCA), as examples for successful kernel-based learning methods.  We first give a short background about Vapnik--Chervonenkis (VC) theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations.  We illustrate the usefulness of kernel algorithms by finally discussing applications such as optical character recognition (OCR) and DNA analysis. 
Regularized Principal Manifolds| Abstract Many settings of unsupervised learning can be viewed as quantization problems - the minimization of the expected quantization error subject to some restrictions.  This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning.  This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding.  We explore this connection in two ways: (1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm.  In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators.  Experimental results demonstrate the feasibility of the approach. 
Semi-supervised Learning on Directed Graphs| Abstract Given a directed graph in which some of the nodes are labeled, we investigate the question of how to exploit the link structure of the graph to infer the labels of the remaining unlabeled nodes.  To that extent we propose a regularization framework for functions defined over nodes of a directed graph that forces the classification function to change slowly on densely linked subgraphs.  A powerful, yet computationally simple classification algorithm is derived within the proposed framework.  The experimental evaluation on real-world Web classification problems demonstrates encouraging results that validate our approach. 
Kernel Methods for Extracting Local Image Semantics| This paper describes an investigation into using kernel methods for extracting semantic information from images.  The specific problem addressed is the local extraction of `man-made' vs `natural' information.  Kernel linear discriminant and support vector methods are compared to the standard linear discriminant using a multi-level hierarchy.  The two kernel methods are found to perform similarly and significantly better than the linear method.  An advantage of the kernel linear discriminant over the SVM method is that accurate class-conditional density estimates can be determined at each level allowing posterior estimates of class membership to be evaluated.  These probabilistic outputs give a principled framework for combining results from a number of semantic labels. 
Learning View Graphs for Robot Navigation| Abstract.  We present a purely vision-based scheme for learning a topological representation of an open environment.  The system represents selected places by local views of the surrounding scene, and finds traversable paths between them.  The set of recorded views and their connections are combined into a graph model of the environment.  To navigate between views connected in the graph, we employ a homing strategy inspired by findings of insect ethology.  In robot experiments, we demonstrate that complex visual exploration and navigation tasks can thus be performed without using metric information. 
Extracting Support Data for a Given Task| Abstract We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (4%) subsets of the data base.  This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task.  In addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines.  This finding is important for cases where the amount of available data is limited. 
SVM and Boosting: One Class| Abstract We show via an equivalence of mathematical programs that a Support Vector (SV) algorithm can be translated into an equivalent boosting-like algorithm and vice versa.  We exemplify this translation procedure for a new algorithm --one-class Leveraging --- starting from the one-class Support Vector Machines (1SVM).  This is a first step towards unsupervised learning in a Boosting framework.  Building on so-called barrier methods known from the theory of constrained optimization, it returns a function, written as a convex combination of basis hypotheses, that characterizes whether a given test point is likely to have been generated from the distribution underlying the training data.  Simulations on one-class classification problems demonstrate the usefulness of our approach. 
Learning to Find Pre-Images| Abstract We consider the problem of reconstructing patterns from a feature map.  Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS.  We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images.  The introduced technique avoids difficult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 
Kernel Methods for Implicit Surface Modeling| Abstract We describe methods for computing an implicit model of a hypersurface that is given only by a finite sampling.  The methods work by mapping the sample points into a reproducing kernel Hilbert space and then determining regions in terms of hyperplanes. 
A Tutorial on -Support Vector Machines| Abstract.  We briefly describe the main ideas of statistical learning theory, support vector machines (SVMs), and kernel feature spaces.  We place particular emphasis on a description of the so-called -SVM, including details of the algorithm and its implementation, theoretical results, and practical applications.  1 An Introductory Example Suppose we are given empirical data (x 1 , y 1 ), .  .  .  , (x m , y m ) 2 X {1}.  (1) Here, the domain X is some nonempty set that the patterns x i are taken from; the y i are called labels or targets.  Unless stated otherwise, indices i and j will always be understood to run over the training set, i. e. , i,
Face Detection --- Efficient and Rank Deficient| Abstract This paper proposes a method for computing fast approximations to support vector decision functions in the field of object detection.  In the present approach we are building on an existing algorithm where the set of support vectors is replaced by a smaller, so-called reduced set of synthesized input space points.  In contrast to the existing method that finds the reduced set via unconstrained optimization, we impose a structural constraint on the synthetic points such that the resulting approximations can be evaluated via separable filters.  For applications that require scanning large images, this decreases the computational complexity by a significant amount.  Experimental results show that in face detection, rank deficient approximations are 4 to 6 times faster than unconstrained reduced set systems. 
Feature Selection for Support Vector Machines by Means of Genetic Algorithms| Abstract The problem of feature selection is a difficult
A Regularization Framework for Learning from Graph Data| Abstract The data in many real-world problems can be thought of as a graph, such as the web, co-author networks, and biological networks.  We propose a general regularization framework on graphs, which is applicable to the classification, ranking, and link prediction problems.  We also show that the method can be explained as lazy random walks.  We evaluate the method on a number of experiments. 
Max--Planck--Institut f ur biologische Kybernetik Max Planck Institute for Biological Cybernetics Technical Report No| TR-110 A kernel view of the dimensionality reduction of manifolds.  Abstract We interpret several well-known algorithms for dimensionality reduction of manifolds as kernel methods.  Isomap, graph Laplacian eigenmap, and locally linear embedding (LLE) all utilize local neighborhood information to construct a global embedding of the manifold.  We show how all three algorithms can be described as kernel PCA on specially constructed Gram matrices, and illustrate the similarities and differences between the algorithms with representative examples. 
Cluster Kernels for Semi-Supervised Learning| Abstract We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label.  This is achieved by modifying the eigenspectrum of the kernel matrix.  Experimental results assess the validity of this approach. 
An Auditory Paradigm for Brain--Computer Interfaces| Abstract Motivated by the particular problems involved in communicating with "locked-in" paralysed patients, we aim to develop a braincomputer interface that uses auditory stimuli.  We describe a paradigm that allows a
Kernel Dependency Estimation| Abstract We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects.  The objects can be for example: vectors, images, strings, trees or graphs.  Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces.  We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images. 
Nonlinear Component Analysis as a Kernel Eigenvalue Problem| A new method for performing a nonlinear form of principal component analysis is proposed.  By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map---for instance, the space of all possible five-pixel products in 1616 images.  We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition. 
Incorporating Invariances in Support Vector Learning Machines| Abstract.  Developed only recently, support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error; however, so far there existed no way of adding knowledge about invariances of a classification problem at hand.  We present a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary. 
Estimating the Support of a High-Dimensional Distribution|
New Support Vector Algorithms|
Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators|
Nonlinear Component Analysis as a Kernel Eigenvalue Problem|
View-based cognitive mapping and path planning|
Sparse Greedy Matrix Approximation for Machine Learning|
Natural regularization from generative models|
Learning from Labeled and Unlabeled Data Using Random Walks|
Engineering support vector machine kernels that recognize translation initiation sites|
On a Kernel-Based Method for Pattern Recognition, Regression, Approximation, and Operator Inversion|
Comparison of View-Based Object Recognition Algorithms Using Realistic 3D Models|
Kernel view of the dimensionality reduction of manifolds|
Kernel Machine Based Learning for Multi-View Face Detection and Pose Estimation|
Support Vector Method for Novelty Detection|
Prior Knowledge in Support Vector Kernels|
Predicting Time Series with Support Vector Machines|
Invariant Feature Extraction and Classification in Kernel Spaces|
editors|
A kernel view of the dimensionality reduction of manifolds|
Semiparametric Support Vector and Linear Programming Machines|
SVMs - A Practical Consequence of Learning Theory,"|
From Regularization Operators to Support Vector Kernels|
Feature selection and transduction for prediction of molecular bioactivity for drug design|
Entropy Numbers, Operators and Support Vector Kernels|
Shrinking the Tube: A New Support Vector Regression Algorithm|
Kernel Principal Component Analysis|
Constructing Boosting Algorithms from SVMs: An Application to One-Class Classification|
A generalized representer theorem, In \Proceedings of the 14th Annual Conference on Computational Learning Theory",|
Support Vector Machines and Kernel Methods: The New Generation of Learning Machines|
"Learning View Graphs for Robot Navigation", Technical Report no 33, Max-Planck Institute fur Biologische Kybernetik, 72076 Tubingen, Germany| Can be downloaded from the website:. 
View-Based Cognitive Mapping and Path Planning", Technical Report no 7, MaxPlanck Institute fur Biologische Kybernetik, 72076 Tubingen, Germany|Canbe downloaded from:. 
and Klaus-Robert Muller| "Support Vector Methods in Learning and Feature Extraction. 
Improving speed and accuracy of support vector learning machines|
Constructing Descriptive and Discriminative Nonlinear Features: Rayleigh Coefficients in Kernel Feature Spaces|

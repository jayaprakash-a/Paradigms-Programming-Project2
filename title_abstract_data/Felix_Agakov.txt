Kernelized Infomax Clustering| Abstract We propose a simple information-theoretic approach to soft clustering based on maximizing the mutual information I(x, y) between the unknown cluster labels y and the training patterns x with respect to parameters of specifically constrained encoding distributions.  The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to specific unknown vectors in the feature space.  The method may be conveniently applied to learning the optimal anity matrix, which corresponds to learning parameters of the kernelized encoder.  The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets. 
MIT Press 2002 Products of Gaussians| Abstract Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data.  Below we consider PoE models in which each expert is a Gaussian.  Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure.  We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of 1-factor PPCA models and (3) a products of experts construction for an AR(1) process.  Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data.  In this paper we consider PoE models in which each expert is a Gaussian.  It is easy to see that in this case the product model will also be Gaussian.  However, if each Gaussian has a simple structure, the product can have a richer structure.  Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be dicult with other models, e. g.  models defined over discrete random variables.  Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of 1-factor PPCA models; (3) A products of experts construction for an AR(1) process.  Products of Gaussians If each expert is a Gaussian p i (xj# i ) # N(# i ; C i ), the resulting distribution of the product of m Gaussians may be expressed as p(xj#) / exp ( 1 2 m X i=1 (x # i ) T C 1 i (x # i ) ) : By completing the square in the exponent it may be easily shown that p(xj#) # N(## ; C# ), where C 1 # = P m i=1 C 1 i .  To simplify the following derivations we will assume that p i (xj# i ) # N(0; C i ) and thus that p(xj#) # N(0; C# ).  fiff 6= 0 can be obtained by translation of the coordinate system. 
Products of Gaussians| Abstract Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data.  Below we consider PoE models in which each expert is a Gaussian.  Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure.  We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of 1-factor PPCA models and (3) a products of experts construction for an AR(1) process.  Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data.  In this paper we consider PoE models in which each expert is a Gaussian.  It is easy to see that in this case the product model will also be Gaussian.  However, if each Gaussian has a simple structure, the product can have a richer structure.  Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be dicult with other models, e. g.  models defined over discrete random variables.  Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of 1-factor PPCA models; (3) A products of experts construction for an AR(1) process.  Products of Gaussians If each expert is a Gaussian p i (xj# i ) # N(# i ; C i ), the resulting distribution of the product of m Gaussians may be expressed as p(xj#) / exp ( 1 2 m X i=1 (x # i ) T C 1 i (x # i ) ) : By completing the square in the exponent it may be easily shown that p(xj#) # N(## ; C# ), where C 1 # = P m i=1 C 1 i .  To simplify the following derivations we will assume that p i (xj# i ) # N(0; C i ) and thus that p(xj#) # N(0; C# ).  fiff 6= 0 can be obtained by translation of the coordinate system. 
Extreme Components Analysis| Abstract Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining.  Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution.  In this paper we present a probabilistic model for "extreme components analysis" (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components.  For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA.  We describe an efficient algorithm to solve for the globally optimal solution.  For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components.  In general, the solution admits a combination of both.  In experiments we explore the properties of XCA on some synthetic and real-world datasets. 
Products of Gaussians and Probabilistic Minor Components Analysis|
Investigations of Gaussian Products-of-Experts Models|
Auxiliary Variational Information Maximization for Dimensionality Reduction|
Correlated sequence learning in a network of spiking neurons using maximum likelihood,|
An Analysis of Contrastive Divergence Learning in Gaussian Boltzmann Machines|

Comparison of Different 3D Edge Detection Methods to Define Landmarks for Point-Based Warping in Autoradiographic Brain Imaging| Abstract.  Warping can be used to reduce interindividual structural variations of 3D image datasets of brains by generating a standard brain and subsequent matching of individual datasets to this reference system.  Point-based warping uses structural information (landmarks) to construct the spatial correspondence between the datasets.  For this we compare the performance of three landmark detection algorithms.  The first two approaches use a threshold-based definition of landmarks, the third spatial derivations of voxels.  The warping is based on a distance-weighted method with an exponential weighting function.  All methods tested are able to reduce structural variations, best results are obtained by the derivation approach. 
A Model for Intrinsic and Activity Dependent Mechanisms Underlying the Regeneration of the Retinotectal Map in Goldfish| Abstract We have developed a mathematical framework to study the interplay of processes thought to be involved in the development of the retinotectal projection.  Our model consists of two input layers which correspond to the retinae of both eyes, and one output layer which corresponds to either the right or the left half of the tectum.  The connections between eyes and tectum are described by effective synaptic weights and their development is governed by four interacting processes: Fiber-tectum interaction, intrinsic within-eye fiber-fiber interaction, intrinsic between-eye fiber-fiber interaction and activity based within-eye fiber-fiber interaction.  We are able to reproduce all reported experiments with the same set of model parameters, as long as the outcome does not depend on "debris" of old retinal fibers. 
Semi-Automatic Quality Determination of 3D Confocal Microscope Scans of Neuronal Cells Denoised by 3D-Wavelet Shrinkage| ABSTRACT The main goal of this work is to denoise 3D confocal microscope scans of neuronal cells taken at high resolution such that neuronal structures of size smaller than 1m become visible.  Although scanning confocal microscopes give much clearer images than ordinary light microscopes do, the images are still noisy and blurred.  Our goal is to filter out the noise in these images without disturbing the smallest neuronal structures which have the same signal amplitude and geometric size as the noise.  In order to obtain a good scale-space representation of the analyzed image, we use the 3D-wavelet transformation.  We extend the denoising method of Donoho 1 for 3D data and obtain several ways of computing thresholds and noise variances.  Finally we develop a quality measure, for images with tree like structures, to determine the denoising method and wavelet form best suited for a particular confocal scan. 
A Stochastic Self-Organizing Map for Proximity Data| Abstract We derive an efficient algorithm for topographic mapping of proximity data (TMP),
A structure preserving image transformation as the goal of visual sensory coding| Abstract Neuronal selectivity is often characterized by stimuli that evoke maximal responses.  But early in the visual system neurons are known to respond to a richer class of stimuli than it can be expected by a feed-forward architecture.  To explain this finding we present a more general functional framework for the exploration of neuronal responses.  To quantify the structure or 'interestingness' of an image patch we introduce a measure of the symmetries that can be found in image patches at the size of local receptive fields of cortical neurons.  We show how this transformation can explain strong responses to complex stimuli as well as responses to simple grating stimuli as found for neurons in V1 and V2. 
Correspondence Analysis for Visualizing Interplay of Pitch Class, Key, and Composer| Abstract We apply correspondence analysis for visualization of interdependence of pitch class & key and key & composer.  A co-occurrence matrix of key & pitch class frequencies is extracted from score (Bach's WTC).  Keys are represented as high-dimensional pitch class vectors.  Correspondence analysis then projects keys on a planar "keyscape".  Vice versa, on "pitchscapes" pitch classes can also be embedded in the key space.  In both scenarios a homogenous circle of fifths emerges in the scapes.  We employ biplots to embed keys and pitch classes in the keyscape to visualize their interdependence.  After a change of co-ordinates the four-dimensional biplots can be interpreted as a configuration on a torus, closely resembling results from music theory and experiments in listener models.  In conjunction with spectral analysis, correspondence analysis constitutes a cognitive auditory model.  Correspondence analysis of the co-occurrence table of intensities of keys and pitch classes lets the circle of fifths evolve in the pitchscape.  This model works on digitized recorded music, does not require averaging or normalization of the data, and does not implicitly use circularity inherent in the model.  Statistics on key preference in composers yields a composer & key cooccurrence matrix.  Then "stylescapes" visualize relations between musical styles of particular composers and schools.  The Biplotting technique links stylistic characteristics to favored keys.  Interdependence of composers and schools is meaningfully visualized according to their key preferences. 
A New Method for Tracking Modulations in Tonal Music in Audio Data Format| Abstract Cq-profiles are 12-dimensional vectors, each component referring to a pitch class.  They can be employed to represent keys.  Cq-profiles are calculated with the constant Q filter bank [4].  They have the following advantages: (i) They correspond to probe tone ratings.  (ii) Calculation is possible in real-time.  (iii) Stability is obtained with respect to sound quality.  (iv) They are transposable.  By using the cq-profile technique as a simple auditory model in combination with the SOM [11] an arrangement of keys emerges, that resembles results from psychological experiments [13], and from music theory [1].  Cq-profiles are reliably applied to modulation tracking by introducing a special distance measure. 
Second-Order Statistics of Natural Images| Abstract.  Assuming adaptation of the visual cortex to its environment, we analyse the invariance found in natural images to explain the selective response of visual cortical neurons.  We argue that the invariant structure of images can be formally expressed by dot-products.  Utilizing the specific structure of the proposed model we show how non-linear functions can be learned efficiently from natural images.  In addition to localized edge detectors we found model neurons that respond to changes in texture and cells that detect edge curvature.  The analysis suggests new types of features neurons in primary visual cortex may be selective to. 
The influence of threshold variability on the response of visual cortical neurons| Abstract Orientation--selective neurons in monkeys and cats show contrast saturation and contrast--invariant orientation tuning (1).  Recently proposed models for orientation selectivity predict contrast invariant orientation tuning but no contrast saturation at high strength of intracortical recurrent couplings, whereas at lower coupling strengths the contrast response saturates but the tuning widths are contrast dependent (3; 7).  In the present work we address the question, if and under which conditions the incorporation of a stochastic distribution of activation thresholds for cortical neurons helps resolving that paradoxon.  We find that both phenomena occurs naturally if two different classes of inhibitory inter-neurons are combined.  Low threshold inhibition keeps the gain of the cortical amplification finite, whereas high threshold inhibition causes contrast saturation. 
Correlation and Stationarity of Speech Radiation: Consequences for Linear Multichannel Filtering| Abstract---Speech processing using multichannel microphone systems is often based on slowly adapting, linear filters.  These systems are able to extract a single source from a mixture (and suppress the others)---if the speech radiation can be described by a linear and time-invariant transfer function.  Here, we test this assumption using a two-channel microphone array and a human talker as the speech source.  We measure correlations between the signals received by the two microphones for individual phonemes using the magnitude squared coherence.  Stationarity is addressed by comparing optimal filters between different phoneme pairs using the system distance.  We find that, in particular for fricatives, the coherence of the speech signals radiated to different directions is very low.  We also find, that the transfer functions from the mouth to the microphones differ significantly between vowels, depending on the locations of the two microphones.  These measurements show that the general mixing model does not hold for speech for arbitrary microphone setups, and that multichannel microphone systems have to be carefully designed. 
Deterministic Annealing for Topographic Vector Quantization and Self-Organizing Maps| Abstract We have developed a robust optimization scheme for self-organizing maps in the framework of noisy vector quantization.  Based on a cost function that takes distortions from channel noise into account we derive a fuzzy algorithm of EM-type for topographic vector quantization (STVQ) which employs deterministic annealing.  This annealing process leads to phase transitions in the cluster representation for which we are able to calculate critical modes and temperatures as a function of the neighbourhood function and the covariance matrix of the data.  Similar results are obtained for the automatic selection of feature dimensions.  Deterministic annealing also offers an alternative to the heuristic stepwise shrinking of the neighbourhood width in the SOM and makes it possible to use the neighbourhood solely to encode desired neighbourhood relations between the clusters.  A soft version of the SOM (SSOM) is derived as a computationally efficient approximation to the E-step of STVQ.  Both methods are numerically tested on a two-dimensional map of the plane and we conclude that the temperature annealing can be precisely controlled and could for many applications be the method of choice. 
Rapid adaptation to internal states as a coding strategy in visual cortex?| Adaptation is a prominent feature of biological neuronal systems.  A common interpretation of adaptation in terms of function is that it provides exibility for a neuronal system to per form well under varying external conditions, for example by adjusting the input/output relation of a sensory system with reference to the ensemble of stimuli the organism currently perceives. This interpretation, however, only applies if the time-scale of adaptation is slower than the time-scale at which the environment changes.  Experimentally it is observed, however, that adaptation can be very rapid.  Spike-frequency adaptation of cortical neurons, for example, occurs on a time-scale of B100ms. Here we show that those ra id ada tation rocesses can also be understood within the framework of information theory.  We start with the hy othesis that neuronal codes are designed to o timize the information a neuronal re resentation conveys about an in ut stimulus for any increasing time window beginning with stimulus onset, and we show that this im lies a ra id ada tation of the neuronal code on the time-scale of stimulus resentation.  Ada tation, however, does not occur because the state of the environmentchanges. Rather it is a reaction to changes oftheorganismsowninternalstate,e. g. thelevelofnoiseinthe neuronal re resentation.  We a ly this a roach to a model of an orientation hy ercolumn in the rimary visual cortex, and redict that inter-columnar interactions should ada t on the time-scale of a ty ical xation eriod (B300ms). 
Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers| Abstract We investigate the problem of learning a classification task for datasets which are described by matrices.  Rows and columns of these matrices correspond to objects, where row and column objects may belong to dierent sets, and the entries in the matrix express the relationships between them.  We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space.  Minimizing a bound for the generalization error of a linear classifier which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization.  The new objective function has the advantage that it allows the analysis of matrices which are not positive definite, and not even symmetric or square.  We then consider the case that row objects are interpreted as features.  We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection.  Finally, we apply this method to data obtained from DNA microarrays, where "column" objects correspond to samples, "row" objects correspond to genes and matrix elements correspond to expression levels.  Benchmarks are conducted using standard one-gene classification and support vector machines and K-nearest neighbors after standard feature selection.  Our new method extracts a sparse set of genes and provides superior classification results. 
Solving Large Systems of Differential Equations in Parallel Using Covers and Skeletons| Abstract The design and implementation of parallel algorithms for distributed memory architectures is much harder than the development of sequential algorithms.  This is mainly due to the communication and synchronization that is necessary to manage distributed data correctly.  This paper applies a methodology for the transformational derivation of parallel programs using data distribution algebras that enable an abstract description of data distribution issues.  Algorithms are formulated using skeletons, that is, specialized higher-order functions with particular parallel implementations.  The methodology is applied to a the solution of a system of ordinary differential equations where convolutions can be computed using the Fast Fourier transformation.  The example illustrates the practical optimization problems for a development model of the visual system that involves large scale neural network simulations.  Finally, this algorithm is compared to an implementation of the same system of equations in the programming language C* on a CM-5. 
1 Protein and World Wide Web Data Sets| Abstract We describe a new technique for the analysis of data which is given in matrix form.  We consider two sets of objects, the "row" and the "column" objects, and we represent these objects by a matrix of numerical values which describe their mutual relationships.  We then introduce a new technique, the "Potential Support Vector Machine" (P-SVM), as a large-margin based method for the construction of classifiers and regression functions for the "column" objects.  Contrary to standard support vector machine (SVM) approaches, the P-SVM minimizes a scale-invariant capacity measure under a new set of constraints.  As a result, the P-SVM can handle data matrices which are neither positive definite nor square, and leads to a usually sparse expansion of the classification boundary or the regression function in terms of the "row" rather than the "column" objects.  We introduce two complementary regularization schemes in order to avoid overfitting for noisy data sets.  The first scheme improves generalization performance for classification and regression problems, the second scheme leads to the selection of a small and informative set of "row" objects and can be applied to feature selection.  A fast optimization algorithm based on the "Sequential Minimal Optimization" (SMO) technique is provided.  We first apply the new method to two kinds of data representation.  The first representation uses a vectorial representation for the "row" and the "column" objects, and constructs a Gram matrix from feature vectors using a kernel function.  Benchmark results show, that the P-SVM method is competitive with or provides superior classification and regression results compared to standard methods and has the additional advantage that the kernel functions are no longer restricted to be positive definite.  The second representation uses a measured matrix of mutual relations between objects rather than vectorial data.  The new classification and regression method again performs very well compared to standard techniques on the benchmark data sets.  More importantly, however, our experiments show that the P-SVM can be very eectively used for feature selection because the resulting predictions are based on a sparse expansion of the "row" objects.  Benchmarks are performed not only with toy data, but also with several real world data sets including data from the UCI repository, protein classification, web-page classification, and DNA microarray data. 
Dynamical Causal Learning| Abstract Current psychological theories of human causal learning and judgment focus primarily on long-run predictions: two by estimating parameters of a causal Bayes nets (though for different parameterizations), and a third through structural learning.  This paper focuses on people's short-run behavior by examining dynamical versions of these three theories, and comparing their predictions to a real-world dataset. 
Principal Component Analysis and Blind Separation of Sources for Optical Imaging of Intrinsic Signals| Abstract The analysis of data sets from optical imaging of intrinsic signals requires the separation of signals, which accurately reflect stimulated neuronal activity (mapping signal), from signals related to background activity.  Here we show that blind separation of sources by Extended Spatial Decorrelation (ESD) is a powerful method for the extraction of the mapping signal from the total recorded signal.  ESD is based on the assumptions, (i) that each signal component varies smoothly across space and (ii) that every component has zero cross-correlation functions with the other components.  In contrast to the standard analysis of optical imaging data, the proposed method (i) is applicable to non-orthogonal stimulus-conditions, (ii) can remove the global signal, blood-vessel patterns and movement artifacts, (iii) works without ad hoc assumptions about the data structure in the frequency domain, and (iv) provides a confidence measure for the signals (Z-score).  We first demonstrate on orientation maps from cat and ferret visual cortex, that Principal Component Analysis (PCA), which acts as a preprocessing step to ESD, can already remove global signals from image stacks, as long as data stacks for at least two -- not necessarily orthogonal -- stimulus conditions are available.  We then show that the full ESD analysis can further reduce global signal components and -- finally -- concentrate the mapping signal within a single component both for differential image stacks and for image stacks recorded during presentation of a single stimulus. 
The Effect of Intracortical Competition on the Formation of Topographic Maps in Models of Hebbian Learning| Abstract Correlation based learning models (CBL) and self-organizing maps (SOM) are two classes of Hebbian models that have both been proposed to explain the activity driven formation of cortical maps.  Both models differ significantly in the way lateral cortical interactions are treated leading to different predictions for the formation of receptive fields.  The linear CBL models predict that receptive field profiles are determined by the average values and the spatial correlations of second order of the afferent activity patterns, wheras SOM models map stimulus features.  Here we investigate a class of models which are characterized by a variable degree of lateral competition and which have the CBL and SOM models as limit cases.  We show that there exists a critical value for intracortical competition below which the model exhibits CBL properties and above which feature mapping sets in.  The class of models is then analyzed with respect to the formation of topographic maps between two layers of neurons.  For Gaussian input stimuli we find that localized receptive fields and topographic maps emerge above the critical value for intracortical competition and we calculate this value as a function of the size of the input stimuli and the range of the lateral interaction function.  Additionally, we show that the learning rule can be derived via the optimization of a global cost function in a framework of probabilistic output neurons which represent a set of input stimuli by a sparse code. 
header for SPIE use Point-based warping with optimized weighting factors of displacement vectors| ABSTRACT The accurate comparison of inter-individual 3D image brain datasets requires non-affine transformation techniques (warping) to reduce geometric variations.  Constrained by the biological prerequisites we use in this study a landmark-based warping method with weighted sums of displacement vectors, which is enhanced by an optimization process.  Furthermore, we investigate fast automatic procedures for determining landmarks to improve the practicability of 3D warping.  This combined approach was tested on 3D autoradiographs of Gerbil brains.  The autoradiographs were obtained after injecting a non-metabolizable radioactive glucose derivative into the Gerbil thereby visualizing neuronal activity in the brain.  Afterwards the brain was processed with standard autoradiographical methods.  The landmark-generator computes corresponding reference points simultaneously within a given number of datasets by Monte-Carlo-techniques.  The warping function is a distance weighted exponential function with a landmark-specific weighting factor.  These weighting factors are optimized by a computational evolution strategy.  The warping quality is quantified by several coefficients (correlation coefficient, overlap-index, and registration error).  The described approach combines a highly suitable procedure to automatically detect landmarks in autoradiographical brain images and an enhanced point-based warping technique, optimizing the local weighting factors.  This optimization process significantly improves the similarity between the warped and the target dataset. 
Spike-frequency adaptation as a mechanism for dynamic coding in V1| Abstract We investigate the representation of visual stimuli and the short-term dynamics of activity within primary visual cortex in a &free-viewing' scenario with &saccading eye movements' modeled as a series of visual stimuli that are flashed onto the retina for the duration of a "xation period (200}300 ms).  We assume that the entire activity pattern from the beginning of "xation until time t constitutes the neural code.  Given a noisy (Poissonian) representation it follows that the signal-to-noise ratio increases with time, because more spikes become available for representation.  Here, we show that for archiving an optimal stimulus representation in any increasing time-window beginning with stimulus onset, the processing strategy of the network should be dynamic in the sense that an initially high recurrent cortical competition between orientation selective cells attenuates with time, i. e.  mediated by the instrinsic property of spike-frequency adaptation of pyramidal cells. 
Blind Signal Separation from Optical Imaging Data| Abstract| Optical imaging is the video recording of twodimensional patterns of changes in light reflectance from cortical tissue evoked by stimulation.  We derived a method, extended spatial decorrelation (ESD), that uses second order statistics in space for separating the intrinsic signals into the stimulus related components and the nonspecific variations.  The performance of ESD on model data is compared to independent component analysis algorithms using statistics of 4th and higher order.  Robustness against sensor noise is scored.  When applied to optical images, ESD separates the stimulus specific signal well from biological noise and artifacts. 
Classification on Pairwise Proximity Data| Abstract We investigate the problem of learning a classification task on data represented in terms of their pairwise proximities.  This representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using Euclidean feature vectors, from which pairwise proximities can always be calculated.  Our first approach is based on a combined linear embedding and classification procedure resulting in an extension of the Optimal Hyperplane algorithm to pseudo-Euclidean data.  As an alternative we present another approach based on a linear threshold model in the proximity values themselves, which is optimized using Structural Risk Minimization.  We show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics w. r. t.  their generalization.  Finally, the algorithms are successfully applied to protein structure data and to data from the cat's cerebral cortex.  They show better performance than K-nearest-neighbor classification. 
Soft Learning Vector Quantization| Abstract Learning Vector Quantization is a popular class of adaptive nearest prototype classifiers for multiclass classification, but learning algorithms from this family have so far been proposed on heuristic grounds. 
CONVOLUTIVE DECORRELATION PROCEDURES FOR BLIND SOURCE| ABSTRACT Convolutive decorrelation algorithms form a class of powerful algorithms for blind source separation.  In contrast to ICA, they are based on vanishing second order cross correlation functions between sources.  We provide an analyze an unifying approach for convolutive decorrelation procedures.  The convolutive decorrelation procedures impose the problem of simultaneously diagonalizing a number of covariance matrices.  We examine different cost functions for simultaneous diagonalization with respect to the demixing matrix.  It turns out, that best performance is achieved for a cost function, that takes the squared sum of the off diagonal elements after the diagonal elements were normalized to unity.  We then provide criteria for convolution kernels, that are optimal for noise robustness and which can guarantee positive definite covariance matrices, which are important for reliable convergence. 
ACTIVE TOPOGRAPHIC MAPPING OF PROXIMITIES| Abstract We deal with the question of how to reduce the computational costs of obtaining and clustering dissimilarity data.  We show that for pairwise clustering, a large portion of the dissimilarity data can be neglected without incurring a serious deterioration of the clustering solution.  This fact can be exploited by selecting the dissimilarity values that are supposed to be most relevant in a well-directed manner.  We present an algorithm for active data selection for topographic pairwise clustering that aims at maximizing the expected reduction in the clustering cost function and propose a computationally more efficient approximation to this algorithm, that yields satisfactory results in cases where the topography is imposed only weakly. 
Solving Combinatorial Optimization Problems Using Neural Networks| Cambridge Univ.  Engineering. 
Adjusting stochastic resonance in a leaky integrate and fire neuron to sub-threshold stimulus distributions| Abstract Here we study in an abstract model how a single neuron could adapt its properties to maximize information processing capabilities in case of weak signal input and additional noise, the natural realm of stochastic resonance.  The dynamics of the membrane potential is described by an Ornstein--Uhlenbeck process in a hazard function approximation.  First we analytically and numerically characterize the e3ect of stochastic resonance as a function of the model's parameters.  Then we derive an activity-dependent learning rule for the adjustment of the noise inputs and show that it only depends on quantities which could be estimated locally by the neuron. 
An Annealed Self-Organizing Map for Source Channel Coding| Abstract We derive and analyse robust optimization schemes for noisy vector quantization on the basis of deterministic annealing.  Starting from a cost function for central clustering that incorporates distortions from channel noise we develop a soft topographic vector quantization algorithm (STVQ) which is based on the maximum entropy principle and which performs a maximum-likelihood estimate in an expectationmaximization (EM) fashion.  Annealing in the temperature parameter fi leads to phase transitions in the existing code vector representation during the cooling process for which we calculate critical temperatures and modes as a function of eigenvectors and eigenvalues of the covariance matrix of the data and the transition matrix of the channel noise.  A whole family of vector quantization algorithms is derived from STVQ, among them a deterministic annealing scheme for Kohonen's self-organizing map (SOM).  This algorithm, which we call SSOM, is then applied to vector quantization of image data to be sent via a noisy binary symmetric channel.  The algorithm's performance is compared to those of LBG and STVQ.  While it is naturally superior to LBG, which does not take into account channel noise, its results compare very well to those of STVQ, which is computationally much more demanding. 
Attention Driven Memory| Abstract Categorization is a skill which is extensively used in everyday life and therefore an important aspect of
Models of orientation and ocular dominance columns in the visual cortex: a critical comparison| Abstract Orientation and ocular dominance maps in the primary visual cortex of mammals are among the most thouroughly investigated of the patterns in the cerebral cortex.  A considerable amount of work has been dedicated to unraveling both their detailed structure and the neural mechanisms which underlie their formation and development.  Many schemes have been proposed, some of which are in competition.  Some models focus on development of receptive fields while others focus on the structure of cortical maps, i. e.  the arrangement of receptive field properties across the cortex.  Each model used different means to determine its success at reproducing experimental map patterns, often relying principally on visual comparison.  Experimental data is becoming available which allows a more careful evaluation of models.  In this contribution more than ten of the most prominent models of cortical map formation and structure are critically evaluated and compared with the most recent experimental findings from macaque striate cortex.  Comparisons are based on properties of the predicted or measured cortical map patterns.  We introduce several new measures for comparing experimental and model map data which reveal important differences between models.  We expect that the use of these measures will improve current models by helping determine parameters to match model maps to experimental data now becomming available from a variety of species.  Our study reveals that (i) despite apparent differences, many models are based on similar principles and consequently make similar predictions, (ii) several models produce orientation map patterns which are not consistent with the experimental data from macaques, regardless of the plausibility of the models' suggested physiological implementations, (iii) no models have yet fully accounted for both the local and the global relationships between orientation and ocular dominance map patterns. 
Automatic Segmentation and Skeletonization of Neurons From Confocal Microscopy Images Based on the 3-D Wavelet Transform| Abstract---In this work, we focus on methods for the preprocessing of neurons from three-dimensional (3-D) confocal microscopy images, which are needed for a subsequent detailed morphologic analysis [7].  Due to the specific image properties of confocal microscopy scans, we had to include several heuristic approaches which are based on multiscale edges [17] to guarantee meaningful results: 1) a reliable segmentation of objects of different sizes independent of image contrast, and, based on it, 2) the computation of skeleton points along the branch central axes, and 3) the reliable detection of branching points and of problematic regions.  These are preprocessing steps to gather information which is needed by the subsequent construction of a graph representing the geometry of the neuron [27] and a final surface reconstruction [31]. 
Classification on Proximity Data with LP--Machines| Abstract We provide a new linear program to deal with classification of data in the case of data given in terms of pairwise proximities.  This allows to avoid the problems inherent in using feature spaces with indefinite metric in Support Vector Machines, since the notion of a margin is purely needed in input space where the classification actually occurs.  Moreover in our approach we can enforce sparsity in the proximity representation by sacrificing training error.  This turns out to be favorable for proximity data.  Similar to --SV methods, the only parameter needed in the algorithm is the (asymptotical) number of data points being classified with a margin.  Finally, the algorithm is successfully compared with --SV learning in proximity space and K--nearest-neighbors on real world data from Neuroscience and molecular biology. 
Soft Nearest Prototype Classification| Abstract--- We propose a new method for the construction of nearest prototype classifiers which is based on a Gaussian mixture ansatz and which can be interpreted as an annealed version of Learning Vector Quantization.  The algorithm performs a gradient descent on a cost-function minimizing the classification error on the training set.  We investigate the properties of the algorithm and assess its performance for several toy data sets and for an optical letter classification task.  Results show4 7 that annealing in the dispersion parameter of the Gaussian kernels improves classification accuracy, 17104 that classification results are better than those obtained with standard Learning Vector Quantization (LVQ 2. 1, LVQ 3) for equal numbers of prototypes andnd 5664 that annealing of the width parameter improved the classification capability.  Additionally, the principled approach provides an explanation of a number of features of the (heuristic) LVQ methods. 
Development and Regeneration of the Retinotectal Map in Goldfish: A Computational Study| Abstract We present a simple computational model to study the interplay of activity dependent and intrinsic processes thought to be involved in the formation of topographic neural projections.  Our model consists of two input layers which project to one target layer.  The connections between layers are described by a set of synaptic weights.  These weights develop according to three interacting developmental rules: (i) an intrinsic fibertarget interaction which generates chemospecific adhesion between afferent fibers and target cells, (ii) an intrinsic fiber-fiber interaction which generates mutual selective adhesion between the afferent fibers and (iii) an activity-dependent fiber-fiber interaction which implements Hebbian learning.  Additionally, constraints are imposed to keep synaptic weights finite.  The model is applied to a set of eleven experiments on the regeneration of the retinotectal projection in goldfish.  We find that the model is able to reproduce the outcome of an unprecedented range of experiments with the same set of model parameters, including details of the size of receptive and projective fields.  We expect this mathematical framework to be a useful tool for the analysis of developmental processes in general. 
On the Anatomical Basis of Field Size, Contrast Sensitivity, and Orientation Selectivity in Macaque Striate Cortex: A Model Study| Abstract.  Neurons in layer 4C in macaque striate cortex show a differential change in receptive field size and achromatic contrast sensitivity with depth, and exhibit orientation selective responses in the upper 4Cff sublayer.  Using a computational model we first demonstrate that the observed change in receptive field size and contrast sensitivity can arise from a differential convergence of afferents from the P and M subdivisions of the lateral geniculate nucleus onto layer 4C spiny stellate cells if one postulates that the two anatomically identified M1 and M2 subpopulations of the M afferents differentially project to different depth in the 4Cff subdivision.  Number ratios and response properties of both M subpopulations are predicted and may now be tested experimentally.  We then show that realistic orientation selective responses in upper 4Cff can emerge intracortically as a result of local lateral interactions, which are anisotropic, between spiny stellate cells and inhibitory interneurons.  The model assumes that orientation bias and tuning are generated by the same cortical circuits and predicts a receptive field dynamics with an initial non orientation specific response. 
To appear in Vision Research A Model for the Depth-Dependence of Receptive Field Size and Contrast Sensitivity of Cells in Layer 4C of Macaque Striate Cortex| Abstract A model of LGN input to layer 4C of macaque primary visual cortex has been used to test the hypothesis that feedforward convergence of P and M inputs onto layer 4C spiny stellate neurons is sufficient to explain the observed gradual change in receptive field size and contrast sensitivity with depth in the layer.  Overlap of dendrites of postsynaptic neurons between M and P input zones proved sufficient to explain change in the lower two-thirds of layer 4C, while more rapid change in upper 4C was matched by proposing two different M inputs with partial overlap in upper 4Cff. 
Self-Organizing Maps and Clustering Methods for Matrix Data| Abstract In this contribution we present extensions of the Self Organizing Map and clustering methods
Spectrum of Pitch Differences Models the Perception of Octave Ambiguous Tones| Abstract In the experiment we proof the paradoxical perception of pitch.  A tone sequence demonstrates this paradox: the sequence is perceived as descending, although the first and last tone are identical.  The intransitivity of pitch perception generalizes (Shepard 1964).  It applies also to harmonic complex tones with variate amplitude envelope and partials that are arranged in intervals different from octaves.  The paradox perception in this experiment as well as in the Shepard scale is modeled by a formula (spectrum of pitch differences) that extends the virtual pitch algorithm (Terhardt 1998) to pitch intervals. 
Automatic 3D-Graph Construction of Nerve Cells from Confocal Microscopy Scans| Abstract We present a method of automatic graph construction for the description of the geometric structure of nerve cells from 3D confocal scans.  The method is tracing the branch center points, in the branch axial direction using as hints the location of difficult regions inside the neuronal branches.  The axes were obtained in previous work by computing pairwise vector products of intersecting gradients associated to across-scales validated boundary edge points of the neuronal branches.  The axes' anchor points are the branch center points which are estimated as the "center of mass" of all intersecting gradient end points.  The difficult regions are the axes' anchor points having a high directional variance of vector products contributing to the associated ax.  The presented algorithm which uses all the information obtained from preprocessing, is robust to variable contrast, little sensitive to boundary irregularities, adaptive to variability of branch geometry, and produces a sparse, topology preserving graph of the neuron under investigation.  A subsequent surface reconstruction based on this graph [21] accompanied by the labeling of the graph with geometric measurements, would be feasible. 
Recurrent cortical competition: Strengthen or weaken?| Abstract We investigate the short term dynamics of the recurrent competition and neural activity in the primary visual cortex in terms of information processing and in the context of orientation selectivity.  We propose that after stimulus onset, the strength of the recurrent excitation decreases due to fast synaptic depression.  As a consequence, the network shifts from an initially highly nonlinear to a more linear operating regime.  Sharp orientation tuning is established in the first highly competitive phase.  In the second and less competitive phase, precise signaling of multiple orientations and long range modulation, e. g. , by intra- and inter-areal connections becomes possible (surround effects).  Thus the network first extracts the salient features from the stimulus, and then starts to process the details.  We show that this signal processing strategy is optimal if the neurons have limited bandwidth and their objective is to transmit the maximum amount of information in any time interval beginning with the stimulus onset. 
A Model for the Intracortical Origin of Orientation Preference and Tuning in Macaque Striate Cortex| Abstract We report results of numerical simulations for a model of generation of orientation selectivity in macaque striate cortex.  In contrast to previous models, where the initial orientation bias is generated by convergent geniculate input to simple cells and subsequently sharpened by lateral circuits, our approach is based on anisotropic intracortical excitatory connections which provide both the initial orientation bias and its subsequent amplification.  Our study shows that the emerging response properties are similar to the response properties that are observed experimentally, hence the hypothesis of an intracortical generation of orientation bias is a sensible alternative to the notion of an afferent bias by convergent geniculocortical projection patterns.  In contrast to models based on an afferent orientation bias, however, the "intracortical hypothesis" predicts that orientation tuning gradually evolves from an initially nonoriented response and a complete loss of orientation tuning when the recurrent excitation is blocked, but new experiments must be designed to unambiguously decide between both hypotheses. 
Structured Models from Structured Data: Emergence of Modular Information Processing within One Sheet of Neurons| Abstract In our contribution we investigate how structured information processing within a neural net can emerge as a result of unsupervised learning from data. 
Warping with Optimized Weighting Factors of Displacement Vectors - A New Method to Reduce Inter-Individual Variations in Brain Imaging| Abstract An accurate comparison of multimodal and/or interindividual 3D image datasets of brains requires geometric transformation techniques (warping) to reduce geometric variations.  Here, a subset of warping techniques, namely point-based warping, is investigated.  For this kind of warping landmarks between datasets have to be defined.  In large 3D datasets manually setting of landmarks is timeconsuming and therefore impracticable.  Consequently we approach this problem by investigating fast automatic procedures for determining landmarks, based on Monte-Carlo-techniques.  The combined methods were tested on 3D autoradiographs of brains of gerbils.  The results are evaluated by three different similarity functions.  We found that the combined approach is highly applicable in processing brain images. 
Contrast adaptation and infomax in visual cortical neurons| Abstract In the primary visual cortex (V1) the contrast response function of many neurons saturates at high contrast and adapts depending on the visual stimulus.  We propose that both effects---contrast saturation and adaptation---can be explained by a fast and a slow component in the synaptic dynamics.  In our model the saturation is an effect of fast synaptic depression with a recovery time constant of about 200 ms.  Fast synaptic depression leads to a contrast response function with a high gain for only a limited range of contrast values.  Furthermore, we propose that slow adaptation of the transmitter release probability at the geniculocortical synapses is the underlying neural mechanism that accounts for contrast adaptation on a time scale of about 7 sec.  For the functional role of contrast adaptation we make the hypothesis that it serves to achieve the best visual cortical representation of the geniculate input.  This representation should maximize the mutual information between the cortical activity and the geniculocortical input by increasing the release probability in a low contrast environment.  We derive an adaptation rule for the transmitter release probability based on this infomax principle.  We show that changes in the transmitter release probability may compensate for changes in the variance of the geniculate inputs---an essential requirement for contrast adaptation.  Also, we suggest that increasing the release probability in a low contrast environment is beneficial for signal extraction, because neurons remain sensitive only to an increase in the presynaptic activity if it is synchronous and, therefore, likely to be stimulus related.  Our hypotheses are tested in numerical simulations of a network of integrate-and-fire neurons for one column of V1 using fast synaptic depression and slow synaptic adaptation.  The simulations show that changing the synaptic release probability of the geniculocortical synapses is a better model for contrast adaptation than the adaptation of the synaptic weights: only in the case of changing the transmitter release probability our model reproduces the experimental finding that the average membrane potential (DC component) adapts much stronger than the stimulus modulated component (F1 component).  In the case of changing synaptic weights, however, the average membrane potential (DC) as well as the stimulus modulated component (F1 component) would adapt.  Furthermore, changing the release probability at the recurrent cortical synapses cannot account for contrast adaptation, but could be responsible for establishing oscillatory activity often observed in recordings from visual cortical cells. 
Blind Source Separation of Single Components from Linear Mixtures| Abstract.  We present a new method, that is able to separate one or a few particular sources from a linear mixture, performing source separation and dimensionality reduction simultaneously.  This is in particular useful in situations in which the number of observations is much larger than the number of underlaying sources, as it allows to drastically reduce the number of the parameters to estimate.  It is well applicable for the long time series recorded in optical imaging experiments.  Here one is basically interested in only one source containing the stimulus response.  The algorithm is based on the technique of joint diagonalization of cross correlation matrices.  To focus the convergence to the desired source, prior knowledge is incorporated.  It can be derived, for instance, from the expected time course of the metabolic response in an optical imaging experiment.  We demonstrate the capabilities of this algorithm on the basis of toy data coming from prototype signals of former optical recording experiments and with time courses that are similar to those obtained in optical recording experiments. 
2 The Potential Support Vector Machine 6| Abstract We describe a new technique for the analysis of data which is given in matrix form.  We consider two sets of objects, the "row" and the "column" objects, and we represent these objects by a matrix of numerical values which describe their mutual relationships.  We then introduce a new technique, the "Potential Support Vector Machine" (P-SVM), as a large-margin based method for the construction of classifiers and regression functions for the "column" objects.  Contrary to standard support vector machine (SVM) approaches, the P-SVM minimizes a scale-invariant capacity measure under a new set of constraints.  As a result, the P-SVM can handle data matrices which are neither positive definite nor square, and leads to a usually sparse expansion of the classification boundary or the regression function in terms of the "row" rather than the "column" objects.  We introduce two complementary regularization schemes in order to avoid overfitting for noisy data sets.  The first scheme improves generalization performance for classification and regression problems, the second scheme leads to the selection of a small and informative set of "row" objects and can be applied to feature selection.  A fast optimization algorithm based on the "Sequential Minimal Optimization" (SMO) technique is provided.  We first apply the new method to two kinds of data representation.  The first representation uses a vectorial representation for the "row" and the "column" objects, and constructs a Gram matrix from feature vectors using a kernel function.  Benchmark results show, that the P-SVM method is competitive with or provides superior classification and regression results compared to standard methods and has the additional advantage that the kernel functions are no longer restricted to be positive definite.  The second representation uses a measured matrix of mutual relations between objects rather than vectorial data.  The new classification and regression method again performs very well compared to standard techniques on the benchmark data sets.  More importantly, however, our experiments show that the P-SVM can be very eectively used for feature selection because the resulting predictions are based on a sparse expansion of the "row" objects.  Benchmarks are performed not only with toy data, but also with several real world data sets including data from the UCI repository, protein classification, web-page classification, and DNA microarray data. 
Unspecific long-term potentiation can evoke functional segregation in a model of area 17| Abstract Recently it has been shown in rat hippocampus that the synapse specificity of Hebbian long-term potentiation breaks down at short distances below 100 m.  Using a neural network model we show that this unspecific component of long term potentiation can be responsible for the robust formation and maintainance of cortical organization during activity driven development.  When the model is applied to the formation of orientation and ocular dominance in visual cortex, we find that the addition of an unspecific component to standard Hebbian learning - in combination with a tendency of left-eye and right-eye driven synapses to initially group together on the postsynaptic neuron - induces the simultaneous emergence and stabilization of ocular dominance and of segregated, oriented ON-/OFF-subfields.  Since standard Hebbian learning cannot account for the simultaneous stabilization of both structures, unspecific LTP thus induces a qualitatively new behaviour.  Since unspecific LTP only acts between synapses which are locally clustered in space, our results imply that details of the local grouping of synapses on the dendritic arbors of postsynaptic cells can considerably influence the formation of the cortical functional organization at the systems level. 
Optimal Kernels for Unsupervised Learning| Abstract--- We investigate the optimal kernel for sample-based model selection in unsupervised learning if maximum likelihood approaches are intractable.  Given a set of training data and a set of data generated by the model, two kernel density estimators are constructed.  A model is selected through gradient descent w. r. t.  the model parameters on the integrated squared difference between the density estimators.  Firstly we prove that convergence is optimal, i. e.  that the cost function has only one global minimum w. r. t.  the locations of the model samples, if and only if the kernel in the reparametrized cost function is a Coulomb kernel.  As a consequence, Gaussian kernels commonly used for density estimators are suboptimal.  Secondly we show that the absolute value of the difference between model and reference density convergences at least with 1/t.  Finally, we apply the new methods to distribution free ICA and to nonlinear ICA. 
Information Theory and Coding | McGraw.  and MacKay, D.  J.  C.  (2000) BSC thresholds for code ensembles based on `typical pairs' decoding.  In Codes, Systems and
Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems| Abstract We introduce a family of classifiers based on a physical analogy to an electrostatic system of charged conductors.  The family, called Coulomb classifiers, includes the two best-known support-vector machines (SVMs), the --SVM and the C--SVM.  In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classification function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy.  The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-definite kernels, regularization techniques that allow for the construction of an optimal classifier in Minkowski space.  Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs.  The experiments include classification tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classifier outperformed standard SVMs. 
Nonlinear Feature Selection with the Potential Support Vector Machine| SVM) which is a new filter method for feature selection.  The idea of the P-SVM feature selection is to exchange the role of features and data points in order to construct "support features".  The "support features" are the selected features.  The P-SVM uses a novel objective function and novel constraints -- one constraint for each feature.  As with standard SVMs, the objective function represents a complexity or capacity measure whereas the constraints enforce low empirical error.  In this contribution we extend the P-SVM in two directions.  First, we introduce a parameter which controls the redundancy among the selected features.  Secondly, we propose a nonlinear version of the P-SVM feature selection which is based on neural network techniques.  Finally, the linear and nonlinear P-SVM feature selection approach is demonstrated on toy data sets and on data sets from the NIPS 2003 feature selection challenge. 
Cortical Map Development Driven by Spontaneous Retinal Activity Waves| Abstract.  Correlation-based learning (CBL) models have been quite successful when applied to the activity driven development of cortical orientation selectivity (OR) or ocular dominance (OD) maps.  We ask under which conditions waves of spontaneous activity as have been observed in the developing retina may provide the proper activity to drive the CBL process.  We demonstrate that the currently assumed statistics of spontaneous activity waves may drive OD and OR development only, if (i) the visual pathways interact before they reach the cortex, and (ii) if the CBL model involves nonlinear mechanisms or constraints to couple OD and OR development.  Finally, simulated cortical maps are examined. 
Classification and Feature Selection on Matrix Data with Application to Gene-Expression Analysis| We consider the classification task for datasets which are described by matrices.  Rows and columns of these matrices correspond to objects where row and column objects may be from dierent sets and column objects are labeled.  Data matrix entries express relationships between row and column objects and are produced by an unknown kernel.  These kernels represent dot products in some (unknown) feature space.  In this feature space a linear column object classifier should be constructed.  However the dot products between column objects are not available.  Therefore standard support vector techniques cannot be utilized.  We derive a new objective function for model selection in such a feature space according to the principle of structural risk minimization.  The new objective allows the analysis of matrices which are not positive definite, and not even symmetric or square.  Because row objects can be interpreted as features and our method assigns support vector weights to the row objects can be used for feature selection.  An additional constraint, which imposes sparseness on the row objects resulting in few selected features.  We analyse data obtained from DNA microarrays, where "column" objects correspond to samples, "row" objects correspond to genes and matrix elements correspond to expression levels.  Benchmarks are conducted using standard one-gene classification and support vector machines and K-nearest neighbors after standard feature selection.  Our new method extracts a sparse set of genes and provides superior classification results. 
Application of Blind Separation of Sources to Optical Recording of Brain Activity| Abstract In the analysis of data recorded by optical imaging from intrinsic signals (measurement of changes of light reflectance from cortical tissue) the removal of noise and artifacts such as blood vessel patterns is a serious problem.  Often bandpass filtering is used, but the underlying assumption that a spatial frequency exists, which separates the mapping component from other components (especially the global signal), is questionable.  Here we propose alternative ways of processing optical imaging data, using blind source separation techniques based on the spatial decorrelation of the data.  We first perform benchmarks on artificial data in order to select the way of processing, which is most robust with respect to sensor noise.  We then apply it to recordings of optical imaging experiments from macaque primary visual cortex.  We show that our BSS technique is able to extract ocular dominance and orientation preference maps from single condition stacks, for data, where standard post-processing procedures fail.  Artifacts, especially blood vessel patterns, can often be completely removed from the maps.  In summary, our method for blind source separation using extended spatial decorrelation is a superior technique for the analysis of optical recording data. 
2003 Special Issue Modeling the adaptive visual system: a survey of principled approaches| Abstract Modeling the visual system can be done at multiple levels of description ranging from computer simulations of detailed biophysical models to firing rate and so-called `black-box' models.  Re-introducing David Marr's analysis levels for the visual system, we motivate the use of more abstract models in order to answer the question of what the visual system is computing.  The approaches we selected to review in this article concentrate on modeling the changes of sensory representations.  The considered time-scales, range from the developmental time-scale of receptive field formation to fast transient neuronal dynamics during a single stimulus presentation.  Common to all approaches is their focus on providing functional interpretations, instead of `only' explanations in terms of mechanisms.  Although the concrete approaches can be distinguished along different lines, a common theme is emerging which may qualify as a paradigm for providing functional interpretations for changes of receptive field properties, i. e.  the dynamic adjustment of sensory representations to varying external or internal conditions. 
Multi Dimensional ICA to Separate Correlated Sources| Abstract We present a new method for the blind separation of sources, which do not fulfill the independence assumption.  In contrast to standard methods we consider groups of neighboring samples (\patches") within the observed mixtures.  First we extract independent features from the observed patches.  It turns out that the average dependencies between these features in different sources is in general lower than the dependencies between the amplitudes of different sources.  We show that it might be the case that most of the dependencies is carried by only a small number of features.  Is this case - provided these features can be identified by some heuristic - we project all patches into the subspace which is orthogonal to the subspace spanned by the \correlated" features.  Standard ICA is then performed on the elements of the transformed patches (for which the independence assumption holds) and robustly yields a good estimate of the mixing matrix. 
Optimal Gradient-Based Learning Using Importance Weights| Abstract--- We introduce a novel "importance weight" method (IW) to speed up gradient based learning.  The method is particularly useful for "difficult" data sets including features like unbalanced data, highly non-linear relationships between variables, or long-term dependencies in sequences.  An importance weight is assigned to every data point of the training set.  The weight controls the contribution of the data point to the total training error according to its informativeness for learning a good predictor.  It can also be interpreted as an individual learning step size for the local gradient at this particular data point.  The importance weights are obtained by solving a quadratic optimization problem which minimizes the absolute value of the change in the parameter vector during a learning step under the (soft) constraint, that the total error should be reduced by at least a given fixed value.  For linear classifiers we show, that the new method is equivalent to standard support vector learning.  We apply the IW method to feedforward multi-layer perceptrons and to recurrent neural networks (LSTM).  Benchmarks with QuickProp and standard gradient descent methods are provided for toy data as well as for "real world" protein datasets.  Results show that the new learning method is usually much faster in terms of epochs as well as in terms of absolute CPU time, and that it provides equal or better prediction results.  In the "latching benchmark" for sequence prediction, the new approach was able to extract and exploit dependencies between sites which are 1,000,000 sequence elements apart -- a new record. 
Gaussian Process Regression: Active Data Selection and Test Point Rejection| Abstract We consider active data selection and test point rejection strategies for Gaussian process regression based on the variance of the posterior over target values.  Gaussian process regression is viewed as transductive regression that provides target distributions for given points rather than selecting an explicit regression function.  Since not only the posterior mean but also the posterior variance are easily calculated we use this additional information to two ends: Active data selection is performed by either querying at points of high estimated posterior variance or at points that minimize the estimated posterior variance averaged over the input distribution of interest or --- in a transductive manner --- averaged over the test set.  Test point rejection is performed using the estimated posterior variance as a confidence measure.  We find for both a two-dimensional toy problem and for a real-world benchmark problem that the variance is a reasonable criterion for both active data selection and test point rejection. 
Correction methods for three-dimensional reconstructions from confocal images: I| tissue shrinking and axial scaling.  Abstract We show here, using locust wholemount ganglia as an example, that scaling artifacts in three-dimensional reconstructions from confocal microscopic images due to refractive index mismatch in the light
Synapse Clustering Can Drive Simultaneous ON-OFF and Ocular-Dominance Segregation in a Model of Area 17| Abstract In the primary visual cortex of monkeys, the development of ocular dominance and orientation selectivity is at least partially driven by neural activity.  We propose a modified Hebb-type learning mechanism, which takes into account non-specific components of activity-dependent synaptic modification.  It is shown analytically, that ocular dominance and ON-OFF-segregation occur simultaneously in a linear network as soon as left-eye and right-eye synapses tend to cluster on the surface of the postsynaptic neuron.  Simulations show, that this mechanism is robust against the introduction of network nonlinearities such as rectifying transfer functions and intracortical recurrency.  The results imply, that details of single cell properties can have considerable influence on the behaviour of high level developmental models. 
A COMPARISON OF MODELS OF VISUAL CORTICAL MAP FORMATION| ABSTRACT Several classes of models of visual cortical map characterization and development are compared.  Characteristics and predictions of the models are compared to one another and to cortical maps observed in animals.  Several models are found to predict incorrect map structure.  Certain observed patterns of visual maps imply constraints on the processes which could be involved in their morphogenesis.  1 FEATURE MAPS IN VISUAL CORTEX Individual cells in the mammalian primary visual cortex, or striate cortex, respond differently to features in visual input, with feature preferences determined in part by the pattern of connection between retinal light receptors and cortex.  Striate cortical receptive fields, descriptions of features to which each cell responds, are often localized in the visual field, may be dominated by input from either eye, and usually show a preference for stimuli with a particular orientation.  Several receptive field properties of neurons are arranged in the cortex in a complicated two-dimensional map such that nearby columns of neurons tend to have similar receptive fields.  Due to the ordered projections from the eyes, a roughly topographic map of visual space is formed on the cortical surface.  An optical imaging technique [1, 2, 4] reveals the embedded maps of ocular dominance and orientation preference over small patches of the cortex.  The details of the cortical maps vary greatly between individual animals, but certain organizing principles appear invariant.  Depending on the species, ocular 1 To whom correspondence should be addressed. 
A NEW ALGORITHM FOR THE EVALUATION OF PTV DATA USING POINT MATCHING DISTANCE MEASURES AND DETERMINISTIC ANNEALING| ABSTRACT We describe a new method for the evaluation of single pulse multiple exposure PTV data which allows the joint estimate of several local flow field parameters, among them components describing local translation, rotation and shear.  The method operates on particle coordinates, which can be extracted from digital images using the CLEAN algorithm by Schwarz (1978), and is based on an algorithm recently proposed by Gold et al.  (1996) for matching problems in stereo vision and statistical pattern recognition.  For flows with small velocity gradients our method compares favorable to cross-correlation techniques.  When velocity gradients become larger, for example near vortices and in shear flows, the new method leads to strongly improved velocity estimates, and additionally allows an estimate of the strength of the local rotation and shear components.  The method can in principle be extended to stereo data, because computational expenses grow linearly with the number of particle coordinates only. 
Orientation Selective Cells Emerge in a Sparsely Coding Boltzmann Machine| Abstract In our contribution we investigate a sparse coded Boltzmann machine as a model for the formation of orientation selective receptive fields in primary visual cortex.  The model consists of two layers of neurons which are recurrently connected and which represent the lateral geniculate nucleus and primary visual cortex.  Neurons have ternary activity values +1, \Gamma 1, and 0, where the 0-state is degenerate being assumed with higher prior probability.  The probability for a (stochastic) activation vector on the net obeys the Boltzmann distribution and maximum-likelihood leads to the standard Boltzmann learning rule.  We apply a mean-field version of this model to natural image processing and find that neurons develop localized and oriented receptive fields. 
Quadratic Optimization for Simultaneous Matrix Diagonalization| Abstract Simultaneous diagonalization of a set of matrices is a technique, which has numerous applications in statistical signal processing and multi-variate statistics.  Although objective functions in a least squares sense can be easily formulated, their minimization is not trivial, because constraints and 4th order terms are usually involved.  Most known optimization algorithms are, therefore, subject to certain restrictions on the class of problems: orthogonal transformations, sets of symmetric, hermitian or positive definite matrices, to name a few.  In this work we present a new algorithm called QDIAG, that splits the overall optimization problem into a sequence of simpler second order sub-problems.  There are no restrictions imposed on the transformation matrix, which may be non-orthogonal, indefinite or even rectangular, and there are no restrictions, except for one, imposed on the matrices to be diagonalized, regarding their symmetry or definiteness.  We apply the new method to Second Order Blind Source Separation and show that the algorithm convergences fast and reliably.  It allows for an implementation with a complexity independent of the number of matrices and, therefore, is particularly suitable for problems dealing with large sets of matrices. 
Support Vector Machines for Dyadic Data| Abstract We describe a new technique for the analysis of dyadic data, where two sets of objects ("row" and "column" objects) are characterized by a matrix of numerical values which describe their mutual relationships.  The new technique, called "Potential Support Vector Machine" (P-
Contrast adaptation in simple cells by changing the transmitter release probability| Abstract The contrast response function (CRF) of many neurons in the primary visual cortex saturates and shifts towards higher contrast values following prolonged presentation of high contrast visual stimuli.  Using a recurrent neural network of excitatory spiking neurons with adapting synapses we show that both effects could be explained by a fast and a slow component in the synaptic adaptation.  (i) Fast synaptic depression leads to saturation of the CRF and phase advance in the cortical response to high contrast stimuli.  (ii) Slow adaptation of the synaptic transmitter release probability is derived such that the mutual information between the input and the output of a cortical neuron is maximal.  This component---given by infomax learning rule---explains contrast adaptation of the averaged membrane potential (DC component) as well as the surprising experimental result, that the stimulus modulated component (F1 component) of a cortical cell's membrane potential adapts only weakly.  Based on our results, we propose a new experiment to estimate the strength of the effective excitatory feedback to a cortical neuron, and we also suggest a relatively simple experimental test to justify our hypothesized synaptic mechanism for contrast adaptation. 
An Analysis of Orientation and Ocular Dominance Patterns in the Visual Cortex of Cats and Ferrets| Abstract We report an analysis of orientation and ocular dominance maps that were recorded optically from area 17 of cats and ferrets.  Similar to a recent study performed in primates (Obermayer and Blasdel, 1997) we find that 80% (for cats and ferrets) of orientation singularities that are nearest neighbors have opposite sign, and that the spatial distribution of singularities deviates from a random distribution of points, because the average distances between nearest neighbors are significantly larger than expected for a random distribution.  Orientation maps of normally raised cats and ferrets show approximately the same typical wavelength, however the density of singularities is higher in ferrets than in cats.  Also, we find the well-known overrepresentation of cardinal vs.  oblique orientations in young ferrets (Chapman and Bonhoeffer, 1998; Coppola, White, Fitzpatrick and Purves, 1998) but only a weak, not quite significant overrepresentation of cardinal orientations in cats, as has been reported previously (Bonhoeffer and Grinvald, 1993).  Orientation and ocular dominance slabs in cats exhibit a tendency of being orthogonal to each other (Hubener, Shoham, Grinvald and Bonhoeffer, 1997), albeit less pronounced as has been reported for primates (Obermayer and Blasdel, 1993).  In chronic recordings from single animals, a decrease of the singularity density and an increase of the ocular dominance wavelength with age but no change of the orientation wavelengths was found.  Orientation maps are compared with two pattern models for orientation preference maps: bandpass-filtered white noise and the field analogy model.  Bandpass-filtered white noise predicts sign correlations between orientation singularities, but the correlations are significantly stronger (87 % opposite sign pairs) than what we have found in the data.  Also, bandpass-filtered noise predicts a deviation of the spatial distribution of singularities from a random dot pattern.  The field analogy model can account for the structure of certain local patches but not for the whole orientation map.  Differences between the predictions of the field analogy model and experimental data are smaller than what has been reported for primates (Obermayer and Blasdel, 1997), which can be explained by the smaller size of the imaged areas in cats and ferrets. 
Learning Quadratic Forms by Density Estimation and its Applications to Image Coding| Abstract We develop a novel method for source separation and apply it to natural images.  It is a specialization of independent factor analysis (IFA) but overcomes generic IFA problems and finds many independent sources in few observations.  A fast and robust EM learning algorithm produces an over-complete basis.  Compared to standard approaches our method generates superior codes in terms of population sparseness and dispersity.  The algorithm learns features which possess properties that are observed in simple as well as complex cells found in V1. 
Regression Models for Ordinal Data: A Machine Learning Approach| Abstract In contrast to the standard machine learning tasks of classification and metric regression we investigate the problem of predicting variables of ordinal scale, a setting referred to as ordinal regression.  The task of ordinal regression arises frequently in the social sciences and in information retrieval where human preferences play a major role.  Also many multi--class problems are really problems of ordinal regression due to an ordering of the classes.  Although the problem is rather novel to the Machine Learning Community it has been widely considered in Statistics before.  All the statistical methods rely on a probability model of a latent (unobserved) variable and on the condition of stochastic ordering.  In this paper we develop a distribution independent formulation of the problem and give uniform bounds for our risk functional.  The main difference to classification is the restriction that the mapping of objects to ranks must be transitive and asymmetric.  Combining our theoretical framework with results from measurement theory we present an approach that is based on a mapping from objects to scalar utility values and thus guarantees transitivity and asymmetry.  Applying the principle of Structural Risk Minimization as employed in Support Vector Machines we derive a new learning algorithm based on large margin rank boundaries for the task of ordinal regression.  Our method is easily extended to nonlinear utility functions.  We give experimental results for an Information Retrieval task of learning the order of documents with respect to an initial query.  Moreover, we show that our algorithm outperforms more naive approaches to ordinal regression such as Support Vector Classification and Support Vector Regression in the case of more than two ranks 1 .  1 This paper is a preliminary version of (Herbrich et al.  1999)
Classification, Regression, and Feature Selection on Matrix Data| Abstract We describe a new technique for the analysis of data which is given in matrix form.  We consider two sets of objects, the "row" and the "column" objects, and we represent these objects by a matrix of numerical values which describe their mutual relationships.  We then introduce a new technique, the "Potential Support Vector Machine" (P-SVM), as a large-margin based method for the construction of classifiers and regression functions for the "column" objects.  Contrary to standard support vector machine (SVM) approaches, the P-SVM minimizes a scale-invariant capacity measure under a new set of constraints.  As a result, the P-SVM can handle data matrices which are neither positive definite nor square, and leads to a usually sparse expansion of the classification boundary or the regression function in terms of the "row" rather than the "column" objects.  We introduce two complementary regularization schemes in order to avoid overfitting for noisy data sets.  The first scheme improves generalization performance for classification and regression problems, the second scheme leads to the selection of a small and informative set of "row" objects and can be applied to feature selection.  A fast optimization algorithm based on the "Sequential Minimal Optimization" (SMO) technique is provided.  We first apply the new method to so-called pairwise data, i. e.  "row" and "column" objects are from the same set.  Pairwise data can be represented in two ways.  The first representation uses vectorial data and constructs a Gram matrix from feature vectors using a kernel function.  Benchmark results show, that the P-SVM method provides superior classification and regression results and has the additional advantages that kernel functions are no longer restricted to be positive definite.  The second representation uses a measured matrix of mutual relations between objects rather than vectorial data.  The new classification and regression method performs very well compared to standard techniques on benchmark data sets.  More importantly, however, experiments show that the P-SVM can be very eectively used for feature selection.  Then we apply the P-SVM to genuine matrix data, where "row" and "column" objects 1 are from dierent sets, and, again, the data matrix is either constructed via a kernel function combining "row" and "column" objects or obtained by measurements.  On various benchmark data sets we demonstrate the new method's excellent performance for classification, regression, and feature selection problems.  For both pairwise and matrix data benchmarks are performed not only with toy data, but also with several real world data sets including data from the UCI repository, protein classification, web-page classification, and DNA microarray data. 
On the Influence of Threshold Variability in a Mean-Field Model of the Visual Cortex| Abstract.  Orientation{selective neurons in monkeys and cats show contrast saturation and contrast{invariant orientation tuning (Albrecht and Hamilton, 1982).  Recently proposed models for orientation selectivity predict contrast invariant orientation tuning but no contrast saturation at high strength of recurrent intracortical coupling, whereas at lower coupling strengths the contrast response saturates but the tuning widths are contrast dependent (Hansel and Sompolinsky, 1997; Bartsch, Stetter and Obermayer, 1997).  In the present work we address the question, if and under which conditions the incorporation of a stochastic distribution of activation thresholds of cortical neurons leads to the saturation of the contrast response curve as a network effect.  We find that contrast saturation occurs naturally if two different classes of inhibitory inter-neurons are combined.  Low threshold inhibition keeps the gain of the cortical amplification finite, whereas high threshold inhibition causes contrast saturation. 
Analysis of Calcium Imaging Signals from the Honeybee Brain by Nonlinear Models| Abstract Recent Ca 2+ -imaging studies on the antennal lobe of the honeybee (Apis mellifera) have shown that olfactory stimuli evoke complex spatiotemporal changes of the intracellular Ca 2+ concentration, in which stimulus-dependent subsets of glomeruli are highlighted.  In this work we use nonlinear models for the quantitative identification of the spatial and temporal properties of the Ca 2+ -dependent fluorescence signal.  This technique describes time-series of the Ca 2+ signal as a superposition of biophysically motivated model functions for photobleaching and Ca 2+ -dynamics, provides optimal estimates of their amplitudes (signal strengths) and time-constants together with error measures.  Using this method, we can reliably identify two different stimulusdependent signal components.  Their delays and rise times, c1 = (0:4 # 0:3) s, # c1 = (3:8 # 1:2) s for the fast component and c2 = (2:4 # 0:6) s, # c2 = (10:3 # 3:2) s for the slow component, are constant over space and across different odors and animals.  In chronical experiments, the amplitude of the fast (slow) component often decreases (increases) with time.  The pattern of the Ca 2+ -dynamics in space and time can be reliably described as a superposition of only two spatiotemporally separable patterns based on the fast and slow components.  However, the distributions of both components over space turn out to differ from each other, and more work has to be done in order to specify their relationship with neuronal activity. 
Neural Networks in Economics: Background, Applications and New Developments| Abstract Neural Networks were developed in the sixties as devices for classification and regression.  The approach was originally inspired from Neuroscience.  Its attractiveness lies in the ability to "learn", i. e.  to generalize to as yet unseen observations.  One aim of this paper is to give an introduction to the technique of Neural Networks and an overview of the most popular architectures.  We start from statistical learning theory to introduce the basics of learning.  Then, we give an overview of the general principles of neural networks and of their use in the field of Economics.  A second purpose is to introduce a recently developed Neural Network Learning technique, so called Support Vector Network Learning, which is an application of ideas from statistical learning theory.  This approach has shown very promising results on problems with a limited amount of training examples.  Moreover, utilizing a technique that is known as the kernel trick, Support Vector Networks can easily be adapted to nonlinear models.  Finally, we present an economic application of this approach from the field of preference learning. 
Active Learning in Self-Organizing Maps| The self-organizing map (SOM) was originally proposed by T.  Kohonen in 1982 on biological grounds and has since then become a widespread tool for exploratory data analysis.  Although introduced as a heuristic, SOMs have been related to statistical methods in recent years, which led to a theoretical foundation in terms of cost functions as well as to extensions to the analysis of pairwise data, in particular of dissimilarity data.  In our contribution, we first relate SOMs to probabilistic autoencoders, re-derive the SOM version for dissimilarity data, and review part of the above-mentioned work.  Then we turn our attention to the fact, that dissimilarity-based algorithms scale with O(D 2 ), where D denotes the number of data items, and may therefore become impractical for real-world datasets.  We find that the majority of the elements of a dissimilarity matrix are redundant and that a sparse matrix with more than 80% missing values suffices to learn a SOM representation of low cost.  We then describe a strategy how to select the most informative dissimilarities for a given set of objects.  We suggest to select (and measure) only those elements whose knowledge maximizes the expected reduction in the SOM cost function.  We find that active data selection is computationally expensive, but may reduce the number of necessary dissimilarities by more than a factor of two compared to a random selection strategy.  This makes active data selection a viable alternative when the cost of actually measuring dissimilarities between data objects becomes high. 
Adaptation using local information for maximizing the global cost| Abstract Recently the information transmission properties of noisy, parallel summing threshold arrays, have been investigated and interpreted in a neural coding context (see Stocks, Phys.  Rev. 
Effects of lateral competition in the primary visual cortex on the development of topographic projections and ocular dominance maps| Abstract We present a Hebbian model for the development of cortical maps in the striate cortex that includes a parameter which represents the degree of lateral competition for activity between neurons.  It has two well known models as limiting cases: for weak competition we obtain a Correlation Based Learning (CBL) model and for strong lateral competition we recover the Self Organizing Map (SOM).  We show that incresing the competition for positively correlated localized stimuli leads to a sharp transition from an unorganized map to a topographic projection and subesequently to a topographic map with OD and locally magnified OD stripes. 
A method for the automatic segmentation of autoradiographic image stacks and spatial normalization of functional cortical activity patterns| Abstract This paper introduces two new methods for the automatic anatomical and functional analysis of neurobiological autoradiographic image stacks, such as 2-fluoro-deoxyglucose (2FDG) images.  The difficulty in the evaluation of these "2 D" datasets is that they do not inherently represent a continuous 3D data volume (as generated by MRI or CT), but consist of a stack of images from single tissue slices, suffering from unavoidable preparation artifacts.  In the first part of the paper, a semi-automatic segmentation method is presented which generates a 3D surface model of certain brain structures and which is robust against different cutting directions with respect to the brain coordinate system.  The method saves man-hours compared to manual segmentation and the results are highly reproducible.  In the second part, a fully automatic method for the extraction, analysis and 3D visualization of functional information is described, which allows not only a more accurate localization of activation sites, but also greatly enhances the comparability of different individuals.  Results are shown for 2FDG autoradiographs from rat brains under acoustical stimulation. 
Large margin rank boundaries for ordinal regression|
A statistical mechanical analysis of self-organization and pattern formation during the development of visual maps|
Self-organizing maps: Generalizations and new optimization techniques|
Phase transitions in stochastic self-organizing maps|
Self-Organizing Maps: Stationary States, Metastability and Convergence Rate|
Self-organizing maps: ordering, convergence properties and energy functions|
Learning a preference relation for information retrieval|
A principle for the formation of the spatial structure of cortical feature maps|
A Comparison between a Neural Network Model for the Formation of Brain Maps and Experimental Data|
Support vector learning for ordinal regression|
Bayesian Transduction|
Simulation of Self-Organizing Neural Nets: a Comparison between a Transputer Ring and a Connection Machine CM-2|
Geometry of orientation and ocular dominance columns in monkey striate cortex|
The Joint Development of Orientation and Ocular Dominance: Role of Constraints|
Linear Correlation-Based Learning Models Require a Two-Stage Process for the Development of Orientation and Ocular Dominance|
Convergence properties of self-organizing maps|
Large-scale simulation of a self-organizing neural network|
New methods for the computer-assisted 3-D reconstruction of neurons from confocal image stacks|
Adaptive Neuronale Netze und ihre Anwendung als Modelle der Entwicklung kortikaler Karten|
Recurrent cortical competition: Strengthen or weaken?,|
Contrast adaptation and infomax in visual cortical neurons|
Large-scale simulations of self-organizing neural networks on parallel computers: application to biological modelling|
NIPS 10|
Blind separation of spatial signal patterns from optical imaging records|
Temporal and spatial analysis of intrinsic signals from cat visual cortex|
Simulation of scanning laser techniques for optical imaging of blood-related intrinsic signals|,. 
Automatic 3-D-Graph Construction of Nerve Cells from Confocal|
Deblurring of confocal microscope images: Tissue properties, optics and simulations,|
Correction schemes for geometrically exact 3D reconstructions of neurons from confocal images|
Optimiertes Warping durch gewichtete Summen von Verschiebungsvektoren - eine neue Methode zur Reduktion von interindividuellen Variabilitten von Hirndaten|
Response stimulus correlation of a leaky integrate-and-fire neuron|
Fast adaptation as a cortical coding strategy|
Pulse detection in a colored noise setting|
Optimal noise-aided signal transmission through populations of neurons|
Approximating the response stimulus correlation in the integrate-and-fire neuron model|
A biologicallybased neural network model for geniculocortical information transfer in the primate visual system|
A mean field model for orientation tunin,contrast saturation and contextual eCects in the primary visual cortex,|
A Model for Orientation Tuning and Contextual Effects of Orientation Selective Receptive Fields|
Using unlabeled data for supervised learning,|
Solving large systems of differential equations in parallel using covers and skeletons|
Contextual effects by short range connections in a mean-field model of V1|
Active data selection for fuzzy topographic mapping of proximities|
Gene selection for microarray data|
Sphered support vector machine|
Rapid adaptation and efficient coding,|
Selforganizing maps and adaptive filters|
------, "Dyadic classification trees via structural risk minimization," in Advances in Neural Information Processing Systems 15,|
A neural network model for the formation of topographic maps in the CNS: Development of receptive fields|
Modeling the adaptive visual system: a survey of principled approaches|
A neural network model of geniculocortical information transfer in primate visual system|
Structural basis of multistationary quantum systems| I.  Effective single-particle dynamics",. 
editors|
Toroidal models of inter-key relations in tonal music|
Singularities in Primate Orientation Maps|
"Scale degree profiles from audio investigated with machine learning",|
A model for generation and sharpening of orientation selectivity with the same cortical microcircuits in macaque striate cortex|
An Analysis of Orientation and Ocular Dominance Patterns in the Visual Cortex of Cats and Ferrets|
Automatic three dimensional reconstruction of neurons from confocal images|
of nerve cells from confocal microscope scans|
A model for the development of the retinotectal projection in goldfish|
Supervised learning of preference relations|
\Regularized second order source separation,"|
Application of Blind Separation of Sources to Optical Recording of Brain Activity|
Classification of pairwise proximity data with support vectors|
Emergent Neural Computational Architectures, chapter Emergence of modularity within one sheet of neurons: a model comparison,|
Weak models in the analysis of optical imaging data: Hemoglobin changes following neural activity|
A statistical neural field approach to orientation selectivity| Neural Computing --- Proceedings of the Computational Neuroscience Meeting CNS 98,. 
Modelling contrast adaptation and contextual effects in primary visual cortex|
Visualization of synaptic markers in the drosophila optic neuropils using a new constrained deconvolution method|
Correction methods for 3D reconstruction of multi-channel confocal images of neurons in wholemount preparations|
eds|,. 
Pfluger, "Correction methods for three-dimensional reconstructions from confocal images: I| tissue shrinking and axial scaling,. 
Networks for separation of sources that are superimposed and delayed," in Application of blind separation of sources to optical recording of brain activity,|
Point-Based Warping With Optimized Weighting Factors of Displacement Vectors|
Contrast adaptation in simple cells by changing the transmitter release probability,|
A self-organizing map for proximity data|
Active learning in self-organizing maps|
Fuzzy topographic kernel clustering,|

Tetris: Experiments with the LP Approach to Approximate DP| Abstract We study the linear programming (LP) approach to approximate dynamic programming (DP) through experiments with the game of Tetris.  Our empirical results suggest that the performance of policies generated by the approach is highly sensitive to how the problem is formulated and the discount factor.  Furthermore, we find that, using a state-sampling scheme of the kind proposed in [7], the simulation time required to generate an adequate number of constraints far exceeds the time taken to solve the resulting LP.  As an extension to the standard approximate LP approach, we examine a bootstrapped version wherein a sequence of LPs is solved, with the policy generated by each solution being used to sample constraints for the next LP.  Our empirical results demonstrate that this bootstrapped approach can amplify performance. 
Algorithms for GPS operation indoors and downtown| Abstract The proliferation of mobile devices and the emergence of wireless location-based services has generated consumer demand for availability of GPS in urban and indoor environments.  This demand calls for enhanced GPS algorithms that accommodate high degrees of signal attenuation and multipath effects unique to the "urban channel. " This paper overviews the market for location-based services and discusses algorithmic innovations that address challenges posed by urban environments. 
Average Cost Temporal--Difference Learning| ABSTRACT We propose a variant of temporal--difference learning that approximates average and differential costs of an irreducible aperiodic Markov chain.  Approximations are comprised of linear combinations of fixed basis functions whose weights are incrementally updated during a single endless trajectory of the Markov chain.  We presentaproof of convergence (with probability 1), and a characterization of the limit of convergence.  We also provide a bound on the resulting approximation error that exhibits an interesting dependence on the "mixing time" of the Markovchain.  The results parallel previous work bythe authors, involving approximations of discounted cost--to--go. 
An Analysis of Temporal-Difference Learning withFunction Approximation| ABSTRACT We discuss thetemporal-di#erence learning algorithm, as applied toapproximatingthe cost-to-go function of an in#nite-horizon discounted Markovchain.  The algorithm weanalyze updates parameters of a linear function approximator on{line, duringasingle endless trajectory of an irreducible aperiodic Markovchain with a finite or in#nitestate space.  We present a proof of convergence (with probability1),acharacterization of the limit of convergence, andaboundonthe resultingapproximation error.  Furthermore, our analysis is based on a new line of reasoningthat provides new intuition aboutthe dynamics of temporal-di#erence learning.  In addition to provingnew and stronger positive resultsthan those previously available, weidentify the significance of on-lineupdatingandpotential hazards associated withthe use of nonlinear function approximators.  First, we provethat divergence may occur when updates are not based on trajectories of the Markovchain.  This fact reconciles positive andnegative resultsthathave been discussed in the literature, regardingthe soundness of temporal-di#erence learning.  Second, we present an example illustratingthe possibilityof divergence when temporal-di#erence learningisusedinthe presence of a nonlinear function approximator. 
Consensus Propagation| Abstract We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network.  We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention.  Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature.  In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge. 
The Linear Programming Approach to Approximate Dynamic Programming| Abstract The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems.  We study an ecient method based on linear programming for approximating solutions to such problems.  The approach \#ts" a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function.  Wedevelop error bounds that offer performance guarantees and also guide the selection of both basis functions and \state-relevance weights" that influence quality of the approximation.  Experimental results in the domain of queueing network control provide empirical support for the methodology. 
Solitaire: Man Versus Machine| Abstract In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire.  This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules.  A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. 
Improving Eigenvector-Based Reputation Systems Against Collusions| Abstract Eigenvector based methods in general, and Google's PageRank algorithm for rating web pages in particular, have become an important component of information retrieval on the Web.  In this paper, we study the ecacy of, and countermeasures for, collusions designed to improve user rating in such systems.  We define a metric, called the amplification factor, which captures the amount of PageRankinflation obtained by a group due to collusions.  We prove that the amplification factor is at most O(1=#), where # is the reset probability of the PageRank random walk.  We show that colluding nodes (web-pages, blogs etc. ) can achieve this amplification upper bound and increase their rank signi#cantly in realistic settings; further, several natural schemes to address this problem are demonstrably inadequate.  We propose a relatively simple modification to PageRank which renders the algorithm insensitive to such collusion attempts.  Our scheme is based on the observation that nodes which cheat do so by \stalling" the random walk in a small portion of the web graph and, hence, their PageRank must be especially sensitive to the reset probability #.  We perform exhaustive simulations on the Web and Weblog graphs to demonstrate that our scheme successfully prevents colluding nodes from improving their rank, yielding an algorithm that is robust to gaming. 
Approximate Dynamic Programming via Linear Programming| Abstract The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large{scale stochastic control problems.  We study an ecient method based on linear programming for approximating solutions to such problems.  The approach \#ts" a linear combination of pre{selected basis functions to the dynamic programming cost{to{go function.  We develop bounds on the approximation error and present experimental results in the domain of queueing network control, providing empirical support for the methodology. 
On Average Versus Discounted Reward Temporal-Difference Learning| ABSTRACT We provide an analytical comparison between discounted and average reward temporal-difference (TD) learning with linearly parameterized approximations.  We first consider the asymptotic behavior of the two algorithms.  We showthat as the discountfactor approaches 1, the value function produced by discounted TD approaches the differential value function generated by average reward TD.  We further argue that if the constant function -- which is typically used as one of the basis functions in discounted TD -- is appropriately scaled, the transient behaviors of the two algorithms are also similar.  Our analysis suggests that the computational advantages of average reward TD that havebeen observed in some prior empirical work may have been caused by inappropriate basis function scaling rather than fundamental differences in problem formulations or algorithms. 
Approximate Linear Programming for Average-Cost Dynamic Programming| Abstract This paper extends our earlier analysis on approximate linear programming as an approach to approximating the cost-to-go function in a discounted-cost dynamic program [6].  In this paper, we consider the average-cost criterion and a version of approximate linear programming that generates approximations to the optimal average cost and differential cost function.  We demonstrate that a naive version of approximate linear programming prioritizes approximation of the optimal average cost and that this may not be well-aligned with the objective of deriving a policy with low average cost.  For that, the algorithm should aim at producing a good approximation of the differential cost function.  We propose a twophase variant of approximate linear programming that allows for external control of the relative accuracy of the approximation of the differential cost function over different portions of the state space via state-relevance weights.  Performance bounds suggest that the new algorithm is compatible with the objective of optimizing performance and provide guidance on appropriate choices for state-relevance weights. 
and an Application to Pricing High--Dimensional Financial Derivatives| ABSTRACT We develop a theory characterizing optimal stopping times for discrete-time ergodic Markov processes with discounted rewards.  The theory differs from prior work by its view of per-stage and terminal reward functions as elements of a certain Hilbert space.  In addition to a streamlined analysis establishing existence and uniqueness of a solution to Bellman's equation, this approach provides an elegant framework for the study of approximate solutions.  In particular, we propose a stochastic approximation algorithm that tunes weights of a linear combination of basis functions in order to approximate a value function.  We prove that this algorithm converges (almost surely) and that the limit of convergence has some desirable properties.  We discuss how variations on this line of analysis can be used to develop similar results for other classes of optimal stopping problems, including those involving independent increment processes, finite horizons, and two--player zero--sum games.  We illustrate the approximation method with a computational case study involving the pricing of a path--dependent financial derivative security that gives rise to an optimal stopping problem with a one--hundred--dimensional state space. 
Regression Methods for Pricing Complex American-Style Options| Abstract---We introduce and analyze a simulation-based approximate dynamic programming method for pricing complex American-style options, with a possibly high-dimensional underlying state space.  We work within a finitely parameterized family of approximate value functions, and introduce a variant of value iteration, adapted to this parametric setting.  We also introduce a related method which uses a single (parameterized) value function, which is a function of the time-state pair, as opposed to using a separate (independently parameterized) value function for each time.  Our methods involve the evaluation of value functions at a finite set, consisting of "representative" elements of the state space.  We show that with an arbitrary choice of this set, the approximation error can grow exponentially with the time horizon (time to expiration).  On the other hand, if representative states are chosen by simulating the state process using the underlying risk-neutral probability distribution, then the approximation error remains bounded. 
TD(0) Leads to Better Policies than Approximate Value Iteration| Abstract We consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost-to-go function over each partition is approximated by a constant.  We establish performance loss bounds for policies derived from approximations associated with fixed points.  These bounds identify benefits to having projection weights equal to the invariant distribution of the resulting policy.  Such projection weighting leads to the same fixed points as TD(0).  Our analysis also leads to the first performance loss bound for approximate value iteration with an average cost objective.  1 Preliminaries Consider a discrete-time communicating Markov decision process (MDP) with a finite state space S = {1, .  .  .  , |S|}.  At each state x 2 S, there is a finite set U x of admissible actions.  If the current state is x and an action u 2 U x is selected, a cost of g u (x) is incurred, and the system transitions to a state y 2 S with probability p xy (u).  For any x 2 S and u 2 U x , P y2S p xy (u) = 1.  Costs are discounted at a rate of # 2 (0, 1) per period.  Each instance of such an MDP is defined by a quintuple (S,
A Neuro-Dynamic Programming Approach to Retailer Inventory Management| ABSTRACT We present a model of two-echelon retailer inventory systems, and we cast the problem of generating optimal control strategies into the framework of dynamic programming.  We formulate two specific case studies, for which the underlying dynamic programming problems involve thirty-three and forty-six state variables, respectively.  Because of the enormity of these state spaces, classical algorithms of dynamic programming are inapplicable.  To address these complex problems, we develop approximate dynamic programming algorithms.  The algorithms are motivated by recent research in artificial intelligence involving simulation--based methods and neural network approximations, and they are representative of algorithms studied in the emerging field of neuro-dynamic programming.  We assess performance of resulting solutions relative to optimized s--type ("order--up--to") policies, which are generally accepted as reasonable heuristics for the types of problems we consider.  In both case studies, we are able to generate control strategies substantially superior to the heuristics, reducing inventory costs by approximately ten percent. 
Analysis of Temporal-Difference Learning with Function Approximation|
Valic inequalities for mixed 0--1 programs|
Controlling Stocks and Flows to Promote Quality: The Environment, with Applications to Physical and Human Capital|
Feature-Based Methods for Large Scale Dynamic Programming|
An analysis of belief propagation on the turbo decoding graph with Gaussian densities|
Distributed Optimization in Adaptive Networks|
Learning and Value Function Approximation in Complex Decision Problems|
On the existence of fixed points for approximate value iteration and temporal-di#erence learning|
Performance loss bounds for approximate value iteration with state aggregation| Under review with. 
Optimal stopping of markov processes: Hilbert space theory, approximation algorithms, and an application to pricing high-dimensional financial derivatives|
The linear programming approach to approximate dynamic programming|
On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming|
Planning Under Uncertainty in Complex Structured Environments|
A neurodynamic programming approach to retailer inventory management| Technical Report LIDS-P-?, Laboratory for Information and Decision Systems,. 
An Analysis of Temporal--Difference Learning with Function Approximation," the|
Learning and Value Function Approximation in Complex Decision Processes|
UV radiation effects on photosynthesis: the importance of near-surface thermoclines in a subarctic lake,|
Learning and value function approximation in complex decision problems|
Approximate Dynamic Programming via Linear Programming|
Average cost temporal-difference learning," submitted to|
\A Neuro-DynamicProgramming|
SODA daylighting resource,|
"A Non-Parametric Approach to MultiProduct Pricing|
Improving GPS Coverage and Continuity: Indoors and Downtown|
Neuro-dynamic programming: Overview and recent trends|
Urban GPS: Algorithms for GPS Operation Indoors and Downtown,"|
An Analysis of Turbo Decoding with Gaussian Densities|
An analysis of temporal-difference learning with function approximation|
Tetris: A study of randomized constraint sampling|
An analysis of temporal-di#erence leanring with function approximation|
Approximate Solutions to Optimal Stopping Problems|
Feature-based methods for large scale DP|
Consensus propagation| Preprint (submitted),. 
An Analysis of Temporal Difference Learning with Function Approximation|
A Generalized Kalman Filter for Fixed Point Approximation and Efficient Temporal Difference Learning|

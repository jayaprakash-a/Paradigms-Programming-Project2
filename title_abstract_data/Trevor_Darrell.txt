Articulatory Features for Robust Visual Speech Recognition| ABSTRACT Visual information has been shown to improve the performance of speech recognition systems in noisy acoustic environments.  However, most audio-visual speech recognizers rely on a clean visual signal.  In this paper, we explore a novel approach to visual speech modeling, based on articulatory features, which has potential benefits under visually challenging conditions.  The idea is to use a set of parallel SVM classifiers to extract different articulatory attributes from the input images, and then combine their decisions to obtain higher-level units, such as visemes or words.  We evaluate our approach in a preliminary experiment on a small audio-visual database, using several image noise conditions, and compare it to the standard viseme-based modeling approach. 
Fast 3D Model Acquisition from Stereo Images| Abstract We propose a fast 3D model acquisition system that aligns intensity and depth images, and reconstructs a textured 3D mesh.  3D views are registered with shape alignment based on intensity gradient constraints and a global registration algorithm.  We reconstruct the 3D model using a new Cubic Ray Projection merging algorithm which takes advantage of a novel data structure: the linked voxel space.  Finally, we present experiments to test the accuracy of our approach on 3D face modeling using real-time stereo images. 
Inferring 3D Structure with a Statistical Image-Based Shape Model| Abstract We present an image-based approach to infer 3D structure parameters using a probabilistic "shape+structure" model.  The 3D shape of an object class is represented by sets of contours from silhouette views simultaneously observed from multiple calibrated cameras, while structural features of interest on the object are denoted by a number of 3D locations.  A prior density over the multi-view shape and corresponding structure is constructed with a mixture of probabilistic principal components analyzers.  Given a novel set of contours, we infer the unknown structure parameters from the new shape's Bayesian reconstruction.  Model matching and parameter inference are done entirely in the image domain and require no explicit 3D construction.  Our shape model enables accurate estimation of structure despite segmentation errors or missing views in the input silhouettes, and it works even with only a single input view.  Using a training set of thousands of pedestrian images generated from a synthetic model, we can accurately infer the 3D locations of 19 joints on the body based on observed silhouette contours from real images. 
Fast Stereo-Based Head Tracking for Interactive Environments| Abstract We present a robust implementation of stereo-based head tracking designed for interactive environments with uncontrolled lighting.  We integrate fast face detection and drift reduction algorithms with a gradient-based stereo rigid motion tracking technique.  Our system can automatically segment and track a user's head under large rotation and illumination variations.  Precision and usability of our approach are compared with previous tracking methods for cursor control and target selection in both desktop and interactive room environments. 
Learning Joint Statistical Models for Audio-Visual Fusion and Segregation| Abstract People can understand complex auditory and visual information, often using one to disambiguate the other.  Automated analysis, even at a lowlevel, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates.  Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships.  We learn the joint distribution of the visual and auditory signals using a non-parametric approach.  First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation.  We then model the complicated stochastic relationships between the signals using a nonparametric density estimator.  These learned densities allow processing across signal modalities.  We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video. 
Conditional Random Fields for Object Recognition| Abstract We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. 
Reducing Drift in Parametric Motion Tracking| Abstract We develop a class of differential motion trackers that automatically stabilize when in finite domains.  Most differential trackers compute motion only relative to one previous frame, accumulating errors indefinitely.  We estimate pose changes between a set of past frames, and develop a probabilistic framework for integrating those estimates.  We use an approximation to the posterior distribution of pose changes as an uncertainty model for parametric motion in order to help arbitrate the use of multiple base frames.  We demonstrate this framework on a simple 2D translational tracker and a 3D, 6-degree of freedom tracker. 
Face-Responsive Interfaces: From Direct Manipulation to Perceptive Presence| and can be used to create perceptually-driven presence artefacts which convey real-time awareness of a remote space. 
Efficient Image Matching with Distributions of Local Invariant Features| Abstract Sets of local features that are invariant to common image transformations are an effective representation to use when comparing images; current methods typically judge feature sets' similarity via a voting scheme (which ignores co-occurrence statistics) or by comparing histograms over a set of prototypes (which must be found by clustering).  We present a method for efficiently comparing images based on their discrete distributions (bags) of distinctive local invariant features, without clustering descriptors.  Similarity between images is measured with an approximation of the Earth Mover's Distance (EMD), which quickly computes minimal-cost correspondences between two bags of features.  Each image's feature distribution is mapped into a normed space with a low-distortion embedding of EMD.  Examples most similar to a novel query image are retrieved in time sublinear in the number of examples via approximate nearest neighbor search in the embedded space.  We evaluate our method with scene, object, and texture recognition tasks. 
The ALIVE System: Wireless, Full-Body Interaction with Autonomous Agents| Abstract The cumbersomenatureof wired interfaces often limits the range of application of virtual environments.  In this paper we discuss the design and implementation of a novel system, called ALIVE, which allows unencumbered full-body interaction between a human participant and a
3D Pose Tracking with Linear Depth and Brightness Constraints| Abstract This paper explores the direct motion estimation problem assuming that video-rate depth information is available, from either stereo cameras or other sensors.  We use these depth measurements in the traditional linear brightness constraint equations, and we introduce a new depth constraint equation.  As a result, estimation of certain types of motion, such as translation in depth and rotations out of the image plane, becomes more robust.  We derive linear brightness and depth change constraint equations that govern the velocity field in 3-D for both perspective and orthographic camera projection models.  These constraints are integrated jointly over image regions according to a rigidbody motion model, yielding a single linear system to robustly track 3D object pose.  Results are shown for tracking the pose of faces in sequences of synthetic and real images.  For a color version of this paper and for video result sequences, see
Evaluating Look-to-Talk: A Gaze-Aware Interface in a Collaborative Environment| ABSTRACT We present "look-to-talk", a gaze-aware interface for directing a spoken utterance to a software agent in a multiuser collaborative environment.  Through a prototype and a Wizard-of-Oz (WOz) experiment, we show that "look-totalk" is indeed a natural alternative to speech and other paradigms. 
Fast Pose Estimation with Parameter-Sensitive Hashing| m a s s a c h u s e t t s i n s t i t u t e o f t e c h n o l o g y , c a m b r i d g e , m a 0 2 1 3 9 u s a --- www.  a i .  m i t .  e d u m a s s a c hu s e t t s i n s t i t u t e o f t e c hno l o g y --- a r t i f i c i a l i n t e l l i g e n c e l a bo r a t o r y @ MIT Abstract Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low.  For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly becme prohibitively high.  We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for locality-sensitive hashing, which finds approximate neighbors in time sublinear in the number of examples.  This method depends critically on the choice of hash functions; we show how to find the set of hash functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images. 
Reaching for Dexterous Manipulation MIT EECS Area Exam| Abstract I review three papers in the area of dexterous manipulation by machines.  These papers focus on different stages of the process of grasping an object.  The first describes a method for generating a suitable hand pose to grasp an object at a given set of contact points.  The second describes a method for adapting an example manipulation task from a given object to a new object.  The third describes a control-theoretic method for iteratively improving the quality of a given grasp of an object of unknown geometry and using only tactile sensory information.  These three papers are good examples of mainstream work in dexterous manipulation.  Following a brief review of the essential idea presented by each of these three papers, I consider the relationships between each of these approaches, focusing on the larger issues that loom in the differences between them.  Finally, I propose some new ideas to try to open the door to dexterous manipulation in more ordinary, unstructured human environments. 
From Conversational Tooltips to Grounded Discourse: Head Pose Tracking in Interactive Dialog Systems| ABSTRACT Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people.  While the machine interpretation of these cues has previously been limited to output modalities, recent advances in facepose tracking allow for systems which are robust and accurate enough to sense natural grounding gestures.  We present the design of a module that detects these cues and show examples of its integration in three different conversational agents with varying degrees of discourse model complexity.  Using a scripted discourse model and off-the-shelf animation and speech-recognition components, we demonstrate the use of this module in a novel "conversational tooltip" task, where additional information is spontaneously provided by an animated character when users attend to various physical objects or characters in the environment.  We further describe the integration of our module in two systems where animated and robotic characters interact with users based on rich discourse and semantic models. 
T| Media Laboratory Perceptual Computing Group Technical Report No.  Abstract Hand and face gestures are modeled using an appearance-based approach in which patterns are represented as a vector of similarity scores to a set of view models defined in space and time.  These view models are learned from examples using unsupervised clustering techniques.  A supervised learning paradigm is used to interpolate view scores into a task-dependent coordinate system appropriate for recognition and control tasks.  We apply this analysis to the problem of context-specific gesture interpolation and recognition, and demonstrate real-time systems which perform these tasks. 
A Virtual Mirror Interface Using Real-Time Robust Face Tracking| Abstract We describe a virtual mirror interface which can react to people using robust, real-time face tracking.  Our display can directly combine a user's face with various graphical effects, performed only on the face region in the image.  We have demonstrated our system in crowded environments with open and moving backgrounds.  Robust performance is achieved using multi-modal integration, combining stereo, color, and grey-scale pattern matching modules into a single real-time system.  Stereo processing is used to isolate the figure of a user from other objects and people in the background.  Skin-hue classification identifies and tracks likely body parts within the foreground region.  Face pattern detection discriminates and localizes the face within the tracked body parts.  We show an initial application of the mirror where the user sees his or her face distorted into various comic poses.  Qualitatively, users of the system felt the display "knew" where their face was, and provided entertaining imagery.  We discuss the failure modes of the individual components, and quantitatively analyze the face localization performance of the complete system with thousands of users in recent trials. 
Constraining Human Body Tracking| Abstract Our paper addresses the problem of enforcing constraints in human body tracking.  A projection technique is derived to impose kinematic constraints on independent multi-body motion: we show that for small motions the multi-body articulated motion space can be approximated by a linear manifold estimated directly from the previous body pose.  We propose a learning approach to model non-linear constraints; we train a support vector classifier from motion capture data to model the boundary of the space of valid poses.  Linear and non-linear body pose constraints are enforced by first projecting unconstrained motions onto the articulated motion space and then optimizing to find points on this linear manifold that lie within the non-linear constraint surface modeled by the SVM classifier. 
Recovering Articulated Model Topology from Observed Rigid Motion| Abstract Accurate representation of articulated motion is a challenging problem for machine perception.  Several successful tracking algorithms have been developed that model human body as an articulated tree.  We propose a learning-based method for creating such articulated models from observations of multiple rigid motions.  This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known.  Our approach is based on finding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions.  The topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body.  We demonstrate the performance of our algorithm on both synthetic and real motion capture data. 
Adaptive View-Based Appearance Models| Abstract We present a method for online rigid object tracking using an adaptive view-based appearance model.  When the object's pose trajectory crosses itself, our tracker has bounded drift and can track objects undergoing large motion for long periods of time.  Our tracker registers each incoming frame against the views of the appearance model using a twoframe registration algorithm.  Using a linear Gaussian filter, we simultaneously estimate the pose of the object and adjust the view-based model as pose-changes are recovered from the registration algorithm.  The adaptive view-based model is populated online with views of the object as it undergoes different orientations in pose space, allowing us to capture non-Lambertian effects.  We tested our approach on a real-time rigid object tracking task using stereo cameras and observed an RMS error within the accuracy limit of an attached inertial sensor. 
Integrated Person Tracking Using Stereo, Color, and Pattern Detection| Abstract.  We present an approach to real-time person tracking in crowded and/or unknown environments using integration of multiple visual modalities.  We combine stereo, color, and face detection modules into a single robust system, and show an initial application in an interactive, face-responsive display.  Dense, real-time stereo processing is used to isolate users from other objects and people in the background.  Skin-hue classification identifies and tracks likely body parts within the silhouette of a user.  Face pattern detection discriminates and localizes the face within the identified body parts.  Faces and bodies of users are tracked over several temporal scales: short-term (user stays within the field of view), medium-term (user exits/reenters within minutes), and long term (user returns after hours or days).  Short-term tracking is performed using simple region position and size correspondences, while medium and long-term tracking are based on statistics of user appearance.  We discuss the failure modes of each individual module, describe our integration method, and report results with the complete system in trials with thousands of users. 
Separation of Transparent Motion into Layers using Velocity-Tuned Mechanisms| ABSTRACT This paper presents a model for the perception of transparently combined moving images.  We advocate a framework consisting of a local motion mechanism which can operate in the presence of transparency, and a global mechanism that integrates information across space.  We present a new method for the local motion testing mechanism, using "donut" velocity selective mechanisms formed from the weighted combination of spatio-temporal energy units.  This method has the advantage over traditional methods that it does not fail when there are multiple motions in the sequence.  The global layer selection mechanism attempts to account for the local velocity distributions with a small set of global functions.  Using donut mechanisms permits a simplified layer selection optimization, in which inhibition between layers is determined by the product of their predicted velocity distributions.  With this scheme, we demonstrate the decomposition of image sequences containing additively combined multiple moving objects into a set of layers corresponding to each object. 
Location Estimation with a Differential Update Network| Abstract Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available.  The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement.  We demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a 2D trajectory.  To update a trajectory of length t, the update takes O(t).  When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simplifies the state covariance matrix after incorporating each measurement. 
Tracking Facial Motion| Abstract We describe a computer system that allows real-time tracking of facial expressions.  Sparse, fast visual measurements using 2-D templates are used to observe the face of a subject.  Rather than track features on the face, the distributed response of a set of templates is used to characterize a given facial region.  These measurements are coupled via a linear interpolation method to states in a physically-based model of facial animation, which includes both skin and muscle dynamics.  By integrating real-time 2D image-processing with 3-D models, we obtain a system that is able to quickly track and interpret complex facial motions. 
Background Estimation and Removal Based on Range and Color| Abstract Background estimation and removal based on the joint use of range and color data produces superior results than can be achieved with either data source alone.  This is increasingly relevant as inexpensive, real-time, passive range systems become more accessible through novel hardware and increased CPU processing speeds.  Range is a powerful signal for segmentation which is largely independent of color, and hence not effected by the classic color segmentation problems of shadows and objects with color similar to the background.  However, range alone is also not sufficient for the good segmentation: depth measurements are rarely available at all pixels in the scene, and foreground objects may be indistinguishable in depth when they are close to the background.  Color segmentation is complementary in these cases.  Surprisingly, little work has been done to date on joint range and color segmentation.  We describe and demonstrate a background estimation method based on a multidimensional (range and color) clustering at each image pixel.  Segmentation of the foreground in a given frame is performed via comparison with background statistics in range and normalized color.  Important implementation issues such as treatment of shadows and low confidence measurements are discussed in detail. 
Active Face Tracking and Pose Estimation in an Interactive Room| Abstract We demonstrate real-time face tracking and pose estimation in an unconstrained office environment with an active foveated camera.  Using vision routines previously implemented for an interactive environment, we determine the spatial location of a user's head and guide an active camera to obtain foveated images of the face.  Faces are analyzed using a set of eigenspaces indexed over both pose and world location.  Closed loop feedback from the estimated facial location is used to guide the camera when a face is present in the foveated view.  Our system can detect the head pose of an unconstrained user in real-time as he or she moves about an open room. 
Nodding in conversations with a robot| ABSTRACT In this demo we describe our ongoing efforts to build a robot that can collaborate with a person in hosting activities.  We illustrate our current robot's conversations, which include gestures of various types, and report on extensions to the robot's existing gestural abilities to be able to recognize nodding in conversations. 
Pose Estimation using 3D View-Based Eigenspaces| Abstract In this paper we present a method for estimating the absolute pose of a rigid object based on intensity and depth viewbased eigenspaces, built across multiple views of example objects of the same class.  Given an initial frame of an object with unknown pose, we reconstruct a prior model for all views represented in the eigenspaces.  For each new frame, we compute the pose-changes between every view of the reconstructed prior model and the new frame.  The resulting pose-changes are then combined and used in a Kalman filter update.  This approach for pose estimation is userindependent and the prior model can be initialized automatically from any view point of the view-based eigenspaces.  To track more robustly over time, we present an extension of this pose estimation technique where we integrate our prior model approach with an adaptive differential tracker.  We demonstrate the accuracy of our approach on face pose tracking using stereo cameras. 
Towards Context-Based Visual Feedback Recognition for Embodied Agents| Abstract Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people.  We investigate how contextual information can improve visual recognition of feedback gestures during interactions with embodied conversational agents.  We present a visual recognition model that integrates cues from the spoken dialogue of an embodied agent with direct observation of a user's head pose.  In preliminary experiments using a discriminative framework, contextual information improved the performance of head nod detection. 
Perceptive Spaces for Performance and Entertainment Untethered Interaction Using Computer Vision and Audition| Abstract Bulky head-mounted displays, data gloves, and severely limited movement have become synonymous with virtual environments.  This is unfortunate since virtual environments have such great potential in applications such as entertainment, animation by example, design interface, information browsing, and even expressive performance.  In this paper we describe an approach to unencumbered, natural interfaces called Perceptive Spaces.  The spaces are unencumbered because they utilize passive sensors that don't require special clothing and large format displays that don't isolate the user from their environment.  The spaces are natural because the open environment facilitates active participation.  Several applications illustrate the expressive power of this approach, as well as the challenges associated with designing these interfaces. 
Fast Contour Matching Using Approximate Earth Mover's Distance| Abstract Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost matching between two shapes' features often reveals how similar the shapes are.  However, due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases.  We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the Earth Mover's Distance (EMD) into a normed space.  Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search with Locality-Sensitive Hashing (LSH).  We demonstrate our shape matching method on a database of 136,500 images of human figures.  Our method achieves a speedup of four orders of magnitude over the exact method, at the cost of only a 4% reduction in accuracy. 
Space-Time Gestures,|
Modeling, Tracking and Interactive Animation of Faces and Heads Using Input from Video|
Robust estimation of a multi-layered motion representation|
The ALIVE system: full-body interaction with autonomous agents|
Cooperative Robust Estimation Using Layers of Support|
Pfinder: real-time tracking of the human body|
Bayesian network for online global pose estimation|
Stereo Tracking Using ICP and Normal Flow Constraint|
Segmentation by minimal description|
Classifying Hand Gestures with a View-Based Distributed Representation|
On Probabilistic Combination of Face and Gait Cues for Identification|
Informative subspaces for audiovisual processing: High-level function from low-level fusion,|
Space-time gesture|
Attention-driven Expression and Gesture Analysis in an Interactive Environment,|
A novel environment for situated vision and behavior|
Cooperative Robust Estimation Using Layers of Support, M|I. T Media Vision and Modeling Group Tech Report,. 
Multimodal Interfaces that Flex, Adapt, and Persist," (eds|) special issue,. 
Nulling' filters and the separation of transparent motions|
Active Gesture Recognition using Learned Visual Attention|
Real-time tracking of the human body|
Modeling and interactive animation of facial expressions using vision|
Plan-View Trajectory Estimation with Dense Stereo Background Models|
Activity Zones for Context-Aware Computing|
Single Range Camera Based on Focal Error|
Visual perception of human bodies and faces for multi-modal interfaces,|
Second-order method for occlusion relationships in motion layers",|
Integrated Face and Gait Recognition From Multiple Views|
Face Recognition from Long-Term Observations|
A probabilistic framework for multi-modal multi-person tracking|
Towards context based vision feedback recognition for embodied agents|
Interactive vision using hidden state decision processes|". 
Conditional random fields for object recognition|
Articulated-Pose Estimation Using Brightness and Depth-Constancy Constraints|
PSFIG - A DITROFF Preprocessor for Postscript Figures|
Pfinder: Real-Time Tracking of the Human Body", MIT Media Lab,|
Recognition of Space-Time Gestures using a Distributed Representation, MIT Media Laboratory Vision and Modeling TR-|
Pyramid based depth from focus|
A simple, real-time range camera|
Evolving Visual Routines|
Privacy In Context ", The journal of HumanComputer Interaction, Special Issue on Context -Aware Computing, volu|
Active gesture recognition using partially observable Markov decision processes|
Task-Specific Gesture Analysis in Real-Time Using Interpolated Views|
Recognition of space-time gestures using a distributed representation|
Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis|
On the use of ``nulling" filters to separate transparent motions|
Visually Guided Animation,|
Gesture + play: full-body interaction for virtual environments|
Probabilistic models and informative subspaces for audiovisual correspondence,|
Motion Estimation from Disparity Images|
A Bayesian Approach to Image-Based Visual Hull Reconstruction|
Combining Simple Models to Approximate Complex Dynamics|
Perceptually-driven Avatars and Interfaces: Active Methods for Direct Control|
Recognition of space--time gestures using distributed representation|
Approximate nearest neighbors methods for learning and vision|
Haptics and biometrics: A multimodal approach for determining speaker location and focus|
Interacting with animated autonomous agents|
Speaker association with signal-level audiovisual fusion,|
Perceptive spaces for peformance and entertainment|
A Simple, Real-Tie Range Camera,|
Untethered gesture acquisition and recognition for a multimodal conversational system|
pfinder: A Real-Time System for Tracking People,|
Dynamic Occluding Contours: A New External-Energy Term for Snakes|
IDeixis - Searching the Web with Mobile Images for Location-Based Information|
Tracking and interactive animation of faces and heads using input from video|
Example-Based Image Synthesis of Articulated Figures|
ALIVE: Artificial Life Interactive Video Environment|
``ALIVE: Dreams and Illusions"|
Fast contour matching using approximate Earth Mover's Distance|
Situated vision and behavior for interactiveenvironments|
Face recognition from long-term observations, in:|
Tracking people with integrated stereo, color, and face detection|
Evolving Visual Routines," in Artificial Life IV,|
Pyramid Match Kernels: Discriminative Classification with Sets of Image Features|
Filters and the Separation of Transparent Motions|
On the representation of occluded shapes|
Magic Morphin' Mirror: Person Detection and Tracking|
Mass hallucinations|
From conversational tooltips to grounded discourse: Head pose tracking in interactive dialog systems|
Nodding in conversations with a robot|
Simultaneous Calibration and Tracking with a Network of Non-Overlapping Sensors|
Interval Research Corp| Unpublished work, including real-time, plan-view, multiple-person tracker using multiple stereo camera heads. 
Integrated person tracking using stereo, color and pattern detection|
Multiple person and speaker activity tracking with a particle filter,|
A ViewBased Appearance Model for 6 DOF Tracking," Proceed-ings of|
Signal Level Fusion for Multimodal Perceptual Interface,|
"Vision interface group|". 
Discontinuity models and multi-layer description networks|
\A radial cumulative transform for robust image correspondence,"|
Robust, real-time people tracking in open environments using integrated stereo, color, and face detection",|
Robust estimation of multiple models using support maps|
Gesture + play: full-body interaction for virtual environments|
Pfinder: Real time tracking of the human body| Media Lab Tech Report 353,. 
Correspondence with Cumulative Similarity Transforms,|

Experts in a Markov Decision Process| Abstract We consider an MDP setting in which the reward function is allowed to change during each time step of play (possibly in an adversarial manner), yet the dynamics remain fixed.  Similar to the experts setting, we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time.  We provide efficient algorithms, which have regret bounds with no dependence on the size of state space.  Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions.  We also show that in the case that the dynamics change over time, the problem becomes computationally hard. 
Worst-Case Bounds for Gaussian Process Models| Abstract We present a competitive analysis of some non-parametric Bayesian algorithms in a worst-case online learning setting, where no probabilistic assumptions about the generation of the data are made.  We consider models which use a Gaussian process prior (over the space of all functions) and provide bounds on the regret (under the log loss) for commonly used non-parametric Bayesian algorithms --- including Gaussian regression and logistic regression --- which show how these algorithms can perform favorably under rather general conditions.  These bounds explicitly handle the infinite dimensionality of these non-parametric classes in a natural way.  We also make formal connections to the minimax and minimum description length (MDL) framework.  Here, we show precisely how Bayesian Gaussian regression is a minimax strategy. 
An Alternate Objective Function for Markovian Fields| Abstract In labelling or prediction tasks, a trained model's test performance is often based on the quality of its single-time marginal distributions over labels rather than its joint distribution over label sequences.  We propose using a new cost function for discriminative learning that more accurately reflects such test time conditions.  We present an ecient method to compute the gradient of this cost for Maximum Entropy Markov Models, Conditional Random Fields, and for an extension of these models involving hidden states.  Our experimental results show that the new cost can give significant improvements and that it provides a novel and effective way of dealing with the 'label-bias' problem. 
Graphical Economics| Abstract: We introduce a graph-theoretic generalization of classical Arrow-Debreu economics,
Dopamine Bonuses| Abstract Substantial data support a temporal difference (TD) model of dopamine (DA) neuron activity in which the cells provide a global error signal for reinforcement learning.  However, in certain circumstances, DA activity seems anomalous under the TD model, responding to non-rewarding stimuli.  We address these anomalies by suggesting that DA cells multiplex information about reward bonuses, including Sutton's exploration bonuses and Ng et al's non-distorting shaping bonuses.  We interpret this additional role for DA in terms of the unconditional attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. 
Deterministic Calibration and Nash Equilibrium| Abstract.  We provide a natural learning process in which the joint frequency of empirical play converges into the set of convex combinations of Nash equilibria.  In this process, all players rationally choose their actions using a public prediction made by a deterministic, weakly calibrated algorithm.  Furthermore, the public predictions used in any given round of play are frequently close to some Nash equilibrium of the game. 
Optimizing Average Reward Using Discounted Rewards| Abstract.  In many reinforcement learning problems, it is appropriate to optimize the average reward.  In practice, this is often done by solving the Bellman equations using a discount factor close to 1.  In this paper, we provide a bound on the average reward of the policy obtained by solving the Bellman equations which depends on the relationship between the discount factor and the mixing time of the Markov chain.  We extend this result to the direct policy gradient of Baxter and Bartlett, in which a discount parameter is used to find a biased estimate of the gradient of the average reward with respect to the parameters of a policy.  We show that this biased gradient is an exact gradient of a related discounted problem and provide a bound on the optima found by following these biased gradients of the average reward.  Further, we show that the exact Hessian in this related discounted problem is an approximate Hessian of the average reward, with equality in the limit the discount factor tends to 1.  We then provide an algorithm to estimate the Hessian from a sample path of the underlying Markov chain, which converges with probability 1. 
Economic Properties of Social Networks| Abstract We examine the marriage of recent probabilistic generative models for social networks with classical frameworks from mathematical economics.  We are particularly interested in how the statistical structure of such networks influences global economic quantities such as price variation.  Our findings are a mixture of formal analysis, simulation, and experiments on an international trade data set from the United Nations. 
2002 Special issue Dopamine: generalization and bonuses| Abstract In the temporal difference model of primate dopamine neurons, their phasic activity reports a prediction error for future reward.  This model is supported by a wealth of experimental data.  However, in certain circumstances, the activity of the dopamine cells seems anomalous under the model, as they respond in particular ways to stimuli that are not obviously related to predictions of reward.  In this paper, we address two important sets of anomalies, those having to do with generalization and novelty.  Generalization responses are treated as the natural consequence of partial information; novelty responses are treated by the suggestion that dopamine cells multiplex information about reward bonuses, including exploration bonuses and shaping bonuses.  We interpret this additional role for dopamine in terms of the mechanistic attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. 
Online Bounds for Bayesian Algorithms| Abstract We present a competitive analysis of Bayesian learning algorithms in the online learning setting and show that many simple Bayesian algorithms (such as Gaussian linear regression and Bayesian logistic regression) perform favorably when compared, in retrospect, to the single best model in the model class.  The analysis does not assume that the Bayesian algorithms' modeling assumptions are "correct," and our bounds hold even if the data is adversarially chosen.  For Gaussian linear regression (using logloss), our error bounds are comparable to the best bounds in the online learning literature, and we also provide a lower bound showing that Gaussian linear regression is optimal in a certain worst case sense.  We also give bounds for some widely used maximum a posteriori (MAP) estimation algorithms, including regularized logistic regression. 
In NIPS 12 Acquisition in Autoshaping| Abstract Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning.  However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude.  We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli play a crucial role. 
Acquisition and Extinction in Autoshaping| Abstract Gallistel and Gibbon (2000) have presented quantitative data on the speed with which animals acquire behavioral responses during autoshaping, together with a statistical model of learning intended to account for them.  Although this model captures the form of the dependencies amongst critical variables, its detailed predictions are substantially at variance with the data.  In the present article, further key data on the speed of acquisition are used to motivate an alternative model of learning, in which animals can be interpreted as paying different amounts of attention to stimuli according to estimates of their differential reliabilities as predictors. 
Trading in Markovian Price Models| Abstract We examine a Markovian model for the price evolution of a stock, in which the probability of local upward or downward movement is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information).  Our main result is a "universally profitable" trading strategy --- a single fixed strategy whose profitability competes with the optimal strategy, which knows all of the underlying parameters of the infinite and possibly nonstationary Markov process. 
Competitive algorithms for VWAP and limit order trading| ABSTRACT We introduce new online models for two important
Correlated equilibria in graphical games| ABSTRACT We examine correlated equilibria in the recently introduced formalism of graphical games, a succinct representation for multiplayer games.  We establish a natural and powerful relationship between the graphical structure of a multiplayer game and a certain Markov network representing distributions over joint actions.  Our first main result establishes that this Markov network succinctly represents all correlated equilibria of the graphical game up to expected payoff equivalence.  Our second main result provides a general algorithm for computing correlated equilibria in a graphical game based on its associated Markov network.  For a special class of graphical games that includes trees, this algorithm runs in time polynomial in the graphical game representation (which is polynomial in the number of players and exponential in the graph degree). 
Exploration in Metric State Spaces| Abstract We present metric-4 , a provably near-optimal algorithm for reinforcement learning in Markov decision processes in which there is a natural metric on the state space that allows the construction of accurate local models.  The algorithm is a generalization of the he algorithm of Kearns and Singh, and assumes a black box for approximate planning.  Unlike the originalri , metricme finds a near optimal policy in an amount of time that does not directly depend on the size of the state space, but instead depends on the covering number of the state space.  Informally, the covering number is the number of neighborhoods required for accurate local modeling. 
Explaining Away in Weight Space| Abstract Explaining away has mostly been considered in terms of inference of states in belief networks.  We show how it can also arise in a Bayesian context in inference about the weights governing relationships such as those between stimuli and reinforcers in conditioning experiments such as backward blocking.  We show how explaining away in weight space can be accounted for using an extension of a Kalman filter model; provide a new approximate way of looking at the Kalman gain matrix as a whitener for the correlation matrix of the observation process; suggest a network implementation of this whitener using an architecture due to Goodall; and show that the resulting model exhibits backward blocking. 
Acquisition in Autoshaping| Abstract Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning.  However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude.  We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli play a crucial role. 
Reinforcement Learning in POMDPs| Abstract We consider the most realistic reinforcement learning setting in which an agent starts in an unknown environment (the POMDP) and must follow one unbroken chain of experience with no access to "resets" or "offline" simulation.  We provide algorithms for general POMDPs that obtain near optimal average reward.  One algorithm we present has a convergence rate which depends exponentially on a certain horizon time of an optimal policy, but has no dependence on the number of (unobservable) states.  The main building block of our algorithms is an implementation of an approximate reset strategy, which we show always exists in every POMDP.  An interesting aspect of our algorithms is how they use this strategy when balancing exploration and exploitation. 
Competitive Analysis of the Explore/Exploit Tradeoff| Abstract We investigate the explore/exploit trade-off in reinforcement learning using competitive analysis applied to an abstract model.  We state and prove lower and upper bounds on the competitive ratio.  The essential conclusion of our analysis is that optimizing the explore/exploit trade-off is much easier with a few pieces of extra knowledge such as the stopping time or upper and lower bounds on the value of the optimal exploitation policy. 
Approximately Optimal Approximate Reinforcement Learning| Abstract In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used.  In this paper, we present the conservative policy iteration algorithm which finds an #approximately# optimal policy, given access to a restart distribution (which draws the next state from a particular distribution) and an approximate greedy policy chooser.  Crudely, the greedy policy chooser outputs a policy that usually chooses actions with the largest state-action values of the current policy, ie it outputs an #approximate# greedy policy.  This greedy policy chooser can be implemented using standard value function approximation techniques.  Under these assumptions, our algorithm: (1) is guaranteed to improve a performance metric (2) is guaranteed to terminate in a #small# number of timesteps and (3) returns an #approximately# optimal policy.  The quantifled statements of (2) and (3) depend on the quality of the greedy policy chooser, but not explicitly on the the size of the state space. 
Maximum Entropy Correlated Equilibria| Abstract We study maximum entropy correlated equilibria in (multi-player) games and provide two gradient-based algorithms that are guaranteed to converge to such equilibria.  Although we do not provide convergence rates for the algorithms, they do have strong connections to similar algorithms that have been found to be effective heuristics in practice for tasks such as statistical estimation. 
A Natural Policy Gradient| Abstract We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space.  Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action.  These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al.  [9].  We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 
Acetylcholine in Cortical Inference| Abstract Acetylcholine (ACh) plays an important role in a wide variety of cognitive tasks, such as perception, selective attention, associative learning, and memory.  Extensive experimental and theoretical work in tasks involving learning and memory has suggested that ACh reports on unfamiliarity and controls plasticity and effective network connectivity.  Based on these computational and mechanistic insights, we develop a theory of cholinergic modulation in perceptual inference.  We propose that ACh levels reflect the uncertainty associated with top-down information, and have the effect of modulating the interaction between top-down and bottom-up processing in determining the appropriate neural representations for inputs.  We illustrate our proposal by means of an hierarchical hidden Markov model, showing that cholinergic modulation of contextual information leads to appropriate perceptual inference. 
Learning and selective attention|
rewarded during each block| In order to discourage the animals. 
Dopamine: Generalization and bonuses|
On the Sample Complexity of Reinforcement Learning|
Worst-case bounds for some non-parametric bayesian methods,|
Deterministic Calibration and Nash Equilibrium," University of Pennsylvania (mimeo)|
Cover trees for nearest neighbor|

Analysis of Switching Dynamics with Competing Support Vector Machines| Abstract We present a framework for the unsupervised segmentation of switching dynamics using support vector machines.  Following the architecture by Pawelzik et al.  [21] where annealed competing neural networks were used to segment a non-stationary time series, in this article we exploit the use of support vector machines, a well-known learning technique.  First, a new formulation of support vector regression is proposed.  Second, an expectation-maximization (EM) step is suggested to adaptively adjust the annealing parameter.  Results indicate that the proposed approach is promising. 
Decomposition Methods for Linear Support Vector Machines| Abstract In this paper, we show that decomposition methods with alpha seeding are extremely useful for solving a sequence of linear SVMs with more data than attributes.  This strategy is motivated from (Keerthi and Lin 2003) which proved that for an SVM with data not linearly separable, after C is large enough, the dual solutions are at the same face.  We explain why a direct use of decomposition methods for linear SVMs is sometimes very slow and then analyze why alpha seeding is much more effective for linear than nonlinear SVMs.  We also conduct comparisons with other methods which are efficient for linear SVMs, and demonstrate the effectiveness of alpha seeding techniques for helping the model selection. 
Vector Classifiers: Theory and Algorithms| Abstract The #-support vector machine (#-SVM) for classification proposed by Scholkopf et al.  has the advantage of using a parameter # on controlling the number of support vectors.  In this paper, we investigate the relation between #-SVM and C-SVM in detail.  We show that in general they are two different problems with the same optimal solution set.  Hence we may expect that many numerical aspects on solving them are similar.  However, comparing to regular C-SVM, its formulation is more complicated so up to now there are no effective methods for solving large-scale #-SVM.  We propose a decomposition method for #-SVM which is competitive with existing methods for C-SVM.  We also discuss the behavior of #-SVM by some numerical experiments. 
LIBSVM: Introduction and Benchmarks| Abstract Most available support vector machines (SVM) software are either quite complicated or are not suitable for large problems.  Instead of seeking a very fast software for dicult problems, we provide a simple, easy-to-use, and moderately ecient package for SVM classification: LIBSVM.  It is a simplification of both SMO by Platt and SV M light by Joachims.  LIBSVM is also a simplification of the modification 2 of SMO by Keerthi et al.  We hope it helps users from other fields to easily use SVM as a tool.  This document includes the introduction of the software and benchmarks. 
An Incomplete Cholesky Factorization for Dense Matrices| Abstract In this paper, we study the use of an incomplete Cholesky factorization (ICF) as a preconditioner for solving dense positive definite linear systems.  Analysis and implementation of this preconditioner are discussed.  We test the proposed ICF on randomly generated systems and large matrices from two practical applications: semidefinite programming and support vector machines.  Numerical comparison with the diagonal preconditioner is also presented. 
Simple Probabilistic Predictions for Support Vector Regression| Abstract Support vector regression (SVR) has been popular in the past decade, but it provides only an estimated target value instead of predictive probability intervals.  Many work have addressed this issue but sometimes the SVR formula must be modified.  This paper presents a rather simple and direct approach to construct such intervals.  We assume that the conditional distribution of the target value depends on its input only through the predicted value, and propose to model this distribution by simple functions.  Experiments show that the proposed approach gives predictive intervals with competitive coverages with Bayesian SVR methods. 
Leave-one-out Bounds for Support Vector Regression Model Selection| Abstract Minimizing bounds of leave-one-out (loo) errors is an important and efficient approach for support vector machine (SVM) model selection.  Past research focuses on their use for classification but not regression.  In this article, we derive various loo bounds for support vector regression (SVR) and discuss the difference from those for classification.  Experiments demonstrate that the proposed bounds are competitive with Bayesian SVR for parameter selection.  We also discuss the differentiability of loo bounds. 
Radius Margin Bounds for Support Vector Machines with the RBF Kernel| Abstract An important approach for efficient support vector machine (SVM) model selection is to use differentiable bounds of the leave-one-out (loo) error.  Past efforts focused on finding tight bounds of loo, for example, radius margin bounds, span bounds, etc.  However, their practical viability is still not very satisfactory.  In (Duan et al. , 2003), it has been shown that radius margin bound gives good prediction for L2-SVM.  In this paper, through the analyses why this bound performs well for L2-SVM, we show that finding a bound whose minima are in a region with small loo values may be more important than its tightness.  Based on this principle we propose modified radius margin bounds for L1-SVM where the original bound is only applicable to the hard-margin case.  Our modification for L1-SVM achieves comparable performance to L2-SVM.  To study whether L1- or L2-SVM should be used, we further analyze other properties such as their differentiability, number of support vectors, and number of free support vectors.  In this aspect, L1-SVM possesses the advantage of having fewer support vectors.  Their implementations are also different so we discuss related issues in detail. 
A Note on Platt's Probabilistic Outputs for Support Vector Machines| Abstract Platt's probabilistic outputs for Support Vector Machines [6] has been popular for applications that require posterior class probabilities.  In this note, we propose an improvement which theoretically converges and avoids numerical difficulties.  A simpler and ready-to-use pseudo code is included. 
EUNITE Network Competition: Electricity Load Forecasting| Abstract.  EUNITE network recently organized a world-wide
Load Forecasting Using Support Vector Machines: A Study on EUNITE Competition 2001| Abstract---Load forecasting is usually made by constructing models on relative information, such as climate and previous load demand data.  In 2001 EUNITE network organized a competition aiming at mid-term load forecasting (predicting daily maximum load of the next 31 days).  During the competition we proposed a support vector machine (SVM) model, which was the winning entry, to solve the problem.  In this paper, we discuss in detail how SVM, a new learning technique, is successfully applied to load forecasting.  In addition, motivated by the competition results and the approaches by other participants, more experiments and deeper analyses are conducted and presented here.  Some important conclusions from the results are that temperature (or other types of climate information) might not be useful in such a midterm load forecasting problem and that the introduction of timeseries concept may improve the forecasting. 
The Analysis of Decomposition Methods for Support Vector Machines| Abstract The support vector machine (
A Practical Guide to Support Vector Classification| Abstract Support vector machine (SVM) is a popular technique for classification.  However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but significant steps.  In this guide, we propose a simple procedure which usually gives reasonable results. 
Projected Gradient Methods for Non-negative Matrix Factorization| Abstract Non-negative matrix factorization (NMF) is a problem of bound-constrained optimization, a well studied area in theory and practice.  However, so far there is no study formally applying bound-constrained optimization techniques to NMF.  In this paper, we propose two projectedgradient type methods for NMF.  The new methods have sound optimization properties.  We discuss their efficient implementations and show that one proposed method converges faster than the popular multiplicative update approach.  A simple MATLAB code is provided. 
Formulations of Support Vector Machines: A Note from an Optimization Point of View| Abstract In this paper, we discuss issues about formulations of support vector machines (SVM) from an optimization point of view.  First, support vector machines map training data into a higher (may be infinite) dimensional space.  Currently primal and dual formulations of SVM are derived in the finite dimensional space and readily extended to the infinite dimensional space.  We rigorously discuss the primal-dual relation in the infinite dimensional spaces.  Second, SVM formulations contain penalty terms which are different from unconstrained penalty functions in optimization.  Traditionally unconstrained penalty functions approximate a constrained problem as the penalty parameter increases.  We are interested in similar properties for SVM formulations.  For two of the most popular SVM formulations, we show that one enjoys properties of exact penalty functions but the other is only like traditional penalty functions which converge when the penalty parameter goes to infinity. 
Preconditioning Dense Linear Systems from Large-Scale Semidefinite Programming Problems| Abstract In this paper we show semidefinite programming (SDP) could be a resource for large dense systems.  The preconditioned conjugate gradient method is used to solve fully dense linear systems during each iteration of an optimization algorithm.  An incomplete Cholesky factorization is proposed, and some numerical results are obtained.  These results generate better lower bounds for the quadratic assignment problem, an application of SDP. 
LIBSVM: a Library for Support Vector Machines| Abstract LIBSVM is a library for support vector machines (SVM).  Its goal is to help users to easily use SVM as a tool.  In this document, we present all its implementation details.  For the use of LIBSVM, the README file included in the package and the LIBSVM FAQ provide the information. 
Probability Estimates for Multi-Class Classification by Pairwise Coupling| Abstract Pairwise coupling is a popular multi-class classification method that combines together all pairwise comparisons for each pair of classes.  This paper presents two approaches for obtaining class probabilities.  Both methods can be reduced to linear systems and are easy to implement.  We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3]. 
A Study on SMO-type Decomposition Methods for Support Vector Machines| Abstract Decomposition methods are currently one of the major methods for training support vector machines.  They vary mainly according to different working set selections.  Existing implementations and analysis usually consider some specific selection rules.  In this article, we study Sequential Minimal Optimization (SMO)-type decomposition methods under a general and flexible way of choosing the two-element working set.  Main results include: 1) a simple asymptotic convergence proof, 2) a general explanation of the shrinking and caching techniques, and 3) the linear convergence of the method.  Extensions to some SVM variants are also discussed. 
A Formal Analysis of Stopping Criteria of Decomposition Methods for Support Vector Machines| Abstract In [8] we proved the convergence of a commonly used decomposition method for SVM. 
Analysis of Nonstationary Time Series Using Support Vector Machines| Abstract.  Time series from alternating dynamics have many important applications.  In [5], the authors propose an approach to solve the drifting dynamics.  Their method directly solves a non-convex optimization problem.  In this paper, we propose a strategy which solves a sequence of convex optimization problems by using modified support vector regression.  Experimental results showing its practical viability are presented and we also discuss the advantages and disadvantages of the proposed approach. 
Asymptotic Behaviors of Support Vector Machines with Gaussian Kernel| Abstract Support vector machines (SVMs) with the Gaussian (RBF) kernel have been popular for practical use.  Model selection in this class of SVMs involves two hyperparameters: the penalty parameter C and the kernel width #.  This paper analyzes the behavior of the SVM classifier when these hyperparameters take very small or very large values.  Our results help in a good understanding of the hyperparameter space that leads to an ecient heuristic method of searching for hyperparameter values with small generalization errors.  The analysis also indicates that if complete model selection using the Gaussian kernel has been conducted, there is no need to consider linear SVM. 
A Note on the Decomposition Methods for Support Vector Regression| Abstract The dual formulation of support vector regression involves with two closely related sets of variables.  When the decomposition method is used, many existing approaches use pairs of indices from these two sets as the working set.  Basically they select a base set first and then expand it so all indices are pairs.  This makes the implementation different from that for support vector classification.  In addition, a larger optimization sub-problem has to be solved in each iteration.  In this paper we provide theoretical proofs and conduct experiments to show that directly using the base set as the working set leads to similar convergence (number of iterations).  Therefore, by using a smaller working set while keeping similar number of iterations, the program can be simpler and more ecient. 
Linear Convergence of a Decomposition Method for Support Vector Machines| Abstract Recently the asymptotic convergence of some commonly used decomposition methods for support vector machines has been established.  However, their local convergence rates are still unknown.  In this paper, under the assumptions that the kernel matrix is positive definite and the problem is non-degenerate, we prove the linear convergence of a popular decomposition method. 
Vector Regression: Theory and Algorithms| Abstract We discuss the relation between #-Support Vector Regression (#-SVR) and #Support Vector Regression (#-SVR).  In particular we focus on properties which are different from those of C-Support Vector Classification (C-SVC) and #-Support Vector Classification (#-SVC).  We then discuss some issues which do not occur in the case of classification: the possible range of # and the scaling of target values.  A practical decomposition method for #-SVR is implemented and computational experiments are conducted.  We show some interesting numerical observations specific to regression. 
A Tutorial on -Support Vector Machines| Abstract.  We briefly describe the main ideas of statistical learning theory, support vector machines (SVMs), and kernel feature spaces.  We place particular emphasis on a description of the so-called -SVM, including details of the algorithm and its implementation, theoretical results, and practical applications.  1 An Introductory Example Suppose we are given empirical data (x 1 , y 1 ), .  .  .  , (x m , y m ) 2 X {1}.  (1) Here, the domain X is some nonempty set that the patterns x i are taken from; the y i are called labels or targets.  Unless stated otherwise, indices i and j will always be understood to run over the training set, i. e. , i,
LIBSVM 2|0: Solving Different Support Vector Formulations.  Abstract LIBSVM (version 2. 0)
Automatic Model Selection for Support Vector Machines| Abstract Automatic model selection is an important issue to make support vector machines (SVM) practically useful.  Most existing approaches use the leave-one-out (loo) related estimators.  As finding the loo rate is time consuming, researchers exploit different techniques to approximate it.  Therefore, they develop methods mainly from a machine learning point of view while numerical experiments are usually not seriously treated.  Up to now, most existing SVM software cannot do effective automatic model selections.  We think that if the calculation of loo is a dicult numerical issue, more investigation from a numerical point of view should be conducted.  In this paper, we study the relation between the loo rate and the stopping criteria of the decomposition method for SVM.  For each particular model, the computation of loo involves a sequence of standard SVM problems.  We show that using a very loose stopping criteria for the decomposition method, the best model can still be obtained.  Such an observation leads us to be able to design simple and practical automatic model selection software. 
A Study on Reduced Support Vector Machines| Abstract Recently the Reduced Support Vector Machine (RSVM) was proposed as an alternate of the standard SVM.  Motivated by resolving the difficulty on handling large data sets using SVM with nonlinear kernels, it preselects a subset of data as support vectors and solves a smaller optimization problem.  However, several issues of its practical use have not been fully discussed yet.  For example, we do not know if it possesses comparable generalization ability as the standard SVM.  In addition, we would like to see for how large problems RSVM outperforms SVM on training time.  In this paper we show that the RSVM formulation is already in a form of linear SVM and discuss four RSVM implementations.  Experiments indicate that in general the test accuracy of RSVM are a little lower than that of the standard SVM.  In addition, for problems with up to tens of thousands of data, if the percentage of support vectors is not high, existing implementations for SVM is quite competitive on the training time.  Thus, from this empirical study, RSVM will be mainly useful for either larger problems or those with many support vectors.  Experiments in this paper also serve as comparisons of (1) different implementations for linear SVM; and (2) standard SVM using linear and quadratic cost functions. 
Training -Support Vector Regression: Theory and Algorithms|
PSBIST: A Partial-Scan Based Built-In Self-Test Scheme|
A comparison of methods for multi-class support vector machines|
Timing-Driven Test Point Insertion for Full-Scan and Partial-Scan BIST|
PASCANT: A partial scan and test generation system,|
Libsvm: a library for support vector machines (version 2|36). 
Training #-support vector regression: theory and|
A Hybrid Algorithm for Test Point Selection for Scan-Based BIST|
Some analysis on #-support vector classi#cation|
Multi-code CDMA wireless personal communications networks",|
Decomposition methods for linear support vector machines|
Analysis of nonstationary time series using support vector machines|
A simple decomposition method for support vector machines|
An integrated implementation for support vector classification and regression|
Libsvm - A Library for SVM,|
A practical guide to SVM classification|

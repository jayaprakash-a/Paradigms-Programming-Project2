Uses of Convexity in Numerical Domain Partitioning| Abstract We study the different manifestations of convexity in classifier-constructing machine learning algorithms.  Two principal ways to apply the consequences of the convexity of the evaluation function to enhance numerical domain partitioning exist: it enables static and dynamic pruning of partition candidates.  Numerical attribute handling is a potential time-consumption bottleneck in classifier learning.  Therefore, speeding it up is important for the practical utility of machine learning algorithms.  In extensive empirical evaluation we review the utility of static and dynamic pruning of partition candidates.  We test for 28 UCI test domains the speed-up gained by both approaches separately and their combined effect.  All pruning methods are able to enhance the efficiency of optimal numerical attribute partitioning.  Best results are obtained by combining static and dynamic pruning. 
Finding Optimal Multi-Splits for Numerical Attributes in Decision Tree Learning| Abstract Handling continuous attribute ranges remains a deficiency of top-down induction of decision trees.  They require special treatment and do not fit the learning scheme as well as one could hope for.  Nevertheless, they are common in practical tasks and, therefore, need to be taken into account.  This topic has attracted abundant attention in recent years.  In particular, Fayyad and Irani showed how optimal binary partitions can be found efficiently.  Later, they based a greedy heuristic multipartitioning algorithm on these results.  Recently, Fulton, Kasif, and Salzberg attempted to develop algorithms for finding the optimal multi-split for a numerical attribute in one phase.  We prove that, similarly as in the binary partitioning, only boundary points need to be inspected in order to find the optimal multipartition of a numerical value range.  We develop efficient algorithms for finding the optimal splitting into more than two intervals.  The resulting partition is guaranteed to be optimal w. r. t.  the function that is used to evaluate the attributes' utility in class prediction.  We contrast our method with alternative approaches in initial empirical experiments.  They show that the new method surpasses the greedy heuristic approach of Fayyad and Irani constantly in the goodness of the produced multi-split, but, with small data sets, cannot quite attain the efficiency of the greedy approach.  Furthermore, our experiments reveal that one of the techniques proposed by Fulton, Kasif, and Salzberg is of scarce use in practical tasks, since its time consumption falls short of all demands.  In addition, it categorically fails in finding the optimal multi-split because of an error in the rationale of the method. 
A Neural Network Tool for Brewery Fermentations| ABSTRACT Fermentation is an important, if not the most important process step in the production of beer.  It is subject to alterations stemming from the variation in the yeast, a living organism, and due to the complex raw materials of biological origin.  The variability may manifest in changes in the fermentation speed or cause off-flavors in beer.  Thus, there is a need for tools, both analytical and computational, to help in monitoring the process and keeping it in a desired course.  In this paper we describe a prediction tool to assist production management in the brewery.  The system relies on a neural network that predicts the course of the fermentation based on yeast history, fermentation recipe and raw material variables.  The system is able to predict the fermentation time with an average error of 10 percent of the remaining fermentation time, which enables the brewery to spot problematic batches days in advance and plan the correct yeast harvesting time with an increased accuracy.  The system has been in daily trial use in Hartwall's brewery in Lahti since March 2000 and the user experiences have been positive. 
General and Efficient Multisplitting of Numerical Attributes| Abstract.  Often in supervised learning numerical attributes require special treatment and do not fit the learning scheme as well as one could hope.  Nevertheless, they are common in practical tasks and, therefore, need to be taken into account.  We characterize the well-behavedness of an evaluation function, a property that guarantees the optimal multi-partition of an arbitrary numerical domain to be defined on boundary points.  Well-behavedness reduces the number of candidate cut points that need to be examined in multisplitting numerical attributes.  Many commonly used attribute evaluation functions possess this property; we demonstrate
On Decision Boundaries of Naïve Bayes in Continuous Domains| Abstract.  Nave Bayesian classifiers assume the conditional independence of attribute values given the class.  Despite this in practice often violated assumption, these simple classifiers have been found efficient, effective, and robust to noise.  Discretization of continuous attributes in nave Bayesian classifiers has achieved a lot of attention recently.  Continuous attributes need not necessarily be discretized, but it unifies their handling with nominal attributes and can lead to improved classifier performance.  We show that optimal partitioning results from decision tree learning carry over to Nave Bayes as well.  In particular, it sets decision boundaries on borders of segments with equal class frequency distribution.  An optimal univariate discretization with respect to the Nave Bayes rule can be found in linear time but, unfortunately, optimal multivariate optimization is intractable. 
On the Splitting Properties of Common Attribute Evaluation Functions| Abstract The complexity of numerical domain partitioning depends on the number of potential cut points.  For a large family of attribute evaluation functions only boundary points need to be considered as candidates.  We prove that an even more general property holds for many commonly-used functions.  They do not obtain their optimal value within a segment of examples in which the relative class frequency distribution of examples is static.  The borders of such segments are a subset of boundary points.  Thus, even less cut points need to be examined for these functions.  The results shed a new light on the splitting properties of common attribute evaluation functions and they have practical value as well.  The functions that are examined also include non-convex ones.  Hence, the property introduced is not just another consequence of the convexity of a function. 
Microscopy image analysis of bread using machine learning methods| Abstract We studied the link between the bread microstructure and sensory properties using machine learning methods.  In the first phase, we extracted starch gelatinization degree and protein mesh properties from microscopy images.  Color histograms and Haralick features were found to be suitable feature extraction techniques.  In both cases, the prediction error of an induced model tree was lower than the standard deviation between independent predictions given by domain experts.  Combining models by boosting gave small gains in accuracy.  In the second phase, we used various analysis-originating attributes to predict bread sensory properties.  Best single attribute predictors and best pairs are reported. 
On Maximum Margin Hierarchical Multilabel Classification| Abstract We present work in progress towards maximum margin hierarchical classification where the objects are allowed to belong to more than one category at a time.  The classification hierarchy is represented as a Markov network equipped with an exponential family defined on the edges.  We present a variation of the maximum margin multilabel learning framework, suited to the hierarchical classification task and allows efficient implementation via gradient-based methods.  We compare the behaviour of the proposed method to the recently introduced hierarchical regularized least squares classifier as well as two SVM variants in Reuter's news article classification.  Often in hierarchical classification, the object to be classified is assumed to belong to exactly one (leaf) node in the hierarchy (c. f.  [5, 2, 4]).  Following [3], in this paper we consider the more general case where a single object can be classified into several categories in the hierarchy, to be specific, the multilabel is a union of partial paths in the hierarchy.  For example, a news article about David and Victoria Beckham could belong to partial paths sport, football and entertainment, music but might not belong to any leaf categories such as champions league or jazz.  In our setting the training data ((x i , y(x i ))) m i=1 consists of pairs (x, y) of vector x 2 R n and a multilabel y 2 {+1,- 1} k consisting of k microlabels.  As the model class we use the exponential family P (y|x) / Y e2E exp w T e # # # e (x, y e ) # = exp w T # # #(x, y) # (1) defined on the edges of a Markov network G = (V, E), where node j 2 V corresponds to the j'th component of the multilabel and the edges e = (j, j 0 ) 2 E correspond # Corresponding author.  to the classification hierarchy given as input.  By y e = (y j , y 0 j ) we denote the restriction of the multilabel y = (y 1 , .  .  .  , y k ) to the edge e = (j, j 0 ).  The edgefeature vector # # # e , in turn, is a concatenation of 'class-sensitive' feature vectors # # # u e (x, y e ) = Jy e = uK# # #(x), where JK denotes an indicator function (c. f.  [1]), and w e is the weight vector for edge e.  The vector # # #(x) could be a bag of words---as in the experiments reported here---or any other feature representation of the document x.  Also, note that although the same feature vector # # #(x) is duplicated for each edge and edge-labeling, in the weight vector w = (w ue e ) e2E,ue we still have a separate weights to represent importance differences of a given feature in different contexts.  There are many ways to define loss functions for hierarchical classification setting (c. f [5, 2, 4, 3]).  Zero-one loss ` 0/1 (y, u) = Jy 6= uK is not very well suited to the task as it ignores the severity of the discrepancy between y and u.  Symmetric difference loss ` # (y, u) = P j Jy i 6= u i K does not suffer from this deficiency.  However, it fails to take the dependency structure of the microlabels into account.  A more appealling choice is the hierarchical loss function of [3].  It penalizes the first mistake along a path, ` PATH (y, u) = P j c j Jy j 6= u j & y k = u k 8k 2 anc(j)K, where the coefficients c root = 1, c j = c pa(j) /|sibl(j)| down-weight mistakes made deeper in the hierarchy.  Here we denoted by anc(j) an ancestor, by pa(j) the immediate parent, and by sibl(j) the set of siblings of node j.  In this paper, we consider a simplified version of ` PATH , namely ` EDGE (y, u) = X j c j Jy j 6= u j & y pa(j) = u pa(j) K, that penalizes a mistake in child if the label of the parent was correct.  This choice lets the loss function to capture some of the hierarchical dependencies (between the parent and the child) but allows us define the loss in terms of edges, which is crucial for the efficiency of our learning algorithm.  This is achieved by dividing the microlabel loss of each node among the edges adjacent to it.  As in [7, 8], our goal is to learn a weight vector w that maximizes the minimum margin on training data the between the correct multilabel y(x i ) and the incorrect multilabels y 6= y(x i ).  Also, we would like the margin to scale as a function of the loss.  Alloting a single slack variable for each training example results in the following soft-margin optimization problem: min w 1 2 ||w|| 2 + C m X i=1 # i s. t.  w T fiff # #(x i , y) # ` ` `(y i , y) - # i , 8i,
Isotopomer distribution computation from tandem mass spectrometric data with overlapping fragment spectra| In tests with isotope-labelled alanine, most accurate results were obtained using multiple reaction monitoring and 15 eV collision energy.  The meausured isotopomer frequecies were in the range 99--106% of the theoretical value and the deviation between repetitions was in the range 1--10%. 
Adaptation Cost as a Criterion for Solution Evaluation| Abstract.  In most case-based systems, the best case is selected by calculating similarities between the surface features of the cases.  Sometimes, the most similar case is not the one that is the easiest to adapt.  On the other hand, judging the adaptation effort can be tedious.  In this paper, we propose caseadaptability to be used as an evaluation criterion as opposed to using it in the case-retrieval phase.  Our method can be seen to combine the computational effectiveness of similarity-based retrieval with the better quality solutions of adaptation-guided retrieval. 
Data Mining: An AI Perspective| 0 Abstract--Data mining, or knowledge discovery in databases (KDD), is an interdisciplinary area that integrates techniques from several fields including machine learning, statistics, and database systems, for the analysis of large volumes of data.  This paper reviews the topics of interest from the IEEE International Conference on Data Mining (ICDM) from an AI perspective.  We discuss common topics in data mining and AI, including key AI ideas that have been used in both data mining and machine learning. 
Improved Computation of| Abstract We present a computational method for determination of the isotopomer distributions of metabolites from the data generated by a tandem mass spetrometer.  The method is an improvement over existing method in two aspects: First, the method is able to deal with overlapping fragments in the daughter ion spectra.  Second, the method intertwines the correction step for natural abundance of higher isotopes with the step relating the mass isotopomers to positional isotopomers.  This results in a more efficient utilization of the spectral data. 
An integrated approach to bioprocess recipe design| Finland Biotechnical and food process industries have a need for tools that support the implementation of modern manufacturing practice.  Flexible production strategies and total quality management require efficient design of the process recipes.  This paper presents the approach used in Sophist, an adaptive recipe planning system.  The system integrates casebased reasoning and a semi-qualitative process model for the design and optimisation of bioprocess recipes.  In particular, we introduce a novel revision selection algorithm that improve the system's adaptivity.  The behavior of the planner is exemplified by a batch fermentation planning experiment. 
On the Well-Behavedness of Important Attribute Evaluation Functions| Abstract The class of well-behaved evaluation functions simplifies and makes efficient the handling of numerical attributes; for them it suffices to concentrate on the boundary points in searching for the optimal partition.  This holds always for binary partitions and also for multisplits if only the function is cumulative in addition to being well-behaved.  The class of wellbehaved evaluation functions is a proper superclass of convex evaluation functions.  Thus, it is clear that a large proportion of the most important attribute evaluation functions are well-behaved.  This paper explores the extent and boundaries of well-behaved functions.  In particular, we examine the convexity and well-behavedness of C4. 5's default attribute evaluation function gain ratio, which has been known to have problems with numerical attributes.  Our empirical experiments show that a very simple cumulative rectification to the poor bias of information gain significantly outperforms gain ratio. 
Efficient computation of gap-weighted string kernels on large alphabets| Abstract We present a sparse dynamic programming algorithm that, given two strings s, t, a gap penalty #, and an integer p, computes the value of the gap-weighted length-p subsequences kernel.  The algorithm works in time O(p|M | log min(|s|, |t|)),
Computer-assisted image-guided surgery using the Regulus Navigator|
Knowledge and power| Toward a political philosophy of science. 
Predicting the Speed of Beer Fermentation in Laboratory and Industrial Scale| Engineering Applications of Bio-Inspired Artificial Neural Networks,. 
Constructing decision trees and lists using the MDL principle (in Finnish)|
Towards CBR for Bioprocess Planning|
A Relation Between the Fourier Coefficients and the Values of Holomorphic Modular Functions,|
Computing positional isotopomer distributions from tandem mass spectrometric data,|
A Method for Estimating Metabolic Fluxes from Incomplete Isotopomer Information|
Speeding Up the Search for Optimal Partitions|
Generalizing Boundary Points|
Efficient Range Partitioning in Classification Learning|
Fostering intergenerational literacy: The Missouri parents as teachers program|
Necessary and Sufficient Pre-processing in Numerical Range Discretization|
Optimal multivariate discretization for naive Bayesian classifiers is NPhard|
A novel kinase cascade triggered by stress and heat shock that stimulates MAPKAP kinase-2 and phosphorylation of the small heat shock proteins|
Interfaces between the detection, signaling, and repair of DNA damage|

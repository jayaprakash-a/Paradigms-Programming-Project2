Inducing Novel Gene-Drug Interactions from the Biomedical Literature| ABSTRACT Motivation: Knowledge about the interactions between genes and drugs is important in determining the efficacy and toxicity of medications.  We present a supervised learning algorithm for inducing previously unknown genedrug interactions by text-mining the biomedical literature.  This algorithm takes as its input a set of known genedrug relationships and a literature source.  New genedrug interactions are induced based on similarity to known gene-drug interactions, where similarity is determined according to the biomedical literature.  Results: Based on limited training data (258 genedrug pairs), the algorithm induces correct unseen genedrug relationships at precisions of over 60%, including gene-drug relationships that are not obvious from name similarity. 
Factored A Search for Models over Sequences and Trees| Abstract We investigate the calculation of A* bounds for sequence and tree models which are the explicit intersection of a set of simpler models or can be bounded by such an intersection.  We provide a natural viewpoint which unifies various instances of factored A* models for trees and sequences, some previously known and others novel, including multiple sequence alignment, weighted finitestate transducer composition, and lexicalized statistical parsing.  The specific case of parsing with a product of syntactic (PCFG) and semantic (lexical dependency) components is then considered in detail.  We show that this factorization gives a modular lexicalized parser which is simpler than comparably accurate non-factored models, and which allows efficient exact inference with large treebank grammars. 
Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger| Abstract This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.  In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.  The best resulting accuracy for the tagger on the Penn Treebank is 96. 86% overall, and 86. 91% on previously unseen words. 
Joint Learning Improves Semantic Role Labeling| Abstract Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.  This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.  We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models.  This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank. 
Soft Constraints Mirror Hard Constraints: Voice and Person in English and Lummi| some material in the LFG 01 conference presentation that will appear in a longer work in preparation by the authors.  ABSTRACT The same categorical phenomena which are attributed to hard grammatical constraints in some languages continue to show up as statistical preferences in other languages, motivating a grammatical model that can account for soft constraints.  The effects of a hierarchy of person (1st, 2nd 3rd) on grammar are categorical in some languages, most famously in languages with inverse systems, but also in languages with person restrictions on passivization.  In Lummi, for example, the person of the subject argument cannot be lower than the person of a nonsubject argument.  If this would happen in the active, passivization is obligatory; if it would happen in the passive, the active is obligatory (Jelinek and Demers 1983).  These facts follow from the theory of harmonic alignment in OT: constraints favoring the harmonic association of prominent person (1st, 2nd) with prominent syntactic function (subject) are hypothesized to be present as subhierarchies of the grammars of all languages, but to vary in their effects across languages depending on their interactions with other constraints (Aissen 1999).  There is a statistical reflection of these hierarchies in English.  The same disharmonic person/argument associations which are avoided categorically in languages like Lummi by making passives either impossible or obligatory, are avoided in the SWITCHBOARD corpus of spoken English by either depressing or elevating the frequency of passives relative to actives.  The English data can be grammatically analyzed within the stochastic OT framework (Boersma 1998, Boersma and Hayes 2001) in a way which provides a principled and unifying explanation for their relation to the crosslinguistic categorical person effects studied by Aissen (1999). 
Dissociating functor-argument structure from surface phrase structure: the relationship of HPSG Order Domains to LFG| Abstract Recent work on order domains and linearization in HPSG, and also conceptually related work in categorial grammar, is argued to mirror the key ideas that motivated the separation between c-structure and f-structure in the earliest work in LFG (Bresnan 1982).  This paper argues that: (i) to be descriptively adequate, all constraint-based frameworks do indeed need a dissociation between functor-argument structure and surface phrase structure, (ii) while linearization HPSG can technically be regarded as a one-level/one-stratum architecture, the complex attribute-value matrix representations proposed are better thought of as encoding multiple `virtual levels', and (iii) once this is noted, there is a general equivalence between the analyses that can be proposed in linearization HPSG and LFG, although certain differences are noted. 
Feature Selection for a Rich HPSG Grammar Using Decision Trees| Abstract This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs).  We show that single decision trees do not make optimal use of the available information; constructed ensembles of decision trees based on different feature subspaces show significant performance gains (14% parse selection error reduction).  We compare the performance of the learned PCFG grammars and log linear models over the same features. 
Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the Penn Treebank| Abstract This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar.  We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs.  bottom-up strategies.  We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations. 
Robust Textual Inference using Diverse Knowledge Sources| Abstract We present a machine learning approach to robust textual inference, in which parses of the text and the hypothesis sentences are used to measure their asymmetric "similarity", and thereby to decide if the hypothesis can be inferred.  This idea is realized in two different ways.  In the first, each sentence is represented as a graph (extracted from a dependency parser) in which the nodes are words/phrases, and the links represent dependencies.  A learned, asymmetric, graph-matching cost is then computed to measure the similarity between the text and the hypothesis.  In the second approach, the text and the hypothesis are parsed into the logical formula-like representation used by (Harabagiu et al. , 2000).  An abductive theorem prover (using learned costs for making different types of assumptions in the proof) is then applied to try to infer the hypothesis from the text, and the total "cost" of proving the hypothesis is used to decide if the hypothesis is entailed. 
Dictionaries and endangered languages| `A good dictionary is one in which you can find the thing you are looking for preferably in the very first place you look' Mary Haas. 
Named Entity Recognition with Character-Level Models| Abstract We discuss two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation.  The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.  Our best model achieves an overall F 1 of 86. 07% on the English test data (92. 31% on the development data).  This number represents a 25% error reduction over the same model without word-internal (substring) features. 
Exploiting the Block Structure of the Web for Computing PageRank| Abstract The web link graph has a nested block structure: the vast majority of hyperlinks link pages on a host to other pages on the same host, and many of those that do not link pages within the same domain.  We show how to exploit this structure to speed up the computation of PageRank by a 3-stage algorithm whereby (1) the local PageRanks of pages for each host are computed independently using the link structure of that host, (2) these local PageRanks are then weighted by the "importance" of the corresponding host, and (3) the standard PageRank algorithm is then run using as its starting vector the weighted concatenation of the local PageRanks.  Empirically, this algorithm speeds up the computation of PageRank by a factor of 2 in realistic scenarios.  Further, we develop a variant of this algorithm that efficiently computes many different "personalized" PageRanks, and a variant that efficiently recomputes PageRank after node updates. 
Distributional Phrase Structure Induction| Abstract Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data.  Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts.  We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features.  The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.  1 Overview While early work showed that small, artificial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective.  For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality.  Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000).  It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text.  However, there are compelling motivations for unsupervised grammar induction.  Building supervised training data requires considerable resources, including time and linguistic expertise.  Furthermore, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within a supervised parser's supervisory information, and, therefore, often not explicitly modeled in such systems.  For example, our system and others have difficulty correctly attaching subjects to verbs above objects.  For a supervised CFG parser, this ordering is implicit in the given structure of VP and S constituents, however, it seems likely that to learn attachment order reliably, an unsupervised system will have to model it explicitly.  Our goal in this work is the induction of highquality, linguistically sensible grammars, not parsing accuracy.  We present two systems, one which does not do disambiguation well and one which does not do it at all.  Both take tagged but unparsed Penn treebank sentences as input.  1 To whatever degree our systems parse well, it can be taken as evidence that their grammars are sensible, but no effort was taken to improve parsing accuracy directly.  There is no claim that human language acquisition is in any way modeled by the systems described here.  However, any success of these methods is evidence of substantial cues present in the data, which could potentially be exploited by humans as well.  Furthermore, mistakes made by these systems could indicate points where human acquisition is likely not being driven by these kinds of statistics.  2 Approach At the heart of any iterative grammar induction system is a method, implicit or explicit, for deciding how to update the grammar.  Two linguistic criteria for constituency in natural language grammars form the basis of this work (Radford, 1988): 1.  External distribution: A constituent is a sequence of words which appears in various structural positions within larger constituents.  1 The Penn tag and category sets used in examples in this paper are documented in Manning and Schutze (1999, 413).  2.  Substitutability: A constituent is a sequence of words with (simple) variants which can be substituted for that sequence.  To make use of these intuitions, we use a distributional notion of context.  Let # be a part-of-speech tag sequence.  Every occurence of # will be in some context x # y, where x and y are the adjacent tags or sentence boundaries.  The distribution over contexts in which # occurs is called its signature, which we denote by #(#).  Criterion 1 regards constituency itself.  Consider the tag sequences IN DT NN and IN DT.  The former is a canonical example of a constituent (of category PP), while the later, though strictly more common, is, in general, not a constituent.  Frequency alone does not distinguish these two sequences, but Criterion 1 points to a distributional fact which does.  In particular, IN DT NN occurs in many environments.  It can follow a verb, begin a sentence, end a sentence, and so on.  On the other hand, IN DT is generally followed by some kind of a noun or adjective.  This example suggests that a sequence's constituency might be roughly indicated by the entropy of its signature, H(#(#)).  This turns out to be somewhat true, given a few qualifications.  Figure 1 shows the actual most frequent constituents along with their rankings by several other measures.  Tag entropy by itself gives a list that is not particularly impressive.  There are two primary causes for this.  One is that uncommon but possible contexts have little impact on the tag entropy value.  Given the skewed distribution of short sentences in the treebank, this is somewhat of a problem.  To correct for this, let # u (#) be the uniform distribution over the observed contexts for #.  Using H(# u (#)) would have the obvious effect of boosting rare contexts, and the more subtle effect of biasing the rankings slightly towards more common sequences.  However, while H(#(#)) presumably converges to some sensible limit given infinite data, H(# u (#)) will not, as noise eventually makes all or most counts non-zero.  Let u be the uniform distribution over all contexts.  The scaled entropy H s (#(#)) = H(#(#))[H(# u (#))=H(u)] turned out to be a useful quantity in practice.  Multiplying entropies is not theoretically meaningful, but this quantity does converge to H(#(#)) given infinite (noisy) data.  The list for scaled entropy still has notable flaws, mainly relatively low ranks for common NPs, which does not hurt system perforSequence Actual Freq Entropy Scaled Boundary GREEDY-RE DT NN
A System For Identifying Named Entities in Biomedical Text: How Results From Two Evaluations Reflect on Both the System and the Evaluations| Abstract We present a maximum-entropy based system for identifying Named Entities (NEs) in biomedical abstracts and present its performance in the only two biomedical Named Entity Recognition (NER) comparative evaluations that have been held to date, namely BioCreative and Coling BioNLP.  Our system obtained an exact match f-score of 83. 2% in the BioCreative evaluation and 70. 1% in the BioNLP evaluation.  We discuss our system in detail including its rich use of local features, attention to correct boundary identification, innovative use of external knowledge resources including parsing and web searches, and rapid adaptation to new NE sets.  We also discuss in depth problems with data annotation in the evaluations which caused the final performance to be lower than the optimal. 
Natural Language Grammar Induction Using a Constituent-Context Model| Abstract This paper presents a novel approach to the unsupervised learning of syntactic analyses of natural language text.  Most previous work has focused on maximizing likelihood according to generative PCFG models.  In contrast, we employ a simpler probabilistic model over trees based directly on constituent identity and linear context, and use an EM-like iterative procedure to induce structure.  This method produces much higher quality analyses, giving the best published results on the ATIS dataset.  1 Overview To enable a wide range of subsequent tasks, human language sentences are standardly given tree-structure analyses, wherein the nodes in a tree dominate contiguous spans of words called constituents, as in figure 1(a).  Constituents are the linguistically coherent units in the sentence, and are usually labeled with a constituent category, such as noun phrase (NP) or verb phrase (VP).  An aim of grammar induction systems is to figure out, given just the sentences in a corpus S, what tree structures correspond to them.  In this sense, the grammar induction problem is an incomplete data problem, where the complete data is the corpus of trees T , but we only observe their yields S.  This paper presents a new approach to this problem, which gains leverage by directly making use of constituent contexts.  It is an open problem whether entirely unsupervised methods can produce linguistically accurate parses of sentences.  Due to the difficulty of this task, the vast majority of statistical parsing work has focused on supervised learning approaches to parsing, where one uses a treebank of fully parsed sentences to induce a model which parses unseen sentences [7, 3].  But there are compelling motivations for unsupervised grammar induction.  Building supervised training data requires considerable resources, including time and linguistic expertise.  Investigating unsupervised methods can shed light on linguistic phenomena which are implicit within a supervised parser's supervisory information (e. g. , unsupervised systems often have difficulty correctly attaching subjects to verbs above objects, whereas for a supervised parser, this ordering is implicit in the supervisory information).  Finally, while the presented system makes no claims to modeling human language acquisition, results on whether there is enough information in sentences to recover their structure are important data for linguistic theory, where it has standardly been assumed that the information in the data is deficient, and strong innate knowledge is required for language acquisition [4].  S NP NN 1 Factory NNS payrolls VP VBD fell PP IN in NN 2 September Node Constituent Context S NN NNS VBD IN NN # -- # NP NN NNS # -- VBD VP VBD IN NN NNS -- # PP IN NN VBD -- # NN 1 NN # -- NNS NNS NNS NN -- VBD VBD VBD NNS -- IN IN IN VBD -- NN NN 2 NNS IN -- # Empty Context # 0 # -- NN # 1 NN -- NNS # 2 NNS -- VBD # 3 VBD -- IN # 4 IN -- NN # 5 NN -- # Figure 1: Example parse tree with the constituents and contexts for each tree node.  2 Previous Approaches One aspect of grammar induction where there has already been substantial success is the induction of parts-of-speech.  Several different distributional clustering approaches have resulted in relatively high-quality clusterings, though the clusters' resemblance to classical parts-of-speech varies substantially [9, 15].  For the present work, we take the part-ofspeech induction problem as solved and work with sequences of parts-of-speech rather than words.  In some ways this makes the problem easier, such as by reducing sparsity, but in other ways it complicates the task (even supervised parsers perform relatively poorly with the actual words replaced by parts-of-speech).  Work attempting to induce tree structures has met with much less success.  Most grammar induction work assumes that trees are generated by a symbolic or probabilistic context-free grammar (CFG or PCFG).  These systems generally boil down to one of two types.  Some fix the structure of the grammar in advance [12], often with an aim to incorporate linguistic constraints [2] or prior knowledge [13].  These systems typically then attempt to find the grammar production parameters 2 which maximize the likelihood P(S|2) using the inside-outside algorithm [1], which is an efficient (dynamic programming) instance of the EM algorithm [8] for PCFGs.  Other systems (which have generally been more successful) incorporate a structural search as well, typically using a heuristic to propose candidate grammar modifications which minimize the joint encoding of data and grammar using an MDL criterion, which asserts that a good analysis is a short one, in that the joint encoding of the grammar and the data is compact [6, 16, 18, 17].  These approaches can also be seen as likelihood maximization where the objective function is the a posteriori likelihood of the grammar given the data, and the description length provides a structural prior.  The "compact grammar" aspect of MDL is close to some traditional linguistic argumentation which at times has argued for minimal grammars on grounds of analytical [10] or cognitive [5] economy.  However, the primary weakness of MDL-based systems does not have to do with the objective function, but the search procedures they employ.  Such systems end up growing structures greedily, in a bottom-up fashion.  Therefore, their induction quality is determined by how well they are able to heuristically predict what local intermediate structures will fit into good final global solutions.  A potential advantage of systems which fix the grammar and only perform parameter search is that they do compare complete grammars against each other, and are therefore able to detect which give rise to systematically compatible parses.  However, although early work showed that small, artificial CFGs could be induced with the EM algorithm [12], studies with large natural language grammars have generally suggested that completely unsupervised EM over PCFGs is ineffective for grammar acquisition.  For instance, Carroll and Charniak [2] describe experiments running the EM algorithm from random starting points, which produced widely varying learned grammars, almost all of extremely poor quality.  1 1 We duplicated one of their experiments, which used grammars restricted to rules of the form x ! x y | y x , where there is one category x for each part-of-speech (such a restricted CFG is isomorphic to a dependency grammar).  We began reestimation from a grammar with uniform rewrite
The LinGO Redwoods Treebank: Motivation and Preliminary Applications| Abstract The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank.  While
What's needed for lexical databases? Experiences with Kirrkirr| Abstract This paper discusses what is required from dictionary databases, and one approach, based on experience with Kirrkirr, a dictionary browser originally developed for Warlpiri, an Indigenous Australian language.  The paper suggests that there is something of a disconnect between the data access needs of lexical databases and most work on semi-structured databases within the database community. 
Template Sampling for Leveraging Domain Knowledge in Information Extraction| Abstract We initially describe a feature-rich discriminative Conditional Random Field (CRF) model for Information Extraction in the workshop announcements domain, which offers good baseline performance in the PASCAL shared task.  We then propose a method for leveraging domain knowledge in Information Extraction tasks, scoring candidate document labellings as one-value-per-field templates according to domain feasibility after generating sample labellings from a trained sequence classifier.  Our relational models evaluate these templates according to our intuitions about agreement in the domain: workshop acronyms should resemble their names, workshop dates occur after paper submission dates.  These methods see a 5% f-score improvement in fields retrieved when sampling labellings from a Maximum-Entropy Markov Model, however we do not observe improvement over a CRF model.  We discuss reasons for this, including the problem of recovering all field instances from a best template, and propose future work in adapting such a model to the CRF, a better standalone system. 
Bilingual Dictionaries for Australian Languages: User studies on the place of paper and electronic dictionaries| Abstract Dictionaries have long been seen as an essential contribution by linguists to work on endangered languages.  We report on preliminary investigations of actual dictionary usage and usability by 76 speakers, semi-speakers and learners of Australian Aboriginal languages.  The dictionaries include: electronic and printed bilingual Warlpiri-English dictionaries, a printed trilingual Alawa-KriolEnglish dictionary, and a printed bilingual Warumungu-English dictionary.  We examine competing demands for completeness of coverage and ease of access, and focus on the prospects of electronic dictionaries for solving many traditional problems, based in particular on observations on the usability of a prototype interface developed in our project.  The flexibility of computer interfaces can help accommodate different needs including those of speakers with emerging literacy skills, but they are not useful in communities where computer access is generally unavailable. 
The Leaf Projection Path View of Parse Trees: Exploring String Kernels for HPSG Parse Selection| Abstract We present a novel representation of parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree.  This representation allows us to achieve significantly higher accuracy in the task of HPSG parse selection than standard models, and makes the application of string kernels natural.  We define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation.  We apply SVM ranking models and achieve an exact sentence accuracy of 85. 40% on the Redwoods corpus. 
LinGO Redwoods A Rich and Dynamic Treebank for HPSG| Abstract The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank.  A treebank is a (typically hand-built) collection of natural language utterances and associated linguistic analyses; typical treebanks---as for example the widely recognized Penn Treebank (
A* Parsing: Fast Exact Viterbi Parse Selection| Abstract We present an extension of the classic A* search procedure to tabular PCFG parsing.  The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions.  We discuss various estimates and give efficient algorithms for computing them.  On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. 
Probabilistic Parsing Using Left Corner Language Models| Abstract We introduce a novel parser based on a probabilistic version of a left-corner parser.  The left-corner strategy is attractive because rule probabilities can be conditioned on both top-down goals and bottom-up derivations.  We develop the underlying theory and explain how a grammar can be induced from analyzed data.  We show that the left-corner approach provides an advantage over simple top-down probabilistic context-free grammars in parsing the Wall Street Journal using a grammar induced from the Penn Treebank.  We also conclude that the Penn Treebank provides a fairly weak testbed due to the flatness of its bracketings and to the obvious overgeneration and undergeneration of its induced grammar. 
Rethinking Text Segmentation Models: An Information Extraction Case Study| Abstract I examine the use of text segmentation methods on semi-structured text within an information extraction application, arguing that: hierarchical models of discourse structure are necessary, that discourse relationships are not, that repetition can indicate cohesion or a topic change, and that generalized cue phrases are underutilized.  I present results and discussion of the evaluation of two models.  This paper discusses one aspect of an information extraction system for real estate classified ads: how to handle advertisements that advertise multiple properties.  Examples of such advertisements appear in Figures 1 and 2.  The system described is in production use.  1 The contributions of this paper include: # Drawing connections and contrasts between approaches to segmentation within the information extraction literature and the related literature on text segmentation # Questioning the utility { for even practical `language engineering' purposes { of linear segmentations of texts # Showing that the notion of topic, used as a criterion for segmentation, has not been well defined # Arguing that, for important practical notions of topic, the lexical cohesion metrics most commonly used for text segmentation do not give the desired results and one often wants repulsion sion # Reasserting the importance of using a generalized notion of cue phrases 1 It runs in the background of
Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency| Abstract We present a generative model for the unsupervised learning of dependency structures.  We also describe the multiplicative combination of this dependency model with a model of linear constituency.  The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing.  We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 
Conditional Structure versus Conditional Estimation in NLP Models| Abstract This paper separates conditional parameter
Max-Margin Parsing| Abstract We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying
A Generative Constituent-Context Model for Improved Grammar Induction| Abstract We present a generative distributional model for the
Agenda-Based Chart Parser for Arbitrary Probabilistic Context-Free Grammars| Abstract While O(n 3 ) methods for parsing probabilistic context-free grammars (PCFGs) are well known, a tabular parsing framework for arbitrary PCFGs which allows for botton-up, topdown, and other parsing strategies, has not yet been provided.  This paper presents such an algorithm, and shows its correctness and advantages over prior work.  The paper finishes by bringing out the connections between the algorithm and work on hypergraphs, which permits us to extend the presented Viterbi (best parse) algorithm to an inside (total probability) algorithm. 
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network| Abstract We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.  Using these ideas together, the resulting tagger gives a 97. 24% accuracy on the Penn Treebank WSJ, an error reduction of 4. 4% on the best previous single automatically learned tagging result. 
Unsupervised Learning of Field Segmentation Models for Information Extraction| Abstract The applicability of many current information
Extrapolation methods for accelerating PageRank computations| ABSTRACT We present a novel algorithm for the fast computation of PageRank, a hyperlink-based estimate of the "importance" of Web pages.  The original PageRank algorithm uses the Power Method to compute successive iterates that converge to the principal eigenvector of the Markov matrix representing the Web link graph.  The algorithm presented here, called Quadratic Extrapolation, accelerates the convergence of the Power Method by periodically subtracting off estimates of the nonprincipal eigenvectors from the current iterate of the Power Method.  In Quadratic Extrapolation, we take advantage of the fact that the first eigenvalue of a Markov matrix is known to be 1 to compute the nonprincipal eigenvectors using successive iterates of the Power Method.  Empirically, we show that using Quadratic Extrapolation speeds up PageRank computation by 25-300% on a Web graph of 80 million nodes, with minimal overhead.  Our contribution is useful to the PageRank community and the numerical linear algebra community in general, as it is a fast method for determining the dominant eigenvector of a matrix that is too large for standard fast methods to be practical. 
Verb Sense and Subcategorization: Using Joint Inference to Improve Performance on Complementary Tasks| Abstract We propose a general model for joint inference in correlated natural language processing tasks when fully annotated training data is not available, and apply this model to the dual tasks of word sense disambiguation and verb subcategorization frame determination.  The model uses the EM algorithm to simultaneously complete partially annotated training sets and learn a generative probabilistic model over multiple annotations.  When applied to the word sense and verb subcategorization frame determination tasks, the model learns sharp joint probability distributions which correspond to linguistic intuitions about the correlations of the variables.  Use of the joint model leads to error reductions over competitive independent models on these tasks. 
Automatic Acquisition of a Large Subcategorization Dictionary from Corpora| Abstract This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora.  It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser.  Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem. 
Exploring the Boundaries: Gene and Protein Identification in Biomedical Text| Abstract We present a maximum-entropy based system incorporating a diverse set of features for identifying genes and proteins in biomedical abstracts.  This system was entered in the BioCreative comparative evaluation and achieved the best performance in the "open" evaluation and the second-best performance in the "closed" evaluation.  Central contributions are rich use of features derived from the training data at multiple levels of granularity, a focus on correctly identifying entity boundaries, and the innovative use of several external knowledge sources including full MEDLINE abstracts and web searches. 
Finding Educational Resources on the Web: Exploiting Automatic Extraction of Metadata| Abstract Searching for educational resources on the web would be greatly facilitated by the availability of classificatory metadata, but most web educational resources put up by faculty members provide no such metadata.  We explore using text classification and information extraction techniques to automatically gather such metadata.  Text classifications orthogonal to topic matter appear possible with high (} 90%) accuracy, and exact-match information extraction has an F measure around 50%. 
Combining Heterogeneous Classifiers for Word-Sense Disambiguation| Abstract This paper discusses ensembles of simple but heterogeneous classifiers for word-sense disambiguation, examining the Stanford-CS224N system entered in the SENSEVAL-2 English lexical sample task.  First-order classifiers are combined by a
Log-Linear Models for Label Ranking| Abstract Label ranking is the task of inferring a total order over a predefined set of labels for each given instance.  We present a general framework for batch learning of label ranking functions from supervised data.  We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent.  This enables us to accommodate a variety of ranking problems.  In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels.  Special cases of our setting are multilabel categorization and hierarchical classification.  We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration.  The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus. 
Optimizing Local Probability Models for Statistical Parsing| Abstract.  This paper studies the properties and performance of models for estimating local probability distributions which are used as components of larger probabilistic systems --- history-based generative parsing models.  We report experimental results showing that memory-based learning outperforms many commonly used methods for this task (
Kirrkirr: Software for browsing and visual exploration of a structured Warlpiri dictionary| tool for XML dictionaries.  1 In particular, we discuss its use with a dictionary for Warlpiri, an Australian Aboriginal language, focusing on its potential for providing practical, educationally useful dictionary access for endangered languages, at a reasonable labor cost. 
Solving Logic Puzzles: From Robust Processing to Precise Semantics| Abstract This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE).  We highlight key challenges, and discuss the representations and performance of the prototype system. 
An 2 2 A?# Agenda-Based Chart Parser for Arbitrary Probabilistic Context-Free Grammars| Abstract While ######### methods for parsing probabilistic context-free grammars (PCFGs) are well known, a tabular parsing framework for arbitrary PCFGs which allows for botton-up, topdown, and other parsing strategies, has not yet been provided.  This paper presents such an algorithm, and shows its correctness and advantages over prior work.  The paper finishes by bringing out the connections between the algorithm and work on hypergraphs, which permits us to extend the presented Viterbi (best parse) algorithm to an inside (total probability) algorithm. 
Learning random walk models for inducing word dependency distributions| Abstract Many NLP tasks rely on accurately estimating word dependency probabilities P(w 1 |w 2 ), where the words w 1 and w 2 have a particular relationship (such as verb-object).  Because of the sparseness of counts of such dependencies, smoothing and the ability to use multiple sources of knowledge are important challenges.  For example, if the probability P(N |V ) of noun N being the subject of verb V is high, and V takes similar objects to V 0 , and V 0 is synonymous to V 00 , then we want to conclude that P(N |V 00 ) should also be reasonably high---even when those words did not cooccur in the training data.  To capture these higher order relationships, we propose a Markov chain model, whose stationary distribution is used to give word probability estimates.  Unlike the manually defined random walks used in some link analysis algorithms, we show how to automatically learn a rich set of parameters for the Markov chain's transition probabilities.  We apply this model to the task of prepositional phrase attachment, obtaining an accuracy of 87. 56%. 
From Instance-level Constraints to Space-Level Constraints: Making the Most of Prior Knowledge in Data Clustering| Abstract We present an improved method for clustering in the presence of very limited supervisory information, given as pairwise instance constraints.  By allowing instance-level constraints to have spacelevel inductive implications, we are able to successfully incorporate constraints for a wide range of data set types.  Our method greatly improves on the previously studied constrained -means algorithm, generally requiring less than half as many constraints to achieve a given accuracy on a range of real-world data, while also being more robust when over-constrained.  We additionally discuss an active learning algorithm which increases the value of constraints even further. 
Interpreting and Extending Classical Agglomerative Clustering Algorithms using a Model-Based approach| Abstract We present two results which arise from a model-based approach to hierarchical agglomerative clustering.  First, we show formally that the common heuristic agglomerative clustering algorithms -- Ward's method, single-link, complete-link, and a variant of group-average -are each equivalent to a hierarchical model-based method.  This interpretation gives a theoretical explanation of the empirical behavior of these algorithms, as well as a principled approach to resolving practical issues, such as number of clusters or the choice of method.  Second, we show how a model-based viewpoint can suggest variations on these basic agglomerative algorithms.  We introduce adjusted complete-link, Mahalanobis-link, and line-link as variants, and demonstrate their utility. 
Accurate Unlexicalized Parsing| Abstract We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.  Indeed, its performance of 86. 36% (LP/LR F 1 ) is better than that of early
Spectral Learning| Abstract We present a simple, easily implemented spectral learning algorithm which applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples.  In the unsupervised case, it performs consistently with other spectral clustering algorithms.  In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the 20 Newsgroups data set.  Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data.  By using normalized affinity matrices which are both symmetric and stochastic, we also obtain both a probabilistic interpretation of our method and certain guarantees of performance. 
A Joint Model for Semantic Role Labeling| Abstract We present a semantic role labeling system submitted to the closed track of the CoNLL-2005 shared task.  The system, introduced in (Toutanova et al. , 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework.  We also describe experiments aimed at increasing the robustness of the system in the presence of syntactic parse errors.  Our final system achieves F1-Measures of 76. 68 and 78. 45 on the development and the WSJ portion of the test set, respectively. 
Extensions to HMM-based Statistical Word Alignment Models| Abstract This paper describes improved HMM-based word level alignment models for statistical machine translation.  We present a method for using part of speech tag information to improve alignment accuracy, and an approach to modeling fertility and correspondence to the empty word in an HMM alignment model.  We present accuracy results from evaluating Viterbi alignments against human-judged alignments on the Canadian Hansards corpus, as compared to a bigram HMM, and IBM model 4.  The results show up to 16% alignment error reduction. 
Robust textual inference via learning and abductive reasoning| Abstract We present a system for textual inference (the task of inferring whether a sentence follows from another text) that uses learning and a logical-formula semantic representation of the text.  More precisely, our system begins by parsing and then transforming sentences into a logical formula-like representation similar to the one used by (Harabagiu et al. , 2000).  An abductive theorem prover then tries to find the minimum "cost" set of assumptions necessary to show that one statement follows from the other.  These costs reflect how likely different assumptions are, and are learned automatically using information from syntactic/semantic features and from linguistic resources such as WordNet.  If one sentence follows from the other given only highly plausible, low cost assumptions, then we conclude that it can be inferred.  Our approach can be viewed as combining statistical machine learning and classical logical reasoning, in the hope of marrying the robustness and scalability of learning with the preciseness and elegance of logical theorem proving.  We give experimental results from the recent PASCAL RTE 2005 challenge competition on recognizing textual inferences, where a system using this inference algorithm achieved the highest confidence weighted score. 
Is it Harder to Parse Chinese, or the Chinese Treebank?| Abstract We present a detailed investigation of the challenges posed when applying parsing models developed against English corpora to Chinese.  We develop a factored-model statistical parser for the Penn Chinese Treebank, showing the implications of gross statistical differences between WSJ and Chinese Treebanks for the most general methods of parser adaptation.  We then provide a detailed analysis of the major sources of statistical parse errors for this corpus, showing their causes and relative frequencies, and show that while some types of errors are due to dicult ambiguities inherent in Chinese grammar, others arise due to treebank annotation practices.  We show how each type of error can be addressed with simple, targeted changes to the independence assumptions of the maximum likelihood-estimated PCFG factor of the parsing model, which raises our F1 from 80. 7% to 82. 6% on our development set, and achieves parse accuracy close to the best published figures for Chinese parsing.  1 Background Even narrow-coverage context-free natural language grammars produce explosive ambiguity (Church and Patil, 1982).  Today's treebankderived broad-coverage CFGs generate even more, some of it genuine linguistic ambiguity and some of it artificial (see (Krotov et al. , 1998) and (Johnson, 1998) for discussion).  Corpusbased statistical parsing is a leading technique to deal with this extreme ambiguity; the vast bulk of work in this field has been done in English, using the Wall Street Journal section of the English Penn Treebank (ETB).  State-ofthe-art statistical parsing techniques now handle most ambiguity adequately; the best statistical parsers for the ETB are now at roughly 90% labeled bracketing accuracy (Charniak, 2000; Collins, 2000).  The remaining dicult-toresolve ambiguities are fairly well-understood for English|perhaps the best-known are {fiat|flat} versus embedded adjunction structures (see (Johnson, 1998) for discussion) and NP-conjunct versus {fiat|flat} NP coordinations|but are hardly analyzed at all for any other language.  More recently, however, a wider variety of parsed corpora has become available in other languages.  We take advantage of the recently released Penn Chinese Treebank (version 2. 0, abbreviated here as CTB; (Xue et al. , 2002)) to address these questions for Chinese, a language with less morphology and more mixed headedness than English.  Chinese, as we will show, has a rather different set of salient ambiguities from the perspective of statistical parsing.  This section provides background on relevant linguistic differences between Chinese and English, and on relevant tree-structural differences between the two treebanks. 
Parse Selection on the Redwoods Corpus: 3rd Growth Results| Abstract This report details experimental results of using stochastic disambiguation models for parsing sentences from the Redwoods treebank (Oepen et al. , 2002).  The goals of this paper are two-fold: (i) to report accuracy results on the more highly ambiguous latest version of the treebank, as compared to already published results achieved by the same stochastic models on a previous version of the corpus, and (ii) to present some newly developed models using features from the HPSG signs, as well as the MRS dependency graphs. 
Fast Exact Inference with a Factored Model for Natural Language Parsing| Abstract We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models.  This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models.  Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efficient, exact inference. 
Voice and grammatical relations in Indonesian: A new perspective| Abstract This paper deals with the voice system of Indonesian, and argues that certain of the constructions traditionally analysed as passives, should be given a different treatment, parallel to arguments by Kroeger (1993) for Tagalog.  We examine the role of different conceptions of subject and their place in binding.  We show that, unlike other Western Austronesian languages, the logical subject -- l-subject for short (i. e. , the semantically most prominent argument) plays little role in binding: being a logicalsubject alone does not make an argument a binder.  Syntactic prominence is crucial, and in particular the data on binding in Indonesian presented here further confirms the notion of syntacticised argument structured (a-str) first proposed in Manning (1994, 1996b) and also adopted in Arka (1998) wherein a central role is given to the notion of a-subject.  Like other Austronesian languages, the (surface) grammatical subject (i. e. , the SUBJ in the f-structure or gr-subject for short) plays little role, especially in the binding of morphologically complex reflexives.  The data from binding is supported by other syntactic tests such as topicalisation with pronominal copy. 
Parse disambiguation for a rich HPSG grammar|
The lexical integrity of Japanese causatives| Paper presented at the 68th annual. 
Accuracte unlexicalized parsing|
Parsing and Hypergraphs|
A theory of non-constituent coordination based on finite-state rules|
Classifying unknown proper noun phrases without context|
The segmentation problem in morphology learning|
Exploring the boundaries: Gene and protein identification in biomedical text|
Using dictionaries of Australian Aboriginal languages"|
Kirrkirr: Interactive visualisation and multimedia from a structured Warlpiri dictionary"|
The LinGo Redwoods treebank: Motivation and preliminary apllications|
A Generative Model for Semantic Role Labeling|
Information spreading and levels of representation in LFG|
Soft constraints mirror hard constraints: Voice and person in English and Lummi|
A dictionary database template for Australian languages|
The lexical integrity of Japanese causatives|
An O(n 3 ) agenda-based chart parser for arbitrary probabilistic context-free grammars|
to appear| The Lexical Integrity of Japanese Causatives. 
Jesus Gimenez and Lluis M` arquez|
Exploting the block structure of the web for computing PageRank|
Optimization, Maxent Models, and Conditional Estimation without Magic|
Generating summaries of multiple news articles|
and Hinrich Schutze,|
Probabilistic parsing using left corner language models|
Complex Predicates and Information Spreading in LFG|
Recognizing genes and proteins in MEDLINE abstracts|
Exploring sentiment summarization|
Foundations of statistical language processing|
Deep Dependencies from Context-Free Statistical Parsers: Correcting the Surface Dependency Approximation|
Argument Structure as a Locus for Binding Theory|
Accurate Unlexicalized Parsing Proceedings of the 41st Annual Meeting of the|
To appear| Probabilistic approaches to syntax. 
Three generative, lexicalised models for statistical parsing|
Combining heterogeneous classifiers for word-sense disambiguation|
Accurate unlexicalised parsing|
A generative constituentcontext model for grammar induction|

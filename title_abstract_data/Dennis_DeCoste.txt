Dynamic Across-Time Measurement Interpretation| Abstract Incrementally maintaining a qualitative understanding of physical system behavior based on observations is crucial to tasks such as real-time control, monitoring, and diagnosis.  This paper describes the DATMI theory for interpretation tasks.  The key idea of DATMI is to dynamically maintain a concise representation of the space of local and global interpretations across time that are consistent with the observations.  This representation has two key advantages.  First, a set of possible interpretations is more useful than a single (best) candidate for many tasks, such as conservative monitoring.  Second, this representation simplifies switching to alternative interpretations when data are faulty or incomplete.  Domain-specific knowledge about state and transition probabilities can be used to suggest the interpretation which is most likely.  Domain-specific knowledge about durations of states and paths of states can also be used to further constrain the interpretation space.  When no consistent interpretation exists, faulty-data hypotheses are generated and then tested by adjusting the interpretation space.  The DATMI theory has been tested via implementation and we describe its performance on two examples. 
Anytime Interval-Valued Outputs for Kernel Machines: Fast Support Vector Machine Classification via Distance Geometry| Abstract Classifying M query examples using a support vector machine containing L support vectors traditionally requires exactly M # L kernel computations.  We introduce a computational geometry method for which classi#cation cost becomes roughly proportional to each query's diculty (e. g.  distance from the discriminant hyperplane).  It produces exactly the same classifications, while typically requiring vastly fewer kernel computations.  Related \reduced set" methods (e. g.  (Burges, 1996)) similarly lower the effective L, but provide neither proportionality with diculty nor guaranteed preservation of classi#cations.  Experiments on UCI and NASA data illustrate 2 to 64-fold speedups, across both SVMs and kernel Fisher discriminants. 
Training Invariant Support Vector Machines| Abstract.  Practical experience has shown that in order to obtain the best possible
Support Vector Machines and Kernel Fisher Discriminants: A Case Study using Electronic Nose Data| ABSTRACT Kernel methods provide a promising new family of algorithms for machine learning and data mining applications.  In particular, kernel-based nonlinear classifiers such as support vector machines (SVMs) and kernel fisher discriminants (KFDs) have been found to work well in practical problems.  In addition, there are methods for training these algorithms on large-scale data sets making them very suitable for use in data mining.  In this paper, we evaluate the performance of SVMs and KFDs on a dataset generated with a conducting polymer composite-based electronic nose.  The ability of SVM and KFD classifiers to correctly identify the functional class (category) of a chemical based on its electronic nose signature is evaluated and compared against other more traditional methods, including nearest neighbors and linear Fisher discriminants.  Tradeoffs between the different kernel methods and performance relative to more traditional methods are discussed. 
Automated Learning and Monitoring of Limit Functions| Abstract In practice, automated monitoring of spacecraft relies heavily on limitsensing and simulation.  Unfortunately, limit-sensing tends to be too imprecise (missing alarms) and simulation tends to be too precise (false alarms).  To help overcome those disadvantages, we present an anytime algorithm called envelope learning and monitoring via error relaxation (ELMER).  It can incrementally generate successively tighter hi/low limit functions envelopes, essentially moving from the wide static limits typical of limit-sensing toward the precise predictions of simulations, while avoiding unacceptable false alarm rates.  We summarize the techniques and motivations underlying ELMER and illustrate its performance on telemetry data from the NASA spacecraft TOPEX. 
Automated Event Detection in Space Instruments: A Case Study Using IPEX-2 Data and Support Vector Machines| ABSTRACT This paper discusses our ongoing experimental work to train classifier software for automated detection of rare events of interest in space instruments.  As a concrete real-world example, this paper focuses on the task of detecting possible thermal snap events, using accelerometer data describing the movements of the boom structure during the 1997 IPEX-2 experiment.  In particular, this paper explores the use of recent promising techniques for automated classification based on support vector machines. 
Alpha seeding for support vector machines| ABSTRACT A key practical obstacle in applying support vector machines to many large-scale data mining tasks is that SVM training time generally scales quadratically (or worse) in the number of examples or support vectors.  This complexity is further compounded when a specific SVM training is but one of many, such as in Leave-One-Out-Cross-Validation (LOOCV) for determining optimal SVM parameters or as in wrapper-based feature selection.  In this paper we explore new techniques for reducing the amortized cost of each such SVM training, by seeding successive SVM trainings with the results of previous similar trainings. 
Anytime Query-Tuned Kernel Machines via Cholesky Factorization| Abstract Kernel machines (including support vector machines) offer powerful new methods for improving the accuracy and robustness of fundamental data mining operations on challenging (e. g.  high-dimensional) data, including classification, regression, dimensionality reduction, and outlier detection.  However, a key tradeoff to this power is that kernel machines typically compute their outputs in terms of a large fraction of the training data, making it dicult to scale them up to train and run over massive data sets typically tackled in data mining contexts.  We recently demonstrated 2 to 64-fold querytime speedups of SVM and Kernel Fisher classifiers via a new computational geometry method for anytime output bounds [4].  This new paper refines our approach in two key ways.  First, we introduce a simple linear algebra formulation based on standard Cholesky factorization, yielding simpler equations and lower computational overhead.  Second, this new formulation suggests new methods for achieving additional speedups, including tuning on query samples.  We demonstrate effectiveness on three benchmark datasets. 
Mining Multivariate Time-Series Sensor Data to Discover Behavior Envelopes| Abstract This paper addresses large-scale regression tasks using a novel combination of greedy input selection and asymmetric cost.  Our primary goal is learning envelope functions suitable for automated detection of anomalies in future sensor data.  We argue that this new approach can be more effective than traditional techniques, such as static red-line limits, variance-based error bars, and general probability density estimation. 
Sparse Greedy MPM Classification|
Making an Impact: Artificial Intelligence at the Jet Propulsion Laboratory|
Bounds estimation via regression with asymmetric cost functions|
Learning and monitoring with families of high/low envelope functions: Maximizing predictive precision while minimizing false alarms|
CATMS: An ATMS Which Avoids Label Explosions|
Catms: An atms which avoids label explosions|
Dynamic Across-Time Measurement Interpretation: Maintaining Qualitative Understandings of Physical System Behavior|
Adaptive Resource Profiling",|

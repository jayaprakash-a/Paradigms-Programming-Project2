Anomaly Detection is Classification| Abstract We show that anomaly detection can be interpreted as a binary classification problem.  Using this interpretation we propose a support vector machine (SVM) for anomaly detection.  We then present some theoretical results which include consistency and learning rates.  Finally, we experimentally compare our SVM with the standard one-class SVM. 
Fast rates to Bayes for kernel methods| Abstract We establish learning rates to the Bayes risk for support vector machines (SVMs) with hinge loss.  In particular, for SVMs with Gaussian RBF kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels.  Finally, we compare our methods with a recent paper of G.  Blanchard et al. . 
A classification framework for anomaly detection| Abstract One way to describe anomalies is by saying that anomalies are not concentrated. 
Fast Rates for Support Vector Machines| Abstract We establish learning rates to the Bayes risk for support vector machines with hinge loss (L1-SVM's).  Since a theorem of Devroye states that no learning algorithm can learn with a uniform rate to the Bayes risk for all probability distributions we have to restrict the class of considered distributions: in order to obtain fast rates we assume a noise condition recently proposed by Tsybakov and an approximation condition in terms of the distribution and the reproducing kernel Hilbert space used by the L1-SVM.  For Gaussian RBF kernels with varying widths we propose a geometric noise assumption on the distribution which ensures the approximation condition.  This geometric assumption is not in terms of smoothness but describes the concentration of the marginal distribution near the decision boundary.  In particular we are able to describe nontrivial classes of distributions for which L1-SVM's using a Gaussian kernel can learn with almost linear rate.  We use various new and recently introduced techniques for establishing our results: the analysis of the estimation error is based on Talagrand's concentration inequality and local Rademacher averages.  We furthermore develope a shrinking technique which allows us to control the typical size of the norm of the L1-SVM solution.  It turns out that the above mentioned approximation assumption has a crucial impact on both the application of Talagrand's inequality and the shrinking technique.  Moreover, for Gaussian kernels we develope a smoothing technique which allows us to treat the approximation error in a way directly linked to the classification problem.  Finally, we prove some new bounds on covering numbers related to Gaussian RBF kernels. 
Sparseness of Support Vector Machines---Some Asymptotically Sharp Bounds| Abstract The decision functions constructed by support vector machines (SVM's) usually depend only on a subset of the training set---the so-called support vectors.  We derive asymptotically sharp lower and upper bounds on the number of support vectors for several standard types of SVM's.  Our results significantly improve recent achievments of the author. 
On the generalization ability of support vector machines| Abstract In this article we study the generalization abilities of the 1-norm soft margin classifier.  We show for several standard kernels, like the Gaussian RBF kernel, that this algorithm yields arbitrarily good generalization results provided that the factor which weighs the sum of the slack variables is chosen well.  This kind of result is completely new.  Indeed, for the first time it can now be explained without a-priori assumptions on the classification problem why the support vector approach may provide good generalization performance.  Our considerations are firstly based on an approximation property of the used kernels which also gives new insight into the role of kernels in these and other algorithms.  Thus it may also be of independent interest.  Secondly, the result is archieved by a precise investigation of the optimization problem which underlies the 1-norm soft margin algorithm. 
Consistency of support vector machines and other regularized kernel classifiers| Abstract We show that various classifiers that are based on a minimization of a regularized risk are universally consistent. 
On Robustness Properties of Convex Risk Minimization Methods for Pattern Recognition| Abstract The paper brings together methods from two disciplines: machine learning theory and robust statistics.  Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition.  Assumptions are given for the existence of the influence function of the classifiers and for bounds of the influence function.  Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases.  A sensitivity analysis of the support vector machine is given. 
Provably Fast Algorithms for Anomaly Detection Los Alamos National Laboratory| Abstract We describe a solution method for one of the most common anomaly detection formulations [23] that is proven to be computationally ecient, universally consistent, and to guarantee near optimal finite sample performance for a large class of (practical) distributions [23, 21].  We also describe an algorithm for this method that accepts the desired accuracy # as an input and produces an approximate solution that is guaranteed to satisfy this accuracy in low order polynomial time.  Experimental results are used to demonstrate the actual run times for a typical problem. 
Los Alamos National Laboratory Stability of Unstable Learning Algorithms| Abstract We introduce a formalism called graphical learning algorithms and use it to produce bounds on error deviance for unstable learning algorithms.  This formalism suggests a flexible class of extensions of existing algorithms for which risk can be decomposed into algorithmic model risk plus estimation error in a way that enables bounds on estimation error and analysis of the algorithmic model risk.  For example we obtain error deviance bounds for support vector machines (SVMs) with variable offset parameter and estimation error bounds for variations of SVM where the offset parameter is selected to minimize empirical risk.  In addition we prove convergence to the Bayes error for variations of SVM that use a universal kernel and choose the regularization parameter to minimize empirical error.  We provide experimental results that suggest that these variations may offer advantages over standard SVMs in both computation and generalization performance. 
Matrix Multiplication: A Case Study of Algorithm Engineering| ABSTRACT Modern machines present two challenges to algorithm engineers and compiler writers: They have superscalar, super-pipelined structure, and they have elaborate memory subsystems specifically designed to reduce latency and increase bandwidth.  Matrix multiplication is a classical benchmark for experimenting with techniques used to exploit machine architecture and to overcome the limitations of contemporary memory subsystems.  This research aims at advancing the state of the art of algorithm engineering by balancing instruction level parallelism, two levels of data tiling, copying to provably avoid any cache conflicts, and prefetching in parallel to algorithmic operations, in order to fully exploit the memory bandwidth.  Measurements show that the resultant matrix multiplication algorithm outperforms IBM's ESSL by 6. 8-31. 8%, is less sensitive to the size of the input data, and scales better.  The techniques presented in this paper have been developed specifically for matrix multiplication.  However, they are quite general and may be applied to other numeric algorithms.  We believe that some of our concepts may be generalized to be used as compile-time techniques. 
Sparseness of Support Vector Machines| Abstract Support vector machines (SVM's) construct decision functions that are linear combinations of kernel evaluations on the training set.  The samples with non-vanishing coecients are called support vectors.  In this work we establish lower (asymptotical) bounds on the number of support vectors.  On our way we prove several results which are of great importance for the understanding of SVM's.  In particular, we describe to which \limit" SVM decision functions tend, discuss the corresponding notion of convergence and provide some results on the stability of SVM's using subdifferential calculus in the associated reproducing kernel Hilbert space. 
On the influence of the kernel on the generalization ability of support vector machines, submitted to|
Support vector machines are universally consistent|
Stability of unstable learning algorithms|
Consistency of support vector machines and other regularized kernel machine|
Learning rates for support vector machines for density level detection|
Matrix Multiplication: A Case Study of Enhanced Data Cache Utilization|
Some estimates for the entropy numbers of convex hulls with finitely many extreme points|
On the Influence of the Kernel on the Consistency of Support Vector Machines|
Fast Rates for Support Vector Machines|
On the Optimal Parameter Choice for v-Support Vector Machines|
A computation{performance bound for SVM classi#ers|
Learning rates for density level detection|
QP algorithms with guaranteed accuracy and run time for support vector machines|
Symmetry and pattern formation in coupled cell networks,|
Which data-dependent bounds are suitable for SVM's?,|
Entropy of C(K)-valued operators,|
Entropy numbers of convex hulls and an application to learning algorithms,|

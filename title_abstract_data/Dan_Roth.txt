Robust Reading: Identification and Tracing of Ambiguous Names| Abstract A given entity, representing a person, a location or an organization, may be mentioned in text in multiple, ambiguous ways.  Understanding natural language requires identifying whether different mentions of a name, within and across documents, represent the same entity.  We develop an unsupervised learning approach that is shown to resolve accurately the name identification and tracing problem.  At the heart of our approach is a generative model of how documents are generated and how names are "sprinkled" into them.  In its most general form, our model assumes: (1) a joint distribution over entities, (2) an "author" model, that assumes that at least one mention of an entity in a document is easily identifiable, and then generates other mentions via (3) an appearance model, governing how mentions are transformed from the "representative" mention.  We show how to estimate the model and do inference with it and how this resolves several aspects of the problem from the perspective of applications such as questions answering. 
Submitted for Publication On generalization bounds, projection profile, and margin distribution| Abstract We study generalization properties of linear learning algorithms and develop a data dependent approach that is used to derive generalization bounds that depend on the margin distribution.  Our method makes use of random projection techniques to allow the use of existing VC dimension bounds in the effective, lower, dimension of the data.  Comparisons with existing generalization bound show that our bounds are tighter and meaningful in cases existing bounds are not. 
Learning to Reason with a Restricted View \Lambda| Abstract The Learning to Reason framework combines the study of Learning and Reasoning into a single task.  Within it, learning is done specifically for the purpose of reasoning with the learned knowledge.  Computational considerations show that this is a useful paradigm; in some cases learning and reasoning problems that are intractable when studied separately become tractable when performed as a task of Learning to Reason.  In this paper we study Learning to Reason problems where the interaction with the world supplies the learner only partial information in the form of partial assignments.  Several natural interpretations of partial assignments are considered and learning and reasoning algorithms using these are developed.  The results presented exhibit a tradeoff between learnability, the strength of the oracles used in the interface, and the range of reasoning queries the learner is guaranteed to answer correctly. 
On Learning Read-k-Satisfy-j DNF| Abstract We study the learnability of Read-k-Satisfy-j (RkSj) DNF formulas.  These are boolean formulas in disjunctive normal form (DNF), in which the maximum number of occurrences of a variable is bounded by k, and the number of terms satisfied by any assignment is at most j.  After motivating the investigation of this class of DNF formulas, we present an algorithm that for any unknown RkSj DNF formula to be learned, with high probability finds a logically equivalent DNF formula using the well-studied protocol of equivalence and membership queries.  The algorithm runs in polynomial time for k \Delta j = O( log n log log n ), where n is the number of input variables. 
A Linear Programming Formulation for Global Inference in Natural Language Tasks| Abstract Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e. g. , named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints.  Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc.  We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations.  Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the "human-like" quality of the inferences. 
Defaults and Relevance in Model Based Reasoning \Lambda| Abstract Reasoning with model-based representations is an intuitive paradigm, which has been shown to be theoretically sound and to possess some computational advantages over reasoning with formula-based representations of knowledge.  In this paper we present more evidence to the value of such representations.  Our results hinge on the notion of relevance, and model based representations are shown to be useful in capturing relevant information, and in allowing to ignore irrelevant information.  In particular, we consider situations where context-specific information is used in the process of reasoning.  We show that reasoning with model-
A SNoW-Based Face Detector| Abstract A novel learning approach for human face detection using a network of linear units is presented.  The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features.  A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces.  Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others.  Furthermore, learning and evaluation using the SNoW-based method are significantly more ecient than with other methods. 
COLING'02 Learning Question Classifiers| Abstract In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer.  These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.  This paper presents a machine learning approach to question classification.  We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.  We show accurate results on a large collection of free-form questions used in TREC 10. 
Finding the Largest Area Axis-Parallel Rectangle in a Polygon \Lambda| Abstract This paper considers the geometric optimization problem of finding the Largest area axis-parallel Rectangle (LR) in an n-vertex general polygon.  We characterize the LR for general polygons by considering different cases based on the types of contacts between the rectangle and the polygon.  A general framework is presented for solving a key subproblem of the LR problem which dominates the running time for a variety of polygon types.  This framework permits us to transform an algorithm for orthogonal polygons into an algorithm for nonorthogonal polygons.  Using this framework, we show that the LR in a general polygon (allowing holes) can be found in O(n log 2 n) time.  This matches the running time of the best known algorithm for orthogonal polygons.  References are given for the application of the framework to other types of polygons.  For each type, the running time of the resulting algorithm matches the running time of the best known algorithm for orthogonal polygons of that type.  A lower bound of time in \Omega(n log n) is established for finding the LR in both self-intersecting polygons and general polygons with holes.  The latter result gives us both a lower bound of \Omega(n log n) and an upper bound of O(n log 2 n) for general polygons. 
Learning Active Classifiers| Abstract Many classification algorithms are "passive", in that they assign a class-label to each instance based only on the description given, even if that description is incomplete.  In contrast, an active classifier can --- at some cost --- obtain the values of missing attributes, before deciding upon a class label.  The expected utility of using an active classifier depends on both the cost required to obtain the additional attribute values and the penalty incurred if it outputs the wrong classification.  This paper considers the problem of learning near-optimal active classifiers, using a variant of the probably-approximatelycorrect (PAC) model.  After defining the framework --- which is perhaps the main contribution of this paper --- we describe a situation where this task can be achieved efficiently, but then show that the task is often intractable. 
Learning a Sparse Representation for Object Detection| Abstract.  We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects.  A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest.  Images are then represented using parts from this vocabulary, along with spatial relations observed among them.  Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class.  The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration.  We report experiments on images of side views of cars.  Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation.  In addition, we discuss and offer solutions to several methodological issues that are significant for the research community to be able to evaluate object detection approaches. 
Generalized Inference with Multiple Semantic Role Labeling Systems| Abstract We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicateargument output by solving an optimization problem.  The optimization stage, which is solved via integer linear programming, takes into account both the recommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the final role labeling.  We illustrate a significant improvement in overall SRL performance through this inference.  1 SRL System Architecture Our SRL system consists of four stages: pruning, argument identification, argument classification, and inference.  In particular, the goal of pruning and argument identification is to identify argument candidates for a given verb predicate.  The system only classifies the argument candidates into their types during the argument classification stage.  Linguistic and structural constraints are incorporated in the inference stage to resolve inconsistent global predictions.  The inference stage can take as its input the output of the argument classification of a single system or of multiple systems.  We explain the inference for multiple systems in Sec.  2.  1. 1 Pruning Only the constituents in the parse tree are considered as argument candidates.  In addition, our system exploits the heuristic introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents.  The heuristic is a recursive process starting from the verb whose arguments are to be identified.  It first returns the siblings of the verb; then it moves to the parent of the verb, and collects the siblings again.  The process goes on until it reaches the root.  In addition, if a constituent is a PP(propositional phrase), its children are also collected.  Candidates consisting of only a single punctuation mark are not considered.  This heuristic works well with the correct parse trees.  However, one of the errors by automatic parsers is due to incorrect PPattachment leading to missing arguments.  To attempt to fix this, we consider as arguments the combination of any consecutive NPand PP, and the split of NPand PPinside the NPthat was chosen by the previous heuristics.  1. 2 Argument Identification The argument identification stage utilizes binary classification to identify whether a candidate is an argument or not.  We train and apply the binary classifiers on the constituents supplied by the pruning stage.  Most of the features used in our system are standard features, which include .  Predicate and POS tag of predicate indicate the lemma of the predicate and its POS tag. 
Efficient, Strongly Consistent Implementations of Shared Memory| Abstract.  We present linearizable implementations for two distributed organizations of multiprocessor shared memory.  For the full caching organization, where each process keeps a local copy of the whole memory, we present a linearizable implementations of read/write memory objects that achieves essentially optimal efficiency and allows quantitative degradation of the less frequently employed operation.  For the single ownership organization, where each memory object is "owned" by a single process which is most likely to access it frequently, our linearizable implementation allows local operations to be performed much faster (almost instantaneously) than remote ones.  We suggest to combine these organizations in a "hybrid" memory structure that allows processes to access local and remote information in a transparent manner, while at a lower level of the memory consistency system, different portions of the memory are allocated to employ the suitable implementation based on their typical usage and sharing pattern. 
Generalization Bounds for the Area Under the ROC Curve| Abstract We study generalization properties of the area under the ROC curve (AUC), a quantity that has been advocated as an evaluation criterion for the bipartite ranking problem.  The AUC is a different term than the error rate used for evaluation in classification problems; consequently, existing generalization bounds for the classification error rate cannot be used to draw conclusions about the AUC.  In this paper, we define the expected accuracy of a ranking function (analogous to the expected error rate of a classification function), and derive distribution-free probabilistic bounds on the deviation of the empirical AUC of a ranking function (observed on a finite data sequence) from its expected accuracy.  We derive both a large deviation bound, which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on a test sequence, and a uniform convergence bound, which serves to bound the expected accuracy of a learned ranking function in terms of its empirical AUC on a training sequence.  Our uniform convergence bound is expressed in terms of a new set of combinatorial parameters that we term the bipartite rank-shatter coefficients; these play the same role in our result as do the standard VC-dimension related shatter coefficients (also known as the growth function) in uniform convergence results for the classification error rate.  A comparison of our result with a recent uniform convergence result derived by Freund et al.  (2003) for a quantity closely related to the AUC shows that the bound provided by our result can be considerably tighter. 
Learning Question Classifiers: The Role of Semantic Information+#| Abstract In order to respond correctly to a free form factual question given a large collection
Proceedings of the 18th Annual Conference on Learning Theory, 2005 Learnability of Bipartite Ranking Functions| Abstract.  The problem of ranking, in which the goal is to learn a real-valued ranking function that induces a ranking or ordering over an instance space, has recently gained attention in machine learning.  We define a model of learnability for ranking functions in a particular setting of the ranking problem known as the bipartite ranking problem, and derive a number of results in this model.  Our first main result provides a sufficient condition for the learnability of a class of ranking functions F : we show that F is learnable if its bipartite rank-shatter coefficients, which measure the richness of a ranking function class in the same way as do the standard VC-dimension related shatter coefficients (growth function) for classes of classification functions, do not grow too quickly.  Our second main result gives a necessary condition for learnability: we define a new combinatorial parameter for a class of ranking functions F that we term the rank dimension of F , and show that F is learnable only if its rank dimension is finite.  Finally, we investigate questions of the computational complexity of learning ranking functions. 
To Appear in IJCAI-99 Relational Learning for NLP using Linear Threshold Elements| Abstract We describe a coherent view of learning and reasoning with relational representations in the context of natural language processing.  In particular, we discuss the Neuroidal Architecture, Inductive Logic Programming and the SNoW system explaining the relationships among these, and thereby offer an explanation of the theoretical basis for the SNoW system.  We suggest that extensions of this system along the lines suggested by the theory may provide new levels of scalability and functionality. 
PhraseNet: Towards Context Sensitive Lexical Semantics| Abstract This paper introduces PhraseNet, a contextsensitive lexical semantic knowledge base system.  Based on the supposition that semantic proximity is not simply a relation between two words in isolation, but rather a relation between them in their context, English nouns and verbs, along with contexts they appear in, are organized in PhraseNet into Consets; Consets capture the underlying lexical concept, and are connected with several semantic relations that respect contextually sensitive lexical information.  PhraseNet makes use of WordNet as an important knowledge source.  It enhances a WordNet synset with its contextual information and refines its relational structure by maintaining only those relations that respect contextual constraints.  The contextual information allows for supporting more functionalities compared with those of WordNet.  Natural language researchers as well as linguists and language learners can gain from accessing PhraseNet with a word token and its context, to retrieve relevant semantic information.  We describe the design and construction of PhraseNet and give preliminary experimental evidence to its usefulness for NLP researches. 
View-Based 3D Object Recognition Using SNoW| ABSTRACT This paper describes a novel view-based algorithm for 3D object recognition using a network of linear units.  The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features.  We use the pixel-level representation in the experiments and compare the performance of SNoW with Support Vector Machines and nearest neighbor methods on 3D object recognition using the 100 objects in the Columbia Image Object Database (COIL100).  Experimental results show that SNoW-based method outperform SVM-based system in terms of recognition rate and the computational cost involved in learning.  The empirical results also provide insight for practical and theoretical considerations on view-based methods for 3D object recognition. 
IJCAI'05: A Poster Presentation An Inference Model for Semantic Entailment in Natural Language| Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another.  This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications.  We present a principled approach to this problem that builds on inducing re-representations of text snippets into a hierarchical knowledge representation along with a sound inferential mechanism that makes use of it to prove semantic entailment. 
Semantic Integration over Text: From Ambiguous Names to Identifiable Entities| Abstract Semantic integration focuses on discovering, representing, and manipulating correspondences between entities in disparate data sources.  The topic has been widely studied in the context of structured data, with problems being considered including ontology and schema matching, matching relational tuples, and reconciling inconsistent data values.  In recent years, however, semantic integration over text has also received increasing attention.  This paper studies a key challenge in semantic integration over text: identifying whether different mentions of real world entities, such as "JFK" and "John Kennedy", within and across
Mapping Dependencies Trees: An Application to Question Answering| Abstract We describe an approach for answer selection in a free form question answering task.  In order to go beyond the key-word based matching in selecting answers to questions, one would like to incorporate both syntactic and semantic information in the question answering process.  We achieve this goal by representing both questions and candidate passages using dependency trees, and incorporating semantic information such as named entities in this representation.  The sentence that best answers a question is determined to be the one that minimizes the generalized edit distance between it and the question tree, computed via an approximate tree matching algorithm.  We evaluate the approach on question-answer pairs taken from previous TREC Q/A competitions.  Preliminary experiments show its potential by significantly outperforming common bag-of-word scoring methods. 
Exploring Evidence for Shallow Parsing| Abstract Significant amount of work has been devoted recently to develop learning techniques that can be used to generate partial (shallow) analysis of natural language sentences rather than a full parse.  In this work we set out to evaluate whether this direction is worthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform | identifying phrases in sentences.  We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts. 
To Appear in COLING-ACL '98 Part of Speech Tagging Using a Network of Linear Separators| Abstract We present an architecture and an on-line learning algorithm and apply it to the problem of part-ofspeech tagging.  The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm.  Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space.  In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction -- selecting the part of speech of a word.  The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems.  The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded.  This has significance in terms of efficiency, as well as quick adaptation to new contexts.  We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS. 
Reasoning with Examples: Propositional Formulae and Database Dependencies| Abstract For humans, looking at how concrete examples behave is an intuitive way of deriving conclusions.  The drawback with this method is that it does not necessarily give the correct results.  However, under certain conditions example-based deduction can be used to obtain a correct and complete inference procedure.  This is the case for Boolean formulae (reasoning with models) and for certain types of database integrity constraints (the use of Armstrong relations).  We show that these approaches are closely related, and use the relationship to prove new results about the existence and sizes of Armstrong relations for Boolean dependencies.  Furthermore, we exhibit close relations between the questions of finding keys in relational databases and that of finding abductive explanations.  Further applications of the correspondence between these two approaches are also discussed. 
The Role of Semantic Information in Learning Question Classifiers| Abstract Question Classification is commonly used in question answering systems to perform a semantic classification of the target answer in an effort to provide additional information to downstream processes.  It is different from the common text categorization task in the sense that questions are relatively short and contain less word-based information compared with classification of the entire text.  This work presents a machine learning approach to this task.  Our approach is to augment the questions with syntactic and semantic analysis, as well as external semantic knowledge, as input to the text classifier.  It is shown that, in the context of question classification, augmenting the input of the classifier with appropriate semantic category information results in significant improvements to classification accuracy. 
On Kernel Methods for Relational Learning| Abstract Kernel methods have gained a great deal of popularity in the machine learning community as a method to learn indirectly in highdimensional feature spaces.  Those interested in relational learning have recently begun to cast learning from structured and relational data in terms of kernel operations.  We describe a general family of kernel functions built up from a description language of limited expressivity and use it to study the benefits and drawbacks of kernel learning in relational domains.  Learning with kernels in this family directly models learning over an expanded feature space constructed using the same description language.  This allows us to examine issues of time complexity in terms of learning with these and other relational kernels, and how these relate to generalization ability.  The tradeoffs between using kernels in a very high dimensional implicit space versus a restricted feature space, is highlighted through two experiments, in bioinformatics and in natural language processing. 
Semantic Role Labeling Via Generalized Inference Over Classifiers| Abstract We present a system submitted to the CoNLL2004 shared task for semantic role labeling.  The system is composed of a set of classifiers and an inference procedure used both to clean the classification results and to ensure structural integrity of the final role labeling.  Linguistic information is used to generate features during classification and constraints for the inference process. 
Clustering Appearances of 3D Objects| Abstract We introduce a method for unsupervised clustering of images of 3D objects.  Our method examines the space of all images and partitions the images into sets that form smooth and parallel surfaces in this space.  It further uses sequences of images to obtain morereliable clustering.  Finally, since our methodrelies on a non-Euclidean similarity measure we introduce algebraic techniques for estimating local properties of these surfaces without first embedding the images in a Euclidean space.  We demonstrate our method by applying it to a large database of images. 
Discriminative Training of Clustering Functions: Theory and Experiments with Entity Identification| Abstract Clustering is an optimization procedure that partitions a set of elements to optimize some criteria, based on a fixed distance metric defined between the elements.  Clustering approaches have been widely applied in natural language processing and it has been shown repeatedly that their success depends on defining a good distance metric, one that is appropriate for the task and the clustering algorithm used.  This paper develops a framework in which clustering is viewed as a learning task, and proposes a way to train a distance metric that is appropriate for the chosen clustering algorithm in the context of the given task.  Experiments in the context of the entity identification problem exhibit significant performance improvements over state-of-the-art clustering approaches developed for this problem. 
Reasoning with Classifiers| The emphasis of the research in machine learning has been on the study of learning single concepts from examples.  In this framework the learner attempts to learn a single hidden function from a collection of examples, assumed to be drawn independently from some unknown probability distribution, and its performance is measured when classifying future examples.  In the context of natural language, for example, work in this direction has allowed researchers and practitioners to address the robust learnability of predicates such as "the part-of-speech of the word can in the given sentence is noun", "the semantic sense of the word "plant" in the given sentence is "an industrial plant", or determine, in a given sentence, the word that starts a noun phrase.  In fact, a large number of disambiguation problems such as part-of speech tagging, word-sense disambiguation, prepositional phrase attachment, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers have been addressed using machine learning techniques -- in each of these problems it is necessary to disambiguate two or more [semantically, syntactically or structurally]-distinct forms which have been fused together into the same representation in some medium; a stand alone classifier can be learned to perform these task quite successfully [10].  ? Paper written to accompany an invited talk at ECML'02.  This research is supported by NSF grants IIS-99-84168,ITR-IIS-00-85836 and an ONR MURI award.  However, in many cases -- as in most natural language and visual processing situations -- higher level decisions depend on the outcomes of several different but mutually dependent classifiers.  Consider, for example, the problem of chunking natural language sentences where the goal is to identify several kinds of phrases (e. g.  noun (NP), verb (VP) and prepositional (PP) phrases) in sentences, as in: [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only $ 1. 8 billion ] [PP in ] [NP September] .  A task of this sort involves multiple predictions that interact in some way.  For example, one way to address the problem is to utilize two classifiers for each type of phrase, one of which recognizes the beginning of the phrase, and the other its end.  Clearly, there are constraints over the predictions; for instance, phrases cannot overlap and there may also be probabilistic constraints over the order of phrases and over their lengths.  The goal is to minimize some global measure of accuracy, not necessarily to maximize the performance of each individual classifier involved in the decision [8].  As a second example, consider the problem of recognizing the kill (KFJ, Oswald) relation in the sentence "J.  V.  Oswald was murdered at JFK after his assassin, R.  U.  KFJ. . . ".  This task requires making several local decisions, such as identifying named entities in the sentence, in order to support the relation identification.  For example, it may be useful to identify that Oswald and KFJ are people, and JFK is a location.  In addition, it is necessary to identify that the action kill is described in the sentence.  All of this information will help to discover the desired relation and identify its arguments.  At the same time, the relation kill constrains its arguments to be people (or at least, not to be locations) and, in turn, helps to enforce that Oswald and KFJ are likely to be people, while JFK is not.  Finally, consider the challenge of designing a free-style natural language user interface that allows users to request in-depth information from a large collection of on-line articles, the web, or other semi-structured information sources.  Specifically, consider the computational processes required in order to "understand" a simple question of the form "what is the fastest automobile in the world?", and respond correctly to it.  A straight forward key-word search may suggest that the following two passages contain the answer: . . .  will stretch Volkswagen's lead in the world's fastest growing vehicle market.  Demand for cars is expected to soar. 
To appear in ECML'01 Understanding Probabilistic Classifiers| Abstract.  Probabilistic classifiers are developed by assuming generative models which are product distributions over the original attribute space (as in naive Bayes) or more involved spaces (as in general Bayesian networks).  While this paradigm has been shown experimentally successful on real world applications,
Submitted for Publication A Learning Approach to Shallow Parsing| Abstract A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally.  The shallow parsing method suggested learns to identify syntactic patterns by combining simple predictors to produce a coherent inference.  Learned predictors are cascaded and their outcome is used as an input to an inference algorithm that produces the final phrases.  We discuss the requirements from a learning system for this approach to be applicable and present a system based on the SNoW learning architecture that satisfies these conditions.  Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented.  In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors. 
A Classification Approach to Word Prediction| Abstract The eventual goal of a language model is to accurately predict the value of a missing word given its context.  We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context.  This approach raises a few new questions that we address.  First, in order to learn good word representations it is necessary to use an expressive representation of the context.  We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction.  Second, since the number of words \competing" for each prediction is large, there is a need to \focus the attention" on a smaller subset of these.  We exhibit the contribution of a \focus of attention" mechanism to the performance of the word predictor.  Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks. 
Submitted for publication Finding the Maximum Area Axis-Parallel Rectangle in a Polygon| Abstract We consider the geometric optimization problem of finding the maximum area axis-parallel rectangle (MAAPR) in an n-vertex general polygon.  We characterize the MAAPR for general polygons by considering different cases based on the types of contacts between the rectangle and the polygon.  We present a general framework for solving a key subcase of the MAAPR problem which dominates the running time for a variety of polygon types.  Using this framework, we obtain the following MAAPR time results: \Theta(n) for xy-monotone polygons, O(nff(n)) for orthogonally convex polygons, O(nff(n) log n) for horizontally (vertically) convex polygons, (where ff(n) is the slowly growing inverse of Ackermann's function), O(n log n) for a special type of horizontally convex polygon, and O(n log 2 n) for general polygons.  For all these types of non-rectilinear polygons, we match the running time of the best known algorithms for their rectilinear counterparts.  We prove a lower bound of time in \Omega(n log n) for finding the MAAPR in both selfintersecting polygons and general polygons with holes.  The latter result gives us both a lower bound of \Omega(n log n) and an upper bound of O(n log 2 n) for general polygons with holes. 
Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms| Abstract We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features.  We demonstrate a tradeoff between the computational efficiency with which these kernels can be computed and the generalization ability of the resulting classifier.  We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions.  We show that these kernels can be used to efficiently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions.  We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions.  While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow's behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efficiently computable. 
AAAI'04 Identification and Tracing of Ambiguous Names: Discriminative and Generative Approaches| Abstract A given entity -- representing a person, a location or an
Learning Coherent Concepts| Abstract This paper develops a theory for learning scenarios where multiple learners co-exist but there are mutual coherency constraints on their outcomes.  This is natural in cognitive learning situations, where "natural" constraints are imposed on the outcomes of classifiers so that a valid sentence, image or any other domain representation is produced.  We formalize these learning situations, after a model suggested in (Roth & Zelenko, 2000) and study generalization abilities of learning algorithms under these conditions in several frameworks.  We show that the mere existence of coherency constraints, even without the learner's awareness of them, deems the learning problem easier than predicted by general theories and explains the ability to generalize well from a fairly small number of examples.  In particular, it is shown that within this model one can develop an understanding to several realistic learning situations such as highly biased training sets and low dimensional data that is embedded in high dimensional instance spaces. 
On the Hardness of Approximate Reasoning \Lambda| Abstract Many AI problems, when formalized, reduce to evaluating the probability that a propositional expression is true. 
Relational Learning for NLP using Linear Threshold Elements| Abstract We describe a coherent view of learning and reasoning with relational representations in the context of natural language processing.  In particular, we discuss the Neuroidal Architecture, Inductive Logic Programming and the SNoW system explaining the relationships among these, and thereby offer an explanation of the theoretical basis for the SNoW system.  We suggest that extensions of this system along the lines suggested by the theory may provide new levels of scalability and functionality. 
Margin Distribution and Learning Algorithms| Abstract Recent theoretical results have shown that improved bounds on generalization error of classifiers can be obtained by explicitly taking the observed margin distribution of the training data into account.  Currently, algorithms used in practice do not make use of the margin distribution and are driven by optimization with respect to the points that are closest to the hyperplane.  This paper enhances earlier theoretical results and derives a practical data-dependent complexity measure for learning.  The new complexity measure is a function of the observed margin distribution of the data, and can be used, as we show, as a model selection criterion.  We then present the Margin Distribution Optimization (MDO) learning algorithm, that directly optimizes this complexity measure.  Empirical evaluation of MDO demonstrates that it consistently outperforms SVM. 
A Large Deviation Bound for the Area Under the ROC Curve| Abstract The area under the ROC curve (AUC) has been advocated as an evaluation criterion for the bipartite ranking problem.  We study large deviation properties of the AUC; in particular, we derive a distribution-free large deviation bound for the AUC which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on an independent test sequence.  A comparison of our result with a corresponding large deviation result for the classification error rate suggests that the test sample size required to obtain an #-accurate estimate of the expected accuracy of a ranking function with #-confidence is larger than that required to obtain an #-accurate estimate of the expected error rate of a classification function with the same confidence.  A simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from finite function classes. 
Learning to Reason with a Restricted View| Abstract The current emphasis of the research in learning theory is on the study of inductive learning (from examples) of concepts (binary classifications of examples).  The work in AI identifies other tasks, such as reasoning, as essential for intelligent agents, but those are not supported by the current learning models.  The Learning to Reason framework was devised to reconcile inductive learning and efficient reasoning.  The framework highlights the fact that new learning questions arise when learning in order to reason.  This paper addresses the task of deductive reasoning, and investigates learning to reason problems in which the examples seen are only partially specified.  The paper presents several interpretations for partial information in the interface with the environment, and develops model based representations and reasoning algorithms that are suitable to deal with partially observable worlds.  Then, learning to reason algorithms that cope with partial information are developed.  These results exhibit a tradeoff between learnability, the strength of the oracles used in the interface and the expressiveness of the queries asked.  This work shows that one can learn to reason with respect to expressive worlds, that cannot be learned efficiently in the traditional learning framework and do not support efficient reasoning in the traditional reasoning framework. 
Feature Extraction Languages for Propositionalized Relational Learning| Abstract We study representations and relational learning over structured domains within a propositionalization framework that decouples feature construction and model construction.  We describe two complementary approaches that address three aspects of the problem: First, we develop and study a flexible knowledge representation for structured data, with an associated language that provides the syntax and a well defined equivalent semantics for expressing complex structured data succinctly.  Second, we use this language to automate the process of feature construction by expressing `types' of objects in the language, which are instantiated in the ground data, allowing us to determine the level at which learning is done.  Finally, this process of re-representation of the domain allows general purpose learning schemes, such as feature efficient linear algorithms and probabilistic representations, to be defined over the resulting space, yielding efficient and expressive learning of relational functions over a structured domain using propositional means. 
Semantic Role Labeling via Integer Linear Programming Inference| Abstract We present a system for the semantic role labeling task.  The system combines a machine learning technique with an inference procedure based on integer linear programming that supports the incorporation of linguistic and structural constraints into the decision process.  The system is tested on the data provided in the CoNLL-2004 shared task on semantic role labeling and achieves very competitive results. 
Linearizable Read/Write Objects \Lambda| Abstract We study the cost of implementing linearizable read/write objects for shared-memory multiprocessors under various assumptions on the available timing information.  We take as cost measure the worst-case response time of performing an operation in distributed implementations of virtual shared memory consisting of such objects.  It is assumed that processes have clocks that run at the same rate as real time and all messages incur a delay in the range [d\Gamma u; d] for some known constants u and d, 0 u d.  In the perfect clocks model, where processes have perfectly synchronized clocks and every message incurs a delay of exactly d, we present a family of optimal linearizable implementations, parameterized by a constant fi, 0 fi 1, for which the worst-case response times for read and write operations are fid and (1\Gamma fi)d, respectively.  The parameter fi may be appropriately chosen to account for the relative frequencies of read and write operations.  Our main result is the first known linearizable implementation for the imperfect clocks model, where clocks are not initially synchronized and message delays can vary, i. e. , u ? 0; it achieves worst-case response times of less than 4u+b (b ? 0 is an arbitrarily small constant) and d + 3u for read and write operations, respectively.  This implementation employs novel synchronization techniques in order to utilize the lower bound on message delay time and achieve bounds on worst-case response times that depend on the message delay uncertainty u.  For a wide range of values of u, these bounds improve upon previously known ones for implementations that support consistency conditions even weaker than linearizability. 
Learning Hebrew Roots: Machine Learning with Linguistic Constraints| Abstract The morphology of Semitic languages is unique in the sense that the major word-formation mechanism is an inherently non-concatenative process of interdigitation, whereby two morphemes, a root and a pattern, are interwoven.  Identifying the root of a given word in a Semitic language is an important task, in some cases a crucial part of morphological analysis.  It is also a non-trivial task, which many humans find challenging.  We present a machine learning approach to the problem of extracting roots of Hebrew words.  Given the large number of potential roots (thousands), we address the problem as one of combining several classifiers, each predicting the value of one of the root's consonants.  We show that when these predictors are combined by enforcing some fairly simple linguistics constraints, high accuracy, which compares favorably with human performance on this task, can be achieved. 
The Necessity of Syntactic Parsing for Semantic Role Labeling| Abstract We provide an experimental study of the role of syntactic parsing in semantic role labeling.  Our conclusions demonstrate that syntactic parse information is clearly most relevant in the very first stage -- the pruning stage.  In addition, the quality of the pruning stage cannot be determined solely based on its recall and precision.  Instead it depends on the characteristics of the output candidates that make downstream problems easier or harder.  Motivated by this observation, we suggest an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves the performance. 
Integer Linear Programming Inference for Conditional Random Fields| Abstract Inference in Conditional Random Fields and Hidden Markov Models is done using the Viterbi algorithm, an ecient dynamic programming algorithm.  In many cases, general (non-local and non-sequential) constraints may exist over the output sequence, but cannot be incorporated and exploited in a natural way by this inference procedure.  This paper proposes a novel inference procedure based on integer linear programming (ILP) and extends CRF models to naturally and eciently support general constraint structures.  For sequential constraints, this procedure reduces to simple linear programming as the inference process.  Experimental evidence is supplied in the context of an important NLP problem, semantic role labeling. 
IAAI'01 Scaling Up Context-Sensitive Text Correction| Abstract The main challenge in an effort to build a realistic system with
A Winnow-Based Approach to Context-Sensitive Spelling Correction| Abstract.  A large class of machine-learning problems in natural language require the
Learning to Reason \Lambda| Abstract We introduce a new framework for the study of reasoning.  The Learning (in order) to Reason approach developed here combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it.  In this framework the intelligent agent is given access to her favorite learning interface, and is also given a grace period in which she can interact with this interface and construct her representation KB of the world W .  Her reasoning performance is measured only after this period, when she is presented with queries ff from some query language, relevant to the world, and has to answer whether W implies ff.  The approach is meant to overcome the main computational difficulties in the traditional treatment of reasoning which stem from its separation from the "world".  First, by allowing the reasoning task to interface the world (as in the known learning models), we avoid the rigid syntactic restriction on the intermediate knowledge representation.  Second, we make explicit the dependence of the reasoning performance on the input from the environment.  This is possible only because the agent interacts with the world when constructing her knowledge representation.  We show how previous results from learning theory and reasoning fit into this framework and illustrate the usefulness of the Learning to Reason approach by exhibiting new results that are not possible in the traditional setting.  First, we give a Learning to Reason algorithm for a class of propositional languages for which there are no efficient reasoning algorithms, when represented as a traditional (formula-based) knowledge base.  Second, we exhibit a Learning to Reason algorithm for a class of propositional languages that is not known to be learnable in the traditional sense. 
First-order Probabilistic Inference Revisited| Abstract Following ideas in Poole [Poo03], which we correct, formalize and extend, this paper presents the first provable algorithm for reasoning with probabilistic first-order representations at the lifted level.  Specifically, the algorithm automates the process of probabilistic reasoning about populations of individuals, their properties and the relations between them, without the need to ground the probabilistic knowledge base.  The algorithm makes use of unification to guide an interleaving of variable ordering and first-order variable elimination.  Importantly, our contribution includes the formalization of concepts necessary to reason about the algorithm's correctness and its correctness proof. 
Reasoning with Examples: Propositional Formulae and Database Dependencies| Abstract For humans, looking at how concrete examples behave is an intuitive way of deriving conclusions.  The drawback with this method is that it does not necessarily give the correct results.  However, under certain conditions example-based deduction can be used to obtain a correct and complete inference procedure.  This is the case for Boolean formulae (reasoning with models) and for certain types of database integrity constraints (the use of Armstrong relations).  We show that these approaches are closely related, and use the relationship to prove new results about the existence and sizes of Armstrong relations for Boolean dependencies.  Further, we study the problem of translating between different representations of relational databases, in particular we consider Armstrong relations and Boolean dependencies, and prove some positive results in that context.  Finally, we discuss the close relations between the questions of finding keys in relational databases and that of finding abductive explanations. 
Reasoning with Models \Lambda| Abstract We develop a model-based approach to reasoning, in which the knowledge base is represented as a set of models (satisfying assignments) rather then a logical formula, and the set of queries is restricted.  We show that for every propositional knowledge base (KB) there exists a set of characteristic models with the property that a query is true in KB if and only if it is satisfied by the models in this set.  We fully characterize a set of theories for which the model-based representation is compact and provides efficient reasoning.  These include cases where the formula-based representation does not support efficient reasoning.  In addition, we consider the model-based approach to abductive reasoning and show that for any propositional KB, reasoning with its model-based representation yields an abductive explanation in time that is polynomial in its size.  Some of our technical results make use of the Monotone Theory, a new characterization of Boolean functions introduced in [Bsh93].  The notion of restricted queries is inherent to our approach.  This is a wide class of queries for which reasoning is very efficient and exact, even when the model-based representation KB provides only an approximate representation of the "world".  Moreover, we show that the theory developed here generalizes the model-based approach to reasoning with Horn theories [KKS93], and captures even the notion of reasoning with Hornapproximations [SK91].  Our result characterizes the Horn theories for which the approach suggested in [KKS93] is useful and the phenomena observed there, regarding the relative sizes of the formula-based representation and model-based representation of KB is explained and put in a wider context. 
Natural Language Inference via Dependency Tree Mapping: An Application to Question Answering| We describe an approach for answer selection in a free form question answering task.  In order to go beyond a key-word based matching in selecting answers to questions, one would like to develop a principled way for the answer selection process that incorporates both syntactic and semantic information.  We achieve this goal by (1) representing both questions and candidate passages using dependency trees, augmented with semantic information such as named entities, and (2) computing a generalized edit distance between a candidate passage representation and the question representation, a distance which aims to capture some level of meaning similarity.  The sentence that best answers a question is determined to be the one that minimizes the generalized edit distance we define, computed via a dynamic programming based approximate tree matching algorithm.  We evaluate the approach on question-answer pairs taken from previous TREC Q/A competitions.  Preliminary experiments show its potential by significantly outperforming common bag-of-word scoring methods. 
IJCAI'05: Workshop on Knowledge and Reasoning for Question Answering Knowledge Representation for Semantic Entailment and Question-Answering| Abstract Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another.  Question-answering can be reduced to this problem by rephrasing the question as a statement that is entailed by correct answers.  In [Braz et al. , ] we present a principled approach to semantic entailment that builds on inducing re-representations of text snippets into a hierarchical knowledge representation along with an optimization-based inferential mechanism that makes use of it to prove semantic entailment.  This paper provides details and analysis of the knowledge representation and knowledge resources issues in the above approach.  We analyze our system's behavior on a collection of question-answer pairs and use it to motivate and explain some of the design decisions in our hierarchical knowledge representation, that is centered around a predicateargument type abstract representation of text. 
Lifted First-Order Probabilistic Inference| Abstract Most probabilistic inference algorithms are specified and processed on a propositional level.  In the last decade, many proposals for algorithms accepting first-order specifications have been presented, but in the inference stage they still operate on a mostly propositional representation level.  [Poole, 2003] presented a method to perform inference directly on the first-order level, but this method is limited to special cases.  In this paper we present the first exact inference algorithm that operates directly on a first-order level, and that can be applied to any first-order model (specified in a language that generalizes undirected graphical models).  Our experiments show superior performance in comparison with propositional exact inference. 
Invited presentation at SOFSEM'96; appeared in LNCS 1175 Learning in Order to Reason: The Approach| Abstract Any theory aimed at understanding commonsense reasoning, the process that humans use to cope with the mundane but complex aspects of the world in evaluating everyday situations, should account for its flexibility, its adaptability, and the speed with which it is performed.  Current theories of reasoning, however, do not satisfy these requirements, a fact we attribute, at least partly, to their separation from learning.  While the central role of learning in cognition is widely acknowledged, most lines of research nevertheless study the phenomenon of "learning" separately from that of "reasoning".  The work presented here is motivated by the belief that learning is at the core of any attempt at understanding high level cognitive tasks.  A formal model for the study of reasoning is developed in which a learning component has a principal role, and its advantages over traditional formalisms for the study of reasoning are shown.  This paper presents an integrated theory of learning, knowledge representation and reasoning within a unified framework called Learning to Reason.  The Learning to Reason framework combines the interfaces to the world used by known learning models with a reasoning task and a performance criterion suitable for it.  It is shown that the framework efficiently supports ``more reasoning" than traditional approaches and at the same time matches our expectations of plausible patterns of reasoning.  Several results are presented to substantiate this claim, presenting cases where learning to reason about the world is feasible but either reasoning from a given representation of the world or learning representations of the world do not have efficient solutions.  Overall, this framework suggests an "operational" approach to reasoning, that is nevertheless rigorous and amenable to analysis.  As such, it may be a step toward a rigorous large-scale empirical study of learning and reasoning.  The paper presents work originally
On generalization bounds, projection profile, and margin distribution| Abstract We study generalization properties of linear learning algorithms and develop a data dependent approach that is used to derive generalization bounds that depend on the margin distribution.  Our method uses random projection techniques to allow the use of existing VC dimension bounds in the effective, lower, dimension of the data.  Our bounds are tighter than existing bounds and (sometimes) give informative generalization bounds for real world, high dimensional problems. 
Learning Question Classifiers| Abstract In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer.  These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.  This paper presents a machine learning approach to question classification.  We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.  We show accurate results on a large collection of free-form questions used in TREC 10. 
An Inference Model for Semantic Entailment in Natural Language| Abstract Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another.  This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications.  This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment.  A preliminary evaluation on the PASCAL text collection is presented. 
Inference with Classifiers: The Phrase Identification Problem| Abstract Machine learning applications often involve learning several dierent classifiers and combining their outcomes to a global decision in a way that provides a coherent inference that satisfies some constraints.  This paper studies three general approaches to this problem concentrating on identifying sequential structure in the text.  In all cases, the classifiers' learning stage is decoupled from the inference stage.  The first two models studied are Markovian approaches.  One is a generative model that extends standard HMMs and the second is a conditional model; both allow the use of a rich observation structure and of general classifiers to model state-observation dependencies.  The last model studied is an extension of constraint satisfaction formalisms.  We develop ecient combination algorithms under all models and study them experimentally in the context of identifying the phrase structure of natural language sentences. 
EMNLP-WVLC'99 A Learning Approach to Shallow Parsing| Abstract A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally.  The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference.  Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented.  In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors. 
To appear in IJCAI 1995 Learning to Reason: The Non-Monotonic Case| Abstract We suggest a new approach for the study of the nonmonotonicity of human commonsense reasoning.  The two main premises that underlie this work are that commonsense reasoning is an inductive phenomenon, and that missing information in the interaction of the agent with the environment may be as informative for future interactions as observed information.  This intuition is formalized and the problem of reasoning from incomplete information is presented as a problem of learning attribute functions over a generalized domain.  We consider examples that illustrate various aspects of the non-monotonic reasoning phenomena, which have been used over the years as "bench-marks" for various formalisms, and translate them into Learning to Reason problems.  We demonstrate that these have concise representations over the generalized domain and prove that these representations can be learned efficiently.  The framework developed suggests an "operational" approach to studying reasoning that is nevertheless rigorous and amenable to analysis.  We show that this approach efficiently supports reasoning with incomplete information and at the same time matches our expectations of plausible patterns of reasoning in cases where other theories do not.  This work continues previous works in the Learning to Reason framework, and supports the thesis that in order to develop a computational account for commonsense reasoning one should study the phenomena of learning and reasoning together. 
Learning via Inference over Structurally Constrained Output| Abstract We experimentally analyze learning structured output in a discriminative framework where values of the output variables are estimated by local classifiers.  In this framework, complex dependencies among the output variables are captured by constraints that dictate how global labels can be inferred.  We compare two strategies, learning plus inference and inference based training, by observing their behaviors in different conditions.  We conclude that using inference during learning helps when the local classifiers are difficult to learn but requires more examples. 
Draft -- Please do not circulate| Inference & Learning with Linear Constraints.  Abstract We present a discriminatory learning framework for the problem of assigning globally optimal values to a set of variables with complex and expressive dependencies among them.  The problem is modeled as an integer linear program (ILP) where the cost values associated with the variables are represented and trained as linear classifiers.  The framework unifies and extends several existing discriminatory approaches; most importantly, it supports more complex dependencies among variables than existing ones.  This presentation concentrates on the benefits of the additional expressivity and on comparing different training paradigms -- with and without global feedback -- in the context of semantic role labeling. 
To Appear COLING-ACL '98: Workshop on Usage of WordNet in Natural Language Processing Systems Incorporating Knowledge in Natural Language Learning: A Case Study| Abstract Incorporating external information during a learning process is expected to improve its efficiency.  We study a method for incorporating noun-class information, in the context of learning to resolve Prepositional Phrase Attachment (PPA) disambiguation.  This is done within a recently introduced architecture, SNOW, a sparse network of threshold gates utilizing the Winnow learning algorithm.  That architecture has already been demonstrated to perform remarkably well on a number of natural language learning tasks.  The knowledge sources used were compiled from the WordNet database for general linguistic purposes, irrespective of the PPA problem, and are being incorporated into the learning algorithm by enriching its feature space.  We study two strategies of using enriched features and the effects of using class information at different granularities, as well as randomly-generated knowledge which serves as a control set.  Incorporating external knowledge sources within SNOW yields a statistically significant performance improvement.  In addition, we find an interesting relation between the granularity of the knowledge sources used and the magnitude of the improvement.  The encouraging results with noun-class data provide a motivation for carrying out more work on generating better linguistic knowledge sources. 
Learning and Inference over Constrained Output| Abstract We study learning structured output in a discriminative framework where values of the output variables are estimated by local classifiers.  In this framework, complex dependencies among the output variables are captured by constraints and dictate which global labels can be inferred.  We compare two strategies, learning independent classifiers and inference based training, by observing their behaviors in different conditions.  Experiments and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifiers are difficult to learn but may require many examples before any discernible difference can be observed. 
Token-level Disambiguation of VerbNet classes| Abstract The automatic disambiguation of verbs in domain independent text becomes more and more important for applications such as Machine Translation, Text Summarization, and Question Answering, mainly because verbs play a key factor in the syntactic and semantic interpretation of sentences.  In this paper we present a system for the automatic classification of token verbs in context based on VerbNet classes.  A supervised machine learning classifier is trained and tested on a portion of PropBank using a set of lexical and syntactic features. 
Relational Learning via Propositional Algorithms: An Information Extraction Case Study| Abstract This paper develops a new paradigm for relational learning which allows for the representation and learning of relational information using propositional means.  This paradigm suggests different tradeoffs than those in the traditional approach to this problem -- the ILP approach -- and as a result it enjoys several significant advantages over it.  In particular, the new paradigm is more flexible and allows the use of any propositional algorithm, including probabilistic algorithms, within it.  We evaluate the new approach on an important and relation-intensive task - Information Extraction - and show that it outperforms existing methods while being orders of magnitude more efficient. 
IJCAI '05 Learning and Inference over Constrained Output| Abstract We study learning structured output in a discriminative framework where values of the output variables are estimated by local classifiers.  In this framework, complex dependencies among the output variables are captured by constraints and dictate which global labels can be inferred.  We compare two strategies, learning independent classifiers and inference based training, by observing their behaviors in different conditions.  Experiments and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifiers are difficult to learn but may require many examples before any discernible difference can be observed. 
Generalized Inference with Multiple Semantic Role Labeling Systems| Abstract We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicateargument output by solving an optimization problem.  The optimization stage, which is solved via integer linear programming, takes into account both the recommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the final role labeling.  We illustrate a significant improvement in overall SRL performance through this inference.  1 SRL System Architecture Our SRL system consists of four stages: pruning, argument identification, argument classification, and inference.  In particular, the goal of pruning and argument identification is to identify argument candidates for a given verb predicate.  The system only classifies the argument candidates into their types during the argument classification stage.  Linguistic and structural constraints are incorporated in the inference stage to resolve inconsistent global predictions.  The inference stage can take as its input the output of the argument classification of a single system or of multiple systems.  We explain the inference for multiple systems in Sec.  2.  1. 1 Pruning Only the constituents in the parse tree are considered as argument candidates.  In addition, our system exploits the heuristic introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents.  The heuristic is a recursive process starting from the verb whose arguments are to be identified.  It first returns the siblings of the verb; then it moves to the parent of the verb, and collects the siblings again.  The process goes on until it reaches the root.  In addition, if a constituent is a PP (propositional phrase), its children are also collected.  Candidates consisting of only a single punctuation mark are not considered.  This heuristic works well with the correct parse trees.  However, one of the errors by automatic parsers is due to incorrect PP attachment leading to missing arguments.  To attempt to fix this, we consider as arguments the combination of any consecutive NP and PP, and the split of NP and PP inside the NP that was chosen by the previous heuristics.  1. 2 Argument Identification The argument identification stage utilizes binary classification to identify whether a candidate is an argument or not.  We train and apply the binary classifiers on the constituents supplied by the pruning stage.  Most of the features used in our system are standard features, which include .  Predicate and POS tag of predicate indicate the lemma of the predicate and its POS tag. 
To Appear in Artificial Intelligence Journal Learning Cost-Sensitive Active Classifiers| Abstract Most classification algorithms are "passive", in that they assign a class label to each instance based only on the description given, even if that description is incomplete.  By contrast, an active classifier can --- at some cost --- obtain the values of some unspecified attributes, before deciding upon a class label.  This can be useful, for instance, when deciding whether to gather information relevant to a medical procedure or experiment.  The expected utility of using an active classifier depends on both the cost required to obtain the values of additional attributes and the penalty incurred if the classifier outputs the wrong classification.  This paper analyzes the problem of learning optimal active classifiers, using a variant of the probably-approximately-correct (PAC) model.  After defining the framework, we show that this task can be achieved efficiently when the active classifier is allowed to perform only (at most) a constant number of tests.  We then show that, in more general environments, this task of learning optimal active classifiers is often intractable. 
Linear Concepts and Hidden Variables: An Empirical Study| Abstract Some learning techniques for classification tasks work indirectly, by first trying to fit a full probabilistic model to the observed data.  Whether this is a good idea or not depends on the robustness with respect to deviations from the postulated model.  We study this question experimentally in a restricted, yet non-trivial and interesting case: we consider a conditionally independent attribute (CIA) model which postulates a single binary-valued hidden variable z on which all other attributes (i. e. , the target and the observables) depend.  In this model, finding the most likely value of any one variable (given known values for the others) reduces to testing a linear function of the observed values.  We learn CIA with two techniques: the standard EM algorithm, and a new algorithm we develop based on covariances.  We compare these, in a controlled fashion, against an algorithm (a version of Winnow) that attempts to find a good linear classifier directly.  Our conclusions help delimit the fragility of using the CIA model for classification: once the data departs from this model, performance quickly degrades and drops below that of the directly-learned linear classifier. 
Question-Answering via Enhanced Understanding of Questions| Abstract We describe a machine learning centered approach to developing an open domain question answering system.  The system was developed in the summer of 2002, building upon several existing machine learning based NLP modules developed within a unified framework.  Both queries and data were pre-processed and augmented with pos tagging, shallow parsing information, and some level of semantic categorization (beyond named entities) using a SNoW based machine learning approach.  Given these as input, the system proceeds as an incremental constraint satisfaction process.  A machine learning based question analysis module extracts structural and semantic constraints on the answer, including a fine classi#cation of the desired answer type.  The system continues in several steps to identify candidate passages and then extracts an answer that best satisfies the constraints.  With the available machine learning technologies, the system was developed in six weeks with the goal of identifying some of the key research issues of QA and challenges to it. 
Applying Winnow to Context-Sensitive Spelling Correction| Abstract Multiplicative weight-updating algorithms such as Winnow have been studied extensively in the COLT literature, but only recently have people started to use them in applications.  In this paper, we apply a Winnow-based algorithm to a task in natural language: context-sensitive spelling correction.  This is the task of fixing spelling errors that happen to result in valid words, such as substituting to for too, casual for causal, and so on.  Previous approaches to this problem have been statistics-based; we compare Winnow to one of the more successful such approaches, which uses Bayesian classifiers.  We find that: (1) When the standard (heavily-pruned) set of features is used to describe problem instances, Winnow performs comparably to the Bayesian method; (2) When the full (unpruned) set of features is used, Winnow is able to exploit the new features and convincingly outperform Bayes; and (3) When a test set is encountered that is dissimilar to the training set, Winnow is better than Bayes at adapting to the unfamiliar test set, using a strategy we will present for combining learning on the training set with unsupervised learning on the (noisy) test set. 
To Appear in AAAI-98 Learning to Resolve Natural Language Ambiguities: A Unified Approach| Abstract We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space.  Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis.  Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators.  We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.  We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems.  The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.  In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging.  In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best. 
In Proceedings of NIPS'01 Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms| Abstract We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features.  We demonstrate a tradeoff between the computational efficiency with which these kernels can be computed and the generalization ability of the resulting classifier.  We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions.  We show that these kernels can be used to efficiently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions.  We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions.  While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow's behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efficiently computable. 
Gene recognition based on DAG shortest paths| ABSTRACT We describe DAGGER, an ab initio gene recognition program which combines the output of high dimensional signal sensors in an intuitive gene model based on directed acyclic graphs.  In the first stage, candidate start, donor, acceptor, and stop sites are scored using the SNoW learning architecture.  These sites are then used to generate a directed acyclic graph in which each sourcesink path represents a possible gene structure.  Training sequences are used to optimize an edge weighting function so that the shortest source-sink path maximizes exon-level prediction accuracy.  Experimental evaluation of prediction accuracy on two benchmark data sets demonstrates that DAGGER is competitive with ab initio gene finding programs based on Hidden Markov Models. 
Finding the Largest Rectangle in Several Classes of Polygons| Abstract This paper considers the geometric optimization problem of finding the Largest area axis-parallel Rectangle (LR) in an n-vertex general polygon.  We characterize the LR for general polygons by considering different cases based on the types of contacts between the rectangle and the polygon.  A general framework is presented for solving a key subproblem of the LR problem which dominates the running time for a variety of polygon types.  This framework permits us to transform an algorithm for orthogonal polygons into an algorithm for nonorthogonal polygons.  Using this framework, we obtain the following LR time results: \Theta(n) for xy-monotone polygons, O(nff(n)) for orthogonally convex polygons, (where ff(n) is the slowly growing inverse of Ackermann's function), O(nff(n) log n) for horizontally (vertically) convex polygons, O(n log n) for a special type of horizontally convex polygon (whose boundary consists of two y-monotone chains on opposite sides of a vertical line), and O(n log 2 n) for general polygons (allowing holes).  For all these types of non-orthogonal polygons, we match the running time of the best known algorithms for their orthogonal counterparts.  A lower bound of time in \Omega(n log n) is established for finding the LR in both selfintersecting polygons and general polygons with holes.  The latter result gives us both a lower bound of \Omega(n log n) and an upper bound of O(n log 2 n) for general polygons. 
Learning to Resolve Natural Language Ambiguities: A Unified Approach|
Mistake-Driven Learning in Text Categorization|
The SNoW learning architecture|
The use of classifiers in sequential inference|
Understanding Probabilistic Classifiers|
Learning in Natural Language|
Towards a theory of coherent concepts|
Part of Speech Tagging Using a Network of Linear Separators|
Reasoning with Models|
SNoW User Guide|
On the Hardness of Approximate Reasoning|
On defaults and relevance in model-based reasoning|
A Learning Approach to Shallow Parsing|
What autism teaches us about metarepresentation|
"Bimodal HCI-related affect recognition,"|
Wen-tau Yih 2004 A Linear Programming Formulation for Global Inference in Natural Language Tasks|
On Learning Visual Concepts and DNF Formulae|
Learning to reason|
Incorporating knowledge in natural language learning: A case study|
Applying System Combination to Base Noun Phrase Identification|
Defaults and Relevance in Model-Based Reasoning|
A Sequential Model for Multi-Class Classification|
Shallow parsing by inferencing with classifiers|
On the hardness of approximating approximate reasoning|
Learning to recognize 3D objects|
Linear Concepts and Hidden Variables|
Learning to Reason: The Non-Monotonic Case|
Learning cost-sensitive active classifiers|
A Connectionist Framework for Reasoning: Reasoning with Examples|
A more accurate method to estimate glomerular filtration rate from serum creatinine: a new prediction equation| Modification of Diet in Renal Disease Study Group. 
Kernel methods for relational learning (|
The exocyst is a multiprotein complex required for exocytosis in Saccharomyces cerevisiae|
A simulation study on reconnection and small-scale plasmoid formation,|
Learning a sparse reprsentation for object detection|
A Classification Approach to Word Prediction|
Stochastic greedy search: Efficiently computing a most probable explanation in Bayesian networks|
A uniform convergence bound for the area under the ROC curve|
On learning read-k satisfy-j DNF|
Scaling Up Context-Sensitive Text Correction|
Gene recognition based on a DAG shortest path algorithm|
Splice site prediction using a sparse network of Winnows|
Learning to Recognize 3D Objects with SNoW|
On kernel methods for relation learning|
Probabilistic reasoning for entities and relation recognition|
Individual blade root control demonstration -- recent activities, 27th European Rotorcraft Forum,|
On the hardness of approximate reasonning|
Strategic Delay in Bargaining as a Coordination Failure," University of Michigan working paper|
The exocyst is an effector for Sec4p, targeting secretory vesicles to sites of exocytosis|
The Win32::ODBC Perl Module|
Learning Components for A Question-Answering System|
Default-Reasoning with Models|
The recognition of attitude conveyed by utterance: a study of preschool and autistic children|
Beliefs about false beliefs: understanding mental states in normal and abnormal development|
Learning with Feature Description Logics|
An inference model for semantic entailment in natural language|
Sequential Consistency and Linearizability: Read/Write Objects,"|
Prepositional phrase attachment|
"A snowbased face detection"|
Reasoning with Models Artificial Intelligence 87,|
Finding the Maximum Area Axis-parallel Rectangle in a Polygon|
A sequential model for multi class classification|
SNoW User Guide", University of Illinois|
Learning to Recognize Objects|
Learning and Inference for Clause Identification|
Finding the Largest Area Axis-parallel Rectangle in a Polygon|
Exploiting relevance through model-based reasoning|
A note on Horn theories|
Applying Winnow to ContextSensitive Spelling Correction,|
Knowledge representation for semantic entailment and question-answering|
Toward a Theory of Learning Coherent Concepts|
Pattern selection in the absolutely unstable regime as a nonlinear eigenvalue problem: Taylor vortices in axial flow,|
Applying winnow to context-sensitive spelling correcton|
Learning to Detect Objects in Images via a Sparse, Part-Based Representation|
Effects of age and IQ on Paced Auditory Serial Addition Task (PASAT) performance|
The effect of stress on the suppression of erroneous competing responses| Anxiety, Stress, &. 
Learning knowledge representations for natural language understanding|

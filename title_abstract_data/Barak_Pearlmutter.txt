Transformations of Gaussian Process Priors| Abstract Gaussian processes-prior systems generally consist of noisy measurements of samples of the putatively Gaussian process of interest, where the samples serve to constrain the posterior estimate.  Here we consider the case where the measurements are instead noisy weighted sums of samples.  This framework incorporates measurements of derivative information and of filtered versions of the process, thereby allowing GPs to perform sensor fusion and tomography, it allows certain group invariances (ie symmetries) to be weakly enforced, can be used to model heteroskedasticity in output variance, and under certain conditions it allows the dataset to be dramatically reduced in size.  The method is applied to a sparsely sampled image, where each sample is taken using a broad and non-monotonic point spread function. 
Dynamic Recurrent Neural Networks| Abstract We survey learning algorithms for recurrent neural networks with hidden units and attempt to put the various techniques into a common framework.  We discuss fixpoint learning algorithms, namely recurrent backpropagation and deterministic Boltzmann Machines, and non-fixpoint algorithms, namely backpropagation through time, Elman's history cutoff nets, and Jordan's output feedback architecture.  Forward propagation, an online technique that uses adjoint equations, is also discussed.  In many cases, the unified presentation leads to generalizations of various sorts.  Some simulations are presented, and at the end, issues of computational complexity are addressed. 
VC Dimension of an Integrate-and-Fire Neuron Model| Abstract We compute the VC dimension of a leaky integrate-and-fire neuron model.  The VC dimension quantifies the ability of
Recovering shape and distribution of delays of repetitive responses in strong noise| Abstract In studies of repetitive responses, such as evoked responses in electro/magnetoencephalography, a signal is often of the same order of magnitude or even weaker than the noise.  In order to recover the original signal shape, people average over many trials.  This approach is problematic when the responses have variable delays, as the shape of the averaged signal become blurred.  In this paper we suggest a method for recovering the original form of the signal and the probability distribution of delays using the average of non-linearly transformed responses.  1 Derivation of the Method Let the observed evoked response be a random process x(t) = s(t #) + #(t) where s(t) is an underlying deterministic signal, # is a random delay distributed according to the probability density f(# ), and #(t) is stationary zero mean noise with standard deviation # n .  We wish to estimate s(t); f(#) and # n from multiple realizations (trials) of x(t).  We begin with the following observation.  For a given deterministic function r(t), shifted randomly by # , the corresponding expectation can be expressed pressed E[r(t # )] = r(t) # f(#) ; where \#" means convolution.  In order to recover the original signal shape and the probability distribution of delays, we will use expectations E[x a (t)], which can be estimated by averaging x a (t) across trials.  Using the fact that E[#(t)] = E[s(t)#(t)] = E[s 2 (t)#(t)] = 0 ; it is easy to get relations of the form E[x a (t)] = F a (s; f; # n ) ; a = 1; 2; 3 (1) where in particular F 1 = s(t) # f(#) F 2 = s 2 (t) # f(#) + # 2 n (2) F 3 = s 3 (t) # f(#) + 3# 2 n s(t) # f(#) We can recover the unknowns s(t); f(#) and # n by solving the system of equations (1), say, in least square sense, i. e.  to minimize the error function R(s; f; # n ) # 3 X a=1 c a # # #E[x a (t)] F a (s; f; # n ) # # # 2 (3) where k # k is the L 2 (or l 2 ) norm of a function of continuous (or discrete) time, and c a are some non-negative weights. 
Detecting Intrusions using System Calls: Alternative Data Models| Abstract Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities.  In this paper we study one such observable--sequences of system calls into the kernel of an operating system.  Using system-call data sets generated by several different programs, we compare the ability of different data modeling methods to represent normal behavior accurately and to recognize intrusions.  We compare the following methods: Simple enumeration of observed sequences, comparison of relative frequencies of different sequences, a rule induction technique, and Hidden Markov Models (HMMs).  We discuss the factors affecting the performance of each method, and conclude that for this particular problem, weaker methods than HMMs are likely sufficient. 
Blind Source Separation by Sparse Decomposition in a Signal Dictionary| Abstract The blind source separation problem is to extract the underlying source signals from a set of linear mixtures, where the mixing matrix is unknown.  This situation is common, in acoustics, radio, medical signal and image processing, hyperspectral imaging, etc. .  We suggest a two-stage separation process.  First, a priori selection of a possibly overcomplete signal dictionary (for instance a wavelet frame, or a learned dictionary) in which the sources are assumed to be sparsely representable.  Second, unmixing the sources by exploiting the their sparse representability.  We consider the general case of more sources than mixtures, but also derive a more ecient algorithm in the case of a non-overcomplete dictionary and an equal numbers of sources and mixtures.  Experiments with artificial signals and with musical sounds demonstrate significantly better separation than other known techniques. 
Equivalence Proofs for Multi-Layer Perceptron Classifiers and the Bayesian Discriminant Function \Lambda| Abstract This paper presents a number of proofs that equate the outputs of a Multi-Layer Perceptron (MLP) classifier and the optimal Bayesian discriminant function for asymptotically large sets of statistically independent training samples.  Two broad classes of objective functions are shown to yield Bayesian discriminant performance.  The first class are "reasonable error measures," which achieve Bayesian discriminant performance by engendering classifier outputs that asymptotically equate to a posteriori probabilities.  This class includes the mean-squared error (MSE) objective function as well as a number of information theoretic objective functions.  The second class are classification figures of merit (CFM mono ), which yield a qualified approximation to Bayesian discriminant performance by engendering classifier outputs that asymptotically identify the maximum a posteriori probability for a given input.  Conditions and relationships for Bayesian discriminant functional equivalence are given for both classes of objective functions.  Differences between the two classes are then discussed very briefly in the context of how they might affect MLP classifier generalization, given relatively small training sets. 
Subject-Independent Magnetoencephalographic Source Localization by a Multilayer Perceptron| Abstract We describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic (MEG) measurements in real time.  At its core is a multilayer perceptron (MLP) trained to map sensor signals and head position to dipole location.  Including head position overcomes the previous need to retrain the MLP for each subject and session.  The training dataset was generated by mapping randomly chosen dipoles and head positions through an analytic model and adding noise from real MEG recordings.  After training, a localization took 0. 7 ms with an average error of 0. 90 cm.  A few iterations of a Levenberg-Marquardt routine using the MLP's output as its initial guess took 15 ms and improved the accuracy to 0. 53 cm, only slightly above the statistical limits on accuracy imposed by the noise.  We applied these methods to localize single dipole sources from MEG components isolated by blind source separation and compared the estimated locations to those generated by standard manually-assisted commercial software. 
SECOND ORDER BLIND SOURCE SEPARATION BY RECURSIVE SPLITTING OF SIGNAL SUBSPACES| ABSTRACT We present an approach to blind source separation based on delayed correlations.  This method recursively splits separation space into subspaces spanned by groups of sources.  The inner loop consists of repeated application of a standard eigenvalue decomposition.  When the number of sources is large this algorithm is significantly faster than joint diagonalization of cross-covariance matrices. 
Playing the Matching-Shoulders Lob-Pass Game with Logarithmic Regret| Abstract The best previous algorithm for the matching shoulders lob-pass game, arthur (Abe and Takeuchi 1993), suffered O(t 1=2 ) regret.  We prove that this is the best possible performance for any algorithm that works by accurately estimating the opponent's payoff lines.  Then we describe an algorithm which beats that bound and meets the information-theoretic lower bound of O(log t) regret by converging to the best lob rate without accurately estimating the payoff lines.  The noise-tolerant binary search procedure that we develop is of independent interest.  1 Background The lob-pass problem has its origins in the animal psychology literature (Herrnstein 1990), but following Abe and Takeuchi (1993) we consider it to be the following simplified tennis game.  For each of a sequence of plays we choose to make either a lob or a passing shot.  The opponent then stochastically hits or misses our shot, and in the latter case we score a point.  The opponent adapts in a way that devalues shots that have been selected frequently.  Specifically, the chance of scoring with a given shot is a linear function of the cumulative lob rate, which is the total number of lobs that we have made divided by the total number of shots that we have made.  Throughout this paper we represent the cumulative lob rate by the variable r.  The payoff functions of two sample opponents are shown in figure 1.  Clearly, the odds of scoring with a lob decreases as one moves to the right on the diagrams, and the odds of scoring with a pass decreases as one moves to the left.  We should emphasise that the x-axis here does not correspond to our instantaneous lob rate, but to the cumulative lob rate that is calculated by the opponent.  This means that
Automatic Learning Rate Maximization in Large Adaptive Machines|
Chaitin-Kolmogorov Complexity and Generalization in Neural Networks|
Results of the Abbadingo One DFA Learning Competition and a New Evidence-Driven State Merging Algorithm|
Abbadingo one: Dfa learning competition|
Oaklisp: an Object-Oriented Scheme with First Class Types|
Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA|
Blind Source Separation via Multinode Sparse Representation|
Oaklisp: An Object-Oriented Dialect of Scheme|
Independent Components of Magnetoencephalography: Localization|
G-maximization: an unsupervised learning procedure for discovering regularities|
An MEG Study of Response Latency and Variability in the Human Visual System During a Visual-Motor Integration Task|
Gradient Descent: Second Order Momentum and Saturating Error|

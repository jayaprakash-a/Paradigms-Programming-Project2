Transformation Systems at NASA Ames| ABSTRACT In this paper, we describe the experiences of the Automated Software Engineering Group at the NASA Ames Research Center in the development and application of three different transformation systems.  The systems span the entire technology range, from deductive synthesis, to logic-based transformation, to almost compiler-like source-to-source transformation.  These systems also span a range of NASA applications, including solving solar system geometry problems, generating data analysis software, and analyzing multithreaded Java code. 
Is Multinomial PCA Multi-faceted Clustering or Dimensionality Reduction?| Abstract Discrete analogues to Principal Components Analysis (PCA) are intended to handle discrete or positive-only data, for instance sets of documents.  The class of methods is appropriately called multinomial PCA because it replaces the Gaussian in the probabilistic formulation of PCA with a multinomial.  Experiments to date, however, have been on small data sets, for instance, from early information retrieval collections.  This paper demonstrates the method on two large data sets and considers two extremes of behaviour: (1) dimensionality reduction where the feature set (i. e. , bag of words) is considerably reduced, and (2) multi-faceted clustering (or aspect modelling) where clustering is done but items can now belong in several clusters at once. 
Topic-Specific Scoring of Documents with Discrete PCA| More sophisticated language models are starting to be used in information retrieval [8] and real successes are being achieved in their use [4].  A document modelling approach based on discrete versions of PCA [7, 1, 2] has been applied to the language modelling task in information retrieval [2, 3].  Here we apply the same discrete PCA method to topic specific versions of page rank [6, 10].  Our intent is that this can be used as a secondary score to add topical scoring to retrieval in conjunction with a separate key-word based score such as TFIDF. 
A Scalable Topic-Based Open Source Search Engine| Abstract Site-based or topic-specific search engines work with mixed success because of the general difficulty of the information retrieval task, and the lack of good link information to allow authorities to be identified.  We are advocating an open source approach to the problem due to its scope and need for software components.  We have adopted a topicbased search engine because it represents the next generation of capability.  This paper outlines our scalable system for site-based or topic-specific search, and demonstrates the developing system on a small 250,000 document collection of EU and UN web pages. 
Theory Refinement on Bayesian Networks \Lambda| Abstract Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance.  The problem of theory refinement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision.  The problem is reduced to an incremental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data.  Algorithms for refinement of Bayesian networks are presented to illustrate what is meant by ``partial theory", "alternative theory representation", etc.  The algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode. 
Static Ranking of Web Pages, and Related Ideas| Abstract This working paper reviews some dierent ideas in link-based analysis for search.  First, results about static ranking of web pages based on the so called randomsurfer model are reviewed and presented in a unified framework.  Second, a topic-based hubs and authorities model using a discrete component method (a variant of ICA and PCA) is developed, and illustrated on the 500,000 page English language Wikipedia collection.  Third, a proposal is presented to the community for a Links/Ranking consortium extracted from the Web Intelligence paper Opportunities from Open Source Search. 
Opportunities from Open Source Search| Abstract Internet search has a strong business model that permits a free service to users, so it is difficult to see why, if at all, there should be open source offerings as well.  This paper first discusses open source search, and a rationale for the computer science community at large to get involved.  Because there is no shortage of core open source components for at least some of the tasks involved, the Alvis Consortium is building infrastructure for open source search engines using peer-to-peer and subject specific technology as its core, based on this rationale.  We view open source search as a rich future playground in which information extraction and retrieval components can be used and intelligent agents can operate. 
Classifiers: A Theoretical and Empirical Study| Abstract This paper describes how a competitive tree learning algorithm can be derived from first principles.  The algorithm approximates the Bayesian decision theoretic solution to the learning task.  Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost.  Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned.  As an illustration, a second learning algorithm is derived for learning Bayesian networks from data.  Implications to incremental learning and the use of multiple models are also discussed. 
Applying Discrete PCA in Data Analysis| Abstract Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture.  In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks.  We show that these methods can be interpreted as a discrete version of ICA.  We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling.  We compare the algorithms on a text prediction task using support vector machines, and to information retrieval. 
Automatic Derivation of the Multinomial PCA Algorithm| Abstract Machine learning has reached a point where probabilistic methods can be understood as variations, extensions, and combinations of a small set of abstract themes, e. g. , as different instances of the EM algorithm, or as exponential family methods.  This allows the automatic derivation of algorithms customized for different models.  One interesting new model is a multinomial version of PCA which has received attention due to its ability to better model documents as having multiple topics.  Here we explain how AutoBayes, an automatic synthesis system for machine learning programs, derives from a high-level statistical specification of this model code similar to the original non-negative matrix factorization algorithm of Lee and Seung.  The derivation combines multiple statistical schemes to solve a new problem not originally envisaged.  This demonstrates that the approach can be scaled up from text-book problems to research-level algorithm design. 
Variational Extensions to EM and Multinomial PCA| Abstract.  Several authors in recent years have proposed discrete analogues to principle component analysis intended to handle discrete or positive only data, for instance suited to analyzing sets of documents.  Methods include non-negative matrix factorization, probabilistic latent semantic analysis, and latent Dirichlet allocation.  This paperbegins with a review of the basic theory of the variational extension to the expectationmaximization algorithm, and then presents discrete component finding algorithms in that light.  Experiments are conducted on both bigram word data and document bag-of-word to expose some of the subtleties of this new class of algorithms. 
Automatic Derivation of Statistical Algorithms: The EM Family and Beyond| Abstract Machine learning has reached a point where many probabilistic methods can be understood as variations, extensions and combinations of a much smaller set of abstract themes, e. g. , as different instances of the EM algorithm.  This enables the systematic derivation of algorithms customized for different models.  Here, we describe the AUTOBAYES system which takes a high-level statistical model specification, uses powerful symbolic techniques based on schema-based program synthesis and computer algebra to derive an efficient specialized algorithm for learning that model, and generates executable code implementing that algorithm.  This capability is far beyond that of code collections such as Matlab toolboxes or even tools for model-independent optimization such as BUGS for Gibbs sampling: complex new algorithms can be generated without new programming, algorithms can be highly specialized and tightly crafted for the exact structure of the model and data, and efficient and commented code can be generated for different languages or systems.  We present automatically-derived algorithms ranging from closed-form solutions of Bayesian textbook problems to recently-proposed EM algorithms for clustering, regression, and a multinomial form of PCA. 
Efficient Computation of Stochastic Complexity| Abstract Stochastic complexity of a data set is defined as the shortest possible code length for the data obtainable by using some fixed set of models.  This measure is of great theoretical and practical importance as a tool for tasks such as model selection or data clustering.  Unfortunately, computing the modern version of stochastic complexity, defined as the Normalized Maximum Likelihood (NML) criterion, requires computing a sum with an exponential number of terms.  Therefore, in order to be able to apply the stochastic complexity measure in practice, in most cases it has to be approximated.  In this paper, we show that for some interesting and important cases with multinomial data sets, the exponentiality can be removed without loss of accuracy.  We also introduce a new computationally efficient approximation scheme based on analytic combinatorics and assess its accuracy, together with earlier approximations, by comparing them to the exact form.  The results suggest that due to its accuracy and efficiency, the new sharper approximation will be useful for a wide class of problems with discrete data. 
Efficient Computation of Stochastic Complexity| Abstract Stochastic complexity of a data set is defined as the shortest possible code length for the data obtainable by using some fixed set of models.  This measure is of great theoretical and practical importance as a tool for tasks such as model selection or data clustering.  Unfortunately, computing the modern version of stochastic complexity, defined as the Normalized Maximum Likelihood (NML) criterion, requires computing a sum with an exponential number of terms.  Therefore, in order to be able to apply the stochastic complexity measure in practice, in most cases it has to be approximated.  In this paper, we show that for some interesting and important cases with multinomial data sets, the exponentiality can be removed without loss of accuracy.  We also introduce a new computationally efficient approximation scheme based on analytic combinatorics and assess its accuracy, together with earlier approximations, by comparing them to the exact form.  The results suggest that due to its accuracy and efficiency, the new sharper approximation will be useful for a wide class of problems with discrete data. 
Web-Based Adaptive Personalization| In this position paper we describe a distributed architecture and other engineering aspects of a web-based, adaptive, personalization system.  The major aspects of the system are: adaptability, lightweight install, scalability, robustness, and fault tolerance.  The personalization component of the system adapts to consumer preferences: it selects items to present to consumers according to their respective individual characteristics, and also adapts simultaneously with the covariance of consumers and items.  Products and services that are based on the architecture are easily integrated with clients' existing web portal with minimum effort and interference.  The marketing aspects of the system are also largely maintained by the marketers themselves with little or no interaction required with IT staff, and providing them with simple tools to make analysis and perform marketing trials.  Like standard high-performance web architectures, the system is also scalable, reliable and maintainable.  It is resilient to component failures, and also easy to upgrade.  The architecture thus makes the cost-benefit analysis for personalization highly favorable in contrast with first generation personalization systems. 
Multi-faceted Learning for Web Taxonomies| Abstract.  A standard problem for internet commerce is the task of building a product taxonomy from web pages, without access to corporate databases.  However, a nasty aspect of the real world is that most web-pages have multiple facets.  A web page might contain information about both cameras and computers, as well as having both specification and sale data.  We are interested in methods for supervised and unsupervised learning of multiple faceted models.  Here we present results for multi-faceted clustering of bigram words data. 
Computing Second Derivatives in Feed-Forward Networks: a Review| Abstract.  The calculation of second derivatives is required by recent training and analyses techniques of connectionist networks, such as the elimination of superfluous weights, and the estimation of confidence intervals both for weights and network outputs.  We here review and develop exact and approximate algorithms for calculating second derivatives.  For networks with jwj weights, simply writing the full matrix of second derivatives requires O(jwj 2 ) operations.  For networks of radial basis units or sigmoid units, exact calculation of the necessary intermediate terms requires of the order of 2h + 2 backward/forward-propagation passes where h is the number of hidden units in the network.  We also review and compare three approximations (ignoring some components of the second derivative, numerical differentiation, and scoring).  Our algorithms apply to arbitrary activation functions, networks, and error functions (for instance, with connections that skip layers, or radial basis functions, or cross-entropy error and Softmax units, etc. ). 
Learning Classification Trees| Abstract Algorithms for learning classification trees have had successes in artificial intelligence and statistics over many years.  This paper outlines how a tree learning algorithm can be derived using Bayesian statistics.  This introduces Bayesian techniques for splitting, smoothing, and tree averaging.  The splitting rule is similar to Quinlan's information gain, while smoothing and averaging replace pruning.  Comparative experiments with reimplementations of a minimum encoding approach, Quinlan's C4 (Quinlan et al. , 1987) and Breiman et al. 's CART (Breiman et al. , 1984) show the full Bayesian algorithm can produce more accurate predictions than versions of these other approaches, though pay a computational price. 
Analysing Rock Samples for the Mars Lander| Abstract In the near future NASA intends to explore various regions of our solar system using robotic devices such as rovers, spacecraft, airplanes, and/or balloons.  Such platforms will carry imaging devices, and a variety of analytical instruments intended to evaluate the chemical and mineralogical nature of the environment(s) that they encounter.  The imaging and/or spectroscopic devices will acquire tremendous volumes of data.  The communication band-widths are restrictive enough so that only a small portion of these data can actually be sent to Earth.  The aim of this research was to develop a system which analyses rock spectra to automatically determine which spectra are interesting, and to compress the spectral data for communication to Earth.  In the research we report here we classify laboratory data using clustering techniques (ACPro an enhanced version of Autoclass) and provide the planetary scientists with a rapid, visually oriented method of evaluating the underlying chemical and mineralogical information contained within the clusters.  We show how clustering can be used to identify interesting rock samples and estimate the compression that using such a system can achieve. 
Building and Maintaining Web Taxonomies| We will present some examples using the Reuters 2000 XML Corpus containing 800,000 Reuters newswires from the year beginning 20th August 1996.  Our system, called Ydin, inputs XML/XHTML pages and displays results using a custom web interface.  It extracts dierent facets and provides a navigation tool for investigating the results. 
The Handbook of Probability in Computing: Interactive Media for Research| Abstract The paper outlines a Handbook being developed for distribution on the World-Wide Web.  The Handbook combines an encyclopaedia with an annotated bibliography, tutorials, and links to community events and software.  The Encyclopaedia and Annotated Bibliography form the center piece of the Handbook.  Their development will be distributed amongst the community, driven by community reviewing and authorship.  This is an experiment in network-based research cooperation that we hope will be a model for future publication and education media.  This paper outlines the proposed Handbook, discusses the benefits to the uncertainty community, and discusses some of the development issues involved.  Finally, this paper calls for community feedback and involvement. 
Tree Classification Software| ABSTRACT This paper introduces the IND Tree Package to prospective users.  IND does supervised learning using classification trees.  This learning task is a basic tool used in the development of diagnosis, monitoring and expert systems.  IND was developed as part of a NASA project to semi-automate the development of data analysis and modelling algorithms using artificial intelligence techniques.  IND integrates features from Breiman et al. 's CART and Quinlan's C4. 5 with newer Bayesian and minimum encoding methods for growing classification trees and graphs.  IND also provides an experimental control suite on top.  The newer features give improved probability estimates often required in diagnostic and screening tasks.  The package comes with a manual, Unix "man" entries, and a guide to tree methods and research.  IND is implemented in C under Unix, and has been beta-tested at university and commercial research laboratories in the United States.  DIAGNOSIS AND CLASSIFICATION A common inference task is where we learn to make a discrete prediction about some case given other details about the case.  For instance, in financial credit assessment we wish to decide whether to accept or reject a customer's application for a loan given particular personal information.  In monitoring a subsystem of the space shuttle, measurements such as flow rates and temperature are continuously recorded and we need to screen those measurements to decide if the system is in normal or abnormal operation.  If the system is in abnormal operation we might further wish to try and predict the type of abnormality present.  This prediction task is the basic task of many expert systems, health monitoring systems, diagnostic systems, etc.  Furthermore, more complex problems can often be broken down into a sequence of simple prediction problems.  For instance, speech understanding, converting the spoken word into written text, is a sequence of prediction tasks about each phoneme.  In medical diagnosis, or diagnosis of equipment subsystems, we need more than just a prediction, we need a careful probabilistic assessment.  A simplistic medical example will bring this point home.  Suppose your doctor suspects you have a cyst in your abdomen.  The options (1 or 2) and outcomes (A or B) give the following set of possibilities: (1A) operates, discovers a cyst, removes it, and you're grateful; (1B) operates, no cyst found, but you're left with the medical bill and a day recovery in hospital; (2A), doesn't operate but the cyst exists and causes medical complications due to lack of treatment; (2B), doesn't operate, no cyst exists.  Each case has important implications to you both financially and in quality of life.  With a careful probabilistic assessment of the existence of a cyst, you can weigh up the options and decide which option (1 or 2) is the most beneficial to you.  For instance, if the medical bill is insignificant compared to the potential medical complications, then you would decide to have the operation even if there was a small chance of having the cyst.  If the potential medical complications were insignificant, you would only decide to operate if there was a very high probability of having the cyst.  This process of decision analysis requires as input probabilities about the new case in question.  In health monitoring and diagnosis, these probability assessments are needed when the system is being used to screen cases, i. e.  the computer systems scans the on-line monitoring data and at certain time points alerts a human expert that a potentially anomalous situation has arisen.  Probability assessments such as the "probability of equipment failure" can be used to determine which of the many cases scanned should be forwarded to the human expert for the more costly process of manual inspection.  I will refer to this prediction problem as classification, where the aim is to classify each new case.  One common technique for developing a system to do prediction or probability assessment about new cases is to examine a database of cases, for instance collected historically.  Assume that hindsight tells us which is the correct classification for each case in the data base, so for each we know which prediction was optimal.  From the data base we use statistical techniques to "discover" or "learn" how to do the predictions for new unseen cases.  This learning technique is represented in Figure 1.  The process requires three main forms of input: an expert who is able to advise on the problem, help configure the system, etc. , a data base of correctly classified cases to use in the learning process, and a model family from which the learning algorithm is to select a "good" model for doing prediction or probabilistic assessment.  expert guidance and intuition data base of cases correctly classified skeletal model for system to build on model discovery (data analysis) system model developed from the data base feedback feedback Figure 1.  Learning prediction models from data.  This model learning or discovery process is a useful technique in almost every industry, finance, manufacturing, etc. , wherever on-line databases are stored and important predictions have to be made on a regular basis about new cases before they enter the data base.  Not surprisingly, there are many different fields of science that address this problem as one of their central concerns.  In artificial intelligence it is referred to as the classification or induction problem.  Techniques include tree and rule learning algorithms of the form I will present in this paper.  In statistics it is referred to as the discrimination problem, and common techniques are the linear models used in the finance and banking industry for credit assessment.  In pattern recognition it is referred to as supervised learning.  In neural networks it is the classification and generalization problem and is routinely investigated using a number of network architectures.  These diverse fields are all studying the same problem, "learning to predict", and present a confusing array of methodologies and paradigms for addressing that problem.  They differ in the following aspects: Model family: Which class of models are being used to do prediction? In Figure 1 this corresponds to the "skeletal model".  I present classification tree and classification graph model families in this paper.  Statistical philosophy: How is learning to occur? That is, what statistical principles if any are used to develop the central box in Figure 1? Computational and optimization methods: What are the basic computational methods used in terms of efficiency, optimality, search method, etc. ? Methodological support: What methodology does the analyst use to go about applying the technique to a real problem? For statisticians this is the "consultancy phase" rarely covered in university courses.  In artificial intelligence this is the process of "knowledge engineering".  I will refer to the general task of learning how to predict (or estimate probabilities) from data as the classification task.  The next section discusses the design of tools for this task.  After this, the model family considered in this paper is addressed, and the IND program presented. 
Theory Refinement on Bayesian Networks|
Machine Invention of First Order Predicates by Inverting Resolution|
Operations for Learning with Graphical Models|
A Guide to the Literature on Learning Probabilistic Networks from Data|
Bayesian back-propagation,|
Matha Del Alto (Editors), Collected Notes on the Workshop for Pattern Discovery in Large Databases,|
A Further Comparison of Splitting Rules for Decision-Tree Induction|
Generalized Subsumption and Its Applications to Induction and Redundancy|
Chain graphs for learning|
A Critique of the Valiant Model|
Computing Second Order Derivatives in Feed-Forward Networks: A Review|
Interactive induction|
Learning in Networks,"|
Stratifying samples to improve learning|
Towards Automated Synthesis of Data Mining Programs|
Decision Tree Induction Systems: A Bayesian Analysis|
Learning Classification Rules Using Bayes|
Automatic Derivation of Statistical Algorithms:|
Brckert H-J| "On Solving Equations and Disequations". 
Generalised Subsumption and its Applications to Induction and Redundancy|
Operations for learning graphical models|
Calculating second derivatives on feedforward networks|
Graphical models for discovering knowledge|
Trends & Controversies: Will Domain-Specific Code Synthesis Become a Silver Bullet?|
Technical note: A further comparison of splitting rules for decision tree induction|
Operati ns f r Learning with Graphical M dels|

Similarity and discrimination in classical conditioning: A latent variable account| Abstract We propose a probabilistic, generative account of configural learning phenomena in classical conditioning.  Configural learning experiments probe how animals discriminate and generalize between patterns of simultaneously presented stimuli (such as tones and lights) that are differentially predictive of reinforcement.  Previous models of these issues have been successful more on a phenomenological than an explanatory level: they reproduce experimental findings but, lacking formal foundations, provide scant basis for understanding why animals behave as they do.  We present a theory that clarifies seemingly arbitrary aspects of previous models while also capturing a broader set of data.  Key patterns of data, e. g.  concerning animals' readiness to distinguish patterns with varying degrees of overlap, are shown to follow from statistical inference. 
Operant behavior suggests attentional gating of dopamine system inputs| Abstract Neurophysiological recording experiments in the dopamine system by Schultz and colleagues (Science 275 (1997) 1593}1598) suggest that neurons there are involved in learning to predict rewards and assess behaviors using the temporal-di!erence algorithm.  One aspect o this theory which is undeveloped and experimentally underconstrained is its assumption o an exhaustive input representing all stimuli and their history over time.  We use the algorithm to model operant choice between concurrent variable interval schedules*a key animal conditioning experiment*and show that animals' subtly suboptimal per ormance resembles the behavior o the algorithm with a more limited input representation.  This limitation may reflect the operation o an attentional mechanism gating the inputs to the dopamine system. 
Dopamine and Inference About Timing| Abstract Temporal-di#erence learning (TD) models explain most responses of primate dopamine neurons in appetitive conditioning.  But because existing models are based in the simple formal setting of Markov processes, they do not provide a realistic account of the partial observability of the state of the world, nor of variation in event timing.  For instance, the TD model of Montague et al.  (1996) mispredicts the dopamine response when an expected reward is delivered early.  We explain such experimental results using a version of TD learning grounded in the richer formalism of partially observable semi-Markov processes.  We propose that the brain infers the likely state of the world from limited observations, using a statistical model of how the world's state evolves.  Inference is necessary for such judgements as whether an expected reward is merely late, versus having been omitted altogether.  The dopamine signal is modeled as a TD error signal for learning to predict future rewards from this inferred state representation. 
How fast to work: Response vigor, motivation and tonic dopamine| Abstract Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior.  However, the bulk of data on animal conditioning comes from free-operant experiments measuring how hard animals will work for reinforcement.  Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor.  They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water.  Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding.  Motivational states such as hunger shift these factors, skewing the tradeoff.  This accounts normatively for the effects of motivation on productivity, as well as many other classic findings.  Finally, we suggest that tonic dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. 
Reinforcement learning models of the dopamine system and their behavioral implications| Abstract This thesis aims to improve theories of how the brain functions and to provide a framework to guide future neuroscientific experiments by making use of theoretical and algorithmic ideas from computer science.  The work centers around the detailed understanding of the dopamine system, an important and phylogenetically venerable brain system that is implicated in such general functions as motivation, decision-making and motor control, and whose dysfunction is associated with disorders such as schizophrenia, addiction, and Parkinson's disease.  A series of influential models have proposed that the responses of dopamine neurons recorded from behaving monkeys can be identified with the error signal from temporal difference (TD) learning, a reinforcement learning algorithm for learning to predict rewards in order to guide decision-making.  Here I propose extensions to these theories that improve them along a number of dimensions simultaneously.  The new models that result eliminate several unrealistic simplifying assumptions from the original accounts; explain many sorts of dopamine responses that had previously seemed anomalous; flesh out nascent suggestions that these neurophysiological mechanisms can also explain animal behavior in conditioning experiments; and extend the theories' reach to incorporate proposals about the computational function of several other brain systems that interact with the dopamine neurons.  Chapter 3 relaxes the assumption from previous models that the system tracks only short-term predictions about rewards expected within a single experimental trial.  It introduces a new model based on averagereward TD learning that suggests that long-run reward predictions affect the slow-timescale, tonic behavior of dopamine neurons.  This account resolves a seemingly paradoxical finding that the dopamine system is excited by aversive events such as electric shock, which had fueled several published attacks on the TD theories.  These investigations also provide a basis for proposals about the functional role of interactions between the dopamine and serotonin systems, and about behavioral data on animal decision-making.  Chapter 4 further revises the theory to account for animals' uncertainty about the timing of events and about the moment-to-moment state of an experimental task.  These issues are handled in the context of a TD algorithm incorporating partial observability and semi-Markov dynamics; a number of other new or extant models are shown to follow from this one in various limits.  The new theory is able to explain a number of previously puzzling results about dopamine responses to events whose timing is variable, and provides an appropriate framework for investigating behavioral results concerning variability in animals' temporal judgments and timescale invariance properties in animal learning.  Chapter 5 departs from the thesis' primary methodology of computational modeling to present a complementary attempt to address the same issues empirically.  The chapter reports the results of an experiment that record from the striatum of behaving rats (a brain area that is one of the major inputs and outputs of ii the dopamine system), during a task designed to probe the functional organization of decision-making in the brain.  The results broadly support the contention of most versions of the TD models that the functions of action selection and reward prediction are segregated in the brain, as in "actor/critic" reinforcement learning systems. 
Timing and Partial Observability in the Dopamine System| Abstract According to a series of influential models, dopamine (DA) neurons signal reward prediction error using a temporal-difference (TD) algorithm.  We address a problem not convincingly solved in these accounts: how to maintain a representation of cues that predict delayed consequences.  Our new model uses a TD rule grounded in partially observable semi-Markov processes, a formalism that captures two largely neglected features of DA experiments: hidden state and temporal variability.  Previous models predicted rewards using a tapped delay line representation of sensory inputs; we replace this with a more active process of inference about the underlying state of the world.  The DA system can then learn to map these inferred states to reward predictions using TD.  The new model can explain previously vexing data on the responses of DA neurons in the face of temporal variability.  By combining statistical model-based learning with a physiologically grounded TD theory, it also brings into contact with physiology some insights about behavior that had previously been confined to more abstract psychological models. 
2002 Special issue Local analysis of behaviour in the adjusting-delay task for assessing choice of delayed reinforcement| Abstract The adjusting-delay task introduced by Mazur (Quantitative analyses of behavior: V.  The effect of delay and of intervening events on reinforcement value, 1987, pp.  55 -- 73) has been widely used to study choice of delayed reinforcers.  This paradigm involves repeated choice between one reinforcer delivered after a fixed delay and another, typically larger, reinforcer delivered after a variable delay; the variable delay is adjusted depending on the subject's choice until an equilibrium point is reached at which the subject is indifferent between the two alternatives.  Rats were trained on a version of this task and their behaviour was examined to determine the nature of their sensitivity to the adjusting delay; these analyses included the use of a cross-correlational technique.  No clear evidence of sensitivity to the adjusting delay was found.  A number of decision rules, some sensitive to the adjusting delay and some not, were simulated and it was observed that some effects usually supposed to be a consequence of delay sensitivity could be generated by delay-independent processes, such as a consistent, unchanging relative preference between the alternatives.  Consequently, the use of explicit analysis of delay sensitivity is advocated in future research on delayed reinforcement. 
Combining Configural and TD Learning on a Robot| Abstract We combine configural and temporal difference learning in a classical conditioning model.  The model is able to solve the negative patterning problem, discriminate sequences of stimuli, and exhibit second order conditioning.  We have implemented the algorithm on the Sony AIBO entertainment robot, allowing us to interact with the conditioning model in real time. 
Model Uncertainty in Classical Conditioning| Abstract We develop a framework based on Bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments.  Traditional accounts of conditioning fit parameters within a fixed generative model of reinforcer delivery; uncertainty over the model structure is not considered.  We apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition, two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes.  According to the theory, second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations; conditioned inhibition results when a more complex model is justified by additional experience. 
rewarded during each block| In order to discourage the animals. 
Behavioral considerations suggest an average reward TD model of the dopamine system|
Long-Term Reward Prediction in TD Models of the Dopamine System|
A computational substrate for incentive salience,|
Motivational effects on behavior: Towards a reinforcement learning model of rates of responding|
Uncertainty-based competition between prefrontal and striatal systems for behavioral control|

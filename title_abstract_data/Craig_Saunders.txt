Machine-Learning Applications of Algorithmic Randomness| Abstract Most machine learning algorithms share the following drawback: they only output bare predictions but not the confidence in those predictions.  In the 1960s algorithmic information theory supplied universal measures of confidence but these are, unfortunately, non-computable.  In this paper we combine the ideas of algorithmic information theory with the theory of Support Vector machines to obtain practicable approximations to universal measures of confidence.  We show that in some standard problems of pattern recognition our approximations work well. 
On Maximum Margin Hierarchical Multilabel Classification| Abstract We present work in progress towards maximum margin hierarchical classification where the objects are allowed to belong to more than one category at a time.  The classification hierarchy is represented as a Markov network equipped with an exponential family defined on the edges.  We present a variation of the maximum margin multilabel learning framework, suited to the hierarchical classification task and allows efficient implementation via gradient-based methods.  We compare the behaviour of the proposed method to the recently introduced hierarchical regularized least squares classifier as well as two SVM variants in Reuter's news article classification.  Often in hierarchical classification, the object to be classified is assumed to belong to exactly one (leaf) node in the hierarchy (c. f.  [5, 2, 4]).  Following [3], in this paper we consider the more general case where a single object can be classified into several categories in the hierarchy, to be specific, the multilabel is a union of partial paths in the hierarchy.  For example, a news article about David and Victoria Beckham could belong to partial paths sport, football and entertainment, music but might not belong to any leaf categories such as champions league or jazz.  In our setting the training data ((x i , y(x i ))) m i=1 consists of pairs (x, y) of vector x 2 R n and a multilabel y 2 {+1,- 1} k consisting of k microlabels.  As the model class we use the exponential family P (y|x) / Y e2E exp w T e # # # e (x, y e ) # = exp w T # # #(x, y) # (1) defined on the edges of a Markov network G = (V, E), where node j 2 V corresponds to the j'th component of the multilabel and the edges e = (j, j 0 ) 2 E correspond # Corresponding author.  to the classification hierarchy given as input.  By y e = (y j , y 0 j ) we denote the restriction of the multilabel y = (y 1 , .  .  .  , y k ) to the edge e = (j, j 0 ).  The edgefeature vector # # # e , in turn, is a concatenation of 'class-sensitive' feature vectors # # # u e (x, y e ) = Jy e = uK# # #(x), where JK denotes an indicator function (c. f.  [1]), and w e is the weight vector for edge e.  The vector # # #(x) could be a bag of words---as in the experiments reported here---or any other feature representation of the document x.  Also, note that although the same feature vector # # #(x) is duplicated for each edge and edge-labeling, in the weight vector w = (w ue e ) e2E,ue we still have a separate weights to represent importance differences of a given feature in different contexts.  There are many ways to define loss functions for hierarchical classification setting (c. f [5, 2, 4, 3]).  Zero-one loss ` 0/1 (y, u) = Jy 6= uK is not very well suited to the task as it ignores the severity of the discrepancy between y and u.  Symmetric difference loss ` # (y, u) = P j Jy i 6= u i K does not suffer from this deficiency.  However, it fails to take the dependency structure of the microlabels into account.  A more appealling choice is the hierarchical loss function of [3].  It penalizes the first mistake along a path, ` PATH (y, u) = P j c j Jy j 6= u j & y k = u k 8k 2 anc(j)K, where the coefficients c root = 1, c j = c pa(j) /|sibl(j)| down-weight mistakes made deeper in the hierarchy.  Here we denoted by anc(j) an ancestor, by pa(j) the immediate parent, and by sibl(j) the set of siblings of node j.  In this paper, we consider a simplified version of ` PATH , namely ` EDGE (y, u) = X j c j Jy j 6= u j & y pa(j) = u pa(j) K, that penalizes a mistake in child if the label of the parent was correct.  This choice lets the loss function to capture some of the hierarchical dependencies (between the parent and the child) but allows us define the loss in terms of edges, which is crucial for the efficiency of our learning algorithm.  This is achieved by dividing the microlabel loss of each node among the edges adjacent to it.  As in [7, 8], our goal is to learn a weight vector w that maximizes the minimum margin on training data the between the correct multilabel y(x i ) and the incorrect multilabels y 6= y(x i ).  Also, we would like the margin to scale as a function of the loss.  Alloting a single slack variable for each training example results in the following soft-margin optimization problem: min w 1 2 ||w|| 2 + C m X i=1 # i s. t.  w T fiff # #(x i , y) # ` ` `(y i , y) - # i , 8i,
Using String Kernels to Identify Famous Performers from Their Playing Style| Abstract.  In this paper we show a novel application of string
Syllables and other String Kernel Extensions| Abstract Recently, the use of string kernels that compare documents as a string of letters has been shown to achieve good results on text classification problems.  In this paper we introduce the application of the string kernel in conjunction with syllables.  Using syllables shortens the representation of documents and as a result reduces computation time.  Moreover syllables provide a more natural representation of text; rather than the traditional coarse representation given by the bag-of-words, or the too fine one resulting from considering individual letters only.  We give some experimental results which show that syllables can be effectively used in textcategorisation problems.  In this paper we also propose two extensions to the string kernel.  The first introduces a new lambdaweighting scheme, where different symbols can be given differing decay weightings.  This may be useful in text and other applications where the insertion of certain symbols may be known to be less significant.  We also introduce the concept of `soft matching', where symbols can match (possibly weighted by relevance) even if they are not identical.  Again, this provides a method of incorporating prior knowledge where certain symbols can be regarded as a partial or exact match and contribute to the overall similarity measure for two data items. 
The typicalness framework: a comparison with the Bayesian approach| Abstract When correct priors are known, Bayesian algorithms give optimal decisions, and accurate confidence values for predictions can be obtained.  If the prior is incorrect however, these confidence values have no theoretical base -- even though the algorithms' predictive performance may be good.  There also exist many successful learning algorithms which only depend on the iid assumption.  Often however they produce no confidence values for their predictions.  Bayesian frameworks are often applied to these algorithms in order to obtain such values, however they can rely on unjustified priors.  In this paper we outline the typicalness framework which can be used in conjunction with many other machine learning algorithms.  The framework provides confidence information based only on the standard iid assumption and so is much more robust to different underlying data distributions.  We show how the framework can be applied to existing algorithms.  We also present experimental results which show that the typicalness approach performs close to Bayes when the prior is known to be correct.  Unlike Bayes however, the method still gives accurate confidence values even when different data distributions are considered. 
Ridge Regression Learning Algorithm in Dual Variables| Abstract In this paper we study a dual version of the Ridge Regression procedure.  It allows us to perform non-linear regression by constructing a linear regression function in a high dimensional feature space.  The feature space representation can result in a large increase in the number of parameters used by the algorithm.  In order to combat this "curse of dimensionality", the algorithm allows the use of kernel functions, as used in Support Vector methods.  We also discuss a powerful family of kernel functions which is constructed using the ANOVA decomposition method from the kernel corresponding to splines with an infinite number of nodes.  This paper introduces a regression estimation algorithm which is a combination of these two elements: the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinitenode splines.  Experimental results are then presented (based on the Boston Housing data set) which indicate the performance of this algorithm relative to other algorithms. 
String Kernels, Fisher Kernels and Finite State Automata| Abstract In this paper we show how the generation of documents can be thought of as a k-stage Markov process, which leads to a Fisher kernel from which the n-gram and string kernels can be re-constructed.  The Fisher kernel view gives a more flexible insight into the string kernel and suggests how it can be parametrised in a way that reflects the statistics of the training corpus.  Furthermore, the probabilistic modelling approach suggests extending the Markov process to consider sub-sequences of varying length, rather than the standard fixed-length approach used in the string kernel.  We give a procedure for determining which sub-sequences are informative features and hence generate a Finite State Machine model, which can again be used to obtain a Fisher kernel.  By adjusting the parametrisation we can also influence the weighting received by the features.  In this way we are able to obtain a logarithmic weighting in a Fisher kernel.  Finally, experiments are reported comparing the different kernels using the standard Bag of Words kernel as a baseline. 
Computationally Efficient Transductive Machines|
Comparing the Bayes and Typicalness Frameworks|
Comparing the Bayesian and typicalness frameworks|
Efficient Implementation and Experimental Testing of Transductive Algorithms for Predicting with Confidence|
Ridge regression in dual variables|

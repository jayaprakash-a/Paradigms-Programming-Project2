Spontaneous Recovery of Associations by Learning Unrelated Tasks| Abstract We demonstrate that imperfect recall of a set of associations can usually be improved by training on a new, unrelated set of associations.  This spontaneous recovery of associations is a consequence of the high dimensionality of weight spaces, and is therefore not peculiar to any single type of neural net.  Accordingly, this work may have implications for spontaneous recovery of memory in the central nervous system. 
Artificial Intelligence and Simulation of Behaviour,| Abstract.  Vision consists of a multiplicity of tasks, of which object identification is only one.  We in the computer vision community have concentrated our efforts on object identification, and have thereby ensured that the formulation of the problem of vision provides methods which are not of general utility for vision.  Ironically, one consequence of this is that computer vision may not even be of use for object identification.  An analysis of why computer vision has become synonymous with object identification is presented.  The implications of this analysis for object identification and for interpreting neurophysiological evidence in terms of `feature detectors' are presented.  A formulation of the problem of vision in terms of spatiotemporal characteristics is proposed.  1 Object Identification in Human and Computer Vision The hardest part of any scientific investigation is not solving a particular problem, but formulating questions that focus attention on important aspects of the phenomena under investigation.  This paper attempts to step back from conventional formulations of the `problem of vision' by making explicit the unspoken assumptions upon which conventional formulations of the problem are based.  Historically, the primary goal of computer vision has been to identify objects.  One of the most influential books [1] on computer vision states in its preface: ``Computer vision is the construction of explicit, meaningful descriptions of physical objects from images. " Ballard and Brown, page xiii, 1982[1].  There is no hint here that computer vision may consist of more than object identification 1 .  And, from a psychophysiological perspective: ``The brain's task then, is to extract the constant, invariant features of objects from the perpetually changing flood of information it receives from them. " Zeki, page 43, 1992[2].  (Italics added).  ``The goal of vision is to inform us of the identity of objects in view and their spatial positions", Cavanagh, page 261, 1989[3].  (Italics added).  The cited texts are more general than these quotes might suggest, but these quotes demonstrate a prevalent view of the role of both human and computer vision.  More recently there has been a move away from traditional formulations of the problem of vision [4, 5].  These approaches, whilst commendable in many respects, do not make explicit the central role of the notion of `object'.  In Ballard's [4] excellent account of the advantages of animate vision there is no reference made as to why objects might be a useful way to represent the visual world, nor why the particular representations of objects used (colour histograms) might be formed by an animate vision system.  Formulations of the problem of computer vision in terms of objects may have arisen because the most obvious concomitant of vision in humans is their ability to identify objects.  However, most of human and animal vision has little to do with identifying objects.  More typically, vision is used to guide limbs, to track motion, to detect changes in motion/lighting/colour/depth, to estimate relative depths of surfaces using parallax/stereo/motion/texture.  Whilst some of these tasks may require the detection of objects, none of these tasks requires that those objects be recognised or identified.  Moreover, these tasks involve computation of quantities (such as position, depth, velocity, colour, gradient) which are not necessarily required in order to compute the identity of an object.  1 I use the term `object identification' instead of the more usual `object recognition' because I utilise a distinction between recognition (e. g.  familiarity with an object) and identification (i. e.  classifying or naming an object).  It is tempting to use easily quantifiable tasks, such as object identification, in order to compare the performance of a seeing machine to that of a human.  A machine that can identify objects provides a tangible demonstration that it can do what humans do.  It is tempting to suppose that if a machine can identify objects then it can `see' as humans do.  However, the fact that humans can identify objects does not imply that this is the only task humans use vision for; and a demonstration that both humans and machines can identify objects is not a demonstration that a machine can `see' in the sense normally associated with seeing humans.  It is less easy to measure how well a human uses vision to aid walking/climbing/reaching/grasping, even though there is ample evidence that vision is essential for these tasks.  The conventional formulation of the problem of object recognition implies that objects consist of well defined features, and that objects can be identified by first extracting these features and then matching them to stored representations.  If we accept that vision includes object identification, but that the visual mechanisms we posses evolved in order to perform many other visual tasks, then the conventional formulation of the problem of vision appears not only simplistic, but also peculiarly biased toward a task (object identification) that is an important, but relatively small part of, what vision is for.  It seems likely that our sophisticated object recognition ability is a relatively recent evolutionary development.  Like most evolutionary innovations the ability to recognise objects was probably synthesized from pre-existing computational mechanisms.  Consequently, if a mechanism is useful for object identification only, then it is unlikely that it forms a part of the solution implemented by human visual systems.  Conversely, if a mechanism subserves other forms of visually guided behaviour and object recognition then it is likely that that mechanism forms part of the human visual system.  Within computer vision we pride ourselves on the correspondence between our methods and the computational mechanisms observed in the human visual system.  However, if we wish to achieve object recognition by modelling the computational properties of the human visual system then we could do so by paying more attention to the types of tasks for which our visual systems evolved to deal with. 
Adaptive Scale Filtering: A General Method for Obtaining Shape From Texture| We introduce adaptive scale filtering, a general method for deriving shape from texture under perspective projection without recourse to prior segmentation of the image into geometric texture elements (texels), and without thresholding of filtered images.  If texels on a given surface can be identified in an image then the orientation of that surface can be obtained [11].  However,
Object Recognition Using Spatiotemporal Signatures| Abstract The sequence of images generated by motion between observer and object specifies a spatiotemporal signature for that object.  Evidence is presented that such spatiotemporal signatures are used in object recognition.  Subjects learned novel, three-dimensional, rotating objects from image sequences in a continuous recognition task.  During learning, the temporal order of images of a given object was constant.  During testing, the order of images in each sequence was reversed, relative to its order during learning.  This image sequence reversal produced significant reaction time increases and recognition rate decreases.  Results are interpreted in terms of object-specific spatiotemporal signatures. 
The Optimal Elastic Net: Finding Solutions to the Travelling Salesman Problem| Abstract We introduce the optimal elastic net (OEN) method for finding solutions to the travelling salesman problem (TSP).  The OEN method is related to the elastic net (EN) method[1].  The EN method encourages each city to become associated with at least one unit, and requires a large number of units in order to ensure that each city is matched to a unit at convergence.  In contrast, the OEN method encourages each city to become associated with at least one unit, and each unit to become associated with at least one city.  One consequence of this is that the OEN method requires fewer units than the EN method.  The OEN method has applications for problems in which there are two sets of variable (movable) components (e. g.  VLSI placement), and an arrangement of components is required which minimises the total interconnect length between the two sets. 
When is now? Perception of simultaneity| We address the following question: Is there a dierence (D) between the amount of time for auditory and visual stimuli to be perceived ? On each of 1000 trials, observers were presented with a light ^ sound pair, separated by a stimulus onset asynchrony (SOA) between 250 ms (sound rst) and # 250 ms.  Observers indicated if the light^ sound pair came on simultaneously by pressing one of two (yes or no) keys.  The SOA most likely to yield armative responses was dened as the point of subjective simultaneity (PSS).  PSS values were between 21 ms (i. e.  sound 21ms before light) and #150 ms.  Evidence is presented that each PSS is observer specic.  In a second experiment, each observer was tested using two observer ^ stimulus distances.  The resultant PSS values are highly correlated (r ^ 0:954, p ^ 0:003), suggesting that each observer ' s PSS is stable.  PSS values were signicantly aected by observer ^ stimulus distance, suggesting that observers do not take account of changes in distance on the resultant dierence in arrival times of light and sound.  The dierence RT d in simple reaction time to single visual and auditory stimuli was also estimated; no evidence that RT d is observer specic or stable was found.  The implications of these ndings for the perception of multisensory stimuli are discussed. 
File: ica tutorial2|tex Independent Component Analysis and Projection Pursuit: A Tutorial Introduction.  Abstract Independent component analysis (ICA) and projection pursuit (PP) are two related techniques for separating mixtures of source signals into their individual components.  These rapidly evolving techniques are currently finding applications in speech separation, ERP, EEG, fMRI, and low-level vision.  Their power resides in the simple and realistic assumption that different physical processes tend to generate statistically independent signals.  We provide an account that is intended as an informal introduction, as well as a mathematical and geometric description of the methods. 
Computer Vision: What Is The Object?| Abstract.  Vision consists of a multiplicity of tasks, of which object identification is only one.  We in the computer vision community have concentrated our efforts on object identification, and have thereby ensured that the formulation of the problem of vision provides methods which are not of general utility for vision.  Ironically, one consequence of this is that computer vision may not even be of use for object identification.  An analysis of why computer vision has become synonymous with object identification is presented.  The implications of this analysis for object identification and for interpreting neurophysiological evidence in terms of `feature detectors' are presented.  A formulation of the problem of vision in terms of spatio-temporal characteristics is proposed. 
Independent Components Analysis For Step-Wise Separation of Signals| Abstract We introduce a method for the step-wise extraction of each of N source signals from a set of M N signal mixtures.  The mixtures x = (x1 ; : : : ; xM ) T are formed by combining different proportions of the independent sources s = (s1 ; : : : ; sN ) T using an M \Theta N mixing matrix A, so that x = As.  Porrill and Stone [Porrill and Stone, 1997] showed that K N signals (s1 ; : : : ; sK ) T could be extracted using a K \Theta M rectangular un0mixing matrix W by maximising the joint entropy of K whitened, unmixed signals Y = oe(Wx), where oe is a monotonic, non-linear function.  Rather than extracting K N signals simultaneously, we use a step-wise procedure in which each step consists of extracting K = 1 source signal, and then subtracting that signal from the signal mixtures.  The relation between this ICA method and exploratory projection pursuit is discussed.  The method is demonstrated on acoustic data. 
Undercomplete Independent Component Analysis for Signal Separation and Dimension Reduction| Abstract We introduce undercomplete independent component analysis (uICA), a method for extracting K signals from M # K mixtures of N # M source signals.  The mixtures x = (x 1 ; : : : ; xM ) T are formed from a linear combination of the independent source signals s = (s 1 ; : : : ; s N ) T using an M # N mixing matrix A, so that x = As.  For the case N = K = M , Bell and Sejnowski [Bell and Sejnowski, 1995] showed that a square N # N unmixing matrix W can be found by maximising the joint entropy of M signals (Y 1 ; : : : ; YN ) T = #(Wx), where # is a monotonic, non-linear function.  Using a similar approach, we show that a K #M unmixing matrix W can be used to recover K # M signals (s 1 ; : : : ; s K ) T by maximising the joint entropy of K signals Y = #(Wx).  The matrix W is essentially a pseudo-inverse of the M #N mixing matrix A.  A different and widely used method for reducing the size of W is to perform principal component analysis (PCA) on the data set, and to use only the L principal components with the largest eigenvalues as input to ICA.  This results in an L # L unmixing matrix W .  However, there is no a priori reason to assume that independent components exist in only the L-D subspace defined by the L principal components with largest eigenvectors.  Thus, discarding some eigenvectors may also corrupt or discard independent components.  In contrast, uICA does not discard any independent components in the data set, and can extract between 1 and M signals from x.  The method is demonstrated on mixtures of high kurtosis (speech and music) and Gaussian signals. 
The Adaptive Bisector Method: Separating Slant and Tilt in Estimating Shape from Texture| Abstract Existing techniques for obtaining shape from texture estimate tilt and slant via a single computational mechanism.  These techniques do not take advantage of the fact that tilt is relatively easy to estimate, and slant can be estimated more easily once tilt is known.  This paper introduces the adaptive bisector method, which allows tilt and slant to be calculated separately, and by different computational mechanisms.  The method makes minimal assumptions regarding the isotropy of surface textures.  Evidence from psychophysical studies suggests that human observers are able to provide accurate estimates of tilt[8], but are poor at estimating slant[4].  Whilst no psychophysical claims are made regarding the means by which tilt and slant are computed in this paper, it is claimed that a method which depends upon two different mechanisms provides a plausible functional model for how human observers obtain shape from texture.  Results for real and synthetic perspective images of textured planar surfaces are presented. 
Evolutionary Robots: Our Hands In Their Brains?| Abstract The study of learning and evolutionary adaptation has yet to provide a theory which is sufficiently detailed to enable the construction of even the most primitive synthetic animal.  I argue that this is so for three reasons.  First, unlike other scientific fields, such as theoretical physics, there is no universally accepted paradigmatic approach to the study of the brain.  Second, there are certain fundamental (highly complex) `functional primitives' (e. g.  types of learning) immanent in nervous systems, which are a necessary prerequisite for perceptual processes, and which are not currently possessed by any animat.  Third, even though genetic algorithms are powerful optimisation techniques, the conventional use of genetic algorithms is flawed because it attempts to model only a restricted set of properties found in natural evolutionary systems. 
Object recognition: View-specificity and motionspecificity|
Learning perceptually salient visual parameters using spatiotemporal smoothness constraints|
A canonical microfunction for learning perceptual invariances|
Shape from texture: textural invariance and the problem of scale in perspective images of surfaces|
The Optimal Elastic Net: Finding Solutions for the Traveling Salesman|
Can Artificial Neural Networks Discover Useful Regularities?| In: Artificial Neural Networks. 
Temporal constraints on visual learning: A computational model|
Spatiotemporal independent component analysis of event-related fMRI data using skewed probability density functions|
Shape From Local and Global Analysis of Texture,|
Learning spatio-temporal invariances|
Independent Component Analysis: An Introduction|
Blind Source Separation Using Temporal Predictability|
Analysis of optical imaging data using weak models:|

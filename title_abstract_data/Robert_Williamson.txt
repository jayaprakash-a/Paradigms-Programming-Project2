A Generalized Representer Theorem| Abstract Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples.  We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a support vector kernel.  The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space. 
Generalization Bounds for Convex Combinations of Kernel Functions| Abstract We derive new bounds on covering numbers for hypothesis classes generated by convex combinations of basis functions.  These are useful in bounding the generalization performance of algorithms such as RBF-networks, boosting and a new class of linear programming machines similar to SV machines.  We show that p-convex combinations with p ? 1 lead to diverging bounds, whereas for p = 1 good bounds in terms of entropy numbers can be obtained.  In the case of kernel expansions, significantly better bounds can be obtained depending on the eigenvalues of the corresponding integral operators. 
Generalization Bounds and Learning Rates for Regularized Principal Manifolds| Abstract We derive uniform convergence bounds and learning rates for regularized principal manifolds.  This builds on previous work of Kegl et al. , however we are able to obtain stronger bounds taking advantage of the decomposition of the principal manifold in terms of kernel functions.  In particular, we are able to give bounds on the covering numbers which are independent of the number of basis functions (line elements) used.  Finally we are able to obtain a nearly optimal learning rate of order O(m\Gamma 1 2 +ff ) for certain types of regularization operators, where m is the sample size and ff an arbitrary positive constant.  A companion paper [4] describes the basic algorithm, details of the implementation and experimental results. 
Online Bayes Point Machines| Abstract.  We present a new and simple algorithm for learning large margin classifiers that works in a truly online manner.  The algorithm generates a linear classifier by averaging the weights associated with several perceptron-like algorithms run in parallel in order to approximate the Bayes point.  A random subsample of the incoming data stream is used to ensure diversity in the perceptron solutions.  We experimentally study the algorithm's performance on online and batch learning settings.  The online experiments showed that our algorithm produces a low prediction error on the training sequence and tracks the presence of concept drift.  On the batch problems its performance is comparable to the maximum margin algorithm which explicitly maximises the margin. 
Efficient agnostic learning of neural networks with bounded fan-in| Abstract We show that the class of two layer neural networks with bounded fan-in is efficiently learnable in a realistic extension to the Probably Approximately Correct (PAC) learning model.  In this model, a joint probability distribution is assumed to exist on the observations and the learner is required to approximate the neural network which minimizes the expected quadratic error.  As special cases, the model allows learning real-valued functions with bounded noise, learning probabilistic concepts and learning the best approximation to a target function that cannot be well approximated by the neural network.  The networks we consider have real-valued inputs and outputs, an unlimited number of threshold hidden units with bounded fan-in, and a bound on the sum of the absolute values of the output weights.  The number of computation
Fat-Shattering and the Learnability of Real-Valued Functions| Abstract We consider the problem of learning real-valued functions from random examples when the function values are corrupted with noise.  With mild conditions on independent observation noise, we provide characterizations of the learnability of a real-valued function class in terms of a generalization of the Vapnik-Chervonenkis dimension, the fat-shattering function, introduced by Kearns and Schapire.  We show that, given some restrictions on the noise, a function class is learnable in our model if and only if its fat-shattering function is finite.  With different (also quite mild) restrictions, satisfied for example by gaussian noise, we show that a function class is learnable from polynomially many examples if and only if its fat-shattering function grows polynomially.  We prove analogous results in an agnostic setting, where there is no assumption of an underlying function class. 
Lower Bounds on the VC-Dimension of Smoothly Parametrized Function Classes| Abstract We examine the relationship between the VC-dimension and the number of parameters of a thresholded smoothly parametrized function class.  We show that the VC-dimension of such a function class is at least k if there exists a k-dimensional differentiable manifold in the parameter space such that each member of the manifold corresponds to a different decision boundary.  Using this result, we are able to obtain lower bounds on the VC-dimension proportional to the number of parameters for several thresholded function classes including two-layer neural networks with certain smooth activation functions and radial basis functions with a gaussian basis.  These lower bounds hold even if the magnitudes of the parameters are restricted to be arbitrarily small.  In Valiant's probably approximately correct learning framework, this implies that the number of examples necessary for learning these function classes is at least linear in the number of parameters. 
What is a Dollar Worth? The Market Value of Cash Holdings| Abstract This study investigates the market value of cash held by firms.  In general, we estimate the value shareholders place on a marginal dollar of cash to be about $0. 97.  However, we find large cross-sectional differences consistent with existing theory.  We document that the quality and volatility of the firm's investment opportunity set as well as the probability of financial distress and capital market access impact the value shareholders place on cash holdings.  Firms with good growth options have their cash valued at a premium to those with poor growth prospects.  Additionally, cash is valued less in firms with stable investment programs and those nearer to financial distress.  Finally, we find that access to the capital markets affects shareholder's valuation of cash holdings, although the results are contrary to expectations.  Overall, it appears that the investment opportunity set rather than the financing opportunity set of the firm affects the value that shareholders place on a firm's cash holdings. 
Correction to 'Lower Bounds on the VC-Dimension of Smoothly Parametrized Function Classes'| Abstract The paper [3] gives lower bounds on the VC-dimension of various smoothly parametrized function classes.  The results were
Learning Nonlinearly Parametrized Decision Regions| Abstract In this paper we present a deterministic analysis of an online scheme for learning very general classes of nonlinearly parametrized decision regions.  The only input required is a sequence ((xk;y k )) k2Z + of data samples, where yk =1ifx kbelongs to the decision region of interest, and yk =, 1 otherwise.  Averaging results and Lyapunov theory are used to prove the stability of the scheme.  In the course of this proof, conditions on both the parametrization and the sequence of input examples arise which are sufficient to guarantee convergence of the algorithm.  Anumber of examples are presented, including the problem of learning an intersection of half spaces using only data samples. 
Classification on Proximity Data with LP--Machines| Abstract We provide a new linear program to deal with classification of data in the case of data given in terms of pairwise proximities.  This allows to avoid the problems inherent in using feature spaces with indefinite metric in Support Vector Machines, since the notion of a margin is purely needed in input space where the classification actually occurs.  Moreover in our approach we can enforce sparsity in the proximity representation by sacrificing training error.  This turns out to be favorable for proximity data.  Similar to --SV methods, the only parameter needed in the algorithm is the (asymptotical) number of data points being classified with a margin.  Finally, the algorithm is successfully compared with --SV learning in proximity space and K--nearest-neighbors on real world data from Neuroscience and molecular biology. 
From Margin to Sparsity| Abstract We present an improvement of Noviko#'s perceptron convergence theorem.  Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC#style generalisation error bound for the classifier learned by the perceptron learning algorithm.  The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel.  Ironically, the bound yields better guarantees than are currently available for the support vector solution itself. 
The Importance of Convexity in Learning with Squared Loss| Abstract We show that if the closure of a function class F under the metric induced by some probability distribution is not convex, then the sample complexity for agnostically learning F with squared loss (using only hypotheses in F ) is \Omega(ln(1=ffi)=ffl 2 ) where 1\Gamma ffi is the probability of success and ffl is the required accuracy.  In comparison, if the class F is convex and has finite pseudo-dimension, then the sample complexity is O\Gamma 1 ffl\Gamma ln 1 ffl + ln 1 ffi \Delta\Delta .  If a non-convex class F has finite pseudodimension, then the sample complexity for agnostically learning the closure of the convex hull of F , is O \Gamma 1 ffl \Gamma 1 ffl ln 1 ffl + ln 1 ffi \Delta\Delta .  Hence, for agnostic learning, learning the convex hull provides better approximation capabilities with little sample complexity penalty. 
Shrinking the Tube: A New Support Vector Regression Algorithm| Abstract A new algorithm for Support Vector regression is described.  For a priori chosen , it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction of the data points lie outside.  Moreover, it is shown how to use parametric tube shapes with non-constant radius.  The algorithm is analysed theoretically and experimentally. 
A PAC Analysis of a Bayesian Estimator| Abstract Bayesian analysis of generalisation can place a prior distribution on the hypotheses and estimate the volume of this space that is consistent with the training data.  The larger this volume the greater the confidence in the classifier obtained.  The key feature of such estimators is that they provide a posteriori estimates of generalisation based on properties of the hypothesis and the training data.  This contrasts with a `classical' PAC analysis which provides only a priori (worst case) bounds.  Following results in [26] showing that Data-sensitive analysis of generalisation in the PAC sense is possible, the paper uses the techniques to give the first PAC style analysis of a Bayesian inspired estimator of generalisation.  The estimator concerned is the size of a ball which can be placed in the consistent region of parameter space.  The ball gives a lower bound on the volume of parameter space consistent with the training set.  The larger the ball the better the bound on the generalisation obtained.  In all cases the bounds are of good generalisation with high confidence, hence bounding the tail of the distribution of generalisation errors that might occur.  The resulting bounds are independent of the complexity of the function class though they depend linearly on the dimensionality of the parameter space. 
A Generalized Kernel Approach to Dissimilarity-based| Abstract Usually, objects to be classified are represented by features.  In this paper, we discuss an alternative object representation based on dissimilarity values.  If such distances separate the classes well, the nearest neighbor method oers a good solution.  However, dissimilarities used in practice are usually far from ideal and the performance of the nearest neighbor rule suers from its sensitivity to noisy examples.  We show that other, more global classification techniques are preferable to the nearest neighbor rule, in such cases.  For classification purposes, two dierent ways of using generalized dissimilarity kernels are considered.  In the first one, distances are isometrically embedded in a pseudo-Euclidean space and the classification task is performed there.  In the second approach, classifiers are built directly on distance kernels.  Both approaches are described theoretically and then compared using experiments with dierent dissimilarity measures and datasets including degraded data simulating the problem of missing values. 
THE MARKET FOR CORPORATE CONTROL AND CORPORATE CASH HOLDINGS| Abstract Conventional wisdom asserts that firms with large cash holdings are likely takeover targets.  Using hostile takeover activity from 1985-1994, I find the probability a firm will be acquired decreases with cash.  This holds for firms with excess cash and those with poor investment opportunities.  Cash decreases acquisition probability by deterring potential bids, but does not increase premiums offered when bids occur.  Finally, cash decreases after passage of antitakeover legislation.  Thus, managers may hold cash to entrench themselves at shareholders' expense.  Consequently, the market for corporate control does not monitor corporate cash holdings.  Rather, cash may decrease such monitoring. 
Regularized Principal Manifolds| Abstract Many settings of unsupervised learning can be viewed as quantization problems - the minimization of the expected quantization error subject to some restrictions.  This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning.  This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding.  We explore this connection in two ways: (1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm.  In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators.  Experimental results demonstrate the feasibility of the approach. 
CHANNEL EQUALIZATION AND THE BAYES POINT MACHINE| ABSTRACT Equalizers trained with a large margin have an ability to better handle noise in unseen data and drift in the target solution.  We present a method of approximating the Bayes optimal strategy which provides a large margin equalizer, the Bayes point equalizer.  The method we use to estimate the Bayes point is to average N equalizers that are run on independently chosen subsets of the data.  To better estimate the Bayes point we investigated two methods to create diversity amongst the N equalizers.  We show experimentally that the Bayes point equalizer for appropriately large step sizes offers improvement on LMS and LMA in the presence of channel noise and training sequence errors.  This allows for shorter training sequences albeit with higher computational requirements. 
The Structure of Version Space| Abstract We investigate the generalisation performance of consistent classifiers, i. e.  classifiers that are contained in the so-called version space, both from a theoretical and experimental angle.  In contrast to classical VC analysis --- where no single classifier within version space is singled out on grounds of a generalisation error bound --- the data dependent structural risk minimisation framework suggests that there is one particular classifiers that is to be preferred because it minimises the generalisation error bound.  This is usually taken to provide a theoretical justification for learning algorithms such as the well known support vector machine.  A reinterpretation of a recent PAC-Bayesian result, however, reveals that given a suitably chosen hypothesis space there is a huge number of classifiers with small generalisation error albeit we cannot identify them for a specific learning task.  This result is complemented with an empirical study for kernel classifiers on the task of handwritten digit recognition which demonstrates that even classifiers with a small margin exhibit excellent generalisation. 
Online Learning with Kernels| Abstract We consider online learning in a Reproducing Kernel Hilbert Space.  Our method is computationally efficient and leads to simple algorithms.  In particular we derive update equations for classification, regression, and novelty detection.  The inclusion of the #-trick allows us to give a robust parameterization.  Moreover, unlike in batch learning where the #-trick only applies to the "-insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber's robust loss. 
Algorithmic Luckiness| Abstract In contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specific learning algorithm used.  Motivated by the luckiness framework [8] we are also able to exploit the serendipity of the training sample.  The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm.  We show how the resulting framework relates to the VC, luckiness and compression frameworks.  Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits both the margin and the distribution of the data in feature space. 
Estimating the Support of a High-Dimensional Distribution|
Investigating the Distribution Assumptions in the Pac Learning Model|
A Framework for Structural Risk Minimisation|
Support vector regression with automatic accuracy control|
Generalization bounds for convex combinations of kernel functions|
Design and use of an integrated cambridge ring",|
SV estimation of a distribution's support|
Large Margin Classification for Moving Targets|
The development of the joint NASA GSFC and the National Imagery and Mapping Agency (NIMA) geopotential model EGM96,|
epsilon-Entropy and the Complexity of Feedforward Neural Networks|
Why do firms in countries with poor protection of investor rights hold more cash, Working paper,|
A PAC Analysis of a Bayesian Estimator|
Kernel-dependent support vector error bounds|
Estimating the support of a high-dimensional distribution|
Dagnelie G and de Juan E 2003 Visual perception in a blind subject with a chronic microelectronic retinal prosthesis|
The Cornell implementation of the SMART system|
A Maximum Margin Miscellany|
e-entropy and the complexity of feedforward neural networks,|
Particle filtering algorithms for tracking an acoustic source in a reverberant environ|
Particle filter beamforming for acoustic source localization in a reverberant environment,|
Corporate control and the home bias, working paper,|
The development of the NASA GSFC and NIMA joint geopotential model, International Association of Geodesy Symposium No| 117,. 
The temporal and spatial characteristics of TOPEX/POSEIDON radial orbit error,|
Martin Anthony, Structural Risk Minimization over Data-Dependent Hierarchies,|
The VC dimension and pseudodimension of two-layer neural networks with discrete inputs|
Entropy numbers for convex combinations and MLPs|
On Efficient Agnostic Learning of Linear Combinations of Basis Functions|
Optically sampled analog-to-digital converters,|
Probabilistic arithmetic| I.  Numerical methods for calculating convolutions and dependency bounds. 
Tectonic Motion and Deformation From Satellite Laser Ranging to LAGEOS|
Extending the performance of optically sampled timedemultiplexed analog-to-digital converters,|
Culture, openness, and finance| Unpublished working paper 8222,. 
Space shuttle precision orbit determination in support of SLA-1 using TDRSS and GPS tracking data,|
The determinants and implications of corporate cash holdings,|
Generalization Bounds via Eigenvalues of the Gram Matrix," NeuroCOLT,|
1243 This manuscript was prepared with the AAS L A T E X macros v3|0.  Table 1.  Difference Between Landolt and FOS V mag. 
The development of the NASA GSFC and NIMA joint geopotential model|
Existence and Uniqueness Results for Neural Network Approximations",|
"Experimental comparison of particle filtering algorithms for acoustic source localization in a reverberant room,|
Fat-Shattering and the Learnability of Real-Valued Functions,|
The GEM-T2|
Norm-based regularization of boosting|
Generalization Bounds and Learning Rates for Regularized Principal Manifolds,|
Regularised principal manifolds,|
Probabilistic Arithmetic,|
Corporate governance and the home bias,|
Superkernels|
Learning the kernel with hyperkernels,|
The Statistical Performance of Some Instantaneous Frequency Estimators|
Smart data dictionary: A knowledge-object-oriented approach for interoperability of heterogeneous information management systems|
Cambridge HSLAN protocol review|' DJ Greaves, ID Wilson.  Proceedings of IFIP TC6 International Workshop on `Protocols for high-speed networks' edited by. 
New support vector algorithms, neurocolt2|
Prior Knowledge and Preferential Structures in Gradient Descent Learning Algorithms|
Temporal variations of the Earth's gravitational field from satellite laser ranging to LAGEOS,|
editors| Protocols For High-Speed Networks. 
Hyperkernels|
A generalized representer theorem, In \Proceedings of the 14th Annual Conference on Computational Learning Theory",|
Regularization with Dot-Product Kernels|
Do firms in countries with poor protection of investor rights hold more cash? unpublished working paper,|
The law of large numbers for fuzzy variables under a general triangular norm extension principle|
Subcellular immunolocalization of NMDA receptor subunit NR-1, 2A, 2B in the rat vestibular periphery|
"System identification in the behavioural framework via risk minimization",|
Local minima and attractors at infinity in gradient descent learning algorithms,|
Hyperkernels|
b) `Vendors form FFFE, coalition to ght Wal-Mart on reps',|
Wholesalers' coalition ready to ght Wal-Mart',|
`Wal-Mart to lock out sales reps',|
Corporate Governance and The Home Bias', Working Paper No| 8680, National Bureau of Economic Research. 
Sample Complexity of Agnostic Learning with Squared Loss|
Dice Center for Research in Financial Economics, Ohio State University|
An extreme limit theorem for dependency bounds of normalized sums of random variables|
Vieno t II report on the boards of directors of listed companies in France, Mouvement des Enterprises de France (|
"A Survey of LightWeight Transport Ptotocols for High-Speed Networks",|

RISK BOUNDS FOR MIXTURE DENSITY ESTIMATION| Abstract.  In this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class. 
The Fidelity of Local Ordinal Encoding| Abstract A key question in neuroscience is how to encode sensory stimuli such as images and sounds.  Motivated by studies of response properties of neurons in the early cortical areas, we propose an encoding scheme that dispenses with absolute measures of signal intensity or contrast and uses, instead, only local ordinal measures.  In this scheme, the structure of a signal is represented by a set of equalities and inequalities across adjacent regions.  In this paper, we focus on characterizing the fidelity of this representation strategy.  We develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure.  We also present a neurally plausible implementation of this computation that uses only local update rules.  The results highlight the robustness and generalization ability of local ordinal encodings for the task of pattern classification. 
Automatic Generation of RBF Networks| Abstract Learning can be viewed as mapping from an input space to an output space.  Examples of these mappings are used to construct a continuous function that approximates the given data and generalizes for intermediate instances.  Radial basis function (RBF) networks are used to formulate this approximating function.  A novel method is introduced that automatically constructs a RBF network for a given mapping and error bound.  This network is shown to be the smallest network within the error bound for the given mapping.  The integral wavelet transform is used to determine the parameters of the network.  Simple one-dimensional examples are used to demonstrate how the network constructed using the transform is superior to one constructed using standard ad hoc optimization techniques.  The paper concludes with the automatic generation of a network for a multidimensional problem, namely, object recognition and pose estimation.  The results of this application are favorable. 
Permutation Tests for Classification| Abstract.  We describe a permutation procedure used extensively in classification problems in computational biology and medical imaging.  We empirically study the procedure on simulated data and real examples from neuroimaging studies and DNA microarray analysis.  A theoretical analysis is also suggested to assess the asymptotic behavior of the test.  An interesting observation is that concentration of the permutation procedure is controlled by a Rademacher average which also controls the concentration of empirical errors to expected errors. 
ESAIM: Probability and Statistics Will be set by the publisher URL:| Abstract.  In this paper we focus on the problem of estimating a bounded density using a finite combination of densities from a given class.  We consider the Maximum Likelihood Estimator (MLE) and the greedy procedure described by Li and Barron [7, 8] under the additional assumption of the compactness of the domain.  We prove an O( 1 p n ) bound on the estimation error which does not depend on the number of densities in the estimated combination.  Under the assumption of compactness of the domain, this improves the bound of Li and Barron by removing the log n factor and also generalizes it to the base classes with converging Dudley integral.  1991 Mathematics Subject Classification.  The dates will be set by the publisher. 
Feature Selection for SVMs| Abstract We introduce a method of feature selection for Support Vector Machines.  The method is based upon finding those features which minimize bounds on the leave-one-out error.  This search can be efficiently performed via gradient descent.  The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA microarray data. 
On the Noise Model of Support Vector Machines Regression| Abstract.  Support Vector Machines Regression (SVMR) is a learning technique where the goodness of fit is measured not by the usual quadratic loss function (the mean square error), but by a different loss function called the #-Insensitive Loss Function (ILF), which is similar to loss functions used in the field of robust statistics.  The quadratic loss function is well justified under the assumption of Gaussian additive noise.  However, the noise model underlying the choice of the ILF is not clear.  In this paper the use of the ILF is justified under the assumption that the noise is additive and Gaussian, where the variance and mean of the Gaussian are random variables.  The probability distributions for the variance and mean will be stated explicitly.  While this work is presented in the framework of SVMR,
AI Memo AIM-2003-019 Permutation Tests for Classification| Abstract We introduce and explore an approach to estimating statistical significance of classification accuracy, which is particularly useful in scientific applications of machine learning where high dimensionality of the data and the small number of training examples render most standard convergence bounds too loose to yield a meaningful guarantee of the generalization ability of the classifier.  Instead, we estimate statistical significance of the observed classification accuracy, or the likelihood of observing such accuracy by chance due to spurious correlations of the high-dimensional data patterns with the class labels in the given training set.  We adopt permutation testing, a non-parametric technique previously developed in classical statistics for hypothesis testing in the generative setting (i. e. , comparing two probability distributions).  We demonstrate the method on real examples from neuroimaging studies and DNA microarray analysis and suggest a theoretical analysis of the procedure that relates the asymptotic behavior of the test to the existing convergence bounds. 
Statistical Learning: Stability is Sufficient for Generalization and Necessary and Sufficient for Consistency of Empirical Risk Minimization| Abstract Solutions of learning problems by Empirical Risk Minimization (ERM) -and almost-ERM when the minimizer does not exist -- need to be consistent, so that they may be predictive.  They also need to be well-posed in the sense of being stable, so that they might be used robustly.  We propose a statistical form of leave-one-out stability, called CVEEE loo stability.  Our main new results are two.  We prove that for bounded loss classes CVEEE loo stability is (a) sufficient for generalization, that is convergence in probability of the empirical error to the expected error, for any algorithm satisfying it and, (b) necessary and sufficient for generalization and consistency of ERM.  Thus CVEEE loo stability is a weak form of stability that represents a sufficient condition for generalization for general learning algorithms while subsuming the classical conditions for consistency of ERM.  We discuss alternative forms of stability.  In particular, we conclude that for ERM a certain form of well-posedness is equivalent to consistency. 
Bagging Regularizes| Abstract Intuitively, we expect that averaging --- or bagging --- different regressors with low correlation should smooth their behavior and be somewhat similar to regularization.  In this note we make this intuition precise.  Using an almost classical definition of stability, we prove that a certain form of averaging provides generalization bounds with a rate of convergence of the same order as Tikhonov regularization --- similar to fashionable RKHSbased learningalgorithms. 
Choosing Multiple Parameters for Support Vector Machines| Abstract The problem of automatically tuning multiple parameters for pattern recognition Support Vector Machines (
Feature Reduction and Hierarchy of Classifiers for Fast Object Detection in Video Images|
Support Vector Method for Multivariate Density Estimation|
Hidden Issues in the Simulation of Fixed Wireless Systems|
Institute for Brain and Neural Systems,|
Automatic Generation of GRBF Networks for Visual Learning|

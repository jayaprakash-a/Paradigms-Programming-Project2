What Are Textons?| Abstract.  Textons refer to fundamental micro-structures in generic natural images and thus constitutes the basic elements in early (pre-attentive) visual perception.  However, the word \texton" remains a vague concept in the literature of computer vision and visual perception, and a precise mathematical definition has yet to be found.  In this article, we argue that the definition of texton should be governed by a sound mathematical model of images, and the set of textons must be learned from, or best tuned to, an image ensemble.  We adopt a generative image model that an image is a superposition of bases from an over-complete dictionary, then a texton is defined as a mini-template that consists of a varying number of image bases with some geometric and photometric configurations.  By analogy to physics, if image bases are like protons, neutrons and electrons, then textons are like atoms.  Then a small number of textons can be learned from training images as repeating micro-structures.  We report four experiments for comparison.  The first experiment computes clusters in features space of filter responses.  The second use transformed component analysis in both feature space and image patches.  The third adopts a two layer generative model where an image is generated by image bases and image bases are generated by textons.  The fourth experiment shows textons from motion image sequences, which we call movetons. 
Minimax Entropy Principle and Its Application to Texture Modeling| Abstract This article proposes a general theory and methodology, called the minimax entropy principle, for building statistical models for images (or signals) in a variety of applications.  This principle consists of two parts.  The first is the maximum entropy principle for feature binding (or fusion): for a given set of observed feature statistics, a distribution can be built to bind these feature statistics together by maximizing the entropy over all distributions that reproduce these feature statistics.  The second part is the minimum entropy principle for feature selection: among all plausible sets of feature statistics, we choose the set whose maximum entropy distribution has the minimum entropy.  Computational and inferential issues in both parts are addressed, in particular, a feature pursuit procedure is proposed for approximately selecting the optimal set of features.  The minimax entropy principle is then corrected by considering the sample variation in the observed feature statistics, and an information criterion for feature pursuit is derived.  The minimax entropy principle is applied to texture modeling, where a novel Markov random field (
Image Segmentation by Data-Driven Markov Chain Monte Carlo| Abstract This paper presents a computational paradigm called Data-Driven Markov Chain Monte Carlo (DDMCMC) for image segmentation in the Bayesian statistical framework.  The paper contributes to image segmentation in four aspects.  Firstly, it designs ecient and well balanced Markov Chain dynamics to explore the complex solution space, and thus achieves a nearly global optimal solution independent of initial segmentations.  Secondly, it presents a mathematical principle and a K-adventurers algorithm for computing multiple distinct solutions from the Markov chain sequence, and thus it incorporates intrinsic ambiguities in image segmentation.  Thirdly, it utilizes data-driven (bottom-up) techniques, such as clustering and edge detection, to compute importance proposal probabilities, which drive the Markov chain dynamics and achieve tremendous speedup in comparison to the traditional jump-diffusion methods[12, 11].  Fourthly, the DDMCMC paradigm provides a unifying framework in which the role of many existing segmentation algorithms, such as, edge detection, clustering, region growing, split-merge, snake/balloon, and region competition, are revealed as either realizing Markov chain dynamics or computing importance proposal probabilities.  Thus the DDMCMC paradigm combines and generalizes these segmentation methods in a principled way.  The DDMCMC paradigm adopts seven parametric and non-parametric image models for intensity and color at various regions.  We test the DDMCMC paradigm extensively on both color and grey level images, and some results are reported in this paper. 
Exploring Texture Ensembles by Ecient Markov| Abstract This article presents a mathematical definition of texture { the Julesz ensemble #(h), which is the set of all images (defined on Z 2 ) that share identical statistics h.  Then texture modeling is posed as an inverse problem: given a set of images sampled from an unknown Julesz ensemble #(h # ), we search for the statistics h # which define the ensemble.  A Julesz ensemble #(h) has an associated probability distribution q(I; h), which is uniform over the images in the ensemble and has zero probability outside.  In a companion paper[32], q(I; h) is shown to be the limit distribution of the FRAME (Filter, Random Field, And Minimax Entropy) model[35] as the image lattice # ! Z 2 .  This conclusion establishes the intrinsic link between the scientific definition of texture on Z 2 and the mathematical models of texture on finite lattices.  It brings two advantages to computer vision.  1).  The engineering practice of synthesizing texture images by matching statistics has been put on a mathematical foundation.  2).  We are released from the burden of learning the expensive FRAME model in feature pursuit, model selection and texture synthesis.  In this paper, an ecient Markov chain Monte Carlo algorithm is proposed for sampling Julesz ensembles.  The algorithm generates random texture images by moving along the directions of filter coecients and thus extends the traditional single site Gibbs sampler.  This paper also compares four popular statistical measures in the literature, namely, moments, rectified functions, marginal histograms and joint histograms of linear filter responses in terms of their descriptive abilities.  Our experiments suggest that a small number of bins in marginal histograms are sucient for capturing a variety of texture patterns.  We illustrate our theory and algorithm by successfully synthesizing a number of natural textures. 
Towards a Mathematical Theory of Primal Sketch and Sketchability| Abstract In this paper, we present a mathematical theory for Marr's primal sketch.  We first conduct a theoretical study of the descriptive Markov random field model and the generative wavelet/sparse coding model from the perspective of entropy and complexity.  The competition between the two types of models defines the concept of "sketchability", which divides image into texture and geometry.  We then propose a primal sketch model that integrates the two models and, in addition, a Gestalt field model for spatial organization.  We also propose a sketching pursuit process that coordinates the competition between two pursuit algorithms: the matching pursuit [8] and the filter pursuit [12], that seek to explain the image by bases and filters respectively.  The model can be used to learn a dictionary of image primitives, or textons in Julesz's language, for natural images.  The primal sketch model is not only parsimonious for image representation, but produces meaningful sketches over a large number of generic images. 
Graph Partition by Swendsen-Wang Cuts| Abstract Vision tasks, such as segmentation, grouping, recognition, can be formulated as graph partition problems.  The recent literature witnessed two popular graph cut algorithms: the Ncut using spectral graph analysis and the minimum-cut using the maximum flow algorithm.  This paper presents a third major approach by generalizing the Swendsen-Wang method-- a well celebrated algorithm in statistical mechanics.  Our algorithm simulates ergodic, reversible Markov chain jumps in the space of graph partitions to sample a posterior probability.  At each step, the algorithm splits, merges, or re-groups a sizable subgraph, and achieves fast mixing at low temperature enabling a fast annealing procedure.  Experiments show it converges in 230 seconds in a PC for image segmentation.  This is 400 times faster than the single-site update Gibbs sampler, and 20-40 times faster than the DDMCMC algorithm.  The algorithm can optimize over the number of models and works for general forms of posterior probabilities, so it is more general than the existing graph cut approaches. 
Modeling Textured Motion : Particle, Wave and Sketch| Abstract In this paper, we present a generative model for textured motion phenomena, such as falling snow, wavy river and dancing grass, etc.  Firstly, we represent an image as a linear superposition of image bases selected from a generic and over-complete dictionary.  The dictionary contains Gabor bases for point/particle elements and Fourier bases for wave-elements.  These bases compete to explain the input images.  The transform from a raw image to a base or a token representation leads to large dimension reduction.  Secondly, we introduce a unified motion equation to characterize the motion of these bases and the interactions between waves and particles, e. g.  a ball floating on water.  We use statistical learning algorithm to identify the structure of moving objects and their trajectories automatically.  Then novel sequences can be synthesized easily from the motion and image models.  Thirdly, we replace the dictionary of Gabor and Fourier bases with symbolic sketches (also bases).  With the same image and motion model, we can render realistic and stylish cartoon animation.  In our view, cartoon and sketch are symbolic visualization of the inner representation for visual perception.  The success of the cartoon animation, in turn, suggests that our image and motion models capture the essence of visual perception of textured motion. 
Image Parsing: Unifying Segmentation, Detection, and Recognition| Abstract We propose a general framework for parsing images into regions and objects.  In this framework, the detection and recognition of objects proceed simultaneously with image segmentation in a competitive and cooperative manner.  We illustrate our approach on natural images of complex city scenes where the objects of primary interest are faces and text.  This method makes use of bottom-up proposals combined with top-down generative models using the Data Driven Markov Chain Monte Carlo (DDMCMC) algorithm which is guaranteed to converge to the optimal estimate asymptotically.  More precisely, we define generative models for faces, text, and generic regions-- e. g.  shading, texture, and clutter.  These models are activated by bottom-up proposals.  The proposals for faces and text are learnt using a probabilistic version of AdaBoost.  The DDMCMC combines reversible jump and diffusion dynamics to enable the generative models to explain the input images in a competitive and cooperative manner.  Our experiments illustrate the advantages and importance of combining bottom-up and top-down models and of performing segmentation and object detection/recognition simultaneously. 
Learning Inhomogeneous Gibbs Model of Faces by Minimax Entropy| Abstract In this paper we propose a novel inhomogeneous Gibbs model by the minimax entropy principle, and apply it to face modeling.  The maximum entropy principle generalizes the statistical properties of the observed samples and results in the Gibbs distribution, while the minimum entropy principle makes the learnt distribution close to the observed one.  To capture the fine details of a face, an inhomogeneous Gibbs model is derived to learn the local statistics of facial feature points.  To alleviate the high dimensionality problem of face models, we propose to learn the distribution in a subspace reduced by principal component analysis or PCA.  We demonstrate that our model effectively captures important and subtle non-Gaussian face patterns and efficiently generates good face models. 
Generalizing Swendsen-Wang to Sampling Arbitrary Posterior Probabilities| Abstract Many vision tasks can be formulated as graph partition problems that minimize energy functions.  For such problems, the Gibbs sampler[9] provides general solution but is very slow, while other methods, such as Ncut[24] and graph cuts[4], [22], are computationally effective but only work for specific energy forms[17] and are not generally applicable.  In this paper, we present a new inference algorithm that generalizes the Swendsen-Wang method[25] to arbitrary probabilities defined on graph partitions.  We begin by computing graph edge weights, based on local image features.  Then the algorithm iterates two steps.  (i) Graph clustering: it forms connected components by cutting the edges probabilistically based on their weights.  (ii) Graph relabeling: it selects one connected component and flips probabilistically, the coloring of all vertices in the component simultaneously.  Thus it realizes the split, merge, and re-grouping of a "chunk" of the graph, in contrast to Gibbs sampler that flips a single vertex.  We prove that this algorithm simulates ergodic and reversible Markov chain jumps in the space of graph partitions and is applicable to arbitrary posterior probabilities or energy functions defined on graphs.  We demonstrate the algorithm on two typical problems in computer vision - image segmentation and stereo vision.  Experimentally we show that it is 100-400 times faster in CPU time than the classical Gibbs sampler and 20-40 times faster then the DDMCMC segmentation algorithm[27].  For stereo, we compare performance with graph cuts and belief propagation.  We also show that our algorithm can automatically infer generative models and obtain satisfactory results (better than the graphic cuts or belief propagation) in the same amount of time. 
Bayesian Reconstruction of 3D Shapes and Scenes From A Single Image| Abstract It's common experience for human vision to perceive full 3D shape and scene from a single 2D image with the occluded parts "filled-in" by prior visual knowledge.  In this paper we represent prior knowledge of 3D shapes and scenes by probabilistic models at two levels -- both are defined on graphs.  The first level model is built on a graph representation for single objects, and it is a mixture model for both man-made block objects and natural objects such as trees and grasses.  It assumes surface and boundary smoothness, 3D angle symmetry etc.  The second level model is built on the relation graph of all objects in a scene.  It assumes that objects should be supported for maximum stability with global bounding surfaces, such as ground, sky and walls.  Given an input image, we extract the geometry and photometric structures through image segmentation and sketching, and represent them in a big graph.  Then we partition the graph into subgraphs each being an object, infer the 3D shape and recover occluded surfaces, edges and vertices in each subgraph, and infer the scene structures between the recovered 3D sub-graphs.  The inference algorithm samples from the prior model under the constraint that it reproduces the observed image/sketch under projective geometry. 
Learning Generic Prior Models for Visual Computation| Abstract Many generic prior models have been widely used in computer vision ranging from image and surface reconstruction to motion analysis, and these models presume that surfaces of objects be smooth, and adjacent pixels in images have similar intensity values.  However, there is little rigorous theory to guide the construction and selection of prior models for a given application.  Furthermore, images are often observed at arbitrary scales, but none of the existing prior models are scale-invariant.  Motivated by these problems, this article chooses general natural images as a domain of application, and proposes a theory for learning prior models from a set of observed natural images.  Our theory is based on a maximum entropy principle, and the learned prior models are of Gibbs distributions.  A novel information criterion is proposed for model selection by minimizing a KullbackLeibler information distance.  We also investigate scale invariance in the statistics of natural images and study a prior model which has scale invariant property.  In this paper, in contrast with all existing prior models, negative potentials in Gibbs distribution are first reported.  The learned prior models are verified in two ways.  Firstly images are sampled from the prior distributions to demonstrate what typical images they stand for.  Secondly they are compared with existing prior models in experiments of image restoration. 
A Multi-scale Generative Model for Animate Shapes and Parts| Abstract This paper presents a multi-scale generative model for representing animate shapes and extracting meaningful parts of objects.  The model assumes that animate shapes (2D simple closed curves) are formed by a linear superposition of a number of shape bases.  These shape bases resemble the multi-scale Gabor bases in image pyramid representation, are well localized in both spatial and frequency domains, and form an over-complete dictionary.  This model is simpler than the popular Bspline representation since it does not engage a domain partition.  Thus it eliminates the interference between adjacent B-spline bases, and becomes a true linear additive model.  We pursue the bases by reconstructing the shape in a coarse-to-fine procedure through curve evolution.  These shape bases are further organized in a tree-structure where the bases in each subtree sum up to an intuitive part of the object.  To build probabilistic model for a class of objects, we propose a Markov random field model at each level of the tree representation to account for the spatial relationship between bases.  Thus the final model integrates a Markov tree (generative) model over scales and a Markov random field over space.  We adopt EM-type algorithm for learning the meaningful parts for a shape class, and show some results on shape synthesis. 
A Mathematical Theory of Primal Sketch and Sketchability| Abstract In this paper, we present a mathematical theory for Marr's primal sketch.  The theory has four components.  The first component is a primal sketch model for natural images, which integrates the descriptive Markov random field model and the generative wavelet model.  The former is applied to textural locations without distinguishable elements, called non-sketchable, and the latter is applied to geometric locations, called sketchable.  The second component is a sketching pursuit process which coordinates the competition between two pursuit algorithms: the matching pursuit [9] and the filter pursuit [14], or the competition of the two families of models that seek to explain the image by bases and filters respectively.  The third component is a theoretical definition of sketchability which states a critical condition, and thus a dividing point, for our perceptual jumps between texture and geometry.  The fourth component is to learn a generic dictionary of image primitives, or textons in Julesz's term, for natural images.  Our dictionary is found to be far more effective than conventional Gabor and LoG bases.  Our model is not only extremely parsimonious for image representation, but produces meaningful sketches, as Marr hoped for, over a large number (4 5 A? ) of generic images. 
Example-Based Facial Sketch Generation with Non-parametric Sampling| Abstract In this paper, we present an example-based facial sketch system.  Our system automatically generates a sketch from an input image, by learning from example sketches drawn with a particular style by an artist.  There are two key elements in our system: a non-parametric sampling method and a flexible sketch model.  Given an input image pixel and its neighborhood, the conditional distribution of a sketch point is computed by querying the examples and finding all similar neighborhoods.  An "expected sketch image" is then drawn from the distribution to reflect the drawing style.  Finally, facial sketches are obtained by incorporating the sketch model.  Experimental results demonstrate the effectiveness of our techniques. 
Int'| Abstract We describe a flexible object recognition and modeling system (FORMS) which represents and recognizes animate objects from their silhouettes.  This consists of a model for generating the shapes of animate objects which gives a formalism for solving the inverse problem of object recognition.  We model all objects at three levels of complexity: (i) the primitives, (ii) the mid-grained shapes, which are deformations of the primitives, and (iii) objects constructed by using a grammar to join mid-grained shapes together.  The deformations of the primitives can be characterized by principal component analysis or modal analysis.  When doing recognition the representations of these objects are obtained in a bottom-up manner from their silhouettes by a novel method for skeleton extraction and part segmentation based on deformable circles.  These representations are then matched to a database of prototypical objects to obtain a set of candidate interpretations.  These interpretations are verified in a top-down process.  The system is demonstrated to be stable in the presence of noise, the absence of parts, the presence of additional parts, and considerable variations in articulation and viewpoint.  Finally, we describe how such a representation scheme can be automatically learnt from examples. 
Fundamental Bounds on Edge Detection: An Information Theoretic Evaluation of Different Edge Cues|
Region Competition: Unifying Snakes, Region Growing, Energy/Bayes/MDL for Multi-band Image Segmentation|
Prior Learning and Gibbs Reaction-Diffusion|
A Generative Method for Textured Motion: Analysis and Synthesis|
Equivalence of Julesz Ensemble and FRAME Models|
Parsing Images into Region and Curve Processes|
Equivalence of Julesz Ensembles and FRAME Models|
FORMS: A Flexible Object Recognition and Modelling System|
Filters, random fields and maximun entropy (FRAME) - towards a unified theory for texture modeling|
The design and evaluation of intelligent image diagnosis data management software|
Statistical Edge Detection: Learning and Evaluating Edge Cues|
FRAME: Filters, Random fields, and Minimax Entropy-- Towards a Unified Theory for Texture Modeling|
Statistical and Computatinal Theories for Image Segmentation, Texture Modeling and Object Recognition|
Mathematical modeling of clutter: Descriptive vs| generative models. 
Effective statistical inference by data-driven markov chain Monte-Carlo|
Filters, random fields and maximum entropy (FRAME) -- towards the unified theory for texture modeling|
Parsing images into regions and curve processes",|
Embedding Gestalt Laws in Markov Random Fields|
Integrating Bottom-Up/Top-Down for Object Recognition by Data Driven Markov Chain Monte Carlo|
\Statistics matching and model pursuit by ecient MCMC"|
FRAME: Filters, random field and maximum entropy: -- towards a unified theory for texture modeling,"|
Order Parameters for Minimax Entropy Distributions: When Does High Level Knowledge Help?|
The role of V1 in shape representation,|
Trade, product cycles and inequality within and between countries,"|
Parsing images into region, curves, and curve processes",|
Multigrid and Multi-Level Swendsen-Wang Cuts for Hierarchic Graph Partition|
A Stochastic Algorithm for 3D Scene Segmentation and Reconstruction|
Stochastic Jump-Diffusion Process for Computing Medial Axes in Markov Random Fields|
Exploring Texture Ensembles by Efficient Markov Chain Monte Carlo-Toward a 'Trichromacy' Theory of Texture|
Stochastic Computation of Medial Axis in Markov Random Fields|
Analysis and Synthesis of Textured Motion: Particle, Wave and Cartoon Sketch",|
\Clutter Modeling and Performance Analysis in Automatic Target Recognition"|
Exploring the Julesz ensemble by efficient Markov chain Monte Carlo|
Visual Learning by Integrating Descriptive and Generative Methods|
Statistical Modeling and Conceptualization of Visual Patterns|
Modeling Visual Patterns by Integrating Descriptive and Generative Methods|
Image segmentation by data-driven MCMC",|
Unifying snake/balloons, region growing and Bayes/MDL/Energy for multi-band image segmentation|

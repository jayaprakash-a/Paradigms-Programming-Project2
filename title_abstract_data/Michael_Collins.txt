Three Generative, Lexicalised Models for Statistical Parsing| Abstract In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.  We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement.  Results on Wall Street Journal text show that the parser performs at 88. 1/87. 5% constituent precision/recall, an average improvement of 2. 3% over (Collins 96). 
A Generalization of Principal Components Analysis to the Exponential Family| Abstract Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction.  PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data.  This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types.  We describe algorithms for minimizing the loss functions, and give examples on simulated data. 
New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron| Abstract This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm.  We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the "all subtrees" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.  We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and namedentity extraction from web data. 
Exponentiated Gradient Algorithms for Large-margin Structured Classification| Abstract We consider the problem of structured classification, where the task is to predict a label y from an input x, and y has meaningful internal structure.  Our framework includes supervised training of Markov random fields and weighted context-free grammars as special cases.  We describe an algorithm that solves the large-margin optimization problem defined in [12], using an exponential-family (Gibbs distribution) representation of structured objects.  The algorithm is efficient---even in cases where the number of labels y is exponential in size---provided that certain expectations under Gibbs distributions can be calculated efficiently.  The method for structured labels relies on a more general result, specifically the application of exponentiated gradient updates [7, 8] to quadratic programs. 
Logistic Regression, AdaBoost and Bregman Distances| Abstract We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances.  The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other.  For both problems, we give new algorithms and explain their potential advantages over existing methods.  These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once).  We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified.  For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique.  As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost.  We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm.  We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings. 
Case-Factor Diagrams for Structured Probabilistic Modeling| Abstract We introduce a probabilistic formalism subsuming Markov random fields of bounded tree width and probabilistic context free grammars.  Our models are based on a representation of Boolean formulas that we call case-factor diagrams (CFDs).  CFDs are similar to binary decision diagrams (BDDs) but are concise for circuits of bounded tree width (unlike BDDs) and can concisely represent the set of parse trees over a given string under a given context free grammar (also unlike BDDs).  A probabilistic model consists of a CFD defining a feasible set of Boolean assignments and a weight (or cost) for each individual Boolean variable.  We give an insideoutside algorithm for simultaneously computing the marginal of each Boolean variable, and a Viterbi algorithm for finding the mininum cost variable assignment.  Both algorithms run in time proportional to the size of the CFD. 
An extended abstract of this journal submission| Abstract We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances.  The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other.  For both problems, we give new algorithms and explain their potential advantages over existing methods.  These algorithms can be divided into two types based on whether the parameters are iteratively updated sequentially (one at a time) or in parallel (all at once).  We also describe a parameterized family of algorithms which interpolates smoothly between these two extremes.  For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique.  As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost.  We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with iterative scaling.  We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings. 
Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm| Abstract This paper describes discriminative language modeling for a large vocabulary speech recognition task.  We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs).  The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer.  The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data.  However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0. 5% reduction in word error rate, for a total 1. 8% absolute reduction from the baseline of 39. 2%. 
IMPROVING INTONATIONAL PHRASING WITH SYNTACTIC INFORMATION| ABSTRACT The prediction of intonational phrase boundaries from raw text is an important step for a text-to-speech system: Locating where to place short pauses enables more natural sounding speech, that can be more easily understood.  We improved upon earlier work [Hirschberg and Prieto, 1996] by adding syntactic information gained from a highaccuracy parser [Collins, 1999].  We report significant improvement using various experimental setups.  We also show that our improved method comes close to interannotator agreement. 
Discriminative Syntactic Language Modeling for Speech Recognition| Abstract We describe a method for discriminative training of a language model that makes use of syntactic features.  We follow a reranking approach, where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second "reranking" model is then used to choose an utterance from these 1000-best lists.  The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.  We describe experiments on the Switchboard speech recognition task.  The syntactic features provide an additional 0. 3% reduction in test--set error rate beyond the model of (Roark et al. , 2004a; Roark et al. , 2004b) (significant at p < 0. 001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1. 2% over the baseline Switchboard system. 
Large margin methods for structured classification: Exponentiated gradient algorithms and PAC-Bayesian generalization bounds| Abstract.  We consider the problem of structured classification, where the task is to predict a label y from an input x, and y has meaningful internal structure.  Our framework includes supervised training of both Markov random fields and weighted context-free grammars as special cases.  We describe an algorithm that solves the large-margin optimization problem defined in [12], using an exponentialfamily (Gibbs distribution) representation of structured objects.  The algorithm is efficient -- even in cases where the number of labels y is exponential in size -provided that certain expectations under Gibbs distributions can be calculated efficiently.  The optimization method we use for structured labels relies on a more general result, specifically the application of exponentiated gradient (EG) updates [4, 5] to quadratic programs (QPs).  We describe a new method for solving QPs based on these techniques, and give bounds on its rate of convergence.  In addition to their application to the structured-labels task, the EG updates lead to simple algorithms for optimizing "conventional" binary or multiclass SVM problems.  Finally, we give a new generalization bound for structured classification, using PAC-Bayesian methods for the analysis of large margin classifiers. 
A Note on Ideal Tripartite Access Structures| Abstract Padro and Saez [PS] introduced the concept of a k-partite access structure for secret sharing and gave a complete characterization of ideal bipartite structures.  We derive a necessary condition for ideal tripartite structures, which we conjecture is necessary for all k. 
Max-Margin Parsing| Abstract We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying
A MULTI-LINGUAL WEB-BASED SURVEY FORM MACHINE TRANSLATION MECHANISM| ABSTRACT: Translation costs restrict the preparation of medical survey questionnaires in migrant and Aboriginal communities in Western Australia.  This is further compounded by a lack of affordable and accurate machine translation (MT) mechanisms.  This research investigates combined strategies intended to provide an efficacious and affordable machine translator to meet these needs by: (a) using an interlingua or hub-language which requires less resources for its construction than contemporary systems and has the additional benefit of significant error reduction; (b) creating word group structures to reduce the complexity of translation rules and enhance correct transfer of meaning between natural languages; and (c) defining smaller operating environments to restrict data input and further enhance translation by reducing error occurrence significantly.  This research aims to produce a prototype MT mechanism that will accept questionnaire texts as discrete questions and suggested answers from which a respondent may select.  The prototype will be designed to accept as input, non-ambiguous English as the source language (SL), translate it to a `hub-language' or interlingua based on Esperanto and thence to a selected target language (TL).  Additionally, an inverse path of translation from French back to English will enable validation of minimal or zero change in both syntax and semantics of the original input. 
The Ice Rink Problem| ABSTRACT Aerial search and rescue (or bombing) missions frequently have to locate a target within some prescribed area, running a search pattern that is constrained by the flight characteristics of the aircraft.  The sweep is done in nearly straight lines at high speed, with the craft's sensors monitoring a narrow strip of ground underneath the craft; in between sweeps, the craft slows down for turns that are much wider than the sensor footprint.  A similar task with a simpler geometry consists of cleaning an ice rink.  We prove that the method used in ice rinks (the "Zamboni algorithm") is optimal and use it to develop heuristics for the more general task of sweeping an arbitrary simple polygon.  We provide upper bounds on the performance of some of our heuristics and give the results of experiments showing that our heuristics produce solutions within a few percent of optimal. 
Discriminative Reranking for Natural Language Parsing| Abstract This paper considers approaches which rerank the output of an existing probabilistic parser.  The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.  A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.  We describe and compare two approaches to the problem: one based on Markov Random Fields, the other based on boosting approaches to reranking problems.  The methods were applied to reranking output of the parser of Collins (1999) on the Wall Street Journal corpus, with a 13% relative decrease in error rate. 
Incremental Parsing with the Perceptron Algorithm| Abstract This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm.  A beam-search algorithm is used during both training and decoding phases of the method.  The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.  We demonstrate that training a perceptron model to combine with the generative model during search provides a 2. 1 percent F-measure improvement over the generative model alone, to 88. 8 percent. 
Clause Restructuring for Statistical Machine Translation| Abstract We describe a method for incorporating syntactic
Convolution Kernels for Natural Language| Abstract We describe the application of kernel methods to Natural Language Processing (NLP) problems.  In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors.  We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures.  We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees. 
AT&T at TREC-8| Abstract In 1999, AT&T participated in the ad-hoc task and the Question Answering (QA), Spoken Document Retrieval (SDR), and Web tracks.  Most of our effort for TREC-8 focused on the QA and SDR tracks.  Results from SDR track show that our document expansion techniques, presented in [8, 9], are very effective for speech retrieval.  The results for question answering are also encouraging.  Our system designed in a relatively short period for this task can find the correct answer for about 45% of the user questions.  This is specially good given the fact that our system extracts only a short phrase as an answer. 
A New Statistical Parser Based on Bigram Lexical Dependencies| Abstract This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.  Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.  Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al.  94), which has the best published results for a statistical parser on this task.  The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes.  With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy. 
Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms| Abstract We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs).  The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates.  We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems.  We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger. 
The Rules Behind Roles: Identifying Speaker Role in Radio Broadcasts| Abstract Previous work has shown that providing information about story structure is critical for browsing audio broadcasts.  We investigate the hypothesis that Speaker Role is an important cue to story structure.  We implement an algorithm that classifies story segments into three Speaker Roles based on several content and duration features.  The algorithm correctly classifies about 80% of segments (compared with a baseline frequency of 35. 4%) when applied to ASR derived transcriptions of broadcast data. 
Improved Lower Bounds for the Link Length of Rectilinear Spanning Paths in Grids| Abstract We give improved lower bounds on the link length of spanning paths in a d-dimensional grid of size n.  Kranakis et al.  conjectured that the optimal link length is (1 + 1 d\Gamma 1 )n d\Gamma 1 , but gave lower bounds not substantially larger than n d\Gamma 1 ; we give a lower bound of (1 + 1 2d )n d\Gamma 1 for all d ? 2. 
More Netflow Tools for Performance and Security| Analysis of network traffic is becoming increasingly important, not just for determining network characteristics and anticipating requirements, but also for security analysis.  Several tool sets have been developed to perform analysis of flow-level network traffic, however none have had security as the primary goal of the analysis, nor has performance been a key consideration.  In this paper we present a suite of tools for network traffic collection and analysis based on Cisco NetFlow. The two primary design considerations were performance and the ability to build richer models of traffic for security analysis.  Thus the data structures and code have been optimized for use on very large networks with a large number of flows.  Data filter rates are approximately 80 million records in less than 1. 5 minutes on a Sun 4800. 
An Empirical Analysis of Target-Resident DoS Filters| Abstract Numerous techniques have been proposed by which an end-system, subjected to a denial-of-service flood, filters the offending traffic.  In this paper, we provide an empirical analysis of several such proposals, using traffic recorded at the border of a large network and including real DoS traffic.  We focus our analysis on four filtering techniques, two based on the addresses from which the victim server typically receives traffic (static clustering and network-aware clustering), and two based on coarse indications of the path each packet traverses (hop-count filtering and path identifiers).  Our analysis reveals challenges facing the proposed techniques in practice, and the implications of these issues for effective filtering.  In addition, we compare techniques on equal footing, by evaluating the performance of one scheme under assumptions made by another.  We conclude with an interpretation of the results and suggestions for further analysis. 
Parameter Estimation for Statistical Parsing Models: Theory and Practice of Distribution-Free Methods|
Answer Extraction|
A Statistical Parser of Czech|
A Statistical Parser for Czech|
Ranking Algorithms for Named Entity Extraction: Boosting and the Voted Perceptron|
A New Statistical Based on Bigram Lexical Dependencies",|
Corrective language modeling for large vocabulary ASR with the perceptron algorithm|
Machine translation of battlefield messages using lexicostructural transfer|
Audibility of short-duration tone-glides as a function of rate of frequency change|
Temporal integration of tone glides|
in submission| Discriminative reranking for natural language parsing. 
The rules behind roles|
Overview of Memory Channel Network for PCI|
Yoram: Unsupervised models for named entity classification|
The Nature of Bank/Client Relations and Industrial Lending by English Commercial Banks Before World War I," paper presented at the B Session Preconference on "Finance and the Making of the|
The evolution of a high-performing Java virtual machine|
An Assessment of Structure and Causation of IS Usage|
Representation and characters of finite groups, volume 22|
Estimating Basic and Applied Research and Development in Industry,|
Incremental parsing with the perceptron algorithm|
Statistical Parser Based on Bigram Lexical Dependencies|
Discriminative n-gram language modeling|

Automatic volumetric segmentation of human visual retinotopic cortex| Abstract Previous identification of early visual cortical areas in humans with phase-encoded retinotopic mapping techniques have relied on an accurate cortical surface reconstruction.  Here a 3D phase-encoded retinotopic mapping technique that does not require a reconstruction of the cortical surface is demonstrated.  The visual field sign identification is completely automatic and the method directly supplies volumes for a region-of-interest analysis, facilitating the application of cortical mapping to a wider population.  A validation of the method is provided by simulations and comparison to cortical surface-based methodology.  2003 Elsevier Science (USA).  All rights reserved. 
The Trajectory Mixture Model for Learning Collections of Nonlinear Functions| Abstract Learning statistical models of nonlinear dynamical systems has long been an important problem in machine learning.  The problem becomes especially hard when the dynamical system is composed of a mixture of nonlinear models, not just a single nonlinear model.  To address this general case of nonlinear time-series modeling, we propose a new hierarchical architecture: the Trajectory Mixture Model (TMM).  The TMM learns collections of different nonlinear "trajectories" through state space.  The model uses an expectation maximization (EM) algorithm to train a collection of nonlinear function approximators based on Gaussian radial basis function units.  State densities are represented using samples estimated by particle filtering and smoothing.  A sample-based representation provides an effective means of representing non-parametric state densities that change arbitrarily over time.  We use entropy-based model selection to ensure that the individual function approximators, as well as the higherlevel mixture model, do not overfit the data.  Our results suggest that TMMs can learn complex nonlinear state space models directly from observations and may offer greater flexibility in modeling time-series data than existing methods such as extended Kalman filters and particle filters. 
Learning Arbitrary State Transition Kernels for Prediction| Abstract Predicting future states of noisy dynamical systems requires efficient inference algorithms that are robust to uncertainty.  Markovian state-space models are especially effective at such tasks, allowing realtime inference and efficient training from data.  Most prior work focuses on learning state-space models where state densities and dynamics have analytically tractable forms.  We present a model that learns Markov processes with arbitrary state transition kernels and non-Gaussian, multi-modal state densities to predict the temporal evolution of nonlinear Markovian systems.  The model uses radial basis function approximators to represent predictive state transition kernels.  Conjugate gradient forms the basis of a generalized expectation-maximization algorithm for unsupervised learning of the function approximators.  A particle-based state representation admits non-Gaussian state distributions.  Particle smoothing estimates maximum a posteriori state sequences for model inference.  After learning, the model can be used for prediction, filtering, or fixed-lag smoothing.  We demonstrate the model's robustness by predicting nonlinear, non-Gaussian state sequences from artificially generated noisy time-series data. 
Probabilistic Gaze Imitation and Saliency Learning in a Robotic Head| Abstract Imitation is a powerful mechanism for transferring knowledge from an instructor to a naive observer.  We first present Bayesian algorithms, based on Meltzoff and Moore's AIM model for imitation in infants, that implement the core of an imitation learning framework.  Next, we present Bayesian algorithms for learning which objects an instructor considers salient in a task.  Finally, we demonstrate the performance of our algorithms in a gaze following and saliency learning task implemented on an active vision robotic head.  Our results suggest that the ability to follow gaze and learn instructor- and task-specific saliency models could play a crucial role in building systems capable of complex forms of humanrobot interaction.  1 Imitation learning and shared attention Imitation is a powerful mechanism for transferring knowledge from a skilled agent (the instructor) to an unskilled agent (or observer) using direct manipulation of the environment.  Several researchers have investigated imitative behavior in apes [21, 5], in children (including infants only 42 minutes old) [15, 16], and in an increasingly diverse selection of machines [9, 14].  The attraction of imitation for robotics is obvious: imitative robots offer drastically reduced programming costs compared to robots requiring programming by an expert.  Imitative robots also offer testbeds for cognitive researchers to test computational theories, and provide modifiable agents for contingent interaction with humans in psychological experiments.  Successful imitation requires that instructor and observer simultaneously attend to the same object or environmental state.  Such simultaneous attention is often referred to as "shared attention" in the psychological literature.  Previous work, notably by Scassellati on the Cog platform [20], has concentrated on deterministic algorithms for shared attention between humans and robots.  Scassellati's work concentrated on tracking the gaze of a human instructor, and on mimicking the motion of the instructor's head in either a vertical or a horizontal direction.  Separately, Movellan and colleagues have used robotic platforms to study shared attention in infants [8].  Although robotic platforms [7, 20] have demonstrated impressive mimicry results, richly contingent human-robot interaction comparable to infant imitation depends on having a model for saliency, i. e. , a model of what components of environmental state are important in a given task.  Ideally, saliency models would be task- or instructor-specific, representing the observer's learned context-dependent knowledge of how to allocate attentional resources.  In this paper, we describe a robotic system that uses probabilistic algorithms to follow the gaze of a human and to identify salient objects in a scene.  Our algorithms employ Bayesian inference because of its robustness to noise and missing data, tractability under large data sets, and unifying mathematical formalism.  Bayesian imitation learning approaches have been proposed to accelerate reinforcement learning [17]; however, that framework chiefly addresses the problem of learning a forward model of the environment [12] via imitation (see Section 3), and its correspondence with cognitive findings in humans is unclear.  Other frameworks have been proposed for imitation learning in machines [3, 20, 1], but most of these are not designed around a coherent probabilistic formalism.  The robotic system described in this paper tracks a human instructor's gaze to an object, then learns a simple instructor-specific, task-specific saliency model.  Our biologicallyinspired, model-based approach extends previous robotic gaze imitation results in three main ways: i) it provides a Bayesian description of imitation in general, and gaze tracking specifically; ii) it incorporates infant imitation findings into a rigorous algorithmic framework; and iii) the system learns simple, context-dependent probabilistic models for saliency.  Preliminary results show how shared attention could be developed between humans and robots.  Sections 2 and 3 respectively discuss our system's modality-independent representation and our Bayesian algorithms for motor planning.  Section 4 describes how our system computes object saliency.  Section 5 concludes with gaze tracking results from our system and a simple example of learning a saliency model. 
Bayesian models of human action understanding| Abstract We present a Bayesian framework for explaining how people reason about and predict the actions of an intentional agent, based on observing its behavior.  Action-understanding is cast as a problem of inverting a probabilistic generative model, which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their environment.  Working in a simple sprite-world domain, we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change.  The model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform, and also fits quantitative predictions that adult observers make in a new experiment. 
Cortical Specialization for Processing Firstand Second-order Motion| Distinct mechanisms underlying the visual perception of luminance(first-order) and contrast-defined (second-order) motion have been proposed from electrophysiological, human psychophysical and neurological studies; however a cortical specialization for these mechanisms has proven elusive.  Here human brain imaging combined with psychophysical methods was used to assess cortical specializations for processing these two kinds of motion.  A common stimulus construction was employed, controlling for differences in spatial and temporal properties, psychophysical performance and attention.  Distinct cortical regions have been found preferentially processing either first- or second-order motion, both in occipital and parietal lobes, producing the first physiological evidence in humans to support evidence from psychophysical studies, brain lesion sites and computational models.  These results provide evidence for the idea that first-order motion is computed in V1 and second-order motion in later occipital visual areas, and additionally suggest a functional dissociation between these two kinds of motion beyond the occipital lobe. 
Centrifugal bias for second-order but not first-order motion| Limited-lifetime Gabor stimuli were used to assess both first- and second-order motion in peripheral vision.  Both first- and second-order motion mechanisms were present at a 20-deg eccentricity.  Second-order motion, unlike first-order, exhibits a bias for centrifugal motion, suggesting a role for the second-order mechanism in optic flow processing.  2001 Optical Society of America OCIS codes:
The "motion-blind" patient: Perception of random dot "limited-lifetime" motion|

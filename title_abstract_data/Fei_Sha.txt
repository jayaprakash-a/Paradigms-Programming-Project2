Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization| Abstract An auditory "scene", composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources.  Pitch is known to be an important cue for auditory scene analysis.  In this paper, with the goal of building agents that operate in human environments, we describe a real-time system to identify the presence of one or more voices and compute their pitch.  The signal processing in the front end is based on instantaneous frequency estimation, a method for tracking the partials of voiced speech, while the pattern-matching in the back end is based on nonnegative matrix factorization, an unsupervised algorithm for learning the parts of complex objects.  While supporting a framework to analyze complicated auditory scenes, our system maintains real-time operability and state-of-the-art performance in clean speech. 
Learning a kernel matrix for nonlinear dimensionality reduction| Abstract We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold.  Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled.  The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors.  The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding.  The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification.  We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods. 
Shallow Parsing with Conditional Random Fields| Abstract Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.  Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods.  We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.  Improved training methods based on modern optimization algorithms were critical in achieving these results.  We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models. 
Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines| Abstract We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs).  The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane.  The updates optimize the traditionally proposed objective function for SVMs.  They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration.  They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration.  We analyze the asymptotic convergence of the updates and show that the coefficients of non-support vectors decay geometrically to zero at a rate that depends on their margins.  In practice, the updates converge very rapidly to good classifiers. 
Statistical Signal Processing with Nonnegativity Constraints| Abstract Nonnegativity constraints arise frequently in statistical learning and pattern recognition.  Multiplicative updates provide natural solutions to optimizations involving these constraints.  One well known set of multiplicative updates is given by the ExpectationMaximization algorithm for hidden Markov models, as used in automatic speech recognition.  Recently, we have derived similar algorithms for nonnegative deconvolution and nonnegative quadratic programming.  These algorithms have applications to low-level problems in voice processing, such as fundamental frequency estimation, as well as high-level problems, such as the training of large margin classifiers.  In this paper, we describe these algorithms and the ideas that connect them. 
V+v| t#8' prf#^hy#H'qryv t#h q#D #ryyvtr ##6tr #+##'#D #rt. . . h#r#Tr v#
Multiplicative Updates for Large Margin Classifiers| Abstract.  Various problems in nonnegative quadratic programming arise in the training of large margin classifiers.  We derive multiplicative updates for these problems that converge monotonically to the desired solutions for hard and soft margin classifiers.  The updates differ strikingly in form from other multiplicative updates used in machine learning.  In this paper, we provide complete proofs of convergence for these updates and extend previous work to incorporate sum and box constraints in addition to nonnegativity. 
Calibrating the Query Optimizer Cost Model of IRO-DB|
Calibrating the Query Optimizer Cost Model of IRO-DB, an Object-Oriented Federated Database System|
XML-based Components for Federating Multiple Heterogeneous Data Sources|
Learning a kernal matrix for nonlinear dimensionality reduction,|
Self-aggregation of DNA oligomers with XGG trinucleotide repeats: kinetic and atomic force microscopy measurements,|
Miro Web: Integrating Multiple Data Sources through Semistructured Data Types|
Multiband statistical learning for f 0 estimation in speeh,|

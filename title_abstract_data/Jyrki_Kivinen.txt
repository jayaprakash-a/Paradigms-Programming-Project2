Relative loss bounds for single neurons| Abstract We analyze and compare the well-known Gradient Descent algorithm and the more recent Exponentiated Gradient algorithm for training a single neuron with an arbitrary transfer function.  Both algorithms are easily generalized to larger neural networks, and the generalization of Gradient Descent is the standard back-propagation algorithm.  In this paper we prove worst-case loss bounds for both algorithms in the single neuron case.  Since local minima make it difficult to prove worst-case bounds for gradient-based algorithms, we must use a loss function that prevents the formation of spurious local minima.  We define such a matching loss function for any strictly increasing differentiable transfer function and prove worst-case loss bounds for any such transfer function and its corresponding matching loss.  For example, the matching loss for the identity function is the square loss and the matching loss for the logistic transfer function is the entropic loss.  The different forms of the two algorithms' bounds indicates that Exponentiated Gradient outperforms Gradient Descent when the inputs contain a large number of irrelevant components.  Simulations on synthetic data confirm these analytical results. 
THE P-NORM GENERALIZATION OF THE LMS ALGORITHM FOR ADAPTIVE FILTERING| Abstract: Recently much work has been done analyzing online machine learning algorithms in a worst case setting, where no probabilistic assumptions are made about the data.  This is analogous to the H setting used in adaptive linear filtering.  Bregman divergences have become a standard tool for analyzing online Machine Learning algorithms.  Using these divergences, we motivate a generalization of the the Least Mean Squared (LMS) algorithm.  The loss bounds for these so-called p-norm algorithms involve other norms than the standard 2-norm.  The bounds can be significantly better if a large proportion of the input variables are irrelevant, i. e. , if the weight vector we are trying to learn is sparse.  We also prove result for nonstationary targets.  We only know how to apply kernel methods to the standard LMS algorithm (i. e. , p = 2).  However even in the general p-norm case we can handle generalized linear models where the output of the system is a linear function combined with a nonlinear transfer function (e. g. , the logistic sigmoid). 
Online Bayes Point Machines| Abstract.  We present a new and simple algorithm for learning large margin classifiers that works in a truly online manner.  The algorithm generates a linear classifier by averaging the weights associated with several perceptron-like algorithms run in parallel in order to approximate the Bayes point.  A random subsample of the incoming data stream is used to ensure diversity in the perceptron solutions.  We experimentally study the algorithm's performance on online and batch learning settings.  The online experiments showed that our algorithm produces a low prediction error on the training sequence and tracks the presence of concept drift.  On the batch problems its performance is comparable to the maximum margin algorithm which explicitly maximises the margin. 
The Perceptron Algorithm Versus Winnow: Linear Versus Logarithmic Mistake Bounds when Few Input Variables are Relevant (Technical Note)| Abstract We give an adversary strategy that forces the Perceptron algorithm to make \Omega(kN ) mistakes in learning monotone disjunctions over N variables with at most k literals.  In contrast, Littlestone's algorithm Winnow makes at most O(k log N) mistakes for the same problem.  Both algorithms use thresholded linear functions as their hypotheses.  However, Winnow does multiplicative updates to its weight vector instead of the additive updates of the Perceptron algorithm.  In general, we call an algorithm additive if its weight vector is always a sum of a fixed initial weight vector and some linear combination of already seen instances.  Thus, the Perceptron algorithm is an example of an additive algorithm.  We show that an adversary can force any additive algorithm to make (N +k\Gamma 1)=2 mistakes in learning a monotone disjunction of at most k literals.  Simple experiments show that for k N , Winnow clearly outperforms the Perceptron algorithm also on nonadversarial random data. 
The Power of Sampling in Knowledge Discovery| FIN-00014 University of Helsinki, Finland E-mail: fjkivinen, mannilag@cs. Helsinki. FI Report C-1993-66, December 1993, 18 pages Abstract We consider the problem of approximately verifying the truth of sentences of tuple relational calculus in a given relation M by considering only a random sample of M .  We define two different measures for the error of a universal sentence in a relation.  For a set of n universal sentences each with at most k universal quantifiers, we give upper and lower bounds for the sample sizes required for having a high probability that all the sentences with error at least '' can be detected as false by considering the sample.  The sample sizes are O((ln n)=") or O((jM j 1\Gamma 1=k ln n)="), depending on the error measure used.  We also consider universal-existential sentences. 
Relative Loss Bounds for Multidimensional Regression Problems| Abstract. 
The Curse of Dimensionality and the Perceptron Algorithm| Abstract We give an adversary strategy that forces the Perceptron algorithm to make (N\Gamma k + 1)=2 mistakes when learning k-literal disjunctions over N variables.  Experimentally we see that even for simple random data, the number of mistakes made by the Perceptron algorithm grows almost linearly with N , even if the number k of relevant variable remains a small constant.  Thus, the Perceptron algorithm suffers from the curse of dimensionality even when the target is extremely simple and almost all of the dimensions are irrelevant.  In contrast, Littlestone's algorithm Winnow makes at most O(k log N ) mistakes for the same problem.  Both algorithms use linear threshold functions as their hypotheses.  However, Winnow does multiplicative updates to its weight vector instead of the additive updates of the Perceptron algorithm. 
Tight worst-case loss bounds for predicting with expert advice| abstract We consider on-line algorithms for predicting binary or continuous-valued outcomes, when the algorithm has available the predictions made by N experts.  For a sequence of trials, we compute total losses for both the algorithm and the experts under a loss function.  At the end of the trial sequence, we compare the total loss of the algorithm to the total loss of the best expert, i. e. , the expert with the least loss on the particular trial sequence.  Vovk has introduced a simple algorithm for this prediction problem and proved that for a large class of loss functions, with binary outcomes the total loss of the algorithm exceeds the total loss of the best expert at most by the amount c ln N , where c is a constant determined by the loss function.  This upper bound does not depend on any assumptions on how the experts' predictions or the outcomes are generated, and the trial sequence can be arbitrarily long.  We give a straightforward alternative method for finding the correct value c and show by a lower bound that for this value of c, the upper bound is asymptotically tight.  The lower bound is based on a probabilistic adversary argument.  The class of loss functions for which the c ln N upper bound holds includes the square loss, the logarithmic loss, and the Hellinger loss.  We also consider another class of loss functions, including the absolute loss, for which we have an \Omega\Gamma p ` log N \Delta lower bound, where ` is the number of trials.  We show that for the square and logarithmic loss functions, Vovk's algorithm achieves the same worst-
Sequential Prediction of Individual Sequences Under General Loss Functions| Abstract We consider adaptive sequential prediction of arbitrary binary sequences when the performance is evaluated using a general loss function.  The goal is to predict on each individual sequence nearly as well as the best prediction strategy in a given comparison class of (possibly adaptive) prediction strategies, called experts.  By using a general loss function, we generalize previous work on universal prediction, forecasting, and data compression.  However, here we restrict ourselves to the case when the comparison class is finite.  For a given sequence, we define the regret as the total loss on the entire sequence suffered by the adaptive sequential predictor, minus the total loss suffered by the predictor in the comparison class that performs best on that particular sequence.  We show that for a large class of loss functions, the minimax regret is either \Theta(log N) or \Omega( p ` log N ), depending on the loss function, where N is the number of predictors in the comparison class and ` is the length of the sequence to be predicted.  The former case \Lambda Preliminary results have appeared in Computational Learning Theory: EuroCOLT '93,
CHANNEL EQUALIZATION AND THE BAYES POINT MACHINE| ABSTRACT Equalizers trained with a large margin have an ability to better handle noise in unseen data and drift in the target solution.  We present a method of approximating the Bayes optimal strategy which provides a large margin equalizer, the Bayes point equalizer.  The method we use to estimate the Bayes point is to average N equalizers that are run on independently chosen subsets of the data.  To better estimate the Bayes point we investigated two methods to create diversity amongst the N equalizers.  We show experimentally that the Bayes point equalizer for appropriately large step sizes offers improvement on LMS and LMA in the presence of channel noise and training sequence errors.  This allows for shorter training sequences albeit with higher computational requirements. 
Online Learning with Kernels| Abstract We consider online learning in a Reproducing Kernel Hilbert Space.  Our method is computationally efficient and leads to simple algorithms.  In particular we derive update equations for classification, regression, and novelty detection.  The inclusion of the #-trick allows us to give a robust parameterization.  Moreover, unlike in batch learning where the #-trick only applies to the "-insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber's robust loss. 
Additive versus exponentiated gradient updates for linear prediction|
Exponentiated Gradient Versus Gradient Descent for Linear Predictors|
Boosting as Entropy Projection|
Approximate Inference of Functional Dependencies from Relations|
Using experts for predicting continuous outcomes|
Worst-case Loss Bounds for Single Neurons|
Large Margin Classification for Moving Targets|
Averaging Expert Predictions|
Learning Hierarchical Rule Sets|
Approximate Dependency Inference from Relations|
Additive versus exponentiated gradient updates for learning linear functions|
Worstcase loss bounds for sigmoided neurons|

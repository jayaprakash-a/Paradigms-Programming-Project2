An Approximate Inference Approach for the PCA Reconstruction Error| Abstract The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method.  Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efficiently with two variational parameters.  A perturbative correction to the result is computed and an alternative simplified derivation is also presented. 
Sparse Online Gaussian Processes| Accepted in Neural Computation Minor corrections included a a The authors acknowledge reader feedbacks Abstract We develop an approach for sparse representations of Gaussian Process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets.  The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the GP model.  By using an appealing parametrisation and projection techniques that use the RKHS norm, recursions for the eective parameters and a sparse Gaussian approximation of the posterior process are obtained.  This allows both for a propagation of predictions as well as of Bayesian error measures.  The significance and robustness of our approach is demonstrated on a variety of experiments. 
A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages| Abstract We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error.  We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling. 
General Bounds on the Mutual Information Between a Parameter and n Conditionally Independent Observations| Abstract Each parameter ` in an abstract parameter space \Theta is associated with a different probability distribution on a set Y .  A parameter ` is chosen at random from \Theta according to some a priori distribution on \Theta, and n conditionally independent random variables Y n = Y 1 ; : : : Yn are observed with common distribution determined by `.  We obtain bounds on the mutual information between the random variable \Theta, giving the choice of parameter, and the random variable Y n , giving the sequence of observations.  We also bound the supremum of the mutual information, over choices of the prior distribution on \Theta.  These quantities have applications in density estimation, computational learning theory, universal coding, hypothesis testing, and portfolio selection theory.  The bounds are given in terms of the metric and information dimensions of the parameter space \Theta with respect to the Hellinger distance. 
A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages| Abstract We compute approximate analytical bootstrap averages for support vector classification using a combination of the replica method of statistical physics and the TAP approach for approximate inference.  We test our method on a few datasets and compare it with exact averages obtained by extensive Monte-Carlo sampling. 
Calculation of the Learning Curve of Bayes Optimal Classification Algorithm for Learning a Perceptron With Noise| Abstract The learning curve of Bayes optimal classification algorithm when learning a perceptron from noisy random training examples is calculated exactly in the limit of large training sample size and large instance space dimension using methods of statistical mechanics.  It is shown that under certain assumptions, in this "thermodynamic" limit, the probability of misclassification of Bayes optimal algorithm is less than that of a canonical stochastic learning algorithm, by a factor approaching ing p 2 as the ratio of number of training examples to instance space dimension grows.  Exact asymptotic learning curves for both algorithms are derived for particular distributions.  In addition, it is shown that the learning performance of Bayes optimal algorithm can be approximated by certain learning algorithms that use a neural net with a layer of hidden units to learn a perceptron. 
Asymptotic Universality for Learning Curves of Support Vector Machines| Abstract Using methods of Statistical Physics, we investigate the rle of model complexity in learning with support vector machines (SVMs).  We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error.  Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 
A Variational Approach to Learning Curves| Abstract We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically.  We apply the method to Gaussian process regression.  As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance. 
Continuous Drifting Games| Abstract We combine the results of [13] and [8] and derive a continuous variant of a large class of drifting games.  Our analysis furthers the understanding of the relationship between boosting, drifting games and Brownian motion and yields a differential equation that describes the core of the problem. 
Query by Committee| Abstract We propose an algorithm called query by committee, in which a committee of students is trained on the same data set.  The next query is chosen according to the principle of maximal disagreement.  The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron.  As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain.  This leads to generalization error that decreases exponentially with the number of examples.  This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law.  We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms. 
Selection of Examples for a Linear Classifier| Abstract.  We investigate the problem of selecting an informative subsample out of a neural network's training data.  Using the replica method of statistical mechanics, we calculate the performance of a heuristic selection algorithm for a linear neural network which avoids overfitting. 
Expectation Consistent Free Energies for Approximate Inference| Abstract We propose a novel a framework for deriving approximations for intractable probabilistic models.  This framework is based on a free energy (negative log marginal likelihood) and can be seen as a generalization of adaptive TAP [1, 2, 3] and expectation propagation (EP) [4, 5].  The free energy is constructed from two approximating distributions which encode different aspects of the intractable model such a single node constraints and couplings and are by construction consistent on a chosen set of moments.  We test the framework on a difficult benchmark problem with binary variables on fully connected graphs and 2D grid graphs.  We find good performance using sets of moments which either specify factorized nodes or a spanning tree on the nodes (structured approximation).  Surprisingly, the Bethe approximation gives very inferior results even on grids. 
TAP Gibbs Free Energy, Belief Propagation and Sparsity| Abstract The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived.  We show how a specific sequential minimization of the free energy leads to a generalization of Minka's expectation propagation.  Lastly, we derive a sparse representation version of the sequential algorithm.  The usefulness of the approach is demonstrated on classification and density estimation with Gaussian processes and on an independent component analysis problem. 
Bounds for Predictive Errors in the Statistical Mechanics of Supervised Learning| Abstract Within a Bayesian framework, by generalizing inequalities known from statistical mechanics, we calculate general upper and lower bounds for a cumulative entropic error, which measures the success in the supervised learning of an unknown rule from examples.  Both bounds match asymptotically, when the number m of observed data grows large.  We find that the information gain from observing a new example decreases universally like d=m.  Here d is a dimension that is defined from the scaling of small volumes with respect to a distance in the space of rules. 
Variational Linear Response| Abstract A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented.  Three applications are given and it is discussed how to use linear response as a general principle for improving mean field approximations. 
An Approximate Analytical Approach to Resampling Averages| Abstract Using a novel reformulation, we develop a framework to compute approximate resampling data averages analytically.  The method avoids multiple retraining of statistical models on the samples.  Our approach uses a combination of the replica "trick" of Statistical Physics and the TAP approach for approximate Bayesian inference.  We demonstrate our approach on regression with Gaussian processes.  A comparison with averages obtained by Monte-Carlo sampling shows that our method achieves good accuracy. 
Bayesian analysis of the scatterometer wind retrieval inverse problems: some new approaches| Summary. The retrieval of wind vectors from satellite scatterometer observations is a non-linear inverse problem.  A common approach to solving inverse problems is to adopt a Bayesian framework and to infer the posterior distribution of the parameters of interest given the observations by using a likelihood model relating the observations to the parameters, and a prior distribution over the parameters.  We show how Gaussian process priors can be used efficiently with a variety of likelihood models, using local forward (observation) models and direct inverse models for the scatterometer.  We present an enhanced Markov chain Monte Carlo method to sample from the resulting multimodal posterior distribution.  We go on to show how the computational complexity of the inference can be controlled by using a sparse, sequential Bayes algorithm for estimation with Gaussian processes. This helps to overcome the most serious barrier to the use of probabilistic, Gaussian process methods in remote sensing inverse problems, which is the prohibitively large size of the data sets. We contrast the sampling results with the approximations that are found by using the sparse, sequential Bayes algorithm. 
Mutual Information, Metric Entropy, and Risk in Estimation of Probability Distributions| Abstract Assume fP ` : ` 2 \Thetag is a set of probability distributions with a common dominating measure on a complete separable metric space Y .  A state ` \Lambda 2 \Theta is chosen by Nature.  A statistician gets n independent observations Y 1 ; : : : ; Y n from Y distributed according to P ` \Lambda .  For each time t between 1 and n, based on the observations Y 1 ; : : : ; Y t\Gamma 1 , the statistician produces an estimated distribution ^ P t for P ` \Lambda , and suffers a loss L(P ` \Lambda ; ^ P t ).  The cumulative risk for the statistician is the average total loss up to time n.  Of special interest in information theory, data compression, mathematical finance, computational learning theory and statistical mechanics is the special case when the loss L(P ` \Lambda ; ^ P t ) is the relative entropy between the true distribution P ` \Lambda and the estimated distribution ^ P t .  Here the cumulative Bayes risk from time 1 to n is the mutual information between the random parameter \Theta \Lambda and the observations Y 1 ; : : : ; Y n .  New bounds on this mutual information are given in terms of the Laplace transform of the Hellinger distance between pairs of distributions indexed by parameters in \Theta. 
Finite-dimensional approximation of Gaussian processes| Abstract Gaussian process (GP) prediction suffers from O(n 3 ) scaling with the data set size n.  By using a finite-dimensional basis to approximate the GP predictor, the computational complexity can be reduced.  We derive optimal finite-dimensional predictors under a number of assumptions, and show the superiority of these predictors over the Projected Bayes Regression method (which is asymptotically optimal).  We also show how to calculate the minimal model size for a given n.  The calculations are backed up by numerical experiments. 
Accepted for publication - to appear in Proceeding of NIPS vol| 13 Sparse Representation for Gaussian Process Models.  Abstract We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets.  The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model.  Experimental results on toy examples and large real-world datasets indicate the efficiency of the approach. 
Gaussian Processes for Classification: Mean-Field Algorithms|
Estimating Average-Case Learning Curves Using Bayesian, Statistical Physics and VC Dimension Methods|
Efficient Approaches to Gaussian Process Classification|
United Nations Environment Programme, Residue utilization management of agricultural and agro-industrial residues, paper presented at Industry Section Seminars,|
Finite-Dimensional Approximation of Gaussian Processes|
Tractable approximations for probabilistic models: The adaptive Thouless-Anderson-Palmer mean field approach,|
Sparse Representation for Gaussian Process Models|
A Mean Field Algorithm for Bayes Learning in Large Feed-forward Neural Networks|
Sparse On-Line Gaussian Processes|
Mean Field Methods for Classification with Gaussian Processes|
TAP Gibbs Free Energy, Belief Propagation and Sparsity|
Sparse online Gaussian processes|
General Bounds on Bayes Errors for Regression with Gaussian Processes|
Mean Field Approach to Bayes Learning in Feedforward Neural Networks|

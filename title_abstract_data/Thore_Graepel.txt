Learning on Graphs in the Game of Go| Abstract We consider the game of Go from the point of view of machine learning and as a well-defined domain for learning on graph representations.  We discuss the representation of both board positions and candidate moves and introduce the common fate graph (CFG) as an adequate representation of board positions for learning.  Single candidate moves are represented as feature vectors with features given by subgraphs relative to the given move in the CFG.  Using this representation we train a support vector machine (SVM) and a kernel perceptron to discriminate good moves from bad moves on a collection of life-and-death problems and on 9 \Theta 9 game records.  We thus obtain kernel machines that solve Go problems and play 9 \Theta 9 Go. 
A Stochastic Self-Organizing Map for Proximity Data| Abstract We derive an efficient algorithm for topographic mapping of proximity data (TMP),
Generalization Bounds for the Area Under the ROC Curve| Abstract We study generalization properties of the area under the ROC curve (AUC), a quantity that has been advocated as an evaluation criterion for the bipartite ranking problem.  The AUC is a different term than the error rate used for evaluation in classification problems; consequently, existing generalization bounds for the classification error rate cannot be used to draw conclusions about the AUC.  In this paper, we define the expected accuracy of a ranking function (analogous to the expected error rate of a classification function), and derive distribution-free probabilistic bounds on the deviation of the empirical AUC of a ranking function (observed on a finite data sequence) from its expected accuracy.  We derive both a large deviation bound, which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on a test sequence, and a uniform convergence bound, which serves to bound the expected accuracy of a learned ranking function in terms of its empirical AUC on a training sequence.  Our uniform convergence bound is expressed in terms of a new set of combinatorial parameters that we term the bipartite rank-shatter coefficients; these play the same role in our result as do the standard VC-dimension related shatter coefficients (also known as the growth function) in uniform convergence results for the classification error rate.  A comparison of our result with a recent uniform convergence result derived by Freund et al.  (2003) for a quantity closely related to the AUC shows that the bound provided by our result can be considerably tighter. 
Correspondence Analysis for Visualizing Interplay of Pitch Class, Key, and Composer| Abstract We apply correspondence analysis for visualization of interdependence of pitch class & key and key & composer.  A co-occurrence matrix of key & pitch class frequencies is extracted from score (Bach's WTC).  Keys are represented as high-dimensional pitch class vectors.  Correspondence analysis then projects keys on a planar "keyscape".  Vice versa, on "pitchscapes" pitch classes can also be embedded in the key space.  In both scenarios a homogenous circle of fifths emerges in the scapes.  We employ biplots to embed keys and pitch classes in the keyscape to visualize their interdependence.  After a change of co-ordinates the four-dimensional biplots can be interpreted as a configuration on a torus, closely resembling results from music theory and experiments in listener models.  In conjunction with spectral analysis, correspondence analysis constitutes a cognitive auditory model.  Correspondence analysis of the co-occurrence table of intensities of keys and pitch classes lets the circle of fifths evolve in the pitchscape.  This model works on digitized recorded music, does not require averaging or normalization of the data, and does not implicitly use circularity inherent in the model.  Statistics on key preference in composers yields a composer & key cooccurrence matrix.  Then "stylescapes" visualize relations between musical styles of particular composers and schools.  The Biplotting technique links stylistic characteristics to favored keys.  Interdependence of composers and schools is meaningfully visualized according to their key preferences. 
Generalisation Error Bounds for Sparse Linear Classifiers| Abstract We provide small sample size bounds on the generalisation error of linear classifiers that are sparse in their dual representation given by the expansion coefficients of the weight vector in terms of the training data.  These results theoretically justify algorithms like the Support Vector Machine, the Relevance Vector Machine and K-nearest-neighbour.  The bounds are a-posteriori bounds to be evaluated after learning when the attained level of sparsity is known.  In a PAC-Bayesian style prior knowledge about the expected sparsity is incorporated into the bounds.  The proofs avoid the use of double sample arguments by taking into account the sparsity that leaves unused training points for the evaluation of classifiers.  We furthermore give a PAC-Bayesian bound on the average generalisation error over subsets of parameter space that may pave the way combining sparsity in the expansion coefficients and margin in a single bound.  Finally, reinterpreting a mistake bound for the classical perceptron algorithm due to Novikoff we demonstrate that our new results put classifiers found by this algorithm on a firm theoretical basis. 
Deterministic Annealing for Topographic Vector Quantization and Self-Organizing Maps| Abstract We have developed a robust optimization scheme for self-organizing maps in the framework of noisy vector quantization.  Based on a cost function that takes distortions from channel noise into account we derive a fuzzy algorithm of EM-type for topographic vector quantization (STVQ) which employs deterministic annealing.  This annealing process leads to phase transitions in the cluster representation for which we are able to calculate critical modes and temperatures as a function of the neighbourhood function and the covariance matrix of the data.  Similar results are obtained for the automatic selection of feature dimensions.  Deterministic annealing also offers an alternative to the heuristic stepwise shrinking of the neighbourhood width in the SOM and makes it possible to use the neighbourhood solely to encode desired neighbourhood relations between the clusters.  A soft version of the SOM (SSOM) is derived as a computationally efficient approximation to the E-step of STVQ.  Both methods are numerically tested on a two-dimensional map of the plane and we conclude that the temperature annealing can be precisely controlled and could for many applications be the method of choice. 
Bayesian Learning in Reproducing Kernel Hilbert Spaces| Abstract Support Vector Machines find the hypothesis that corresponds to the centre of the largest hypersphere that can be placed inside version space, i. e.  the space of all consistent hypotheses given a training set.  The boundaries of version space touched by this hypersphere define the support vectors.  An even more promising approach is to construct the hypothesis using the whole of version space.  This is achieved by the Bayes point: the midpoint of the region of intersection of all hyperplanes bisecting version space into two volumes of equal magnitude.  It is known that the centre of mass of version space approximates the Bayes point [31].  The centre of mass is estimated by averaging over the trajectory of a billiard in version space.  We derive bounds on the generalisation error of Bayesian classifiers in terms of the volume ratio of version space and parameter space.  This ratio serves as an effective VC dimension and greatly influences generalisation.  We present experimental results indicating that Bayes Point Machines consistently outperform Support Vector Machines.  Moreover, we show theoretically and experimentally how Bayes Point Machines can easily be extended to admit training errors. 
Classification on Pairwise Proximity Data| Abstract We investigate the problem of learning a classification task on data represented in terms of their pairwise proximities.  This representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using Euclidean feature vectors, from which pairwise proximities can always be calculated.  Our first approach is based on a combined linear embedding and classification procedure resulting in an extension of the Optimal Hyperplane algorithm to pseudo-Euclidean data.  As an alternative we present another approach based on a linear threshold model in the proximity values themselves, which is optimized using Structural Risk Minimization.  We show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics w. r. t.  their generalization.  Finally, the algorithms are successfully applied to protein structure data and to data from the cat's cerebral cortex.  They show better performance than K-nearest-neighbor classification. 
An Annealed Self-Organizing Map for Source Channel Coding| Abstract We derive and analyse robust optimization schemes for noisy vector quantization on the basis of deterministic annealing.  Starting from a cost function for central clustering that incorporates distortions from channel noise we develop a soft topographic vector quantization algorithm (STVQ) which is based on the maximum entropy principle and which performs a maximum-likelihood estimate in an expectationmaximization (EM) fashion.  Annealing in the temperature parameter fi leads to phase transitions in the existing code vector representation during the cooling process for which we calculate critical temperatures and modes as a function of eigenvectors and eigenvalues of the covariance matrix of the data and the transition matrix of the channel noise.  A whole family of vector quantization algorithms is derived from STVQ, among them a deterministic annealing scheme for Kohonen's self-organizing map (SOM).  This algorithm, which we call SSOM, is then applied to vector quantization of image data to be sent via a noisy binary symmetric channel.  The algorithm's performance is compared to those of LBG and STVQ.  While it is naturally superior to LBG, which does not take into account channel noise, its results compare very well to those of STVQ, which is computationally much more demanding. 
Classification on Proximity Data with LP--Machines| Abstract We provide a new linear program to deal with classification of data in the case of data given in terms of pairwise proximities.  This allows to avoid the problems inherent in using feature spaces with indefinite metric in Support Vector Machines, since the notion of a margin is purely needed in input space where the classification actually occurs.  Moreover in our approach we can enforce sparsity in the proximity representation by sacrificing training error.  This turns out to be favorable for proximity data.  Similar to --SV methods, the only parameter needed in the algorithm is the (asymptotical) number of data points being classified with a margin.  Finally, the algorithm is successfully compared with --SV learning in proximity space and K--nearest-neighbors on real world data from Neuroscience and molecular biology. 
A Large Deviation Bound for the Area Under the ROC Curve| Abstract The area under the ROC curve (AUC) has been advocated as an evaluation criterion for the bipartite ranking problem.  We study large deviation properties of the AUC; in particular, we derive a distribution-free large deviation bound for the AUC which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on an independent test sequence.  A comparison of our result with a corresponding large deviation result for the classification error rate suggests that the test sample size required to obtain an #-accurate estimate of the expected accuracy of a ranking function with #-confidence is larger than that required to obtain an #-accurate estimate of the expected error rate of a classification function with the same confidence.  A simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from finite function classes. 
From Margin to Sparsity| Abstract We present an improvement of Noviko#'s perceptron convergence theorem.  Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC#style generalisation error bound for the classifier learned by the perceptron learning algorithm.  The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel.  Ironically, the bound yields better guarantees than are currently available for the support vector solution itself. 
Semi-Definite Programming by Perceptron Learning| Abstract We present a modified version of the perceptron learning algorithm (PLA) which solves semidefinite programs (SDPs) in polynomial time.  The algorithm is based on the following three observations: (i) Semidefinite programs are linear programs with infinitely many (linear) constraints; (ii) every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints; (iii) in general, the perceptron learning algorithm solves a constraint satisfaction problem with linear constraints in finitely many updates.  Combining the PLA with a probabilistic rescaling algorithm (which, on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time.  We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior point methods. 
Combining Conjugate Direction Methods with Stochastic Approximation of Gradients| Abstract The method of conjugate directions provides a very effective way to optimize large, deterministic systems by gradient descent.  In its standard form, however, it is not amenable to stochastic approximation of the gradient.  Here we explore ideas from conjugate gradient in the stochastic (online) setting, using fast Hessian-gradient products to set up low-dimensional Krylov subspaces within individual mini-batches.  In our benchmark experiments the resulting online learning algorithms converge orders of magnitude faster than ordinary stochastic gradient descent. 
Bayes Point Machines: Estimating the Bayes Point in Kernel Space| Abstract From a Bayesian perspective Support Vector Machines choose the hypothesis corresponding to the largest possible hypersphere that can be inscribed in version space, i. e.  in the space of all consistent hypotheses given a training set.  Those boundaries of version space which are tangent to the hypersphere define the support vectors.  An alternative and potentially better approach is to construct the hypothesis using the whole of version space.  This is achieved by using a Bayes Point Machine which finds the midpoint of the region of intersection of all hyperplanes bisecting version space into two halves of equal volume (the Bayes point).  It is known that the center of mass of version space approximates the Bayes point [ Watkin, 1993 ] .  We suggest estimating the center of mass by averaging over the trajectory of a billiard ball bouncing in version space.  Experimental results are presented indicating that Bayes Point Machines consistently outperform Support Vector Machines. 
Invariant Pattern Recognition by Semi-Definite Programming Machines| Abstract Knowledge about local invariances with respect to given pattern transformations can greatly improve the accuracy of classification.  Previous approaches are either based on regularisation or on the generation of virtual (transformed) examples.  We develop a new framework for learning linear classifiers under known transformations based on semidefinite programming.  We present a new learning algorithm--the Semidefinite Programming Machine (SDPM)---which is able to find a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points.  The solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real-valued output as virtual support vectors.  Extensions to segments of trajectories, to more than one transformation parameter, and to learning with kernels are discussed.  In experiments we use a Taylor expansion to locally approximate rotational invariance in pixel images from USPS and find improvements over known methods. 
Sparsity vs| Large Margins for Linear Classifiers.  Abstract We provide small sample size bounds on the generalisation error of linear classifiers that take advantage of large observed margins on the training set and sparsity in the data dependent expansion coefficients.  It is already known from results in the luckiness framework that both criteria independently have a large impact on the generalisation error.  Our new results show that they can be combined which theoretically justifies learning algorithms like the Support Vector Machine [4] or the Relevance Vector Machine [12].  In contrast to previous studies we avoid using the classical technique of symmetrisation by a ghost sample but directly using the sparsity for the estimation of the generalisation error.  We demonstrate that our result leads to practical useful results even in case of small sample size if the training set witnesses our prior belief in sparsity and large margins. 
The Kernel Gibbs Sampler| Abstract We present an algorithm that samples the hypothesis space of kernel classifiers.  Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS).  The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen.  The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise.  For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 
Bayes Point Machines| the improvement over support vector machines is shown to be reduced.  Finally, we demonstrate that the realvalued output of single Bayes points on novel test points is a valid confidence measure and leads to a steady decrease in generalisation error when used as a rejection criterion. 
Gaussian Process Regression: Active Data Selection and Test Point Rejection| Abstract We consider active data selection and test point rejection strategies for Gaussian process regression based on the variance of the posterior over target values.  Gaussian process regression is viewed as transductive regression that provides target distributions for given points rather than selecting an explicit regression function.  Since not only the posterior mean but also the posterior variance are easily calculated we use this additional information to two ends: Active data selection is performed by either querying at points of high estimated posterior variance or at points that minimize the estimated posterior variance averaged over the input distribution of interest or --- in a transductive manner --- averaged over the test set.  Test point rejection is performed using the estimated posterior variance as a confidence measure.  We find for both a two-dimensional toy problem and for a real-world benchmark problem that the variance is a reasonable criterion for both active data selection and test point rejection. 
Large Scale Bayes Point Machines| Abstract The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning.  Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) # also known as the Bayes point # exhibits excellent generalisation abilities.  However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires O m 2 # of memory and O N # m 2 # computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution.  In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback.  The method is algorithmically simple and is easily extended to the multi-class case.  We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine.  In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior.  Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e. g.  0:1% generalisation error for a given rejection rate of 10%. 
Robust Bayes Point Machines| Abstract.  Support Vector Machines choose the hypothesis corresponding to the centre of the largest hypersphere that can be inscribed in version space.  If version space is elongated or irregularly shaped a potentially superior approach is take into account the whole of version space.  We propose to construct the Bayes point which is approximated by the centre of mass.  Our implementation of a Bayes Point Machine (BPM) uses an ergodic billiard to estimate this point in the kernel space.  We show that BPMs outperform hard margin Support Vector Machines (SVMs) on real world datasets.  We introduce a technique that allows the BPM to construct hypotheses with non{zero training error similar to soft margin SVMs with quadratic penelisation of the margin slacks.  An experimental study reveals that with decreasing penelisation of training error the improvement of BPMs over SVMs decays, a finding that is explained by geometrical considerations. 
Regression Models for Ordinal Data: A Machine Learning Approach| Abstract In contrast to the standard machine learning tasks of classification and metric regression we investigate the problem of predicting variables of ordinal scale, a setting referred to as ordinal regression.  The task of ordinal regression arises frequently in the social sciences and in information retrieval where human preferences play a major role.  Also many multi--class problems are really problems of ordinal regression due to an ordering of the classes.  Although the problem is rather novel to the Machine Learning Community it has been widely considered in Statistics before.  All the statistical methods rely on a probability model of a latent (unobserved) variable and on the condition of stochastic ordering.  In this paper we develop a distribution independent formulation of the problem and give uniform bounds for our risk functional.  The main difference to classification is the restriction that the mapping of objects to ranks must be transitive and asymmetric.  Combining our theoretical framework with results from measurement theory we present an approach that is based on a mapping from objects to scalar utility values and thus guarantees transitivity and asymmetry.  Applying the principle of Structural Risk Minimization as employed in Support Vector Machines we derive a new learning algorithm based on large margin rank boundaries for the task of ordinal regression.  Our method is easily extended to nonlinear utility functions.  We give experimental results for an Information Retrieval task of learning the order of documents with respect to an initial query.  Moreover, we show that our algorithm outperforms more naive approaches to ordinal regression such as Support Vector Classification and Support Vector Regression in the case of more than two ranks 1 .  1 This paper is a preliminary version of (Herbrich et al.  1999)
Neural Networks in Economics: Background, Applications and New Developments| Abstract Neural Networks were developed in the sixties as devices for classification and regression.  The approach was originally inspired from Neuroscience.  Its attractiveness lies in the ability to "learn", i. e.  to generalize to as yet unseen observations.  One aim of this paper is to give an introduction to the technique of Neural Networks and an overview of the most popular architectures.  We start from statistical learning theory to introduce the basics of learning.  Then, we give an overview of the general principles of neural networks and of their use in the field of Economics.  A second purpose is to introduce a recently developed Neural Network Learning technique, so called Support Vector Network Learning, which is an application of ideas from statistical learning theory.  This approach has shown very promising results on problems with a limited amount of training examples.  Moreover, utilizing a technique that is known as the kernel trick, Support Vector Networks can easily be adapted to nonlinear models.  Finally, we present an economic application of this approach from the field of preference learning. 
The Structure of Version Space| Abstract We investigate the generalisation performance of consistent classifiers, i. e.  classifiers that are contained in the so-called version space, both from a theoretical and experimental angle.  In contrast to classical VC analysis --- where no single classifier within version space is singled out on grounds of a generalisation error bound --- the data dependent structural risk minimisation framework suggests that there is one particular classifiers that is to be preferred because it minimises the generalisation error bound.  This is usually taken to provide a theoretical justification for learning algorithms such as the well known support vector machine.  A reinterpretation of a recent PAC-Bayesian result, however, reveals that given a suitably chosen hypothesis space there is a huge number of classifiers with small generalisation error albeit we cannot identify them for a specific learning task.  This result is complemented with an empirical study for kernel classifiers on the task of handwritten digit recognition which demonstrates that even classifiers with a small margin exhibit excellent generalisation. 
Large margin rank boundaries for ordinal regression|
Self-organizing maps: Generalizations and new optimization techniques|
Phase transitions in stochastic self-organizing maps|
A PAC-Bayesian margin bound for linear classifiers|
Learning a preference relation for information retrieval|
Support vector learning for ordinal regression|
Bayesian Transduction|
From Margin To Sparsity| Advances in Neural Information Processing Systems 13,. 
Stable Adaptive Momentum for Rapid Online Learning in Nonlinear Systems|
Conjugate Directions for Stochastic Gradient Descent|
NIPS 10|
in:|
The geometry of kernel canonical correlation analysis|
Using unlabeled data for supervised learning,|
Modelling uncertainty in the game of Go|
Kernel Matrix Completion by Semidefinite Programming|
Supervised learning of preference relations|
Learning to fight|
Generalization error bounds for sparse linear classifiers|
Kernal method for document filtering|
Kernel Methods for Document Filtering|
A self-organizing map for proximity data|
Fuzzy topographic kernel clustering,|

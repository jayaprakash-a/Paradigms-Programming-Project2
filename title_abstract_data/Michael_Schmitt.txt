RBF Neural Networks and Descartes' Rule of Signs| Abstract.  We establish versions of Descartes' rule of signs for radial basis function (RBF) neural networks.  These RBF rules of signs provide tight bounds for the number of zeros of univariate networks with certain parameter restrictions.  Moreover, they can be used to derive tight bounds for the Vapnik-Chervonenkis (VC) dimension and pseudo-dimension of these networks.  In particular, we show that these dimensions are no more than linear.  This result contrasts with previous work showing that RBF neural networks with two and more input nodes have superlinear VC dimension.  The rules give rise also to lower bounds for network sizes, thus demonstrating the relevance of network parameters for the complexity of computing with RBF neural networks. 
Some dichotomy theorems for neural learning problems| Abstract The computational complexity of learning from binary examples is investigated for linear threshold neurons.  We introduce combinatorial measures that create classes of infinitely many learning problems with sample restrictions.  We analyze how the complexity of these problems depends on the values for the measures.  The results are established as dichotomy theorems showing that each problem is either NP-complete or solvable in polynomial time.  In particular, we consider consistency and maximum consistency problems for neurons with binary weights, and maximum consistency problems for neurons with arbitrary weights.  We determine for each problem class the dividing line between the NP-complete and polynomial-time solvable problems.  Moreover, all efficiently solvable problems are shown to have constructive algorithms that require no more than linear time on a random access machine model.  Similar dichotomies are exhibited for neurons with bounded threshold.  The results demonstrate on the one hand that the consideration of sample constraints can lead to the discovery of new efficient algorithms for non-trivial learning problems.  On the other hand, hard learning problems may remain intractable even for severely restricted samples. 
Service Sockets: A Uniform User-Level Interface for Networking Applications| Abstract As network services have evolved, their communication structures have become increasingly complex.  This has led to a corresponding increase in complexity of applications that interact with these services.  In this paper, we propose the Service Socket abstraction to help alleviate this complexity.  Conceptually, a Service Socket represents a communication channel to a service instead of a particular host.  The goal of Service Sockets is to allow application programmers to interact with diverse complex services using the familiar Berkeley Sockets programming interface.  The Service Sockets framework provides service-independent functionality in a small core library and encapsulates service-dependent functionality in dynamically linked libraries with a welldefined interface.  It provides a simple speci#cation interface that can be used by application programmers to describe the desired configuration of a Service Socket.  We present several examples of how Service Sockets can be used to provide a common interface for complex end-point functionality.  A microbenchmark-based evaluation indicates that such an extensible user-level framework does not necessarily incur a substantial performance penalty. 
On the Implications of Delay Adaptability for Learning in Pulsed Neural Networks| ABSTRACT We consider a model for networks of neurons that compute and communicate in terms of pulses.  In addition to weights and thresholds, which are commonly the parameters of artificial neural networks, these pulsed neural networks have adaptable delays.  We present and discuss the most recent and prominent results on the complexity of computing and learning using this new type of parameter. 
On Computing Boolean Functions by a Spiking Neuron| Abstract Computations by spiking neurons are performed using the timing of action potentials.  We investigate the computational power of a simple model for such a spiking neuron in the Boolean domain by comparing it with traditional neuron models such as threshold gates (or McCulloch-Pitts neurons) and sigma-pi units (or polynomial threshold gates).  In particular, we estimate the number of gates required to simulate a spiking neuron by a disjunction of threshold gates and we establish tight bounds for this threshold number.  Furthermore, we analyze the degree of the polynomials that a sigma-pi unit must use for the simulation of a spiking neuron.  We show that this degree cannot be bounded by any fixed value.  Our results give evidence that the use of continuous time as a computational resource endows single-cell models with substantially larger computational capabilities. 
On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?| Abstract Fast and frugal heuristics are well studied models of bounded rationality.  Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources.  Take-thebest searches for a sufficiently good ordering of cues (features) in a task where objects are to be compared lexicographically.  We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies.  We show that no efficient algorithm can approximate the optimum to within any constant factor, if P 6= NP.  We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm.  This algorithm is proven to perform better than take-the-best. 
VC Dimension Bounds for Product Unit Networks| Abstract A product unit is a formal neuron that multiplies its input values instead of summing them.  Furthermore, it has weights acting as exponents instead of being factors.  We investigate the complexity of learning for networks containing product units.  We establish bounds on the Vapnik-Chervonenkis (VC) dimension that can be used to assess the generalization capabilities of these networks.  In particular, we show that the VC dimension for these networks is not larger than the best known bound for sigmoidal networks.  For higher-order networks we derive upper bounds that are independent of the degree of these networks.  We also contrast these results with lower bounds. 
On the Complexity of Learning for a Spiking Neuron| Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity.  They provide a way of analyzing neural computation that is not captured by the traditional neuron models such as sigmoidal and threshold gates (or "Perceptrons").  We introduce a simple model of a spiking neuron that, in addition to the weights that model the plasticity of synaptic strength, also has variable transmission delays between neurons as programmable parameters.  For coding of input and output values two modes are taken into account: binary coding for the Boolean and analog coding for the real-valued domain.  We investigate the complexity of learning for a single spiking neuron within the framework of PAC-learnability.  With regard to sample complexity, we prove that the VC-dimension is \Theta(n log n) and, hence, strictly larger than that of a threshold gate.  In particular, the lower bound holds for binary coding and even if all weights are kept fixed.  The upper bound is valid for the case of analog coding if weights and delays are programmable.  With regard to computational complexity, we show that there is no polynomial-time PAClearning algorithm, unless RP = NP, for a quite
Service Sockets: Design, Implementation and Interfaces| Abstract As network services have evolved, their communication structures have become increasingly complex.  This increase in the complexity in the communication structure of services is mirrored by an increase in the complexity of the client code that interacts with and administers these services.  In this paper, we propose the Service Socket abstraction.  Conceptually, a Service Socket (or an S-socket) is a communication channel to a service instead of a particular host.  Service Sockets abstract away the details of interacting with complex and dynamic services while retaining the conceptual simplicity, the familiar programming interface and the ease-of-
In: Proceedings of the Sixth Italian Workshop on Neural Networks| ABSTRACT The maximum absolute value of integral weights sufficient to represent any linearly separable Boolean function is investigated.  It is shown that upper bounds exhibited by Muroga (1971) for rational weights satisfying the so-called normalized system of inequalities also hold for integral weights.  This improves previously known upper bounds. 
On the Complexity of Learning for Spiking Neurons with Temporal Coding \Lambda| Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal pattern of their activity.  In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection).  It is known that these delays are tuned in biological neural systems through a variety of mechanisms.  We investigate the VC-dimension of networks of spiking neurons where the delays are viewed as programmable parameters and we prove tight bounds for this VC-dimension.  Thus we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays.  In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights.  The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons.  Results about the computational complexity of such algorithms are also given. 
Unsupervised Learning in Networks of Spiking Neurons Using Temporal Coding| Abstract.  We propose a mechanism for unsupervised learning in networks of spiking neurons which is based on the timing of single firing events.  Our results show that a topology preserving behaviour quite similar to that of Kohonen's self-organizing map can be achieved using temporal coding.  In contrast to previous approaches, which use rate coding, the winner among competing neurons can be determined fast and locally.  Hence our model is a further step towards a more realistic description of unsupervised learning in biological neural systems. 
Descartes' Rule of Signs for Radial Basis Function Neural Networks| Abstract We establish versions of Descartes' rule of signs for radial basis function (RBF) neural networks.  The RBF rules of signs provide tight bounds for the number of zeros of univariate networks with certain parameter restrictions.  Moreover, they can be used to infer that the Vapnik-Chervonenkis (VC) dimension and pseudo-dimension of these networks are no more than linear.  This contrasts with previous work showing that RBF neural networks with two and more input nodes have superlinear VC dimension.  The rules give rise also to lower bounds for network sizes, thus demonstrating the relevance of network parameters for the complexity of computing with RBF neural networks. 
Adaptive Receiver Notification for Non-Dedicated Workstation Clusters| Abstract Efficient communication in a NOW environment can be a challenging task.  Depending on the application, the architecture of the nodes and the characteristics of other processes running on the nodes, different communication mechanisms can be appropriate.  In this paper, we evaluate an adaptive strategy which provides multiple communication mechanisms and selects between them depending on the current situation.  We focus on strategies for receiver notification under varying load conditions.  Previous research has shown that in a non-dedicated environment, spin-block strategies can avoid scheduling anomalies which can cause several orders of magnitude of performance degradation.  However, research on these strategies have focused on fixed spin-delay.  We investigate an adaptive strategy that uses runtime information to select the spin-delay in an online manner.  Our strategy achieves better performance than static spin-block strategies on workstation clusters with varying load. 
On the Sample Complexity for Neural Trees| Abstract.  A neural tree is a feedforward neural network with at most one edge outgoing from each node.  We investigate the number of examples that a learning algorithm needs when using neural trees as hypothesis class.  We give bounds for this sample complexity in terms of the VC dimension.  We consider trees consisting of threshold, sigmoidal and linear gates.  In particular, we show that the class of threshold trees and the class of sigmoidal trees on n inputs both have VC dimension \Omega (n log n).  This bound is asymptotically tight for the class of threshold trees.  We also present an upper bound for this class where the constants involved are considerably smaller than in a previous calculation.  Finally, we argue that the VC dimension of threshold or sigmoidal trees cannot become larger by allowing the nodes to compute linear functions.  This sheds some light on a recent result that exhibited neural networks with quadratic VC dimension. 
Lower Bounds on the Complexity of Approximating Continuous Functions by Sigmoidal Neural Networks| Abstract We calculate lower bounds on the size of sigmoidal neural networks that approximate continuous functions.  In particular, we show that for the approximation of polynomials the network size has to grow as \Omega((log k) 1=4 ) where k is the degree of the polynomials.  This bound is valid for any input dimension, i. e.  independently of the number of variables.  The result is obtained by introducing a new method employing upper bounds on the Vapnik-Chervonenkis dimension for proving lower bounds on the size of networks that approximate continuous functions. 
Identification Criteria and Lower Bounds for Perceptron-LikeLearning Rules| Abstract Perceptron-like learning rules are known to require exponentially many correction steps in order to identify Boolean threshold functions exactly.  We introduce criteria that are weaker than exact identification and investigate whether learning becomes significantly faster if exact identification is replaced by one of these criteria: PAC identification, order identification, and sign identification.  PAC identification is based on the learning paradigm introduced by Valiant and known to be easier than exact identification.  Order identification uses the fact that each threshold function induces an ordering relation on the input variables which can be represented by weights of linear size.  Sign identification is based on a property of threshold functions known as unateness and requires only weights of constant size.  We show that Perceptron-like learning rules cannot satisfy these criteria when the number of correction steps is to be bounded by a polynomial.  We also present an exponential lower bound for order identification with the learning rules introduced by Littlestone.  Our results show that efficiency imposes severe restrictions on what can be learned with local learning rules. 
On the Complexity of Computing and Learning with Multiplicative Neural Networks| Abstract In a great variety of neuron models neural inputs are combined using the summing operation.  We introduce the concept of multiplicative neural networks which contain units that multiply their inputs instead of summing them and, thus, allow inputs to interact nonlinearly.  The class of multiplicative networks comprises such widely known and well studied network types as higher-order networks and product unit networks.  We investigate the complexity of computing and learning for multiplicative neural networks.  In particular, we derive upper and lower bounds on the Vapnik-Chervonenkis (VC) dimension and the pseudo dimension for various types of networks with multiplicative units.  As the most general case, we consider feedforward networks consisting of product and sigmoidal units, showing that their pseudo dimension is bounded from above by a polynomial with the same order of magnitude as the currently best known bound for purely sigmoidal networks.  Moreover, we show that this bound holds even in the case when the unit type, product or sigmoidal, may be learned.  Crucial for these results are calculations of solution set components bounds for new network classes.  As to lower bounds we construct product unit networks of fixed depth with superlinear VC dimension.  For higher-order sigmoidal networks we establish polynomial bounds that, in contrast to previous results, do not involve any restriction of the network order.  We further consider various classes of higher-order units, also known as sigma-pi units, characterized by connectivity constraints.  In terms of these we derive some asymptotically tight bounds.  Multiplication plays an important role both in neural modeling of biological behavior and in applications of artificial neural networks.  We also briefly survey research in biology and in applications where multiplication is considered an essential computational element.  The results we present
Complexity of Learning for Networks of Spiking Neurons with Nonlinear Synaptic Interactions| Abstract.  We study model networks of spiking neurons where synaptic inputs interact in terms of nonlinear functions.  These nonlinearities are used to represent the spatial grouping of synapses on the dendrites and to model the computations performed at local branches.  We analyze the complexity of learning in these networks in terms of the VC dimension and the pseudo dimension.  Polynomial upper bounds on these dimensions are derived for various types of synaptic nonlinearities. 
Radial Basis Function Neural Networks Have Superlinear VC Dimension| Abstract We establish superlinear lower bounds on the Vapnik-Chervonenkis (VC) dimension of neural networks with one hidden layer and local receptive field neurons.  As the main result we show that every reasonably sized standard network of radial basis function (RBF) neurons has VC dimension \Omega(W log k), where W is the number of parameters and k the number of nodes.  This significantly improves the previously known linear bound.  We also derive superlinear lower bounds for networks of discrete and continuous variants of centersurround neurons.  The constants in all bounds are larger than those obtained thus far for sigmoidal neural networks with constant depth.  The results have several implications with regard to the computational power and learning capabilities of neural networks with local receptive fields.  In particular, they imply that the pseudo dimension and the fat-shattering dimension of these networks is superlinear as well, and they yield lower bounds even when the input dimension is fixed.  The methods developed here appear suitable for obtaining similar results for other kernel-based function classes. 
Product Unit Neural Networks with Constant Depth and Superlinear VC Dimension| Abstract.  It has remained an open question whether there exist product unit networks with constant depth that have superlinear VC dimension.  In this paper we give an answer by constructing two-hidden-layer networks with this property.  We further show that the pseudo dimension of a single product unit is linear.  These results bear witness to the cooperative effects on the computational capabilities of product unit networks as they are used in practice. 
Self-Organization of Spiking Neurons Using Action Potential Timing| Abstract We propose a mechanism for unsupervised learning in networks of spiking neurons which is based on the timing of single firing events.  Our results show that a topology preserving behaviour quite similar to that of
Hebbian Learning in Networks of Spiking Neurons Using Temporal Coding| Abstract.  Computational tasks in biological systems that require short response times can be implemented in a straightforward way by networks of spiking neurons that encode analogue values in temporal coding.  We investigate the question how spiking neurons can learn on the basis of differences between firing times.  In particular, we provide learning rules of the Hebbian type in terms of single spiking events of the pre- and postsynaptic neuron and show that the weights approach some value given by the difference between pre- and postsynaptic firing times with arbitrary high precision.  Our learning rules give rise to a straightforward possibility for realizing very fast pattern analysis tasks with spiking neurons. 
On the Power of Boolean Computations in Generalized RBF Neural Networks| Abstract Generalized radial basis function (RBF) neurons are extensions of the RBF neuron model where the Euclidean norm is replaced by a weighted norm.  We study binary-valued variants of generalized RBF neurons and compare their computational power in the Boolean domain with linear threshold neurons.  As one of the main results, we show that generalized binary RBF neurons with any weighted norm can compute every Boolean function that is computed by a linear threshold neuron.  While this inclusion turns into an equality if the RBF neuron uses the Euclidean norm, we exhibit a weighted norm where the inclusion is proper.  Applications of the results yield bounds on the Vapnik-Chervonenkis (VC) dimension of RBF neural networks with binary inputs. 
On the sample complexity of learning for networks of spiking neurons with nonlinear synaptic interactions| Abstract We study networks of spiking neurons that use the timing of pulses to encode information.  Nonlinear interactions model the spatial groupings of synapses on the dendrites and describe the computations performed at local branches.  We analyze the question of how many examples these networks must receive during learning to be able to generalize well.  Bounds for this sample complexity of learning are derived in terms of the pseudo-dimension.  In particular, we obtain almost linear and quadratic upper bounds in terms of the number of adjustable parameters for depth-restricted and general feedforward architectures, respectively.  These bounds are also shown to be asymptotically tight for networks that satisfy realistic constraints. 
On the Sample Complexity of Learning for Networks of Spiking Neurons with Nonlinear Synaptic Interactions \Lambda| Abstract We study networks of spiking neurons that use the timing of pulses to encode information.  Nonlinear interactions model the spatial groupings of synapses on the neural dendrites and describe the computations performed at local branches.  Within a theoretical framework of learning we analyze the question of how many training examples these networks must receive to be able to generalize well.  Bounds for this sample complexity of learning can be obtained in terms of a combinatorial parameter known as the pseudo-dimension.  This dimension characterizes the computational richness of a neural network and is given in terms of the number of network parameters.  Two types of feedforward architectures are considered: constant-depth networks and networks of unconstrained depth.  We derive asymptotically tight bounds for each of these network types: Constant depth networks are shown to have an almost linear pseudo-dimension, whereas the pseudo-dimension of general networks is quadratic.  Networks of spiking neurons that use temporal coding are becoming
Complexity of Boolean Computations for a Spiking Neuron \Lambda| Abstract We investigate the computational power of a model for a spiking neuron in the Boolean domain by comparing it with traditional neuron models such as threshold gates (or McCulloch-Pitts neurons) and sigma-pi units (or polynomial threshold gates).  In particular, we estimate the number of gates required to simulate a spiking neuron by a disjunction of threshold gates and we establish tight bounds for this threshold number.  Furthermore, we analyze the degree of the polynomials that a sigma-pi unit must use for the simulation of a spiking neuron.  We show that this degree cannot be bounded by any fixed value.  Our results give evidence that the use of continuous time as a computational resource endows single-cell models with substantially larger computational capabilities. 
Exact VC-Dimension of Boolean Monomials \Lambda| Abstract We show that the Vapnik-Chervonenkis dimension of Boolean monomials over n variables is at most n for all n 2.  It follows that the VC-dimension is determined exactly and is, except for n = 1, equal to the VC-dimension of the proper subclass of monotone monomials. 
VC Dimension Bounds for Higher-Order Neurons| Abstract We investigate the sample complexity for learning using higher-order neurons.  We calculate upper and lower bounds on the Vapnik-Chervonenkis dimension and the pseudo dimension for higher-order neurons that allow unrestricted interactions among the input variables.  In particular, we show that the degree of interaction is irrelevant for the VC dimension and that the individual degree of the variables plays only a minor role.  Further, our results reveal that the crucial parameters that affect the VC dimension of higher-order neurons are the input dimension and the maximum number of occurrences of each variable.  The lower bounds that we establish are asymptotically almost tight.  In particular, they show that the VC dimension in super-linear in the input dimension.  Bounds for higher-order neurons with sigmoidal activation function are also derived. 
VC Dimension Bounds for Networks of Spiking Neurons| Abstract.  We calculate bounds on the VC dimension and pseudo dimension for networks of spiking neurons.  The connections between network nodes are parameterized by transmission delays and synaptic weights.  We provide bounds in terms of network depth and number of connections that are almost linear.  For networks with few layers this yields better bounds than previously established results for networks of unrestricted depth. 
Neural Networks with Local Receptive Fields and Superlinear VC Dimension| Abstract Local receptive field neurons comprise such well-known and widely used unit types as radial basis function neurons and neurons with center-surround receptive field.  We study the Vapnik-Chervonenkis (VC) dimension of feedforward neural networks with one hidden layer of these units.  For several variants of local receptive field neurons we show that the VC dimension of these networks is superlinear.  In particular, we establish the bound \Omega(W log k) for any reasonably sized network with W parameters and k hidden nodes.  This bound is shown to hold for discrete center-surround receptive field neurons, which are physiologically relevant models of cells in the mammalian visual system, for neurons computing a difference of Gaussians, which are popular in computational vision, and for standard radial basis function (RBF) neurons,
On the Capabilities of Higher-Order Neurons: A Radial Basis Function Approach| Abstract Higher-order neurons with k monomials in n variables are shown to have Vapnik-Chervonenkis (VC) dimension at least nk + 1.  This result supersedes the previously known lower bound obtained via k-term monotone disjunctive normal form (DNF) formulas.  Moreover, it implies that the VC dimension of higher-order neurons with k monomials is strictly larger than the VC dimension of k-term monotone DNF.  The result is achieved by introducing an exponential approach that employs Gaussian radial basis function (RBF) neural networks for obtaining classifications of points in terms of higherorder neurons. 
On Methods to Keep Learning Away from Intractability| Abstract We investigate the complexity of learning from restricted sets of training examples.  With the intention to make learning easier we introduce two types of restrictions that describe the permitted training examples.  The strength of the restrictions can be tuned by choosing specific parameters.  We ask how strictly their values must be limited to turn NP-complete learning problems into polynomial-time solvable ones.  Results are presented for Perceptrons with binary and arbitrary weights.  We show that there exist bounds for the parameters that sharply separate efficiently solvable from intractable learning problems. 
Inner Product Spaces for Bayesian Networks| Abstract Bayesian networks have become one of the major models used for statistical inference.  We study the question whether the decisions computed by a Bayesian network can be represented within a low-dimensional inner product space.  We focus on two-label classification tasks over the Boolean domain.  As main results we establish upper and lower bounds on the dimension of the inner product space for Bayesian networks with an explicitly given (full or reduced) parameter collection.  In particular, these bounds are tight up to a factor of 2.  For some nontrivial cases of Bayesian networks we even determine the exact values of this dimension.  We further consider logistic autoregressive Bayesian networks and show that every sufficiently expressive inner product space must have dimension at least #(n 2 ), where n is the number of network nodes.  We also derive the bound 2 #(n) for an artificial variant of this network, thereby demonstrating the limits of our approach and raising an interesting open question.  As a major technical contribution, this work reveals combinatorial and algebraic structures within Bayesian networks such that known methods for the derivation of lower bounds on the dimension of inner product spaces can be brought into play. 
An Improved VC Dimension Bound for Sparse Polynomials| Abstract.  We show that the function class consisting of k-sparse polynomials in n variables has Vapnik-Chervonenkis (VC) dimension at least nk + 1.  This result supersedes the previously known lower bound via k-term monotone disjunctive normal form (DNF) formulas obtained by Littlestone (1988).  Moreover, it implies that the VC dimension for ksparse polynomials is strictly larger than the VC dimension for k-term monotone DNF.  The new bound is achieved by introducing an exponential approach that employs Gaussian radial basis function (RBF) neural networks for obtaining classifications of points in terms of sparse polynomials. 
Charginos and Neutralinos in the Light of Radiative Corrections: Sealing the Fate of Higgsino Dark Matter| Abstract We analyze the LEP constraints from searches for charginos \Sigma and neutralinos i , taking into account radiative corrections to the relations between their masses and the underlying Higgs-mixing and gaugino-mass parameters ; m 1=2 and the trilinear mass parameter A t .  Whilst radiative corrections do not alter the excluded domain in m \Sigma as a function of m \Sigma\Gamma m , its mapping into the ; m 1=2 plane is altered.  We update our previous lower limits on the mass of gaugino dark matter and on tanfi, the ratio of Higgs vacuum expectation values, in the light
On the Sample Complexity for Nonoverlapping Neural Networks| Abstract A neural network is said to be nonoverlapping if there is at most one edge outgoing from each node.  We investigate the number of examples that a learning algorithm needs when using nonoverlapping neural networks as hypotheses.  We derive bounds for this sample complexity in terms of the Vapnik-Chervonenkis dimension.  In particular, we consider networks consisting of threshold, sigmoidal and linear gates.  We show that the class of nonoverlapping threshold networks and the class of nonoverlapping sigmoidal networks on n inputs both have Vapnik-Chervonenkis dimension \Omega(n log n).  This bound is asymptotically tight for the class of nonoverlapping threshold networks.  We also present an upper bound for this class where the constants involved are considerably smaller than in a previous calculation.  Finally, we argue that the Vapnik-Chervonenkis dimension of nonoverlapping threshold or sigmoidal networks cannot become larger by allowing the nodes to compute linear functions.  This sheds some light on a recent result that exhibited neural networks with quadratic VC dimension. 
On the Complexity of Learning for Spiking Neurons with Temporal Coding| Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity.  In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection).  It is known that these delays are tuned in biological neural systems through a variety of mechanisms.  In this article we consider the arguably most simple model for a spiking neuron, which can also easily be implemented in pulsed VLSI.  We investigate the VC dimension of networks of spiking neurons where the delays are viewed as programmable parameters and we prove tight bounds for this VC dimension.  Thus we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays.  In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights.  The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons.  Results about the computational complexity of such algorithms are also given. 
Bayesian Networks and Inner Product Spaces| Abstract.  In connection with two-label classification tasks over the Boolean domain, we consider the possibility to combine the key advantages of Bayesian networks and of kernel-based learning systems.  This leads us to the basic question whether the class of decision functions induced by a given Bayesian network can be represented within a lowdimensional inner product space.  For Bayesian networks with an explicitly given (full or reduced) parameter collection, we show that the "natural" inner product space has the smallest possible dimension up to factor 2 (even up to an additive term 1 in many cases).  For a slight modification of the so-called logistic autoregressive Bayesian network with n nodes, we show that every sufficiently expressive inner product space has dimension at least 2 n=4 .  The main technical contribution of our work consists in uncovering combinatorial and algebraic structures within Bayesian networks such that known techniques for proving lower bounds on the dimension of inner product spaces can be brought into play. 
Beauty Semileptonic Branching Ratios| Abstract Recent measurements of inclusive and exclusive semileptonic b branching ratios are reviewed.  Model-independent analyses from ARGUS and CLEO confirm the value B(b ! `X) = 10:4%, while new analyses from ALEPH obtain higher values above 11%.  The difference is not consistent with measured values of b hadron lifetime.  New measurements of B(B ! D ? `) from CLEO and ALEPH are in good agreement.  First measurements of B ! D ? `branching ratios from OPAL, DELPHI, and ALEPH confirm indirect evidence for a large D ?? component in semileptonic B decays. 
Shared Memory vs Message Passing on SCI: A Case Study Using Split-C|
Catalog of Infrared Observations (|
A New Approach of Combined Block Adjustment Using GPS-Satellite Constellation,|
Distributed Load Balancing Using a Physical Analogy|
Absolute Receiver Antenna Calibrations with a Robot| Presentation at IGS Workshop "Towards Real-Time",. 
Learning Temporally Encoded Patterns in Networks of Spiking Neurons|
Eine Verteilung von 13 Punkten auf einem Quadrat,|
A&AS,|
Decentralized Dynamic Load Balancing: The Particles Approach|
Catalogue of Infrared Observations,|
The NASA/IPAC Extragalactic Database, ASSL Vol| 203: Information &. 
Functional elements in infants' speech processing: the role of determiners in the syntactic categorization of lexical elements|
Formation of a thermodynamically metastable structure containing hairpin II is critical for infectivity of potato spindle tuber viroid RNA|
An Efficient Global Address Space Model with SCI|
Predictors of treatment discontinuity in outpatient mental health care|

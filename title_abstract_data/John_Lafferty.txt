Hyperplane margin classifiers on the multinomial manifold| Abstract The assumptions behind linear classifiers for categorical data are examined and reformulated in the context of the multinomial manifold, the simplex of multinomial models furnished with the Riemannian structure induced by the Fisher information.  This leads to a new view of hyperplane classifiers which, together with a generalized margin concept, shows how to adapt existing margin-based hyperplane models to multinomial geometry.  Experiments show the new classification framework to be effective for text classification, where the categorical structure of the data is modeled naturally within the multinomial family. 
Expectation-Propagation for the Generative Aspect Model| Abstract The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents.  Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning.  This paper demonstrates that the simple variational methods of Blei et al.  (2001) can lead to inaccurate inferences and biased learning for the generative aspect model.  We develop an alternative approach that leads to higher accuracy at comparable cost.  An extension of ExpectationPropagation is used for inference and then embedded in an EM algorithm for learning.  Experimental results are presented for both synthetic and real data sets. 
Ordered Binary Decision Diagrams and Minimal Trellises| Abstract Ordered binary decision diagrams (OBDDs) are graph-based data structures for representing Boolean functions.  They have found widespread use in computer-aided design and in formal verification of digital circuits.  Minimal trellises are graphical representations of error-correcting codes that play a prominent role in coding theory.  This paper establishes a close connection between these two graphical models, as follows.  Let C be a binary code of length n, and let f C (x 1 ;:::;x n ) be the Boolean function that takes the value 0 at x 1 ;:::;x n if and only if (x 1 ;:::;x n ) 2 C .  Given this natural one-to-one correspondence between Boolean functions and binary codes, we prove that the minimal proper trellis for a code C with minimum distance d } 1 is isomorphic to the singleterminal OBDD for its Boolean indicator function f C (x 1 ;:::;x n ).  Prior to this result, the extensive research during the past decade on binary decision diagrams { in computer engineering { and on minimal trellises { in coding theory { has been carried out independently.  As outlined in this work, the realization that binary decision diagrams and minimal trellises are essentially the same data structure opens up a range of promising possibilities for transfer of ideas between these disciplines. 
To appear in Maximum Entropy and Bayesian Methods,| Abstract.  The maximum entropy method has recently been successfully introduced to a variety of natural language applications.  In each of these applications, however, the power of the maximum entropy method is achieved at the cost of a considerable increase in computational requirements.  In this paper we present a technique, closely related to the classical cluster expansion from statistical mechanics, for reducing the computational demands necessary to calculate conditional maximum entropy language models. 
Additive Models, Boosting, and Inference for Generalized Divergences| Abstract We present a framework for designing incremental learning algorithms derived from generalized entropy functionals.  Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform.  A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et al. , as the natural parameter of the family varies.  We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion.  This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani. 
A Study of Four-Fermion Final States with High Multiplicity at LEP The OPAL Collaboration| Abstract A search for ` + `\Gamma qq (`=e,) four-fermion final states in high multiplicity events has been performed at p s m Z and above the Z peak at p s = 130 GeV and p s = 136 GeV.  The data taken with OPAL correspond to an integrated luminosity of 132. 4 pb\Gamma 1 and 5. 3 pb\Gamma 1 at p s m Z and p s = 130-136 GeV, respectively.  While at the Z resonance the data are in good agreement with the Standard Model predictions, we find more events than expected at p s = 130--136 GeV, especially in the + \Gamma qq channel, where 5 events remain after all cuts, with only 0. 6 predicted by the Monte Carlo simulation.  For all center-of-mass energies the observed shapes of the differential distributions are consistent with the predictions. 
Kernel Conditional Random Fields: Representation, Clique Selection, and Semi-Supervised Learning| Abstract Kernel conditional random fields are introduced as a framework for discriminative modeling of graph-structured data.  A representer theorem for conditional graphical models is given which shows how kernel conditional random fields arise from risk minimization procedures defined using Mercer kernels on labeled graphs.  A procedure for greedily selecting cliques in the dual representation is then proposed, which allows sparse representations.  By incorporating kernels and implicit feature spaces into conditional graphical models, the framework enables semi-supervised learning algorithms for structured data through the use of graph kernels.  The clique selection and semisupervised methods are demonstrated in synthetic data experiments, and are also applied to the problem of protein secondary structure prediction. 
Spectral Techniques for Expander Codes| Abstract This paper introduces methods based on generalized Fourier analysis for working with a class of errorcorrecting codes constructed in terms of Cayley graphs.  Our work is motivated by the recent results of Sipser and Spielman [15] showing graph expansion to be essential for efficient decoding of certain low-density parity-check codes.  They leave open the problem of sub-quadratic encoding for this class of codes, and it is this problem that we address.  We show that when the codes are constructed in terms of Cayley graphs, the symmetry of the graphs can be exploited by using the representation theory of the underlying group to devise a sub-quadratic encoding algorithm that, in the case where the group is PSL 2 (Z=qZ), requires O(n 4=3 ) operations, where n = O(q 3 ) is the block length.  Our results indicate that this new class of codes may combine many of the strengths of two of the most powerful and successful, but previously disparate areas of coding theory: the class of cyclic codes where the rich algebraic structure yields a large collection of techniques for finding and manipulating the codes, and the class of low-density paritycheck codes which have simple and efficient decoding algorithms and good asymptotic properties. 
A study of smoothing methods for language models applied to information retrieval| ABSTRACT Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition.  The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model.  A core problem in language model estimation is smoothing , which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness.  In this paper, we study the problem of language model smoothing and its influence on retrieval performance.  We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. 
Kernel conditional random fields: representation and clique selection| Abstract Kernel conditional random fields (KCRFs) are introduced as a framework for discriminative modeling of graph-structured data.  A representer theorem for conditional graphical models is given which shows how kernel conditional random fields arise from risk minimization procedures defined using Mercer kernels on labeled graphs.  A procedure for greedily selecting cliques in the dual representation is then proposed, which allows sparse representations.  By incorporating kernels and implicit feature spaces into conditional graphical models, the framework enables semi-supervised learning algorithms for structured data through the use of graph kernels.  The framework and clique selection methods are demonstrated in synthetic data experiments, and are also applied to the problem of protein secondary structure prediction. 
A Risk Minimization Framework for Information Retrieval| Abstract This paper presents a probabilistic information retrieval framework in which the retrieval problem is formally treated as a statistical decision problem.  In this framework, queries and documents are modeled using statistical language models, user preferences are modeled through loss functions, and retrieval is cast as a risk minimization problem.  We discuss how this framework can unify existing retrieval models and accommodate systematic development of new retrieval models.  As an example of using the framework to model nontraditional retrieval problems, we derive retrieval models for subtopic retrieval, which is concerned with retrieving documents to cover many different subtopics of a general query topic.  These new models differ from traditional retrieval models in that they relax the traditional assumption of independent relevance of documents. 
Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning| Abstract We present an algorithm based on convex optimization for constructing kernels for semi-supervised learning.  The kernel matrices are derived from the spectral decomposition of graph Laplacians, and combine labeled and unlabeled data in a systematic fashion.  Unlike previous work using diffusion kernels and Gaussian random field kernels, a nonparametric kernel approach is presented that incorporates order constraints during optimization.  This results in flexible kernels and avoids the need to choose among different parametric forms.  Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally feasible for large datasets.  We evaluate the kernels on real datasets using support vector machines, with encouraging results. 
EUROPEAN LABORATORY FOR PARTICLE PHYSICS| Abstract We have searched for decays of the lepton into seven or more charged particles, using data collected with the OPAL detector from 1990 to 1995 in e + e\Gamma collisions at p s MZ .  No candidate events were found and an upper limit on the branching ratio for decays into seven charged particles of 1:8 \Theta 10\Gamma 5 at the 95% confidence level was determined. 
The Candide System for Machine Translation| ABSTRACT We present an overview of Candide, a system for automatic translation of French text to English text.  Candide uses methods of information theory and statistics to develop a probability model of the translation process.  This model, which is made to accord as closely as possible with a large body of French and English sentence pairs, is then used to generate English translations of previously unseen French sentences.  This paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results. 
Boosting and Maximum Likelihood for Exponential Models| Abstract We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels.  In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood.  Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression. 
The Weaver System for Document Retrieval| Abstract This paper introduces Weaver, a probabilistic document retrieval system under development at Carnegie Mellon University, and discusses its performance in the TREC-8 ad hoc evaluation.  We begin by describing the architecture and philosophyoftheWeaver system, which represents a departure from traditional approaches to retrieval.  The central ingredient is a statistical model of how a user might distill or "translate" a given documentinto a query.  The retrieval-as-translation approach is based on the noisy channel paradigm and statistical language modeling, and has much in common with other recently proposed models [12, 10].  After the initial high-level overview, the bulk of the paper contains a discussion of implementation details and the empirical performance of the Weaver retrieval system. 
Spectral Techniques for Expander Codes and Generalized Cyclic Codes| Abstract --- We show that spectral techniques can be used to encode the explicit expander codes recently constructed by Sipser and Spielman.  These codes can be viewed as generalized cyclic codes for certain nonabelian groups.  Many of the classical spectral techniques for cyclic codes extend to this general setting. 
Diffusion Kernels on Graphs and Other Discrete Input Spaces| Abstract The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings.  In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea.  In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels, based on the heat equation, called diffusion kernels, and show that these can be regarded as the discretisation of the familiar Gaussian kernel of Euclidean space. 
CYBERPUNC: A LIGHTWEIGHT PUNCTUATION ANNOTATION SYSTEM FOR SPEECH| ABSTRACT This paper describes a lightweight method for the automatic insertion of intra-sentence punctuation into text.  Despite the intuition that pauses in an acoustic stream are a positive indicator for some types of punctuation, this work will demonstrate the feasibility of a system which relies solely on lexical information.  Besides its potential role in a speech recognition system, such a system could serve equally well in non-speech applications such as automatic grammar correction in a word processor and parsing of spoken text.  After describing the design of a punctuationrestoration system, which relies on a trigram language model and a straightforward application of the Viterbi algorithm, we summarize results, both quantitative and subjective, of the performance and behavior of a prototype system. 
A Model of Lexical Attraction and Repulsion \Lambda| Abstract This paper introduces new methods based on exponential families for modeling the correlations between words in text and speech.  While previous work assumed the effects of word co-occurrence statistics to be constant over a window of several hundred words, we show that their influence is nonstationary on a much smaller time scale.  Empirical data drawn from English and Japanese text, as well as conversational speech, reveals that the "attraction" between words decays exponentially, while stylistic and syntactic contraints create a "repulsion" between words that discourages close co-occurrence.  We show that these characteristics are well described by simple mixture models based on twostage exponential distributions which can be trained using the EM algorithm.  The resulting distance distributions can then be incorporated as penalizing features in an exponential language model. 
Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data| Abstract We present conditional random fields, a framework for building probabilistic models to segment and label sequence data.  Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models.  Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states.  We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 
Using Maximum Entropy for Text Classification| Abstract This paper proposes the use of maximum entropy techniques for text classification.  Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation.  The underlying principle of maximum entropy is that without external knowledge, one should prefer distributions that are uniform.  Constraints on the distribution, derived from labeled training data, inform the technique where to be minimally non-uniform.  The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm.  In this paper, maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document.  In experiments on several text datasets we compare accuracy to naive Bayes and show that maximum entropy is sometimes significantly better, but also sometimes worse.  Much future work remains, but the results indicate that maximum entropy is a promising technique for text classification. 
Iterative Markov Chain Monte Carlo Computation of Reference Priors and Minimax Risk| Abstract We present an iterative Markov chain Monte Carlo algorithm for computing reference priors and minimax risk for general parametric families.  Our approach uses MCMC techniques based on the Blahut-Arimoto algorithm for computing channel capacity in information theory.  We give a statistical analysis of the algorithm, bounding the number of samples required for the stochastic algorithm to closely approximate the deterministic algorithm in each iteration.  Simulations are presented for several examples from exponential families.  Although we focus on applications to reference priors and minimax risk, the methods and analysis we develop are applicable to a much broader class of optimization problems and iterative algorithms. 
Cranking: Combining Rankings Using Conditional Probability Models on Permutations| Abstract A new approach to ensemble learning is introduced that takes ranking rather than classification as fundamental, leading to models on the symmetric group and its cosets.  The approach uses a generalization of the Mallows model on permutations to combine multiple input rankings.  Applications include the task of combining the output of multiple search engines and multiclass or multilabel classification, where a set of input classifiers is viewed as generating a ranking of class labels.  Experiments for both types of applications are presented. 
Semi-Supervised Learning: From Gaussian Fields to Gaussian Processes| Abstract We show that the
Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions| Abstract An approach to semi-supervised learning is proposed that is based on a Gaussian random field model.  Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances.  The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation.  The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory.  We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning.  We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection.  Promising experimental results are presented for synthetic data, digit classification, and text classification tasks. 
Cheating with Imperfect Transcripts| ABSTRACT Most speech recognition systems try to reconstruct a word sequence given an acoustic input, using prior information about the language being spoken.  In some cases, there is more information available to the decoder than simply the acoustics.  When decoding a television news broadcast, for example, the closed-caption information that is often recorded for hearing impaired viewers may also be available.  While these captions are generally not completely accurate transcriptions, they can be considered to be a strong hint as to what was actually spoken.  In this paper, we present a formalization of this problem in terms of the source channel paradigm.  We propose a simple translation model for mapping caption sequences to word sequenceswhich updates the language model with the prior information inherent in the captions.  We also describe an efficient implementation of the search in a Viterbi decoder, and present results using this system in the broadcast news domain. 
WORD CLUSTERING WITH PARALLEL SPOKEN LANGUAGE CORPORA \Lambda| ABSTRACT In this paper we introduce a word clustering algorithm which uses a bilingual, parallel corpus to group together words in the source and target language.  Our method generalizes previous mutual information clustering algorithms for monolingual data by incorporating a statistical translation model.  Preliminary experiments have shown that the algorithm can effectively employ the constraints implicit in bilingual data to extract classes which are well-suited to machine translation tasks. 
LEVEL SPACINGS FOR| Abstract.  We investigate the eigenvalue spacing distributions for randomly generated 4-regular Cayley graphs on SL 2 (Fp ) by numerically calculating their spectra.  We present strong evidence that the distributions are Poisson and hence do not follow the Gaussian orthogonal ensemble.  Among the Cayley graphs of SL 2 (Fp ) we consider are the new expander graphs recently discovered by Y.  Shalom.  In addition, we use a Markov chain method to generate random 4-regular graphs, and observe that the average eigenvalue spacings are closely approximated by the Wigner surmise. 
Information Diffusion Kernels| Abstract A new family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models.  Based on the heat equation on the Riemannian manifold defined by the Fisher information metric, information diffusion kernels generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning.  As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data.  Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for text classification. 
Combining Active Learning and Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions| Abstract Active and semi-supervised learning are important techniques when labeled data are scarce.  We combine the two under a Gaussian random field model.  Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances.  The semi-supervised learning problem is then formulated in terms of a Gaussian random field on this graph, the mean of which is characterized in terms of harmonic functions.  Active learning is performed on top of the semisupervised learning scheme by greedily selecting queries from the unlabeled data to minimize the estimated expected classification error (risk); in the case of Gaussian fields the risk is efficiently computed using matrix methods.  We present experimental results on synthetic data, handwritten digit recognition, and text classification tasks.  The active learning scheme requires a much smaller number of queries to achieve high accuracy compared with random query selection. 
Text Segmentation Using Exponential Models| Abstract This paper introduces a new statistical approach to partitioning text automatically into coherent segments.  Our approach enlists both short-range and long-range language models to help it sniff out likely sites of topic changes in text.  To aid its search, the system consults a set of simple lexical hints it has learned to associate with the presence of boundaries through inspection of a large corpus of annotated data.  We also propose a new probabilistically motivated error metric for use by the natural language processing and information retrieval communities, intended to supersede precision and recall for appraising segmentation algorithms.  Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the effectiveness of our approachintwovery different domains, Wall Street Journal articles and the TDT Corpus, a collection of newswire articles and broadcast news transcripts. 
Simultaneous Word and Document Clustering| Abstract --- In this paper we outline some of our current research on unsupervised learning algorithms for clustering words and documents.  The focus of our work is on the use of statistical methods and builds upon previous work on mutual information word clustering, document clustering, and feature selection for text classification. 
Model-based Feedback in the Language Modeling Approach to Information Retrieval| ABSTRACT The language modeling approach to retrieval has been shown to perform well empirically.  One advantage of this new approach is its statistical foundations.  However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it.  Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query.  In this paper, we present a more principled approach to feedback in the language modeling approach.  Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents.  Such a model-based feedback strategy easily fits into an extension of the language modeling approach.  We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the KL-divergence over feedback documents.  Experiment results show that both approaches are effective and outperform the Rocchio feedback approach. 
EUROPEAN LABORATORY FOR PARTICLE PHYSICS| Abstract Deep inelastic electron-photon scattering is studied using e + e\Gamma data collected by the OPAL detector at centre-of-mass energies p s ee M Z 0 .  The photon structure function F fl 2 (x; Q 2 ) is explored in a Q 2 range of 1. 1 to 6. 6 GeV 2 at lower x values than ever before.  To probe this kinematic region events are selected with a beam electron scattered into one of the OPAL luminosity calorimeters at scattering angles between 27 and 55 mrad.  A measurement is presented of the photon structure function F fl 2 (x; Q 2 ) at
Grammatical Trigrams: A Probabilistic Model of Link Grammar \Lambda| Abstract In this paper we present a new class of language models.  This class derives from link grammar, a context-free formalism for the description of natural language.  We describe an algorithm for determining maximum-likelihood estimates of the parameters of these models.  The language models which we present differ from previous models based on stochastic context-free grammars in that they are highly lexical.  In particular, they include the familiar n-gram models as a natural subclass.  The motivation for considering this class is to estimate the contribution which grammar can make to reducing the relative entropy of natural language. 
Using Unlabeled Data to Improve Text| The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the NSF, DARPA, the U. S.  government or any other entity. 
Variational Chernoff Bounds for Graphical Models| Abstract Recent research has made significant progress on the problem of bounding log partition functions for exponential family graphical models.  Such bounds have associated dual parameters that are often used as heuristic estimates of the marginal probabilities required in inference and learning.  However these variational estimates do not give rigorous bounds on marginal probabilities, nor do they give estimates for probabilities of more general events than simple marginals.  In this paper we build on this recent work by deriving rigorous upper and lower bounds on event probabilities for graphical models.  Our approach is based on the use of generalized Chernoff bounds to express bounds on event probabilities in terms of convex optimization problems; these optimization problems, in turn, require estimates of generalized log partition functions.  Simulations indicate that this technique can result in useful, rigorous bounds to complement the heuristic variational estimates, with comparable computational cost. 
Duality and Auxiliary Functions for Bregman Distances| Abstract We formulate and prove a convex duality theorem for Bregman distances and present a technique based on auxiliary functions for deriving and proving convergence of iterative algorithms to minimize Bregman distance subject to linear constraints. 
Document Language Models, Query Models, and Risk Minimization for Information Retrieval| ABSTRACT We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory.  The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval.  A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization.  The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses.  While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models.  The Markov chain method has connections to algorithms from link analysis and social networks.  The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio.  Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data. 
Statistical Learning Algorithms Based on Bregman Distances| Abstract --- We present a class of statistical learning algorithms formulated in terms of minimizing Bregman distances, a family of generalized entropy measures associated with convex functions.  The inductive learning scheme is akin to growing a decision tree, with the Bregman distance filling the role of the impurity function in tree-based classifiers.  Our approach is based on two components.  In the feature selection step, each linear constraint in a pool of candidate features is evaluated by the reduction in Bregman distance that would result from adding it to the model.  In the constraint satisfaction step, all of the parameters are adjusted to minimize the Bregman distance subject to the chosen constraints.  We introduce a new iterative estimation algorithm for carrying out both the feature selection and constraint satisfaction steps, and outline a proof of the convergence of these algorithms. 
Statistical Models for Text Segmentation| Abstract.  This paper introduces a new statistical approach to automatically partitioning text into coherent segments.  The approach is based on a technique that incrementally builds an
Numerical Investigation of the Spectrum for Certain Families of Cayley Graphs| Abstract In this paper we extend some earlier computations [8].  In particular, the expanding behavior of Cayley graphs of PSL2 (F107) is compared with that of the Cayley graphs for the group A10 .  These computations support the (up to now) unvoiced conjecture of Lubotzky that the symmetric groups and projective linear groups have asymptotically different average expanding behavior.  We also give a thorough spectral analysis for a natural family of Cayley graphs which does not admit analysis by Selberg's theorem. 
International Symposium on Information Theory and Its Applications| Abstract The notion of graph expansion was introduced as a tool in coding theory by Sipser and Spielman, who used it to bound the minimum distance of a class of low-density codes, as well as the performance of various iterative decoding algorithms for these codes.  In spite of its usefulness in establishing theoretical bounds on iterative decoding, graph expansion has not been widely used to design codes.  Instead, random graphs are the primary means used to obtain graphs for codes, raising the question of whether comparable performance can be achieved using explicit constructions.  In this paper we investigate the use of explicit algebraic expander graphs and algebraic subcodes, and show that the resulting coding schemes achieve excellent performance, competitive with standard low-density paritycheck codes over a wide range of block lengths.  Since the code constructions are based on graphs of groups, the Fourier transform can be used to obtain fast encoding algorithms for these codes. 
Topic Detection and Tracking Pilot Study Final Report| \Lambda UMass Amherst, y CMU, z DARPA, x Dragon Systems, and y CMU ABSTRACT Topic Detection and Tracking (TDT) is a DARPA-sponsored
Text Segmentation Using Exponential Models \Lambda| Abstract This paper introduces a new statistical approach to partitioning text automatically into coherent segments.  Our approach enlists both short-range and long-range language models to help it sniff out likely sites of topic changes in text.  To aid its search, the system consults a set of simple lexical hints it has learned to associate with the presence of boundaries through inspection of a large corpus of annotated data.  We also propose a new probabilistically motivated error metric for use by the natural language processing and information retrieval communities, intended to supersede precision and recall for appraising segmentation algorithms.  Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the effectiveness of our approach in two very different domains, Wall Street Journal articles and the TDT Corpus, a collection of newswire articles and broadcast news transcripts. 
Information Retrieval as Statistical Translation| Abstract We propose a new probabilistic approach to information retrieval based upon the ideas and methods of statistical machine translation.  The central ingredientinthis approach is a statistical model of how a user might distill or "translate" a given document into a query.  To assess the relevance of a document to a user's query,we estimate the probability that the query would havebeen generated as a translation of the document, and factor in the user's general preferences in the form of a prior distribution over documents.  We propose a simple, well motivated model of the document-to-query translation process, and describe an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents.  As we show, one can view this approach as a generalization and justification of the "language modeling" strategy recently proposed by Ponte and Croft.  In a series of experiments on TREC data, a simple translation-based retrieval system performs well in comparison to conventional retrieval techniques.  This prototype system only begins to tap the full potential of translation-based retrieval. 
CMU Report on TDT-2: Segmentation, Detection and Tracking| ABSTRACT This paper reports the results achieved by Carnegie Mellon University on the Topic Detection and Tracking Project's secondyear evaluation for the segmentation, detection, and tracking tasks.  Additional post-evaluation improvements are also presented. 
Semi-supervised learning using randomized mincuts| Abstract In many application domains there is a large amount of unlabeled data but only a very limited amount of labeled training data.  One general approach that has been explored for utilizing this unlabeled data is to construct a graph on all the data points based on distance relationships among examples, and then to use the known labels to perform some type of graph partitioning.  One natural partitioning to use is the minimum cut that agrees with the labeled data (Blum & Chawla, 2001), which can be thought of as giving the most probable label assignment if one views labels as generated according to a Markov Random Field on the graph.  Zhu et al.  (2003) propose a cut based on a relaxation of this field, and Joachims (2003) gives an algorithm based on finding an approximate min-ratio cut.  In this paper, we extend the mincut approach by adding randomness to the graph structure.  The resulting algorithm addresses several shortcomings of the basic mincut approach, and can be given theoretical justification from both a Markov random field perspective and from sample complexity considerations.  In cases where the graph does not have small cuts for a given classification problem, randomization may not help.  However, our experiments on several datasets show that when the structure of the graph supports small cuts, this can result in highly accurate classifiers with good accuracy/coverage tradeoffs.  In addition, we are able to achieve good performance with a very simple graph-construction procedure. 
Basic Methods of Probabilistic Context Free Grammars|
Towards History-Based Grammars: Using Richer Models for Probabilistic Parsing|
Inducing Features of Random Fields|
A Model of Lexical Attraction and Repulsion|
A robust parsing algorithm for link grammars|
Mixed-membership models of scientific publications|
Expectation-Propogation for the Generative Aspect Model|
Computation of the Probability of Initial Substring Generation by Stochastic Context-Free Grammars|
Probabilistic IR models based on document and query generation|
Decision tree models applied to the labeling of text with parts-of-speech|
Inference and Estimation of a Long-Range Trigram Model|
Fast Fourier analysis for SL2 over a finite field and related numerical experiments,|
Combining Simple Discriminators for Object Discrimination|
Dual role of smoothing in the language modeling approach|
Decision tree parsing using a hidden derivational model|
Model-based feedback in the KL-divergence retrieval model|
A statistical approach to machine Table 5: Translation results on the Verbmobil task|
Diffusion kernels in continuous spaces| Tech report CMU,. 
Statistical learning algorithms based on Bregman distances,"|
Cluster expansions and iterative scaling of maximum entropy language models|
Basic methods of probabilistic context free grammars| Technical report, Continuous Speech Recognition Group IBM -. 
Bregman distances, iterative scaling, and auxiliary functions,"|
JAVELIN: Justification-based Answer Valuation through Language Interpretation, proposal submitted to ARDA BAA 01-01|
Link Grammar Parser Online Demo|
A derivation of the inside-outside algorithm from the EM algorithm|
Two-stage language models for information retrieval|
Using maximum entropy for text classification|
Codes and iterative decoding on algebraic expander graphs|
Language Modeling for Information Retrieval|
Combining rankings using conditional probability models on permutations|
edu/link/,|
Ures The Candide System for Machine Translation Proceedings of the ARPA Human Language Technology Workshop Plainsbourgh,|
Boosting and ML for exponential models|
Statistical model for text segmentation|
Automatic classification using features of spelling|
Altered behaviour of parasitised killifish increases susceptibility to predation by bird hosts|
An Introduction to the Link Grammar Parser|
For a review see for example|
\Bregman distances, iterative scaling, and auxiliary functions,"|
"Personal communication,"|
Two-stage language model for information retrieval|
'Switchboard April 1996 Evaluation Report',|
The evolution of trophic transmission|
Figure 2:|
Barking up the right tree: Estimating 's for decision trees using the EM algorithm|
Probabilistic IR models based on query and document generation|
Towards historybased grammars: using richer models for probabilistic parsers',|
Wetlands of California - Part II: Classification and Description of Wetlands of the Central and Southern California Coast and Coastal Watersheds|
Parasitology meets ecology on its own terms: Margolis et al| revisited. 

Stochastic Neighbor Embedding| Abstract We describe a probabilistic approach to the task of embedding highdimensional objects into a low-dimensional space in a way that preserves neighbor identities.  A Gaussian is centered on each object in the highdimensional space and the densities under this Gaussian are used to define a probability distribution over all the potential neighbors of the object.  The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed in the low-dimensional space.  The natural cost function is a sum of Kullback-Leibler divergences, one per object.  This leads to a simple gradient for adjusting the positions of objects in the low-dimensional space.  Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each high-dimensional object by a mixture of widely separated low-dimensional points.  This allows the embeddings of ambiguous objects, like the document count vector for the word "bank", to be close to the embeddings of both "river" and "finance" without forcing outdoor concepts to be located close to corporate ones. 
An Alternate Objective Function for Markovian Fields| Abstract In labelling or prediction tasks, a trained model's test performance is often based on the quality of its single-time marginal distributions over labels rather than its joint distribution over label sequences.  We propose using a new cost function for discriminative learning that more accurately reflects such test time conditions.  We present an ecient method to compute the gradient of this cost for Maximum Entropy Markov Models, Conditional Random Fields, and for an extension of these models involving hidden states.  Our experimental results show that the new cost can give significant improvements and that it provides a novel and effective way of dealing with the 'label-bias' problem. 
Factorial Models and Refiltering for Speech Separation and Denoising| Abstract This paper proposes the combination of several ideas, some old and some new, from machine learning and speech processing.  We review the max approximation to log spectrograms of mixtures, show why this motivates a "refiltering" approach to separation and denoising, and then describe how the process of inference in factorial probabilistic models performs a computation useful for deriving the masking signals needed in refiltering.  A particularly simple model, factorial-max vector quantization (MAXVQ), is introduced along with a branch-and-bound technique for efficient exact inference and applied to both denoising and monaural separation.  Our approach represents a return to the ideas of Ephraim, Varga and Moore but applied to auditory scene analysis rather than to speech recognition. 
Hierarchical Clustering of a Mixture Model| Abstract In this paper we propose an efficient algorithm for reducing a large mixture of Gaussians into a smaller mixture while still preserving the component structure of the original model; this is achieved by clustering (grouping) the components.  The method minimizes a new, easily computed distance measure between two Gaussian mixtures that can be motivated from a suitable stochastic model and the iterations of the algorithm use only the model parameters, avoiding the need for explicit resampling of datapoints.  We demonstrate the method by performing hierarchical clustering of scenery images and handwritten digits. 
Optimization with EM and Expectation-Conjugate-Gradient| Abstract We show a close relationship between the Expectation - Maximization (EM) algorithm and direct optimization algorithms such as gradient-based methods for parameter learning.  We identify analytic conditions under which EM exhibits Quasi-Newton behavior, and conditions under which it possesses poor, first-order convergence.  Based on this analysis, we propose two novel algorithms for maximum likelihood estimation of latent variable models, and report empirical results showing that, as predicted by the theory, the proposed new algorithms can substantially outperform standard EM in terms of speed of convergence in certain cases. 
Adaptive Overrelaxed Bound Optimization Methods| Abstract We study a class of overrelaxed bound optimization algorithms,
Constrained Hidden Markov Models| 1 Latent variable models for structured sequence data Structured time-series are generated by systems whose underlying state variables change in a continuous way but whose state to output mappings are highly nonlinear, many to one and not smooth.  Probabilistic unsupervised learning for such sequences requires models with two essential features: latent (hidden) variables and topology in those variables.  Hidden Markov models (HMMs) can be thought of as dynamic generalizations of discrete state static data models such as Gaussian mixtures, or as discrete state versions of linear dynamical systems (LDSs) (which are themselves dynamic generalizations of continuous latent variable models such as factor analysis).  While both HMMs and LDSs provide probabilistic latent variable models for time-series, both have important limitations.  Traditional HMMs have a very powerful model of the relationship between the underlying state and the associated observations because each state stores a private distribution over the output variables.  This means that any change in the hidden state can cause arbitrarily complex changes in the output distribution.  However, it is extremely difficult to capture reasonable dynamics on the discrete latent variable because in principle any state is reachable from any other state at any time step and the next state depends only on the current state.  LDSs, on the other hand, have an extremely impoverished representation of the outputs as a function of the latent variables since this transformation is restricted to be global and linear.  But it is somewhat easier to capture state dynamics since the state is a multidimensional vector of continuous variables on which a matrix "flow" is acting; this enforces some continuity of the latent variables across time.  Constrained hidden Markov models address the modeling of state dynamics by building some topology into the hidden state representation.  The essential idea is to constrain the transition parameters of a conventional HMM so that the discretevalued hidden state evolves in a structured way.  1 In particular, below I consider parameter restrictions which constrain the state to evolve as a discretized version of a continuous multivariate variable, i. e.  so that it inscribes only connected paths in some space.  This lends a physical interpretation to the discrete state trajectories in an HMM.  1 A standard trick in traditional speech applications of HMMs is to use "left-to-right" transition matrices which are a special case of the type of constraints investigated in this paper.  However, leftto-right (Bakis) HMMs force state trajectories that are inherently one-dimensional and uni-directional whereas here I also consider higher dimensional topology and free omni-directional motion.  2 An illustrative game Consider playing the following game: divide a sheet of paper into several contiguous, nonoverlapping regions which between them cover it entirely.  In each region inscribe a symbol, allowing symbols to be repeated in different regions.  Place a pencil on the sheet and move it around, reading out (in order) the symbols in the regions through which it passes.  Add some noise to the observation process so that some fraction of the time incorrect symbols are reported in the list instead of the correct ones.  The game is to reconstruct the configuration of regions on the sheet from only such an ordered list(s) of noisy symbols.  Of course, the absolute scale, rotation and reflection of the sheet can never be recovered, but learning the essential topology may be possible.  2 Figure 1 illustrates this setup. 
Modeling Systematic Variations in Pronunciation via a Language-Dependent Hidden Speaking Mode| ABSTRACT This paper describes the research efforts of the "Hidden Speaking Mode" group participating in the 1996 summer workshop on speech recognition.  The goal of this project is to model pronunciation variations that occur in conversational speech in general and, more specifically, to investigate the use of a hidden speaking mode to represent systematic variations that are correlated with the word sequence (e. g.  predictable from syntactic structure).  This paper describes the theoretical formulation of hidden mode modeling, as well as some results in error analysis, language modeling and pronunciation modeling. 
One Microphone Source Separation| Abstract Source separation, or computational auditory scene analysis, attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment.  Unmixing algorithms such as ICA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available.  I present a technique called refiltering which recovers sources by a nonstationary reweighting ("masking") of frequency sub-bands from a single recording, and argue for the application of statistical algorithms to learning this masking function.  I present results of a simple factorial HMM system which learns on recordings of single speakers and can then separate mixtures using only one observation signal by computing the masking function and then refiltering.  1 Learning from data in computational auditory scene analysis Imagine listening to many pianos being played simultaneously.  If each pianist were striking keys randomly it would be very difficult to tell which note came from which piano.  But if each were playing a coherent song, separation would be much easier because of the structure of music.  Now imagine teaching a computer to do the separation by showing it many musical scores as "training data".  Typical auditory perceptual input contains a mixture of sounds from different sources, altered by the acoustic environment.  Any biological or artificial hearing system must extract individual acoustic objects or streams in order to do successful localization, denoising and recognition.  Bregman [1] called this
A Unifying Review of Linear Gaussian Models| nonlinearity.  Through the use of other nonlinearities we show how independent component analysis (ICA) is also a variation of the same basic generative model.  We show that factor analysis and mixtures of Gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term.  We introduce a new model for static data known as sensible principal component analysis (SPCA) as well as a novel concept of spatially adaptive observation noise.  We also review some of the literature involving global and local mixtures of the basic models and provide pseudo-code for inference and learning for all the basic models.  1 A Unifying Review Many common statistical techniques for modeling multidimensional static datasets and multidimensional time series can be seen as variants of one underlying model.  As we will show later in the paper, these include factor analysis (FA), principal component analysis (PCA), mixtures of Gaussian clusters, vector quantization (VQ), independent component analysis models (ICA), Kalman filter models (a. k. a.  linear dynamical systems) and hidden Markov models (HMMs).  The relationships between some of these models has been noted in passing in the recent literature.  For example, Hinton et al.  (1995) note that factor analysis and PCA are closely related and Digalakis et al.  (1993) relate the forward--backward algorithm for HMMs to Kalman filtering.  In this paper we unify many of the disparate observations made by previous authors (Rubin and Thayer, 1982; Delyon, 1993; Digalakis et al. , 1993; Hinton et al. , 1995; Elliott et al. , 1995; Ghahramani and Hinton, 1996a, 1996b, 1997; Hinton and Ghahramani, 1997) and present a review of all these algorithms as instances of a single basic generative model.  This unified view allows us to show some interesting relations between previously disparate algorithms.  For example, factor analysis and mixtures of Gaussians can be implemented using autoencoder neural networks with different nonlinearities but learned using a squared error cost penalized by the same regularization term.  ICA can be seen as a nonlinear version of factor analysis.  The framework also makes it possible to derive a new model for static data which is based on PCA but has a sensible probabilistic interpretation as well as a novel concept of spatially adaptive observation noise.  We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode (in the appendix) for inference and learning for all the basic models.  2 The Basic Model The basic models we will work with are discrete time linear dynamical systems with Gaussian noise.  In such models we assume that the state of the process in question can at any time be summarized by a k-vector of state variables or causes x which we cannot observe directly.  However, the system also produces at each time step an output or observable p-vector y to which we do have access.  The state x is assumed to evolve according to simple first-order Markov dynamics; each output vector y is generated from the current state by a simple linear observation process.  Both the state evolution and the observation processes are corrupted by additive Gaussian noise which is also hidden.  If we work with a continuous valued state variable x, the basic generative model can be written 1 as: x t+1 = Ax t +w t = Ax t +w ffl w ffl s N (0; Q) (1a) y t = Cx t + v t = Cx t + v ffl v ffl s N (0; R) (1b) where A is the k \Theta k state transition matrix and C is the p \Theta k observation, measurement, or generative matrix.  The k-vector w and p-vector v are random variables representing the state evolution and observation noises respectively which are independent of each other and of the values of x and y.  1 All vectors are column vectors.  To denote the transpose of a vector or matrix we use the notation x T .  The determinant of a matrix is denoted by jAj and matrix inversion by A\Gamma 1 .  The symbol s means "distributed according to".  A multivariate normal (Gaussian) distribution with mean and covariance matrix \Sigma is written as N (; \Sigma).  The same Gaussian evaluated at the point z is denoted N (; \Sigma) j z . 
On Applying Molecular Computation to the Data Encryption Standard| and Lipton described thepotential use of molecular computation in attacking the United States Data Encryption Standard (DES).  Here, we provideadescription of suchan attackusingthesticker model of molecular computation.  Our analysis suggests that such an attack might be mounted on a table-top machine, using approximately a gram of DNA and might succeed even in the presence of a large number of errors. 
Signal Reconstruction from Zero-Crossings| Abstract We present a method for recovering (to within a constant factor) periodic, octave band-limited signals given the times of the zero-crossings.  Recovery involves taking the singular-value decomposition of a size N \Theta 2M matrix, where N is the number of zero-crossings within one period and M is product of the octave bandwidth and the period length.  We also discuss approximate approaches which can be used to reconstruct aperiodic or very-long-period signals.  Our algorithm achieves an inversion of Logan's theorem in the case where such is possible.  1 Logan's Theorem Sampling theorems provide conditions under which continuous signals may be represented by countable sets of real numbers.  In the usual setting, we agree on a regular grid of time points and provide samples of the signal amplitude at those times.  The Nyquist-Shannon theorems tell us that a lowpass signal may be reconstructed exactly, so long as the times of samples are spaced at least as densely as half the period of the highest frequency.  However, we could also agree upon a regular grid of amplitude levels and provide the times at which the signal crossed those levels.  This has been dubbed implicit sampling by Bar-David [Bond and Cahn, 1958, Bar-David, 1974]; although there is no clear theory relating the frequency content of the signal and the number of levels required.  Logan's theorem [Logan, Jr. , 1977] addresses a special case of this general problem of signal reconstruction from level crossings.  It states that if a signal is band-limited to a single octave then the times of the zero crossings are sufficient to reconstruct the signal -to within a constant factor of course.  1 For Nyquist-style sampling (uniform 1 There are some additional important technical caveats.  The signal must also have in time), the reconstruction procedure is well known and involves simply convolving the given samples with a particular kernel (the sinc function) that performs the required interpolation.  However, to our knowledge no practical inversion scheme exists for Logan's theorem.  All strictly band-limited signals must be infinite in time.  Such signals are either periodic or aperiodic.  If an infinite signal is aperiodic, an infinite number of zero-crossing times must be measured and the signal itself requires an infinite amount of information to specify.  However, if an infinite signal is periodic, only a finite number of zero-crossing times (those within one period) need to be measured and the signal is entirely specified by one period.  Because Logan's paper also shows that signals with free zeros cannot be uniquely reconstructed from their zero-crossings, it is practical (and indeed meaningful) to consider reconstruction only of periodic signals with no free zeros.  In what follows, we address exactly this problem.  We present a method for recovering (to within a constant factor) periodic octave band-limited signals (with no free zeros) given the times of the zero-crossings within the period.  Recovery requires taking the singular-value decomposition of a size N \Theta 2M matrix, where N is the number of zero-crossings and M is product of the octave bandwidth and the period length.  We also discuss approximate approaches which can be used to reconstruct aperiodic or very-long-period signals. 
Learning Nonlinear Dynamical Systems using an EM Algorithm| Abstract The Expectation--Maximization (EM) algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables [2].  It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9].  We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems.  The "expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the "maximization" step re-estimates the parameters using these uncertain state estimates.  In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states.  However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.  1 Stochastic Nonlinear Dynamical Systems We examine inference and learning in discrete-time dynamical systems with hidden state x t , inputs u t , and outputs y t .  1 The state evolves according to stationary nonlinear dynamics driven by the inputs and by additive noise x t+1 = f(x t ; u t ) + w (1) 1 All lowercase characters (except indices) denote vectors.  Matrices are represented by uppercase characters.  where w is zero-mean Gaussian noise with covariance Q.  2 The outputs are nonlinearly related to the states and inputs by y t = g(x t ; u t ) + v (2) where v is zero-mean Gaussian noise with covariance R.  The vector-valued nonlinearities f and g are assumed to be differentiable, but otherwise arbitrary.  Models of this kind have been examined for decades in various communities.  Most notably, nonlinear state-space models form one of the cornerstones of modern systems and control engineering.  In this paper, we examine these models within the framework of probabilistic graphical models and derive a novel learning algorithm for them based on EM.  With one exception, 3 this is to the best of our knowledge the first paper addressing learning of stochastic nonlinear dynamical systems of the kind we have described within the framework of the EM algorithm.  The classical approach to system identification treats the parameters as hidden variables, and applies the Extended Kalman Filtering algorithm (described in section 2) to the nonlinear system with the state vector augmented by the parameters [5].  4 This approach is inherently on-line, which may be important in certain applications.  Furthermore, it provides an estimate of the covariance of the parameters at each time step.  In contrast, the EM algorithm we present is a batch algorithm and does not attempt to estimate the covariance of the parameters.  There are three important advantages the EM algorithm has over the classical approach.  First, the EM algorithm provides a straightforward and principled method for handing missing inputs or outputs.  Second, EM generalizes readily to more complex models with combinations of discrete and real-valued hidden variables.  For example, one can formulate EM for a mixture of nonlinear dynamical systems.  Third, whereas it is often very difficult to prove or analyze stability within the classical on-line approach, the EM algorithm is always attempting to maximize the likelihood, which acts as a Lyapunov function for stable learning.  In the next sections we will describe the basic components of the learning algorithm.  For the expectation step of the algorithm, we infer the conditional distribution of the hidden states using Extended Kalman Smoothing (section 2).  For the maximization step we first discuss the general case (section 3) and then describe the particular case where the nonlinearities are represented using Gaussian radial basis function (RBF; [6]) networks (section 4).  2 Extended Kalman Smoothing Given a system described by equations (1) and (2), we need to infer the hidden states from a history of observed inputs and outputs.  The quantity at the heart of this inference problem is the conditional density P (x t ju 1 ; : : : ; u T ; y 1 ; : : : ; y T ), for 1 t T , which captures the fact that the system is stochastic and therefore our inferences about x will be uncertain.  2 The Gaussian noise assumption is less restrictive for nonlinear systems than for linear systems since the nonlinearity can be used to generate non-Gaussian state noise.  3 The authors have just become aware that Briegel and Tresp (this volume) have applied EM to essentially the same model.  Briegel and Tresp's method uses multilayer perceptrons (MLP) to approximate the nonlinearities, and requires sampling from the hidden states to fit the MLP.  We use Gaussian radial basis functions (RBFs) to model the nonlinearities, which can be fit analytically without sampling (see section 4). 
An Introduction to Locally Linear Embedding| Abstract Many problems in information processing involve some form of dimensionality reduction.  Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data.  LLE attempts to discover nonlinear structure in high dimensional data by exploiting the local symmetries of linear reconstructions.  Notably, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations--though capable of generating highly nonlinear embeddings---do not involve local minima.  We illustrate the method on images of lips used in audiovisual speech synthesis. 
Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifold| Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data.  The data, assumed to lie on a nonlinear manifold, is mapped into a single global coordinate system of lower dimensionality.  The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem.  Notably, the optimizations in LLE--though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima.  We describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance.  The algorithm is applied to manifolds of known structure, as well as real data sets consisting of images of faces, digits, and lips.  We provide extensive illustrations of the algorithm's performance. 
EM Algorithms for PCA and SPCA| Abstract I present an expectation-maximization (EM) algorithm for principal component analysis (PCA).  The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data.  It is computationally very efficient in space and time.  It also naturally accommodates missing information.  I also introduce a new variant of PCA called sensible principal component analysis (SPCA) which defines a proper density model in the data space.  Learning for SPCA is also done with an EM algorithm.  I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions.  1 Why EM for PCA? Principal component analysis (PCA) is a widely used dimensionality reduction technique in data analysis.  Its popularity comes from three important properties.  First, it is the optimal (in terms of mean squared error) linear scheme for compressing a set of high dimensional vectors into a set of lower dimensional vectors and then reconstructing.  Second, the model parameters can be computed directly from the data -- for example by diagonalizing the sample covariance.  Third, compression and decompression are easy operations to perform given the model parameters -- they require only matrix multiplications.  Despite these attractive features however, PCA models have several shortcomings.  One is that naive methods for finding the principal component directions have trouble with high dimensional data or large numbers of datapoints.  Consider attempting to diagonalize the sample covariance matrix of # vectors in a space of # dimensions when # and # are several hundred or several thousand.  Difficulties can arise both in the form of computational complexity and also data scarcity.  1 Even computing the sample covariance itself is very costly,
Metric Learning by Collapsing Classes| Abstract We present an algorithm for learning a quadratic Gaussian metric (Mahalanobis distance) for use in classification tasks.  Our method relies on the simple geometric intuition that a good metric is one under which points in the same class are simultaneously near each other and far from points in different classes.  We construct a convex optimization problem which generates such metrics by trying to collapse all examples in the same class to a single point and push examples in other classes infinitely far away.  We show that when the metric we learn is used in a simple classifier, it yields substantial improvements over standard alternatives on a variety of problems; we also discuss how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space allowing more efficient classification with very little reduction in performance. 
Computing with Action Potentials| Abstract Most computational engineering based loosely on biology uses continuous variables to represent neural activity.  Yet most neurons communicate with action potentials.  The engineering view is equivalent to using a rate-code for representing information and for computing.  An increasing number of examples are being discovered in which biology may not be using rate codes.  Information can be represented using the timing of action potentials, and efficiently computed with in this representation.  The "analog match" problem of odour identification is a simple problem which can be efficiently solved using action potential timing and an underlying rhythm.  By using adapting units to effect a fundamental change of representation of a problem, we map the recognition of words (having uniform time-warp) in connected speech into the same analog match problem.  We describe the architecture and preliminary results of such a recognition system.  Using the fast events of biology in conjunction with an underlying rhythm is one way to overcome the limits of an eventdriven view of computation.  When the intrinsic hardware is much faster than the time scale of change of inputs, this approach can greatly increase the effective computation per unit time on a given quantity of hardware. 
Simultaneous Localization and Surveying with Multiple Agents| Abstract We apply constrained Hidden Markov Model architecture to the problem of simultaneous localization and surveying from sensor logs of mobile agents navigating in unknown environments.  We show the solution of this problem for the case of one robot and extend our model to the more interesting case of multiple agents, that interact with each other through proximity sensors.  Since exact learning in this case becomes exponentially expensive, we develop an approximate method for inference using loopy belief propagation and apply it to the localization and surveying problem with multiple interacting robots.  In support of our analysis, we report experimental results showing that with the same amount of data, approximate learning with the interaction signals outperforms exact learning ignoring interactions.  1 Constrained HMMs for Map Learning By identifying each state in a hidden Markov model with some small spatial region of a continuous space, it is possible to naturally define neighboring states as those which correspond to connected regions in the underlying space.  The transition matrix of the HMM can then be constrained to allow transitions only between neighbors; this means that all valid state sequences correspond to connected paths in the continuous space.  The transition matrix does not need to be explicitly stored or learned, it is merely computed by a function that respects the state topology; the remaining parameters of the model scale only linearly with the number of states.  We apply this constrained HMM architecture[10] to the problem of simultaneous localization and surveying from sensor logs of mobile agents navigating in unknown environments.  The surveying problem is distinct from the mapping problem: we are not trying to learn the occupancy grid of a world.  Rather, we are trying to learn the values that various sensors (e. g.  altitude, temperature, light level, beacon signals) take on as a function of position in the unknown environment.  Our problem is motivated by mobile planetary rovers, which generally operate on open plains, collect temporal histories of multiple sensors, and cannot rely on the odometry of self-locomotion because they are navigating extremely rough terrain, often not using conventional wheels.  Figure 1 shows the typical input to a single agent (robot) in the scenario we are studying.  Each robot moves through the environment under the control of an external navigation algorithm that we cannot influence.  As it proceeds, it logs readings from multiple noisy sensors, some of which may be smoothly varying and others of which may be intermittent and discontinuous.  Crucially, the agents can also detect each other when in close proximity.  No odometry or other information about navigational control signals (either intended
Linear Heteroencoders| Abstract This note gives a closed form expression for the linear transform computed by an optimally trained linear heteroencoder network of arbitrary topology trained to minimize squared error.  The transform can be thought of as a restricted rank version of the basic linear least-squares regression (discrete Wiener filter) between input and output.  The rank restriction is set by the "bottleneck" size of the network -- the minimum number of hidden units in any layer.  A special case of this expression is the well known result that linear autoencoders with a bottleneck of size r perform a transform equivalent to projecting into the subspace spanned by the first r principal components of the data.  This result eliminates the need to explicitly train linear heteroencoder networks. 
Multiple Alignment of Continuous Time Series| Abstract Multiple realizations of continuous-valued time series from a stochastic process often contain systematic variations in rate and amplitude.  To leverage the information contained in such noisy replicate sets, we need to align them in an appropriate way (for example, to allow the data to be properly combined by adaptive averaging).  We present the Continuous Profile Model (CPM), a generative model in which each observed time series is a non-uniformly subsampled version of a single latent trace, to which local rescaling and additive noise are applied.  After unsupervised training, the learned trace represents a canonical, high resolution fusion of all the replicates.  As well, an alignment in time and scale of each observation to this trace can be found by inference in the model.  We apply CPM to successfully align speech signals from multiple speakers and sets of Liquid Chromatography-Mass Spectrometry proteomic data. 
Global Coordination of Local Linear Models| Abstract High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. 
A Sticker-Based Model for DNA Computation| Abstract We introduce a new model of molecular computation that we call the sticker model.  Like many previous proposals it makes use of DNA strands as the physical substrate in which information is represented and of separation by hybridization as a central mechanism.  However, unlike previous models, the stickers model has a random access memory that requires no strand extension, uses no enzymes, and (at least in theory) its materials are reusable.  The paper describes computation under the stickers model and discusses possible means for physically implementing each operation.  We go on to propose a specific machine architecture for implementing the stickers model as a microprocessor-controlled parallel robotic workstation.  Finally, we discuss several methods for achieving acceptable overall error rates for a computation using basic operations that are error prone.  In the course of this development a number of previous general concerns about molecular computation [Smith, Hartmanis, Letters to Science] are addressed.  First, it is clear that generalpurpose algorithms can be implemented by DNA-based computers, potentially solving a wide class of search problems.  Second, we find that there are challenging problems, for which only modest volumes of DNA should suffice.  Third, we demonstrate that the formation and breaking of covalent bonds is not intrinsic to DNA-based computation.  This means that costly and shortlived materials such as enzymes are not necessary, nor are energetically costly processes such as PCR.  Fourth, we show that a single essential biotechnology, sequence-specific separation, suffices for constructing a general-purpose molecular computer.  Fifth, we illustrate that separation errors can theoretically be reduced to tolerable levels by invoking a trade-off between time, space, and error rates at the level of algorithm design; we also outline several specific ways in which this can be done and present numerical calculations of their performance. 
EM Algorithms for PCA and Sensible PCA| Abstract I present an expectation-maximization (EM) algorithm for principal component analysis (PCA).  The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data.  It is computationally ecient in space and time and does not require computing the sample covariance of the data.  It also naturally accommodates missing information.  I introduce a new variation of PCA known as sensible principal component analysis (SPCA) which defines a proper density model in the data space.  Learning for SPCA is also done with an EM algorithm.  I include results of simulations showing that, in only a few iterations, these EM algorithms correctly and eciently find the leading eigenvectors of the covariance of large datasets (having up to half a million datapoints in tens of thousands of dimensions).  1 Why EM for PCA? Principal component analysis (PCA) is a widely used dimensionality reduction technique in data analysis.  Its popularity comes from three important properties.  First, it is the optimal (in terms of mean squared error) linear scheme for compressing a set of high dimensional vectors into a set of lower dimensional vectors and then reconstructing.  Second, the model parameters can be computed directly from the data { for example by diagonalizing the sample covariance of the data.  Third, compression and decompression are easy operations to perform given the model parameters { they require only matrix multiplications.  Despite these attractive features, however, PCA models have several shortcomings.  One is that naive methods for finding the principal component directions have trouble with high dimensional data.  Consider attempting to diagonalize the sample covariance matrix of n vectors in a space of p dimensions when n and p are several hundred or several thousand.  Diculties can arise both in the form of computational complexity and also data scarcity.  On the data scarcity front, we often do not have enough data in high dimensions for the sample covariance to be of full rank and so we must be careful to employ techniques such as the singular value decomposition which do not require full rank matrices.  On the complexity front, direct diagonalization of a symmetric matrix thousands of rows in size can be extremely costly since this operation is O(p 3 ) for p # p inputs.  Fortunately, several techniques exist for reducing this cost when only the first few leading eigenvectors and eigenvalues are required (for example the power method [9] which is only O(p 2 )).  However, even computing the sample covariance itself is very costly, requiring O(np 2 ) operations.  In general it is best to avoid altogether computing the sample covariance explicitly.  Methods such as the snap-shot algorithm [8] do this by assuming that the eigenvectors being searched for are linear combinations of the datapoints; their complexity is O(n 3 ).  In this note, I present a version of the expectation-maximization (EM) algorithm [1] for learning the principal components of a dataset.  The algorithm does not require computing the sample covariance and has a complexity limited by O(knp) operations where k is the number of leading eigenvectors to be learned.  For large n and p I am not aware of any other methods which possess this attractive scaling.  Another shortcoming of standard approaches to PCA is that it is not obvious how to deal properly with missing data.  Most of the methods discussed above cannot accommodate missing values and so incomplete points must either be discarded or completed using a variety of ad-hoc interpolation methods.  (For example, a standard solution is to replace missing coordinates with the mean of the known values in that coordinate or with a linear regression of the unknown value from the known values. ) On the other hand, the EM algorithm for PCA enjoys all the benefits [4] of other EM algorithms in terms of estimating the maximum likelihood values for missing information directly at each iteration.  Finally, the PCA model itself suffers from a critical flaw which is independent of the technique used to compute its parameters: it does not define a proper probability model in the space of inputs.  This is because the density is not normalized within the principal subspace.  In other words, if we perform PCA on some data and then ask how well new data are fit by the model, the only criterion available is the squared distance of the new data from their projections into the principal subspace.  A datapoint far away from the training data but nonetheless near the principal subspace will be assigned a high \pseudo-likelihood" or low error (see figure 1).  Similarly, it is not possible to generate \fantasy" data from a PCA model.  In this note I introduce a new model called sensible principal component analysis (SPCA), an obvious modification of PCA which does define a proper covariance structure in the data space.  Its parameters can also be learned with an EM algorithm, given below.  Figure 1: PCA does not define a proper density model in the data space.  The only measure of how well new data fits the model is the distance from the principal subspace; thus points far away from the bulk of the data but nonetheless near the principal subspace will have low reconstruction error.  In the figure, the two points denoted with X have the same reconstruction error.  2 Whence EM for PCA? Principal component analysis can be viewed as a limiting case of a particular class of linear-Gaussian models.  The goal of such models is to capture the covariance structure of an observed p-dimensional variable y using fewer than the p(p + 1)=2 free parameters required in a full covariance matrix.  Linear-Gaussian models do this by assuming that y was produced as a linear transformation of some k-dimensional latent variable x plus additive Gaussian noise.  Denoting the transformation by
TOWARDS ARTICULATORY SPEECH RECOGNITION: LEARNING SMOOTH MAPS TO RECOVER ARTICULATOR INFORMATION| Abstract We present a novel method for recovering articulator movements from speech acoustics based on a constrained form [9] of a hidden Markov model.  The model attempts to explain sequences of high dimensional data using smooth and slow trajectories in a latent variable space.  The key insight is that this continuity constraint when applied to speech helps to solve the \ill-posed" problem of acoustic to articulatory mapping.  By working with sequences of spectra rather than looking only at individual spectra, it is possible to choose between competing articulatory configurations for any given spectrum by selecting the configuration \closest" to those at nearby times.  We present results of applying this algorithm to recover articulator movements from acoustics using data from the Wisconsin X-ray microbeam project [3].  We find that the recovered traces are highly correlated with the measured articulator movements under a single linear transform.  Such recovered traces have the potential to be used for speech recognition, an application we are currently investigating. 
A SEGMENT-BASED PROBABILISTIC GENERATIVE MODEL OF SPEECH| ABSTRACT We present a purely time domain approach to speech processing which identifies waveform samples at the boundaries between glottal pulse periods (in voiced speech) or at the boundaries of unvoiced segments.  An efficient algorithm for inferring these boundaries and estimating the average spectra of voiced and unvoiced regions is derived from a simple probabilistic generative model.  Competitive results are presented on pitch tracking, voiced/unvoiced detection and timescale modification; all these tasks and several others can be performed using the single segmentation provided by inference in the model. 
Learning Nonlinear Dynamical Systems Using an EM Algorithm| Abstract The Expectation{Maximization (EM) algorithm is an iterativeprocedure for maximum likelihood parameter estimation from data sets with missing or hidden variables [2].  It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9].  We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems.  The \expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the \maximization" step re-estimates the parameters using these uncertain state estimates.  In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states.  However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.  1 Stochastic Nonlinear Dynamical Systems We examine inference and learning in discrete-time dynamical systems with hidden state x t , inputs u t , and outputs y t .  1 The state evolves according to stationary nonlinear dynamics driven by the inputs and by additive noise x t+1 = f(x t ;u t )+w (1) 1 All lowercase characters (except indices) denote vectors.  Matrices are represented by uppercase characters.  where w is zero-mean Gaussian noise with covariance Q.  2 The outputs are nonlinearly related to the states and inputs by y t = g(x t ;u t )+v (2) where v is zero-mean Gaussian noise with covariance R.  The vector-valued nonlinearities f and g are assumed to be differentiable, but otherwise arbitrary.  Models of this kind have been examined for decades in various communities.  Most notably, nonlinear state-space models form one of the cornerstones of modern systems and control engineering.  In this paper, we examine these models within the framework of probabilistic graphical models and deriveanovel learning algorithm for them based on EM.  With one exception, 3 this is to the best of our knowledge the first paper addressing learning of stochastic nonlinear dynamical systems of the kind wehave described within the framework of the EM algorithm.  The classical approach to system identification treats the parameters as hidden variables, and applies the Extended Kalman Filtering algorithm (described in section 2) to the nonlinear system with the state vector augmented by the parameters [5].  4 This approach is inherently on-line, whichmay be important in certain applications.  Furthermore, it provides an estimate of the covariance of the parameters at each time step.  In contrast, the EM algorithm we presentisabatch algorithm and does not attempt to estimate the covariance of the parameters.  There are three important advantages the EM algorithm has over the classical approach.  First, the EM algorithm provides a straightforward and principled method for handing missing inputs or outputs.  Second, EM generalizes readily to more complex models with combinations of discrete and real-valued hidden variables.  For example, one can formulate EM for a mixture of nonlinear dynamical systems.  Third, whereas it is often very difficult to prove or analyze stability within the classical on-line approach, the EM algorithm is always attempting to maximize the likelihood, which acts as a Lyapunov function for stable learning.  In the next sections we will describe the basic components of the learning algorithm.  For the expectation step of the algorithm, we infer the conditional distribution of the hidden states using Extended Kalman Smoothing (section 2).  For the maximization step we#rstdiscuss the general case (section 3) and then describe the particular case where the nonlinearities are represented using Gaussian radial basis function (RBF; [6]) networks (section 4).  2 Extended Kalman Smoothing Given a system described by equations (1) and (2), we need to infer the hidden states from a history of observed inputs and outputs.  The quantity at the heart of this inference problem is the conditional density P (x t ju 1 ;:::;u T ;y 1 ;:::;y T ), for 1 # t # T,which captures the fact that the system is stochastic and therefore our inferences about x will be uncertain.  2 The Gaussian noise assumption is less restrictive for nonlinear systems than for linear systems since the nonlinearity can be used to generate non-Gaussian state noise.  3 The authors have just become aware that Briegel and Tresp (this volume) have applied EM to essentially the same model.  Briegel and Tresp's method uses multilayer perceptrons (MLP) to approximate the nonlinearities, and requires sampling from the hidden states to fit the MLP. We use Gaussian radial basis functions (RBFs) to model the nonlinearities, which can be fit analytically without sampling (see section 4). 
Neighbourhood Components Analysis| Abstract In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm.  The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set.  It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification.  Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them.  The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction. 
An EM Algorithm for Identification of Nonlinear Dynamical Systems| Abstract| We provide a novel solution to the problem of simultaneously estimating the unknown parameters and hidden states of a nonlinear dynamical system.  Our solution is based on the expectation{maximization (EM) algorithm, an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables [1].  EM has been applied to system identification in linear statespace models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [2], [3], [4].  Here we generalize the EM algorithm to estimate parameters of nonlinear dynamical state-space models.  The \expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the \maximization" step re-estimates the parameters using these uncertain state estimates.  In general, the nonlinear maximization step is dicult because it requires integrating out the uncertainty in the states.  However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.  We derive an online version of this EM-EKS algorithm, as well as a version for non-stationary time series.  We also consider the identifiability and expressive power of nonlinear dynamical systems and relate our learning algorithm with more traditional system identi#cation procedures based on dual and joint Extended Kalman Filtering.  Finally, we demonstrate our algorithm on several synthetic problems and one real time series.  I.  Learning stochastic nonlinear dynamics S INCE the advent of cybernetics, dynamical systems have been an important modeling tool in fields ranging from engineering to the physical and social sciences.  Most realistic dynamical systems models have two essential features.  First, they are stochastic|the observed outputs are a noisy function of the inputs, and the dynamics itself may be driven by some unobserved noise process.  Second, they can be characterized by some finite-dimensional internal state which, while not directly observable, summarizes at any time all information about the past behaviour of the process relevant to predicting its future evolution.  From a modeling standpoint, stochasticity is essential to allow a model with a few fixed parameters to generate a rich variety of time-series outputs.  1 Explicitly modeling the internal state makes it possible to decouple the internal dynamics from the observation process.  For example, to model a sequence of video images of a balloon floating
Automatic Alignment of Local Representations| Abstract We present an automatic alignment procedure which maps the disparate internal representations learned by several local dimensionality reduction experts into a single, coherent global coordinate system for the original data space.  Our algorithm can be applied to any set of experts, each of which produces a low-dimensional local representation of a highdimensional input.  Unlike recent efforts to coordinate such models by modifying their objective functions [1, 2], our algorithm is invoked after training and applies an efficient eigensolver to post-process the trained models.  The post-processing has no local optima and the size of the system it must solve scales with the number of local models rather than the number of original data points, making it more efficient than model-free algorithms such as Isomap [3] or LLE [4]. 
Relationship between gradient and EM steps in latent variable models| Abstract We present a close relationship between Expectation - Maximization algorithm and direct optimization approaches such as gradient-based methods for parameter learning.  We show that the step EM takes in the parameter space and true gradient are related by the symmetric positive definite P matrix, and provide an explicit form of this matrix for several widely used latent variable models.  We then go on deriving a general form of the P matrix for the regular exponential family in terms of its natural parameters. 
Probabilistic Inference of Speech Signals from Phaseless Spectrograms| Abstract Many techniques for complex speech processing such as denoising and deconvolution, time/frequency warping, multiple speaker separation, and multiple microphone analysis operate on sequences of short-time power spectra (spectrograms), a representation which is often well-suited to these tasks.  However, a significant problem with algorithms that manipulate spectrograms is that the output spectrogram does not include a phase component, which is needed to create a time-domain signal that has good perceptual quality.  Here we describe a generative model of time-domain speech signals and their spectrograms, and show how an efficient optimizer can be used to find the maximum a posteriori speech signal, given the spectrogram.  In contrast to techniques that alternate between estimating the phase and a spectrally-consistent signal, our technique directly infers the speech signal, thus jointly optimizing the phase and a spectrally-consistent signal.  We compare our technique with a standard method using signal-to-noise ratios, but we also provide audio files on the web for the purpose of demonstrating the improvement in perceptual quality that our technique offers. 
On the Convergence of Bound Optimization Algorithms| Abstract Many practitioners who use EM and related
Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models| Abstract We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations.  This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM).  An update begins with the creation of "pools" of candidate states at each time.  We then define an embedded HMM whose states are indexes within these pools.  Using a forward-backward dynamic programming algorithm, we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools.  We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error.  We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 
IS EM REALLY BETTER?| Abstract Unsupervised learning plays an important role in many areas of machine learning.  Two major research fields in unsupervied learning are density estimation, where one seeks to find a descriptive model of data, and dimensionality reduction, where one tries to discover a compact representation of data.  Both of those classic problems often involve fitting models with unobserved or latent variables.  A commonly used technique for Maximum Likelihood Learning of model parameters in the presence of latent variables is Expectation-Maximization (EM) algorithm We propose a new approach for Maximum Likelihood Estimation of the model parameters using the method of conjugate gradients, which we call the Expectation Conjuagte Gradient (ECG) algorithm.  The key idea is that is it possible to find exact gradients of the likelihood function in many latent variable models.  Computing the gradient often involves finding the posterior distribution over latent variables, which corresponds to the E-step of the EM algorithm.  The goal of the present project is to compare the performance and convergence properties of EM and ECG as optimization techniques; 3 both of which require inference as a subroutine as well as to study the relationship between EM and gradient based algorithms.  General Framework A lot of probablistic models have unobserved or hidden variables.  Hidden variables are usually introduced to help model correlations in observed variables.  .  Let X ) observed random variable Z ) hidden, or latent variable. 
Nonlinear dimensionality reduction by locally linear embedding|
A sticker based architecture for DNA computation|
"Nonlinear dimensionality analysis by locally linear embedding,"|
EM algorthms for PCA and SPCA|
NIPS abstracts, 1987--|
"Nonlinear dimensionality reduction by locally linear embedding,"|
Neighbourhood component analysis|
Automatic alignment of hidden representations|
Locally linear embedding|
A Sticker Based Architecture for DNA Computation, 2nd Annual Meeting on DNA Based Computers,|
Chapter 6: An EM algorithm for identification of nonlinear dynamical systems|
Optimisation with EM and Expectation-Conjugate-Gradient,|
"Gaussian identities,"|
Neural Information Processing|
Global coordination of linear models|
"Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode," in Research Report #24, Large Vocabulary Continuous Speech Recognition Workshop Technical Report Series|
An EM algorithm for indentification of nonlinear dynamical systems|
EM algorithm for pca and spca|
Model systematic variations in pronunciation via a language-dependent hidden speaking mode|
Data driven production models for speech processing|
Speaker recognition system|
n em algorithm for identification of nonlinear dynamical systems|
Levenberg-marquardt optimization|
Learning nonlinear dynamical systems using the expectationmaximization algorithm|
LLE Code Page| Internet URL. 

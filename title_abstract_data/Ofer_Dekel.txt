Multiclass Learning by Probabilistic Embeddings| Abstract We describe a new algorithmic framework for learning multiclass categorization problems.  In this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space.  In this space each instance is assigned the label it is nearest to.  We outline and analyze an algorithm, termed Bunching, for learning the pair of embeddings from labeled data.  A key construction in the analysis of the algorithm is the notion of probabilistic output codes, a generalization of error correcting output codes (ECOC).  Furthermore, the method of multiclass categorization using ECOC is shown to be an instance of Bunching.  We demonstrate the advantage of Bunching over ECOC by comparing their performance on numerous categorization problems. 
Large margin hierarchical classification| Abstract We present an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure.  This structure is encoded by a rooted tree which induces a metric over the label set.  Our approach combines ideas from large margin kernel methods and Bayesian analysis.  Following the large margin principle, we associate a prototype with each label in the tree and formulate the learning task as an optimization problem with varying margin constraints.  In the spirit of Bayesian methods, we impose similarity requirements between the prototypes corresponding to adjacent labels in the hierarchy.  We describe new online and batch algorithms for solving the constrained optimization problem.  We derive a worst case loss-bound for the online algorithm and provide generalization analysis for its batch counterpart.  We demonstrate the merits of our approach with a series of experiments on synthetic, text and speech data. 
The Forgetron: A Kernel-Based Perceptron on a Fixed Budget| Abstract The Perceptron algorithm, despite its simplicity, often performs well in online classification tasks.  The Perceptron becomes especially effective when it is used in conjunction with kernels.  However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly.  In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget.  To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores and, on the other hand, entertains a relative mistake bound.  In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach. 
The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees| Abstract Prediction suffix trees (PST) provide a popular and effective tool for tasks such as compression, classification, and language modeling.  In this paper we take a decision theoretic view of PSTs.  Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a mistake bound for it.  We then describe a self-bounded enhancement of our learning algorithm for which the learning process automatically grows a bounded-depth PST.  We also prove a similar mistake-bound for the self-bounded algorithm.  The result is an efficient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters.  To our knowledge, this is the first provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any fixed PST determined in hindsight. 
Log-Linear Models for Label Ranking| Abstract Label ranking is the task of inferring a total order over a predefined set of labels for each given instance.  We present a general framework for batch learning of label ranking functions from supervised data.  We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent.  This enables us to accommodate a variety of ranking problems.  In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels.  Special cases of our setting are multilabel categorization and hierarchical classification.  We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration.  The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus. 
Smooth "-Insensitive Regression by Loss Symmetrization| Abstract We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions.  These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss.  The resulting symmetric logistic loss can be viewed as a smooth approximation to the "-insensitive hinge loss used in support vector regression.  We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses.  The first family employs an iterative log-additive update which provides a regression counterpart for recent boosting algorithms.  The second family utilizes an iterative additive update step.  We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the logistic loss.  A byproduct of our work is a new simple form of regularization for boosting-based classification and regression algorithms.  Our regression framework also has implications on classification algorithms, namely, a new additive batch algorithm for the log-loss and exp-loss used in boosting.  We demonstrate the merits of our algorithms in a series of experiments including an experiment that boosts the accuracy of support vector regressors on a benchmark dataset. 
Data-Driven Online to Batch Conversions| Abstract Online learning algorithms are typically fast, memory efficient, and simple to implement.  However, many common learning problems fit more naturally in the batch learning setting.  The power of online learning algorithms can be exploited in batch settings by using online-to-batch conversions, techniques which build a new batch algorithm from an existing online algorithm.  We first give a unified overview of three existing online-to-batch conversion techniques which do not use training data in the conversion process.  We then build upon these data-independent conversions to derive and analyze data-driven conversions.  Our conversions find hypotheses with a small risk by explicitly minimizing datadependent generalization bounds.  We experimentally demonstrate the usefulness of our approach, and in particular show that the data-driven conversions consistently outperform the data-independent conversions. 
Smooth e-Intensive Regression by Loss Symmetrization|

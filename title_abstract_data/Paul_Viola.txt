Alignment by Maximization of Mutual Information International Journal of Computer Vision,| Abstract A new information-theoretic approach is presented for finding the pose of an object in an image.  The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination.  In our derivation few assumptions are made about the nature of the imaging process.  As a result the algorithms are quite general and may foreseeably be used in a wide variety of imaging situations.  Experiments are presented that demonstrate the approach registering magnetic resonance (MR) images, aligning a complex 3D object model to real scenes including clutter and occlusion, tracking a human head in a video sequence and aligning a view-based 2D object model to real images.  The method is based on a formulation of the mutual information between the model and the image.  As applied here the technique is intensity-based, rather than feature-based.  It works well in domains where edge or gradient-magnitude based methods have difficulty, yet it is more robust than traditional correlation.  Additionally, it has an efficient implementation that is based on stochastic approximation. 
Learning Informative Statistics: A Nonparametric Approach| Abstract We discuss an information theoretic approach for categorizing and modeling dynamic processes.  The approach can learn a compact and informative statistic which summarizes past states to predict future observations.  Furthermore, the uncertainty of the prediction is characterized nonparametrically by a joint density over the learned statistic and present observation.  We discuss the application of the technique to both noise driven dynamical systems and random processes sampled from a density which is conditioned on the past.  In the first case we show results in which both the dynamics of random walk and the statistics of the driving noise are captured.  In the second case we present results in which a summarizing statistic is learned on noisy random telegraph waves with differing dependencies on past states.  In both cases the algorithm yields a principled approach for discriminating processes with differing dynamics and/or dependencies.  The method is grounded in ideas from information theory and nonparametric statistics. 
Learning from One Example in Machine Vision by Sharing Probability Densities| Abstract Human beings exhibit rapid learning when presented with a small number of images of a new object.  A person can identify an object under a wide variety of visual conditions after having seen only a single example of that object.  This ability can be partly explained by the application of previously learned statistical knowledge to a new setting.  This thesis presents an approach to acquiring knowledge in one setting and using it in another.  Specifically, we develop probability densities over common image changes.  Given a single image of a new object and a model of change learned from a different object, we form a model of the new object that can be used for synthesis, classification, and other visual tasks.  We start by modeling spatial changes.  We develop a framework for learning statistical knowledge of spatial transformations in one task and using that knowledge in a new task.  By sharing a probability density over spatial transformations learned from a sample of handwritten letters, we develop a handwritten digit classifier that achieves 88. 6% accuracy using only a single hand-picked training example from each class.  The classification scheme includes a new algorithm, congealing, for the joint alignment of a set of images using an entropy minimization criterion.  We investigate properties of this algorithm and compare it to other methods of addressing spatial variability in images.  We illustrate its application to binary images, gray-scale images, and a set of 3-D neonatal magnetic resonance brain volumes.  Next, we extend the method of change modeling from spatial transformations to color transformations.  By measuring statistically common joint color changes of a scene in an office environment, and then applying standard statistical techniques such as principal components analysis, we develop a probabilistic model of color change.  We show that these color changes, which we call color flows, can be shared effectively between certain types of scenes.  That is, a probability density over color change developed by observing one scene can provide useful information about the variability of another scene.  We demonstrate a variety of applications including image synthesis, image matching, and shadow detection. 
Learning Joint Statistical Models for Audio-Visual Fusion and Segregation| Abstract People can understand complex auditory and visual information, often using one to disambiguate the other.  Automated analysis, even at a lowlevel, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates.  Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships.  We learn the joint distribution of the visual and auditory signals using a non-parametric approach.  First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation.  We then model the complicated stochastic relationships between the signals using a nonparametric density estimator.  These learned densities allow processing across signal modalities.  We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video. 
Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade| Abstract This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e. g.  face detection or database retrieval).  In such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final detector with many desirable features: including high detection rates, very low false positive rates, and fast performance.  Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms.  We propose a new variant of AdaBoost as a mechanism for training the simple classifiers used in the cascade.  Experimental results in the domain of face detection show the training algorithm yields significant improvements in performance over conventional AdaBoost.  The final face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000. 
Fast Multi-view Face Detection| Abstract This paper extends the face detection framework proposed by Viola and Jones 2001 to handle profile views and rotated faces.  As in the work of Rowley et al 1998.  and Schneiderman et al.  2000, we build different detectors for different views of the face.  A decision tree is then trained to determine the viewpoint class (such as right profile or rotated 60 degrees) for a given window of the image being examined.  This is similar to the approach of Rowley et al.  1998.  The appropriate detector for that viewpoint can then be run instead of running all detectors on all windows.  This technique yields good results and maintains the speed advantage of the Viola-Jones detector. 
Unsupervised Improvement of Visual Detectors using Co-Training| Abstract One significant challenge in the construction of visual detection systems is the acquisition of sufficient labeled data.  This paper describes a new technique for training visual detectors which requires only a small quantity of labeled data, and then uses unlabeled data to improve performance over time.  Unsupervised improvement is based on the cotraining framework of Blum and Mitchell, in which two disparate classifiers are trained simultaneously.  Unlabeled examples which are confidently labeled by one classifier are added, with labels, to the training set of the other classifier.  Experiments are presented on the realistic task of automobile detection in roadway surveillance video.  In this application, co-training reduces the false positive rate by a factor of 2 to 11 from the classifier trained with labeled data alone. 
Empirical Entropy Manipulation for Real-World Problems| Abstract No finite sample is sufficient to determine the density, and therefore the entropy, of a signal directly. 
Robust Real-Time Face Detection| Abstract.  This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates.  There are three key contributions.  The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly.  The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features.  The third contribution is a method for combining classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions.  A set of experiments in the domain of face detection is presented.  The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al. , 1998; Schneiderman and Kanade, 2000; Roth et al. , 2000).  Implemented on a conventional desktop, face detection proceeds at 15 frames per second. 
SECOND INTERNATIONAL WORKSHOP ON STATISTICAL AND COMPUTATIONAL THEORIES OF| Abstract This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates.  There are three key contributions.  The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly.  The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6].  The third contribution is a method for combining classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions.  A set of experiments in the domain of face detection are presented.  The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1].  Implemented on a conventional desktop, face detection proceeds at 15 frames per second. 
Fast Pose Estimation with Parameter-Sensitive Hashing| m a s s a c h u s e t t s i n s t i t u t e o f t e c h n o l o g y , c a m b r i d g e , m a 0 2 1 3 9 u s a --- www.  a i .  m i t .  e d u m a s s a c hu s e t t s i n s t i t u t e o f t e c hno l o g y --- a r t i f i c i a l i n t e l l i g e n c e l a bo r a t o r y @ MIT Abstract Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low.  For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly becme prohibitively high.  We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task.  Our algorithm extends a recently developed method for locality-sensitive hashing, which finds approximate neighbors in time sublinear in the number of examples.  This method depends critically on the choice of hash functions; we show how to find the set of hash functions that are optimally relevant to a particular estimation problem.  Experiments demonstrate that the resulting algorithm, which we call Parameter-Sensitive Hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images. 
Boosting Image Retrieval| Abstract We present an approach for image retrieval using a very large number of highly selective features and efficient online learning.  Our approach is predicated on the assumption that each image is generated by a sparse set of visual "causes" and that images which are visually similar share causes.  We propose a mechanism for computing a very large number of highly selective features which capture some aspects of this causal structure (in our implementation there are over 45,000 highly selective features).  At query time a user selects a few example images, and a technique known as "boosting" is used to learn a classification function in this feature space.  By construction, the boosting procedure learns a simple classifier which only relies on 20 of the features.  As a result a very large database of images can be scanned rapidly, perhaps a million images per second.  Finally we will describe a set of experiments performed using our retrieval system on a database of 3000 images. 
Restructuring Sparse High Dimensional Data for Effective Retrieval| Abstract The task in text retrieval is to find the subset of a collection of documents relevant to a user's information request, usually expressed as a set of words.  Classically, documents and queries are represented as vectors of word counts.  In its simplest form, relevance is defined to be the dot product between a document and a query vector--a measure of the number of common terms.  A central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query.  Linear dimensionality reduction has been proposed as a technique for extracting underlying structure from the document collection.  In some domains (such as vision) dimensionality reduction reduces computational complexity.  In text retrieval it is more often used to improve retrieval performance.  We propose an alternative and novel technique that produces sparse representations constructed from sets of highly-related words.  Documents and queries are represented by their distance to these sets, and relevance is measured by the number of common clusters.  This technique significantly improves retrieval performance, is efficient to compute and shares properties with the optimal linear projection operator and the independent components of documents. 
Restructuring Sparse High Dimensional Data for Effective Retrieval| Abstract The task in text retrieval is to find the subset of a collection of documents relevant to a user's information request, usually expressed as a set of words.  Classically, documents and queries are represented as vectors of word counts.  In its simplest form, relevance is defined to be the dot product between a document and a query vector--a measure of the number of common terms.  A central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query.  Linear dimensionality reduction has been proposed as a technique for extracting underlying structure from the document collection.  In some domains (such as vision) dimensionality reduction reduces computational complexity.  In text retrieval it is more often used to improve retrieval performance.  We propose an alternative and novel technique that produces sparse representations constructed from sets of highly-related words.  Documents and queries are represented by their distance to these sets.  and relevance is measured by the number of common clusters.  This technique significantly improves retrieval performance, is efficient to compute and shares properties with the optimal linear projection operator and the independent components of documents. 
Exact Voxel Occupancy with Graph Cuts| Abstract Voxel occupancy is one approach for reconstructing the 3-dimensional shape of an object from multiple views.  In voxel occupancy, the task is to produce a binary labeling of a set of voxels, that determines which voxels are filled and which are empty.  In this paper, we give an energy minimization formulation of the voxel occupancy problem.  The global minimum of this energy can be rapidly computed with a single graph cut, using a result due to Greig, Porteous and Seheult [7].  The energy function we minimize contains a data term and a smoothness term.  The data term is a sum over the individual voxels, where the penalty for a voxel is based on the observed intensities of the pixels that intersect it.  The smoothness term is the number of empty voxels adjacent to filled ones.  Our formulation can be viewed as a generalization of silhouette intersection, with two advantages: we do not compute silhouettes, which are a major source of errors; and we can naturally incorporate spatial smoothness.  We give experimental results showing reconstructions from both real and synthetic imagery.  Reconstruction using this smoothed energy function is not much more time consuming than simple silhouette intersection; it takes about 10 seconds to reconstruct a one million voxel volume. 
Learning from One Example through Shared Densities on Transforms| Abstract We define a process called congealing in which elements of a dataset (images) are brought into correspondence with each other jointly, producing a data-defined model.  It is based upon minimizing the summed component-wise (pixelwise) entropies over a continuous set of transforms on the data.  One of the biproducts of this minimization is a set of transforms, one associated with each original training sample.  We then demonstrate a procedure for effectively bringing test data into correspondence with the data-defined model produced in the congealing process.  Subsequently, we develop a probability density over the set of transforms that arose from the congealing process.  We suggest that this density over transforms may be shared by many classes, and demonstrate how using this density as "prior knowledge" can be used to develop a classifier based on only a single training example for each class. 
Learning Silhouette Features for Control of Human Motion| Abstract We present a vision-based performance interface for controlling animated human characters.  The system combines information about the user's motion contained in silhouettes from several viewpoints with domain knowledge contained in a motion capture database to interactively produce a high quality animation.  Such an interactive system will be useful for authoring, teleconferencing, or as a control interface for a character in a game.  In our implementation, the user performs in front of three video cameras; the resulting silhouettes are used to estimate his orientation and body configuration based on a set of discriminative local features.  Those features are selected by a machine learning algorithm during a preprocessing step.  Sequences of motions that approximate the user's actions are extracted from the motion database and scaled in time to match the speed of the user's motion.  We use swing dancing, an example of complex human motion, to demonstrate the effectiveness of our approach.  We compare the results obtained with our approach to those obtained with a set of global features, Hu moments, and to ground truth measurements from a motion capture system.  Figure 1: A dancing user drives the motion of two animated characters. 
Complex Feature Recognition: A Bayesian Approach for Learning to Recognize Objects| Abstract We have developed a new Bayesian framework for visual object recognition which is based on the insight that images of objects can be modeled as a conjunction of local features.  This framework can be used to both derive an object recognition algorithm and an algorithm for learning the features themselves.  The overall approach, called complex feature recognition or CFR, is unique for several reasons: it is broadly applicable to a wide range of object types, it makes constructing object models easy, it is capable of identifying either the class or the identity of an object, and it is computationally efficient -- requiring time proportional to the size of the image.  Instead of a single simple feature such as an edge, CFR uses a large set of complex features that are learned from experience with model objects.  The response of a single complex feature contains much more class information than does a single edge.  This significantly reduces the number of possible correspondences between the model and the image.  In addition, CFR takes advantage of a type of image processing called oriented energy.  Oriented energy is used to efficiently pre-process the image to eliminate some of the difficulties associated with changes in lighting and pose. 
Image Guided Microscopic Surgery System Using Mutual-Information Based Registraion| Abstract.  We have developed an image guided microscopic surgery system for the navigation of surgical procedures that can overlay renderings of anatomical structures that are otherwise invisible.  A new histogram-based mutual information maximization technique was applied for alignment of the scope view and three-dimensional computer graphics model.  This technique doesn't require any pre-processing nor marker setting but is directly applied to the microscope view and the graphics rendering.  Therefore, any special set up in image scanning or preoperative preparation is not necessary.  Graphics technique were implemented to compute three-dimensional scene information by using a graphics accelerator, increasing the algorithm's performance significantly.  Experiments are presented that demonstrate the approach registering a plastic skull to its three-dimensional reconstruction model generated from a CT scan.  The tracking performance in the experiments were nearly real-time. 
Interactive Information Extraction with Constrained Conditional Random Fields| Abstract Information Extraction methods can be used to automatically "fill-in" database forms from unstructured data such as Web documents or email.  State-of-the-art methods have achieved low error rates but invariably make a number of errors.  The goal of an interactive information extraction system is to assist the user in filling in database fields while giving the user confidence in the integrity of the data.  The user is presented with an interactive interface that allows both the rapid verification of automatic field assignments and the correction of errors.  In cases where there are multiple errors, our system takes into account user corrections, and immediately propagates these constraints such that other fields are often corrected automatically.  Linear-chain conditional random fields (
Alignment by Maximization of Mutual Information| Abstract A new approachispresented for finding the pose of an object model in an image.  The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination.  In our derivation, few assumptions are made about the nature of the imaging process.  As a result the algorithms are quite general and can forseeably be used in a wide variety of imaging situations.  Experiments are presented that demonstrate the approach registering MR images, aligning a smooth 3D object model to images, aligning a complex 3D object model to real scenes including clutter and occlusion, and aligning a view-based 2D object model to real images.  The method is based on a new formulation of the mutual information between the model and the image.  As applied here the technique is intensity-based, rather than feature-based.  It works well in domains where edge or gradient-magnitude based methods have difficulty,yet it is more robust than traditional correlation.  Additionally, it has an efficient implementation. 
Rapid Object Detection using a Boosted Cascade of Simple Features| Abstract This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates.  This work is distinguished by three key contributions.  The first is the introduction of a new image representation called the Integral Image which allows the features used by our detector to be computed very quickly.  The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6].  The third contribution is a method for combining increasingly more complex classifiers in a cascade which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions.  The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest.  In the domain of face detection the system yields detection rates comparable to the best previous systems.  Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection. 
Texture Recognition Using a Non-parametric Multi-Scale Statistical Model| Abstract We describe a technique for using the joint occurrence of local features at multiple resolutions to measure the similarity between texture images.  Though superficially similar to a number of "Gabor" style techniques, which recognize textures through the extraction of multi-scale feature vectors, our approach is derived from an accurate generative model of texture, which is explicitly multiscale and non-parametric.  The resulting recognition procedure is similarly non-parametric, and can model complex non-homogeneous textures.  We report results on publicly available texture databases.  In addition, experiments indicate that this approach may have sufficient discrimination power to perform target detection in synthetic aperture radar images (SAR). 
Detecting Pedestrians Using Patterns of Motion and Appearance| Abstract This paper describes a pedestrian detection system that integrates image intensity information with motion information.  We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence.  The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person.  Past approaches have built detectors based on motion information or detectors based on appearance information, but ours is the first to combine both sources of information in a single detector.  The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20x15 pixels), and has a very low false positive rate.  Our approach builds on the detection work of Viola and Jones.  Novel contributions of this paper include: i) development of a representation of image motion which is extremely efficient, and ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow). 
Roxels: Responsibility Weighted 3D Volume Reconstruction|
Structure Driven Image Database Retrieval|
Alignment by Maximisation of Mutual Information|
A Cluster-based Statistical Model for Object Detection|
Poxels: Probabilistic voxelized volume reconstruction|
Mobile robot evolution|
Cooperative Control of a SemiAutonomous Mobile Robot|
Rapid object detection using a boosted cascade of sample features|
Variable Viewpoint Reality,|
Asymmetric AdaBoost and a detector cascade"|
Network Based Autonomous Robot Motor Control: from Hormones to Learning",|
MIMIC: Finding Optima by Estimating Probability Densities|
Alignment and Tracking using Graphics Hardware|
A Non-Parametric Multi-Scale Statistical Model for Natural Images|
From TCR engagement to T cell activation: A kinetic view of T cell behavior|
T cell activation determined by T cell receptor number and tunable thresholds|
A Unified Learning Framework for Real Time Face Detection and Classification|
Distinct kinetics of cytokine production and cytolysis in effector and memory T cells after viral infection|
Aligment by Maximization of Mutual Information|
III, "Alignment by maximization of mutual information,"|
Adaptive Gaze Control",|
Feature-based recognition of objects|
Neurally Inspired Plasticity in Oculomotor Processes|
A Forest of Sensors" DARPA - VSAM workshop,|
Texture Recognition Using a Non-Parametric Multi-Scale Statistical Model|
Mutual information: An approach for the registration of object models and images|
Detecting Pedestrians using Patterns of Motion Appearance"|
T lymphocyte costimulation mediated by reorganization of membrane microdomains|
Rollout-based policy retraining| Artificial Intelligence Laboratory, MIT,. 
Boosting Image Retrieval", CVPR00,|
Bayesian Model of Surface Perception|
Face recognition using boosted local features|
iRestructuring sparse high dimensional data for eoeective retrieval,j|
Ambiguity and Constraint in Mathematical Expression Recognition|
Approximate nearest neighbors methods for learning and vision|
Alignment by Maxmization of Mutual Information|
A forest of sensors|
Analysis of quickfind with small subfiles|
Analysis of hashing algorithms and a new mathematical transform|
A forest of sensores|
Advances in Neural Information Processing Systems, Vol| 9, chapter MIMIC: Finding Optima by Estimating Probability Densities,. 
Automatic Fax Routing|
Learning informative statistics: A nonparametric approach|
Alignement by maximization of mututal information,|
Exact voxel occupancy using graph cuts|
T-cell activation and the dynamic world of rafts|

Theoretically Grounded Conversational Interfaces to Digital Information| Abstract This paper introduces a general methodology for the development and study of conversational systems.  Closer coupling of linguistic and psycholinguistic theories about dialogue with the development of dialogue systems allows each to inform the other more fully.  Three activities support our goals.  We develop theories through the empirical investigation of dialogue in conversational settings.  We use this to inform our theoretical research on dialogue and our design of advanced conversational interface systems.  We then experiment with these systems to compare their behavior to human-to-human interactions.  This paper discusses a study and analysis of conversations about library information, the development of a system based on this analysis, lessons learned, and future goals. 
Learning Domain Structures| Psychologists have argued that cognition in different domains draws on qualitatively different mental representations.  Tree structures appear well-suited to representing relationships between animal species [1, 2, 10], while a one-dimensional structure (the liberalconservative spectrum) seems better for representing people's political views.  The possibility of different structures raises a fundamental question: how do people learn what kind of structure is appropriate in each domain? The standard approach to this question is to reject one of its assumptions.  Nativists deny that core structures are learned, at least for evolutionarily important domains like folkbiology.  Instead, infants come equipped with innate knowledge about which structures are appropriate for which domains.  Atran [1], for example, argues that folkbiology is a core domain of human knowledge, and that the tendency to group living kinds into hierarchies reflects an "innately determined cognitive structure. " More generally, Keil [8] has argued that ontological knowledge obeys an innate "M-constraint", requiring the extensions of predicates to conform to rigidly treestructured hierarchies of objects.  Alternatively, empiricists generally deny that structured representations are present at all.  Domain-specific representations are merely emergent properties of unstructured, domain-general associative learning architectures.  McClelland and Rogers [12], for example, have recently suggested that the acquisition of semantic knowledge in domains such as intuitive biology can be explained as learning in a generic connectionist network.  Their architecture never explicitly represents any tree structure, although with repeated training, its hidden unit representations may implicitly come to approximate the taxonomic relations between biological species.  This paper proposes an alternative approach -- structure learning -- that combines important insights from both of these traditions.  Our key contribution is to show how structured domain representations can be acquired within a domain-general framework for Bayesian inference.  Like nativists, we suggest that different domains are represented with qualitatively different structures, and we show how these structured representations serve as critical constraints on inductive generalization.  Like empiricists, though, we emphasize the importance of learning, and attempt to show how domain structures can be acquired through domain-general statistical inference.  This is not only more parsimonious than the nativist position, but allows us to explain the origin of structured representations in novel domains, where the prior existence of domain-specific innate structure is highly implausible.  After describing our structure learning framework, we present two empirical tests of its performance.  First, we show that it chooses the appropriate domain structure for both synthetic and real-world data sets.  It correctly chooses a tree structure for a biological domain (animal feature judgments), and a linear structure for a political domain (US Supreme Court decisions).  Second, we model two classic data sets of inductive judgments in biology [13] and show that our framework performs better than an unstructured connectionist approach.  Bayesian structure learning Our proposal takes the form of a rational analysis.  We aim to demonstrate the computational plausibility and explanatory value of Bayesian structure learning, but leave for future work the question of how these computations might be implemented or approximated by cognitive processes.  Assume the learner's data consist of a binary-valued object-feature matrix D specifying the features of each object in a given domain.  In biology, for instance, the rows of D might correspond to species, and the columns to anatomical and behavioral attributes.  The entry in row i and column j would then specify the value of feature j for species i.  Structurelearning includes computational problems at two levels.  First, which structure class is most appropriate for the domain? Second, given a structure class, which structure in that class provides the best account of the data? For instance, suppose that a learner exposed to biological data ends up organizing animal species into a taxonomic tree.  The first problem asks how she knew to use a tree rather than some other kind of structure.  The second problem asks why she settled on one specific tree instead of the many other trees she might have chosen.  Our focus here is on the first problem -- the problem of inferring the right structure class for a domain.  A solution to the second problem, however, falls out of our probabilistic approach.  We assume that learners come to a domain equipped with a hypothesis space of structure classes, either constructed from innate primitives or based on analogies with previously learned domains.  For simplicity, this paper considers a hypothesis space of just three canonical classes: taxonomic trees, one-dimensional (linear) spaces, and independent feature models.  People surely have access to other classes, including higherdimensional spaces, flat (non-hierarchical) clusterings, and causal networks.  We leave it to future work to characterize the full range of structure classes accessible to human cognition.  In particular, it is an open question whether this space is small enough to be explicitly enumerated as we do here, or is so large (perhaps infinite or uncountable) that it can be specified only implicitly through some generating mechanism.  Future work should also consider the possibility that multiple structures may apply within a single domain.  Given a set of probabilistic models, Bayesian techniques can be used to evaluate which of the models is most likely to have generated some data [7].  Before these techniques can be applied to inferring domain structures, we need to associate each structure class in our hypothesis space with a probabilistic generative model for the features of objects.  The next section defines these models, but here we show how Bayesian inference can be used to choose between them.  Let D be an object-feature matrix generated from one of several structure classes.  The posterior probability of each class C i is proportional to the product of the likelihood p(D|C i ) and the prior probability p(C i ).  If we assign equal prior probabilities to each class (as we do throughout this paper), the best class is the class that makes the data most likely.  Computing the likelihood p(D|C i ) requires integrating over all structures S belonging to structure class C i : p(D|C i ) = Z p(D|S, C i )p(S|C i )dS, (1) Intuitively, this means that a structure class C i provides a good account of object-feature data D if the data are highly probable under a range of structures S in class C i , and if these structures themselves have high prior probability within C i .  The following section explains how the fit of each structure to the data, p(D|S, C i ), is computed for several structure classes.  We estimate the integral in Equation 1 using stochastic approximations.  First we run a Markov chain Monte Carlo simulation to draw a sample of m structures, {S j }, from the distribution p(S|D, C i ).  We then approximate p(D|C i ) by the harmonic mean estimator [7]: p(D|C i ) = 0 @ 1 m m X j=1 1 p(D|S j , C i ) 1 A1 1 .  (2) This estimator does not satisfy a central limit theorem, and can be thrown off by a sample with very low likelihood.  Despite its limitations, it is often sufficient to identify a model that is very much better than its competitors.  In future work we plan to estimate these integrals more accurately using path sampling [4].  From structures to probabilistic models We will work with three probabilistic models, each appropriate for a different structure class, and show how to compute the likelihoods p(D|S, C i ) for structures in each class.  For simplicity we assume here that all features are binary, but our framework extends naturally to multi-valued or continuous features.  C T : Taxonomic trees Class C T is the set of taxonomic trees --- rooted trees with the objects in D as their leaves.  This is a natural representation when the objects are the outcome of an evolutionary process.  We restrict ourselves to ultrametric trees --- trees where each leaf node is at the same distance from the root.  Assume that each feature is generated by a mutation process over the tree.  We formalize the mutation process using a simple biological model [11].  Suppose that a feature F is defined at every point along every branch, not just at the leaf nodes where the data points lie.  Imagine F spreading out over the tree from root to leaves --- it starts out at the root with some value and could switch values at any point along any branch.  Whenever a branch splits, both lower branches inherit the value of F at the point immediately before the split.  Figure 1(a) shows one mutation history for a binary feature on a tree with four objects.  A B C D (a) A B C D (b) Figure 1: (a) A tree with four objects (A, B, C and D) and three internal nodes.  A mutation history for a single feature is shown.  The feature is off at the root, but switches on at two places in the tree.  Shaded nodes have value 1, clear nodes have value 0, and crosses indicate mutations.  (b) A line with four objects.  We formalize this model of mutation using a Poisson arrival process.  Under this process, the probability that
Simulated Evolution of Language: a Review of the Field|

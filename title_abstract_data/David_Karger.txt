Random sampling and greedy sparsification for matroid optimization problems| Abstract Random sampling is a powerful tool for gathering information about a group by considering only a small part of it.  We discuss some broadly applicable paradigms for using random sampling in combinatorial optimization, and demonstrate the effectiveness of these paradigms for two optimization problems on matroids: finding an optimum matroid basis and packing disjoint matroid bases.  Applications of these ideas to the graphic matroid led to fast algorithms for minimum spanning trees and minimum cuts.  An optimum matroid basis is typically found by a greedy algorithm that grows an independent set into an the optimum basis one element at a time.  This continuous change in the independent set can make it hard to perform the independence tests needed by the greedy algorithm.  We simplify matters by using sampling to reduce the problem of finding an optimum matroid basis to the problem of verifying that a given fixed basis is optimum, showing that the two problems can be solved in roughly the same time.  Another application of sampling is to packing matroid bases, also known as matroid partitioning.  Sampling reduces the number of bases that must be packed.  We combine sampling with a greedy packing strategy that reduces the size of the matroid.  Together, these techniques give accelerated packing algorithms.  We give particular attention to the problem of packing spanning trees in graphs, which has applications in network reliability analysis.  Our results can be seen as generalizing certain results from random graph theory.  The techniques have also been effective for other packing problems. 
A scalable location service for geographic ad hoc routing| Simple geographic forwarding combined with GLS compares favorably with Dynamic Source Routing (DSR): in larger networks (over 200 nodes) our approach delivers more packets, but consumes fewer network resources. 
Randomization in Graph Optimization Problems: A Survey| Abstract Randomization has become a pervasive technique in combinatorial optimization.  We survey our thesis and subsequent work, which uses four common randomization techniques to attack numerous optimization problems on undirected graphs. 
Random sampling in cut, flow, and network design problems| Abstract We use random sampling as a tool for solving undirected graph problems.  We show that the sparse graph, or skeleton, that arises when we randomly sample a graph's edges will accurately approximate the value of all cuts in the original graph with high probability.  This makes sampling effective for problems involving cuts in graphs.  We present fast randomized (Monte Carlo and Las Vegas) algorithms for approximating and exactly finding minimum cuts and maximum flows in unweighted, undirected graphs.  Our cut-approximation algorithms extend unchanged to weighted graphs while our weighted-graph flow algorithms are somewhat slower.  Our approach gives a general paradigm with potential applications to any packing problem.  It has since been used in a near-linear time algorithm for finding minimum cuts, as well as faster cut and flow algorithms.  Our sampling theorems also yield faster algorithms for several other cutbased problems, including approximating the best balanced cut of a graph, finding a k-connected orientation of a 2k-connected graph, and finding integral multicommodity flows in graphs with a great deal of excess capacity.  Our methods also improve the efficiency of some parallel cut and flow algorithms.  Our methods also apply to the network design problem, where we wish to build a network satisfying certain connectivity requirements between vertices.  We can purchase edges of various costs and wish to satisfy the requirements at minimum total cost.  Since our sampling theorems apply even when the sampling probabilities are different for different edges, we can apply randomized rounding to solve network design problems.  This gives approximation algorithms that guarantee much better approximations than previous algorithms whenever the minimum connectivity requirement is large.  As a particular example, we improve the best approximation bound for the minimum k-connected subgraph problem from 1:85 to 1 +O( p (log n)=k). 
Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model| ABSTRACT Much work in information retrieval focuses on using a model of documents and queries to derive retrieval algorithms.  Model based development is a useful alternative to heuristic development because in a model the assumptions are explicit and can be examined and refined independent of the particular retrieval algorithm.  We explore the explicit assumptions underlying the naive Bayesian framework by performing computational analysis of actual corpora and queries to devise a generative document model that closely matches text.  Our thesis is that a model so developed will be more accurate than existing models, and thus more useful in retrieval, as well as other applications.  We test this by learning from a corpus the best document model.  We find the learned model better predicts the existence of text data and has improved performance on certain IR tasks. 
Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web| Abstract We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network.  Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network.  The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead.  The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows.  Our caching protocols are based on a special kind of hashing that we call consistent hashing.  Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes.  Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network.  We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems. 
Observations on the Dynamic Evolution of Peer-to-Peer Networks| factor.  We also outline and analyze an algorithm that converges to a correct routing state from an arbitrary initial condition. 
Arpeggio: Metadata Searching and Content Sharing with Chord| Abstract Arpeggio is a peer-to-peer file-sharing network based on the Chord lookup primitive.  Queries for data whose metadata matches a certain criterion are performed efficiently by using a distributed keyword-set index, augmented with index-side filtering.  We introduce index gateways, a technique for minimizing index maintenance overhead.  Because file data is large, Arpeggio employs subrings to track live source peers without the cost of inserting the data itself into the network.  Finally, we introduce postfetching, a technique that uses information in the index to improve the availability of rare files.  The result is a system that provides efficient query operations with the scalability and reliability advantages of full decentralization, and a content distribution system tuned to the requirements and capabilities of a peer-to-peer network.  1 Overview and Related Work Peer-to-peer file sharing systems, which let users locate and obtain files shared by other users, have many advantages: they operate more efficiently than the traditional client-server model by utilizing peers' upload bandwidth, and can be implemented without a central server.  However, many current file sharing systems trade-off scalability for correctness, resulting in systems that scale well but sacrifice completeness of search results or vice-versa.  Distributed hash tables have become a standard for constructing peer-to-peer systems because they overcome the difficulties of quickly and correctly
INS/Twine: A Scalable Peer-to-Peer Architecture for Intentional Resource Discovery| Abstract.  The decreasing cost of computing technology is speeding the deployment of abundant ubiquitous computation and communication.  With increasingly large and dynamic computing environments comes the challenge of scalable resource discovery, where client applications search for resources (services, devices, etc. ) on the network by describing some attributes of what they are looking for.  This is normally achieved through directory services (also called resolvers), which store resource information and resolve queries.  This paper describes the design, implementation, and evaluation of INS/Twine, an approach to scalable intentional resource discovery, where resolvers collaborate as peers to distribute resource information and to resolve queries.  Our system maps resources to resolvers by transforming descriptions into numeric keys in a manner that preserves their expressiveness, facilitates even data distribution and enables ecient query resolution.  Additionally, INS/Twine handles resource and resolver dynamism by treating all data as soft-state. 
User Interaction Experience for Semantic Web Information| ABSTRACT The Semantic Web project [2] aims to add semantics to the existing World Wide Web.  We propose an extension of the Web's user interaction experience to take advantage of the added semantics.  This user experience maintains the Web's original navigation paradigm, although all URIs, not just URLs, can be used to address information objects.  URIs form the superset of URLs and can name resources other than just those with retrievable contents.  Information is displayed in webpage-like presentations in which each UI element is associated with the information object that the element represents.  UI elements serve as proxies through which the user can manipulate information objects.  Uniform support for direct manipulation complements the Web's navigation paradigm to create an informationcentric environment for interacting with information.  In addition, we advocate the use of small cooperative tools over large standalone applications to further promote this information-centric paradigm. 
Augmenting Undirected Edge Connectivity in| Abstract We give improved randomized (Monte Carlo) algorithms for undirected edge splitting and edge connectivity augmentation problems.  Our algorithms run in time ~ O(n 2 ) on n-vertex graphs, making them an ~ \Omega(m=n) factor faster than the best known deterministic ones on m-edge graphs. 
A theoretical and practical approach to instruction scheduling on spatial architectures| Abstract.  This paper studies the problem of instruction assignment and scheduling on spatial architectures.  Spatial architectures are architectures whose resources are organized in clusters, with non-zero communication delays between the clusters.  On these architectures, instruction scheduling includes both space scheduling, where instructions are mapped to clusters, and the traditional time scheduling.  This paper considers the problem from both the theoretical and practical perspectives.  It presents two integer linear program formulations with known performance bounds.  We present an 8-approximation algorithm for constant number of tiles and constant communication delays.  Then, we introduce three heuristic algorithms based on list scheduling.  Then we study a layer partitioning method.  Our final algorithm is a combination of layer partitioning and the third heuristic.  Two of the better algorithms are evaluated on the Raw machine.  Results show that they are competitive with previously published results for regular, dense matrix type applications. 
Mathematical Programming Society Newsletter PAGE TWO| Optimization Problems.  The jury found his contribution, on the interface between computer science and optimization, deep and elegant.  To make David's work even better known among the membership, I invited him to write a feature article on some of his very interesting techniques and results. 
An Experimental Study of Polylogarithmic, Fully Dynamic, Connectivity Algorithms| We present an experimental study of different variants of the amortized O(log 2 n)-time fullydynamic connectivity algorithm of Holm, de Lichtenberg, and Thorup (STOC'98).  The experiments build upon experiments provided by Alberts, Cattaneo, and Italiano (SODA'96) on the randomized amortized O(log 3 n) fully-dynamic connectivity algorithm of Henzinger and King (STOC'95).  Our experiments shed light upon similarities and differences between the two algorithms.  We also present a slightly modified version of the Henzinger-King algorithm that runs in O(log 2 n) time, which resulted from our experiments. 
Towards using the network as a switch: On the use of TDM in linear optical networks| Abstract A common problem in optical networking is that the large quantity of raw bandwidth available in such networks is often dicult to access.  We show that time-division multiplexing (TDM) can be used to operate bus and ring architectures in a manner akin to a switch.  We consider a time-varying approach, akin to that used in switching theory, instead of the static approach, related to the knapsack problem, often associated with fixed allocation of trac onto circuits, often termed the grooming problem.  Our approach is probabilistic in nature, and requires significant generalization beyond the Birkho-von Neumann statistical multiplexing approaches have that have been successful in switching theory.  Our techniques rely on decompositions of fractional matchings (for architectures without erasures) and fractional interval graph colorings (for architectures with erasures) into integral matchings and colorings.  We show, moreover, that such TDM using statistical multiplexing substantially reduces the amount of hardware (particularly ADMs) needed to utilize fully the available bandwidth in a range of optical networks.  We show that a significant fraction (and in some cases all) of the bandwidth available to the system can be utilized, even if each node in the system has only a single ADM. 
Using Random Sampling to Find Maximum Flows in Uncapacitated Undirected Graphs| Abstract We present new algorithms, based on random sampling, that find maximum flows in undirected uncapacitated graphs.  Our algorithms dominate augmenting paths over all parameter values (number of vertices and edges and flow value).  They also dominate blocking flows over a large range of parameter values.  Furthermore, they achieve time bounds on graphs with parallel (equivalently, capacitated) edges that previously could only be achieved on graphs without them.  The key contribution of this paper is to demonstrate that such an improvement is possible.  This shows that augmenting paths and blocking flows are non-optimal, and reopens the question of how fast we can find a maximum flow.  We improve known time bounds by only a small (but polynomial) factor, and the complicated nature of our algorithms suggests they will not be practical.  A new idea of our algorithm is to find flow by diminishing cuts instead of augmenting paths.  Rather than finding a way to push flow from the source to the sink, we identify and delete edges that are not needed in a maximum flow.  When no more edges can be deleted, we know that every remaining edge must be saturated to give a maximum flow. 
On Randomized Network Coding| Abstract We consider a randomized network coding approach for multicasting from several sources over a network, in which nodes independently and randomly select linear mappings from inputs onto output links over some field.  This approach was first described in [3], which gave, for acyclic delay-free networks, a bound on error probability, in terms of the number of receivers and random coding output links, that decreases exponentially with code length.  The proof was based on a result in [2] relating algebraic network coding to network flows.  In this paper, we generalize these results to networks with cycles and delay.  We also show, for any given acyclic network, a tighter bound in terms of the probability of connection feasibility in a related network problem with unreliable links.  From this we obtain a success probability bound for randomized network coding in link-redundant networks with unreliable links, in terms of link failure probability and amount of redundancy. 
Techniques for scheduling with rejection| Abstract We consider the general problem of scheduling a set of jobs where we may choose not to schedule certain jobs, and thereby incur a penalty for each rejected job.  More specifically, we focus on choosing a set of jobs to reject and constructing a schedule for the remaining jobs so as to optimize the sum of the weighted completion times of the jobs scheduled plus the sum of the penalties of the jobs rejected.  We give several techniques for designing scheduling algorithms under this criterion.  Many of these techniques show how to reduce a problem with rejection to a (potentially more complex) scheduling problem without rejection.  Some of the reductions are based on general properties of certain kinds of linear-programming relaxations of optimization problems, and therefore are applicable to problems outside of scheduling; we demonstrate this by giving an approximation algorithm for a variant of the facility-location problem. 
New Algorithms for Load Balancing in Peer-to-Peer Systems| Abstract Load balancing is a critical issue for the efficient operation of peer-to-peer networks.  We give new protocols for several scenarios, whose provable performance guarantees are within a constant factor of optimal.  First, we give an improved version of consistent hashing, a scheme used for item to node assignments in the Chord system.  In its original form, it required every network node to operate O(log n) virtual nodes to achieve a balanced load, causing a corresponding increase in space and bandwidth usage.  Our protocol eliminates the necessity of virtual nodes while maintaining a balanced load.  Improving on related protocols, our scheme allows for the deletion of nodes and admits a simpler analysis, since the assignments do not depend on the history of the network.  We then analyze several simple protocols for load sharing by movements of data from higher loaded to lower loaded nodes.  These protocols can be extended to preserve the ordering of data items.  As an application, we use the last protocol to give an efficient implementation of a distributed data structure for range searches on ordered data. 
Linear Programming-Based Decoding of Turbo-Like Codes and its Relation to Iterative Approaches| Abstract In recent work (Feldman and Karger [8]), we introduced a new approach to decoding turbo-like codes based on linear programming (LP).  We gave a precise characterization of the noise patterns that cause decoding error under the binary symmetric and additive white Gaussian noise channels.  We used this characterization to prove that the word error rate is bounded by an inverse polynomial in the code length.  Furthermore, for any turbo-like code, our algorithm has the ML certificate property: whenever it outputs a code word, it is guaranteed to be the maximum-likelihood (ML) code word.  In this paper we extend these results and give an iterative decoder whose output is equivalent to that of the LP decoder.  We also extend the ML certificate property to the more efficient iterative tree reweighted max-product message-passing algorithm developed by Wainwright, Jaakkola, and Willsky [13]: we show that whenever this algorithm converges to a code word, it must be the ML code word.  Finally, we demonstrate experimentally that the noise patterns that cause decoding error in the LP decoder also cause decoding error in the standard iterative sum-product and max-product (min-sum) message-passing algorithms.  Consequently, the deterministically constructible interleaver used by the LP decoder to achieve its bounds on error rate is useful in practice not only for the LP decoder, but for these standard iterative decoders as well. 
OverCite: A Cooperative Digital Research Library| Abstract CiteSeer is a well-known online resource for the
Distributed Job Scheduling in Rings| Abstract We give a distributed approximation algorithm for job scheduling in a ring architecture.  In contrast to many other parallel scheduling models, the model we consider captures the influence of the underlying communications network by specifying that task migration from one processor to another takes time proportional to the distance between those two processors in the network.  As a result, our algorithm must balance both computational load and communication time.  The algorithm is simple, requires no global control, and yields schedules of length at most 4:22 times optimal.  We also give a lower bound on the performance of any distributed algorithm, and the results of simulation experiments which suggest better performance than does our worst-case analysis. 
Scatter/Gather: A Cluster-based Approach to Browsing Large Document Collections| Abstract Document clustering has not been well received as an information retrieval tool.  Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval.  We argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques.  However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm.  We present a document browsing technique that employs document clustering as its primary operation.  We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm. 
Wrapper Induction for End-User Semantic Content Development| ABSTRACT The transition from existing World Wide Web content to the Semantic Web relies on the labeling and classification of existing information before it is useful to end-users and their agents.  This paper presents a wrapper induction system designed to allow end-users to create, modify, and utilize semantic patterns on unlabeled World Wide Web documents.  These patterns allow users to overlay documents with RDF classes and properties, and then to interact with this labeled content within a larger Semantic Web application, such as Haystack. 
Approximation Algorithms for Orienteering and Discounted-Reward TSP| Abstract In this paper, we give the first constant-factor approximation algorithm for the rooted Orienteering problem, as well as a new problem that we call the DiscountedReward TSP, motivated by robot navigation.  In both problems, we are given a graph with lengths on edges and prizes (rewards) on nodes, and a start node s.  In the Orienteering Problem, the goal is to find a path that maximizes the reward collected, subject to a hard limit on the total length of the path.  In the Discounted-Reward TSP, instead of a length limit we are given a discount factor #, and the goal is to maximize total discounted reward collected, where reward for a node reached at time t is discounted by # t .  This is similar to the objective considered in Markov Decision Processes (MDPs) except we only receive a reward the first time a node is visited.  We also consider tree and multiple-path variants of these problems and provide approximations for those as well.  Although the unrooted orienteering problem, where there is no fixed start node s, has been known to be approximable using algorithms for related problems such as k-TSP (in which the amount of reward to be collected is fixed and the total length is approximately minimized), ours is the first to approximate the rooted question, solving an open problem [3, 1]. 
A Randomized Linear-Time Algorithm to Find Minimum Spanning Trees| Abstract We present a randomized linear-time algorithm to find a minimum spanning tree in a
The role of context in question answering systems| ABSTRACT Despite recent advances in natural language question answering technology, the problem of designing effective user interfaces has been largely unexplored.  We conducted a user study to investigate the problem and discovered that overall, users prefer a paragraph-sized chunk of text over just an exact phrase as the answer to their questions.  Furthermore, users generally prefer answers embedded in context, regardless of the perceived reliability of the source documents.  When users research a topic, increasing the amount of text returned to users significantly decreases the number of queries that they pose to the system, suggesting that users utilize supporting text to answer related questions.  We believe that these results can serve to guide future developments in question answering user interfaces. 
On the costs and benefits of procrastination: approximation algorithms for stochastic combinatorial optimization problems| Abstract Combinatorial optimization is often used to \plan ahead," purchasing and allocating resources for demands that are not precisely known at the time of solution.  This advance planning may be done because resources become very expensive to purchase or dicult to allocate at the last minute when the demands are known.  In this work we study the tradeoffs involved in making some purchase/allocation decisions early to reduce cost while deferring others at greater expense to take advantage of additional, late-arriving information.  We consider a number of combinatorial optimization problems in which the problem instance is uncertain|modeled by a probability distribution|and in which solution elements can be purchased cheaply now or at greater expense after the distribution is sampled.  We show how to approximately optimize the choice of what to purchase in advance and what to defer. 
On Coding for Non-Multicast Networks| Abstract We consider the issue of coding for non-multicast networks.  For multicast networks, it is known that linear operations over a field no larger than the number of receivers are sucient to achieve all feasible connections.  In the case of nonmulticast networks, necessary and sucient conditions are known, if we restrict ourselves to linear codes over a finite field [1].  However, no linearity suciency results exist for non-multicast networks.  Indeed, [2] shows that linearity over a field is not sucient in general.  We present a coding theorem that provides necessary and sucient conditions, in terms of receiver entropies, for an arbitrary set of connections to be achievable on any network.  We conjecture that linearity is sucient to satisfy the coding theorem, when linear operations are performed over vectors rather than scalars in a field.  We illustrate the intuition of this conjecture with an example.  This work is part of an ongoing cooperation with R.  Koetter. 
Not Too Hot, Not Too Cold: The Bundled-SVM is Just Right!| Abstract The Support Vector Machine (SVM) typically outperforms other algorithms on text classification problems, but requires training time roughly quadratic in the number of training documents.  In contrast, linear time algorithms like Naive Bayes have lower performance, but can easily handle huge training sets.  In this paper, we describe a technique that creates a continuum of classifiers between the SVM and a Naive Bayes like algorithm.  Included in that continuum is a classifier that approximates SVM performance with linear training time.  Another classifier on this continuum can outperform the SVM, yielding a breakeven point that beats other published results on Reuters-21578.  We give empirical and theoretical evidence that our hybrid approach successfully navigates the tradeoffs between speed and performance. 
Wide-Area Cooperative Storage with CFS| Abstract The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval.  CFS does this with a completely decentralized architecture that can scale to large systems.  CFS servers provide a distributed hash table (DHash) for block storage.  CFS clients interpret DHash blocks as a file system.  DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection.  DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers.  CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD.  Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP.  Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers.  The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail. 
Experimental Study of Minimum Cut Algorithms| -Abstract Abstract Recently, several new algorithms have been developed for the minimum cut problem.  These algorithms are very different from the earlier ones and from each other and substantially improve the worst-case time bounds for the problem.  In this paper, we conduct experimental evaluation the relative performance of these algorithms.  In the process, we develop heuristics and data structures that substantially improve the practical performance of the algorithms.  We also develop problem families for testing minimum cut algorithms.  Our work leads to a better understanding of the practical performance of minimum cut algorithms and produces very efficient codes for the problem. 
Using Randomized Sparsification to Approximate Minimum Cuts| Abstract We introduce the concept of randomized sparsification of a weighted, undirected graph.  Randomized sparsification yields a sparse unweighted graph which closely approximates the minimum cut structure of the original graph.  As a consequence, we show that a cut of weight within a (1 + ffl) multiplicative factor of the minimum cut in a graph can be found in O(m + n(log 3 n)=ffl 4 ) time; thus any constant factor approximation can be achieved in ~ O(m) time.  Similarly, we show that a cut within a multiplicative factor of ff of the minimum can be found in RNC using m + n 2=ff processors.  We also investigate a parametric version of our randomized sparsification approach.  Using it, we show that for a graph undergoing a series of edge insertions and deletions, an O( p 1 + 2=ffl)-approximation to the minimum cut value can be maintained at a cost of ~ O(n ffl+1=2 ) time per insertion or deletion.  If only insertions are allowed, the approximation can be maintained at a cost of ~ O(n ffl ) time per insertion. 
Thwarting Web Censorship with Untrusted Messenger Discovery| Abstract.  All existing anti-censorship systems for the Web rely on proxies to grant clients access to censored information.  Therefore, they face the proxy discovery problem: how can clients discover the proxies without having the censor discover and block these proxies? To avoid widespread discovery and blocking, proxies must not be widely published and should be discovered in-band.  In this paper, we present a proxy discovery mechanism called keyspace hopping that meets this goal.  Similar in spirit to frequency hopping in wireless networks, keyspace hopping ensures that each client discovers only a small fraction of the total number of proxies.  However, requiring clients to independently discover proxies from a large set makes it practically impossible to verify the trustworthiness of every proxy and creates the possibility of having untrusted proxies.  To address this, we propose separating the proxy into two distinct components|the messenger, which the client discovers using keyspace hopping and which simply acts as a gateway to the Internet; and the portal, whose identity is widely-published and whose responsibility it is to interpret and serve the client's requests for censored content.  We show how this separation, as well as in-band proxy discovery, can be applied to a variety of anti-censorship systems. 
How to make a semantic web browser| ABSTRACT Two important architectural choices underlie the success of the Web: numerous, independently operated servers speak a common protocol, and a single type of client---the Web browser---provides point-and-click access to the content and services on these decentralized servers.  However, because HTML marries content and presentation into a single representation, end users are often stuck with inappropriate choices made by the Web site designer of how to work with and view the content.  RDF metadata on the Semantic Web does not have this limitation: users can gain direct access to the underlying information and control how it is presented for themselves.  This principle forms the basis for our Semantic Web browser---an end user application that automatically locates metadata and assembles point-and-click interfaces from a combination of relevant information, ontological specifications, and presentation knowledge, all described in RDF and retrieved dynamically from the Semantic Web.  With such a tool, nave users can begin to discover, explore, and utilize Semantic Web data and services.  Because data and services are accessed directly through a standalone client and not through a central point of access (e. g. , a portal), new content and services can be consumed as soon as they become available.  In this way we take advantage of an important sociological force that encourages the production of new Semantic Web content by remaining faithful to the decentralized nature of the Web. 
On Approximating the Longest Path in a Graph| Abstract We consider the problem of approximating the longest path in undirected graphs.  In an attempt to pin down the best achievable performance ratio of an approximation algorithm for this problem, we present both positive and negative results.  First, a simple greedy algorithm is shown to find long paths in dense graphs.  We then consider the problem of finding paths in graphs that are guaranteed to have extremely long paths.  We devise an algorithm that finds paths of a logarithmic length in Hamiltonian graphs.  This algorithm works for a much larger class of graphs (weakly Hamiltonian), where the result is the best possible.  Since the hard case appears to be that of sparse graphs, we also consider sparse random graphs.  Here we show that a relatively long path can be obtained, thereby partially answering an open problem of Broder, Frieze and Shamir.  To explain the difficulty of obtaining better approximations, we prove some strong hardness results.  We show that, for any ffl ! 1, the problem of finding a path of length n\Gamma n ffl in an n-vertex Hamiltonian graph is NP-hard.  We then show that no polynomial-time algorithm can find a constant factor approximation to the longest path problem unless P = NP.  We conjecture that the result can be strengthened to say that for some constant ffi ? 0, finding an approximation of ratio n ffi is also NP-hard.  As evidence towards this conjecture, we show that if any polynomial-time algorithm can approximate the longest path to a ratio of 2 O(log 1\Gamma ffl n) , for any ffl ? 0, then NP has a quasi-
Fast Connected Components Algorithms for the EREW PRAM| Abstract We present fast and efficient parallel algorithms for finding the connected components of an undirected graph.  These algorithms run on the exclusive-read, exclusive-write (EREW) PRAM.  On a graph with n vertices and m edges, our randomized algorithm runs in O(log n) time using (m+n 1+ffl )= log n EREW processors (for any fixed ffl ? 0).  A variant uses (m+n)= log n processors and runs in O(log n log log n) time.  A deterministic version of the algorithm runs in O(log 1:5 n) time using m+ n EREW processors. 
Prerequisites for a Personalizable User Interface| ABSTRACT Interfaces that support customization and can adapt to individuals' specific use patterns may be more effective than ones designed to be "one size fits all".  However, to test whether such interfaces benefit users in actual everyday applications, a lot of underlying application infrastructure must be reimplemented to accommodate customization and adaptation.  In this paper we discuss Haystack, a platform for building information applications in which user interface concepts such as commands, views, and widgets and even application data itself is described as a semantic network, providing a flexible environment for prototyping notions of customization.  Haystack's data model can host information that used to be managed by multiple applications, meaning that users can use Haystack to interact with their data under a single paradigm, and adaptability and customization capabilities can be provided across multiple domains at once. 
of the Requirements for the Degree of Masters of Science Title: Automatic Record Extraction for the World Wide Web| As the amount of information on the World Wide Web grows, there is an increasing demand for software that can automatically process and extract information from web pages.  Despite the fact that the underlying data on most web pages is structured, we cannot automatically process these web sites/pages as structured data.  We need robust technologies that can automatically understand human-readable formatting and induce the underlying data structures.  Supervision Agreement: The program outlined in this proposal is adequate for a Master's thesis.  The supplies and facilities
Chord: A scalable peer-to-peer lookup service for internet applications| Abstract A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item.  This paper presents Chord, a distributed lookup protocol that addresses this problem.  Chord provides support for just one operation: given a key, it maps the key onto a node.  Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps.  Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing.  Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes. 
Network Coding for Wireless Applications: A Brief Tutorial| Abstract--- The advent of network coding promises to change many aspects of networking.  Network coding moves away from the classical approach of networking, which treats networks as akin to physical transportation systems.  We overview some of the main features of network coding that are most relevant to wireless networks.  In particular, we discuss the fact that random distributed network coding is asymptotically optimal for wireless networks with and without packet erasures.  These results are extremely general and allow packet loss correlation, such as may occur in fading wireless channels.  The coded network lends itself, for multicast connections, to a cost optimization which not only outperforms traditional routing tree-based approaches, but also lends itself to a distributed implementation and to a dynamic implementation when changing conditions, such as mobility, arise.  We illustrate the performance of such optimization methods for energy efficiency in wireless networks and propose some new directions for research in the area. 
In the Working Notes of the 1995 AAAI Fall Symposium on AI Applications in Knowledge Navigation and Retrieval| Scatter/Gather as a Tool for the Navigation of Retrieval Results.  Abstract An important information access problem arises when
LP Decoding| Abstract.  Linear programming (LP) relaxation is a common technique used to find good solutions to complex optimization problems.  We present the method of "LP decoding": applying LP relaxation to the problem of maximum-likelihood (ML) decoding.  An arbitrary binary-input memoryless channel is considered.  This treatment of the LP decoding method places our previous work on turbo codes [6] and low-density parity-check (LDPC) codes [8] into a generic framework.  We define the notion of a proper relaxation, and show that any LP decoder that uses a proper relaxation exhibits many useful properties.  We describe the notion of pseudocodewords under LP decoding, unifying many known characterizations for specific codes and channels.  The fractional distance of an LP decoder is defined, and it is shown that LP decoders correct a number of errors equal to half the fractional distance.  We also discuss the application of LP decoding to binary linear codes.  We define the notion of a relaxation being symmetric for a binary linear code.  We show that if a relaxation is symmetric, one may assume that the all-zeros codeword is transmitted. 
Building Steiner Trees with Incomplete Global Knowledge| Abstract A networking problem of present day interest is that of distributing a single data item to multiple clients while minimizing network usage.  Steiner tree algorithms are a natural solution method, but only when the set of clients requesting the data is known.  We study what can be done without this global knowledge, when a given vertex knows only the probability that any other client will wish to be connected, and must simply specify a fixed path to the data to be used in case it is requested.  Our problem is an example of a class of network design problems with concave cost functions (which arise when the design problem exhibits economies of scale).  In order to solve our problem, we introduce a new version of the facility location problem: one in which every open facility is required to have some minimum amount of demand assigned to it.  We present a simple bicriterion approximation for this problem, one which is loose in both assignment cost and minimum demand, but within a constant factor of the optimum for both.  This suffices for our application.  We leave open the question of finding an algorithm that produces a truly feasible approximate solution. 
Using urls and table layout for web classification tasks| ABSTRACT We propose new features and algorithms for automating Web-page classification tasks such as content recommendation and ad blocking.  We show that the automated classification of Web pages can be much improved if, instead of looking at their textual content, we consider each links's URL and the visual placement of those links on a referring page.  These features are unusual: rather than being scalar measurements like word counts they are tree structured--describing the position of the item in a tree.  We develop a model and algorithm for machine learning using such tree-structured features.  We apply our methods in automated tools for recognizing and blocking Web advertisements and for recommending "interesting" news stories to a reader.  Experiments show that our algorithms are both faster and more accurate than those based on the text content of Web documents. 
Parallel processor scheduling with delay constraints| Abstract We consider the problem of scheduling unit-length jobs on identical parallel machines such that the makespan of the resulting schedule is minimized.  Precedence constraints impose a partial order on the jobs, and both communication and precedence delays impose relative timing constraints on dependent jobs.  The combination of these two types of timing constraints naturally models the instruction scheduling problem that occurs during software compilation for stateof-the-art VLIW (Very Long Instruction Word) processors and multiprocessor parallel machines.  We present the first known polynomial-time algorithm for the case where the precedence constraint graph is a forest of in-trees (or a forest of out-trees), the number of machines m is fixed, and the delays (which are a function of both the job pair and the machines on which they run) are bounded by a constant D.  Our algorithm relies on a new structural theorem for scheduling jobs with arbitrary precedence constraints.  Given an instance with many independent dags, the theorem shows how to convert, in linear time, a schedule S for only the largest dags into a complete schedule that is either optimal or has the same makespan as S. 
A Better Algorithm for an Ancient Scheduling Problem| Abstract One of the oldest and simplest variants of multiprocessor scheduling is the on-line scheduling problem studied by Graham in 1966.  This problem can also be formulated as an on-line load balancing problem, and we will use this formulation throughout this paper.  The jobs arrive online and must be immediately and irrevocably assigned to one of m identical machines without any knowledge of future jobs.  The size of a job is known on arrival.  The load of a machine is the sum of the sizes of the jobs assigned to it.  The goal of the load balancer is to minimize the maximum load on any machine.  Graham proved that the List Processing Algorithm which assigns each job to the currently least loaded machine has competitive ratio (2\Gamma 1=m).  Recently algorithms with smaller competitive ratios than List Processing have been discovered, culminating in Bartal, Fiat, Karloff, and Vohra's construction of an algorithm with competitive ratio bounded away from 2.  Their algorithm has a competitive ratio of at most (2\Gamma 1=70) 1:986 for all m; hence for m ? 70, their algorithm is provably better than List Processing.  We present the CHASM ff algorithm that outperforms List Processing for any m 6, outperforms all previously published algorithms for any m 8, and has a competitive ratio of at most 1:945 for all m, which is significantly closer to the best known lower bound of 1:837 for the problem.  We show that our analysis of CHASM ff is almost tight by presenting a lower bound of 1:9378 on CHASM ff 's competitive ratio for large m.  We also explore some of the tradeoffs between any algorithm's worst case and average case performance.  All these results assume that jobs have infinite duration.  We also consider the case where jobs arrive with finite, unknown duration, and we show that List Processing is the optimal on-line algorithm in this setting with a competitive ratio of 2 \Gamma 1 m . 
An NC Algorithm for Minimum Cuts| Abstract We show that the minimum cut problem for weighted undirected graphs can be solved in NC using three separate and independently interesting results.  The first is an (m 2 =n)-processor NC algorithm for finding a (2 + ffl)-approximation to the minimum cut.  The second is a randomized reduction from the minimum cut problem to the problem of obtaining a (2 + ffl)-approximation to the minimum cut.  This reduction involves a natural combinatorial Set-Isolation Problem that can be solved easily in RNC.  The third result is a derandomization of this RNC solution that requires a combination of two widely used tools: pairwise independence and random walks on expanders.  We believe that the set-isolation approach will prove useful in other derandomization problems.  The techniques extend to two related problems: we describe NC algorithms finding minimum k-way cuts for any constant k and finding all cuts of value within any constant factor of the minimum.  Another application of these techniques yield an NC algorithm for finding a sparse kconnectivity certificate for all polynomially bounded values of k.  Previously, an NC construction was only known for polylogarithmic values of k. 
Approximation Schemes for Minimizing Average Weighted Completion Time with Release Dates| Abstract We consider the problem of scheduling n jobs with release dates on m machines so as to minimize their average weighted completion time.  We present the first known polynomial time approximation schemes for several variants of this problem.  Our results include PTASs for the case of identical parallel machines and a constant number of unrelated machines with and without preemption allowed.  Our schemes are efficient: for all variants the running time for a (1 + ffl) approximation is of the form f(1=ffl; m)poly(n). 
Approximating Minimum Cuts in () Time| Abstract We improve on randomsampling techniquesfor approximately solving problems that involve cuts in graphs.  We give a lineartime construction that transforms any graph on n vertices into an O(n log n)-edge graphon the same vertices whose cuts have approximately the same value as the original graph's.  In this new graph, for example, we can run the ~ O(mn)-time maximum flow algorithm of Goldberg and Tarjan to find an s--t minimum cut in ~ O(n 2 ) time.  This corresponds to a (1 + ffl)-times minimum s--t cut in the original graph.  In a similar way, we can approximate a sparsest cut in ~ O(n 2 ) time. 
Global Min-cuts in RNC, and Other Ramifications of a Simple Min-Cut Algorithm| Abstract This paper presents a new algorithm for finding global min-cuts in weighted, undirected graphs.  One of the strengths of the algorithm is its extreme simplicity.  This randomized algorithm can be implemented as a strongly polynomial sequential algorithm with running time ~ O(mn 2 ), even if space is restricted to O(n), or can be parallelized as an RNC algorithm which runs in time O(log 2 n) on a CRCW PRAM with mn 2 log n processors.  In addition to yielding the best known processor bounds on unweighted graphs, this algorithm provides the first proof that the min-cut problem for weighted undirected graphs is in RNC.  The algorithm does more than find a single min-cut; it finds all of them.  The algorithm also yields numerous results on network reliability, enumeration of cuts, multi-way cuts, and approximate min-cuts. 
A New Approach to the Minimum Cut Problem| Abstract This paper presents a new approach to finding minimum cuts in undirected graphs.  The fundamental principle is simple: the edges in a graph's minimum cut form an extremely small fraction of the graph's edges.  Using this idea, we give a randomized, strongly polynomial algorithm that finds the minimum cut in an arbitrarily weighted undirected graph with high probability.  The algorithm runs in O(n 2 log 3 n) time, a significant improvement over the previous ~ O(mn) time bounds based on maximum flows.  It is simple and intuitive and uses no complex data structures.  Our algorithm can be parallelized to run in RNC with n 2 processors; this gives the first proof that the minimum cut problem can be solved in RNC.  The algorithm does more than find a single minimum cut; it finds all of them.  With minor modifications, our algorithm solves two other problems of interest.  Our algorithm finds all cuts with value within a multiplicative factor of ff of the minimum cut's in expected ~ O(n 2ff ) time, or in RNC with n 2ff processors.  The problem of finding a minimum multiway cut of a graph into r pieces is solved in expected ~ O(n 2(r\Gamma 1) ) time, or in RNC with n 2(r \Gamma 1) processors.  The "trace"
Building peer-to-peer systems with Chord, a distributed lookup service| Abstract We argue that the core problem facing peer-to-peer systems is locating documents in a decentralized network and propose Chord, a distributed lookup primitive.  Chord provides an efficient method of locating documents while placing few constraints on the applications that use it.  As proof that Chord's functionality is useful in the development of peer-to-peer applications, we outline the implementation of a peer-to-peer file sharing system based on Chord. 
Haystack: A Platform for Creating, Organizing and Visualizing Information Using RDF| ABSTRACT The Resource Definition Framework (RDF) is designed to support agent communication on the Web, but it is also suitable as a framework for modeling and storing personal information.  Haystack is a personalized information repository that employs RDF in this manner.  This flexible semistructured data model is appealing for several reasons.  First, RDF supports ontologies created by the user and tailored to the user's needs.  At the same time, system ontologies can be specified and evolved to support a variety of high-level functionalities such as flexible organization schemes, semantic querying, and collaboration.  In addition, we show that RDF can be used to engineer a component architecture that gives rise to a semantically rich and uniform user interface.  We demonstrate that by aggregating various types of users' data together in a homogeneous representation, we create opportunities for agents to make more informed deductions in automating tasks for users.  Finally, we discuss the implementation of an RDF information store and a programming language specifically suited for manipulating RDF. 
Implementing a Fully Polynomial Time Approximation Scheme for All Terminal Network Reliability| 1 Abstract The classic all-terminal network reliability problem posits a graph, each of whose edges fails (disappears) independently with some given probability.  The goal is to determine the probability that the network becomes disconnected due to edge failures.  The practical applications of this question to communication networks are obvious, and the problem has therefore been the subject of a great deal of study.  Since it is ]P-complete, and thus believed hard to solve exactly, a great deal of research has been devoted to estimating the failure probability.  A comprehensive survey can be found in [Col87].  The first author recently presented an algorithm for approximating the probability of network disconnection under random edge failures.  In this paper, we report on our experience implementing this algorithm.  Our implementation shows that the algorithm is practical on networks of moderate size, and indeed works better than the theoretical bounds predict.  Part of this improvement arises from heuristic modifications to the theoretical algorithm, while another part suggests that the theoretical running time analysis of the algorithm might not be tight.  Based on our observation of the implementation, we were able to devise analytic explanations of at least some of the improved performance.  As one example, we formally prove the accuracy of a simple heuristic approximation for the reliability.  We also discuss other questions raised by the implementation which might be susceptible to analysis.  1. 1 The Problem.  Formally, a network is modeled as a a graph G, each of whose edges e is presumed to fail (disappear) with some probability p e , and thus to survive with probability q e = 1 \Gamma p e (a simplified version that we will focus on assumes each p e = p, but our techniques apply to the general case).  Network
Finding nearest neighbors in growth-restricted metrics| ABSTRACT Most research on nearest neighbor algorithms in the literature has been focused on the Euclidean case.  In many practical search problems however, the underlying metric is non-Euclidean.  Nearest neighbor algorithms for general metric spaces are quite weak, which motivates a search for other classes of metric spaces that can be tractably searched.  In this paper, we develop an efficient dynamic data structure for nearest neighbor queries in growth-constrained metrics.  These metrics satisfy the property that for any point q and distance d the number of points within distance 2d of q is at most a constant factor larger than the number of points within distance d.  Spaces of this kind may occur in networking applications, such as the Internet or Peer-to-peer networks, and vector quantization applications, where feature vectors fall into low-dimensional manifolds within high-dimensional vector spaces. 
Infranet: Circumventing Web Censorship and Surveillance| Abstract An increasing number of countries and companies routinely block or monitor access to parts of the Internet.  To counteract these measures, we propose Infranet, a system that enables clients to surreptitiously retrieve sensitive content via cooperating Web servers distributed across the global Internet.  These Infranet servers provide clients access to censored sites while continuing to host normal uncensored content.  Infranet uses a tunnel protocol that provides a covert communication channel between its clients and servers, modulated over standard HTTP transactions that resemble innocuous Web browsing.  In the upstream direction, Infranet clients send covert messages to Infranet servers by associating meaning to the sequence of HTTP requests being made.  In the downstream direction, Infranet servers return content by hiding censored data in uncensored images using steganographic techniques.  We describe the design, a prototype implementation, security properties, and performance of Infranet.  Our security analysis shows that Infranet can successfully circumvent several sophisticated censoring techniques. 
Chord: a scalable peer-to-peer lookup protocol for internet applications| the node to which the key maps.  Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing.  Results from theoretical analysis and simulations show that Chord is scalable: communication cost and the state maintained by each node scale logarithmically with the number of Chord nodes. 
Approximate Graph Coloring by Semidefinite Programming| Abstract We consider the problem of coloring k-colorable graphs with the fewest possible colors.  We present a randomized polynomial time algorithm which colors a 3-colorable graph on n vertices with minfO(\Delta 1=3 log 1=2 \Delta log n); O(n 1=4 log 1=2 n)g colors where \Delta is the maximum degree of any vertex.  Besides giving the best known approximation ratio in terms of n, this marks the first non-trivial approximation result as a function of the maximum degree \Delta.  This result can be generalized to k-colorable graphs to obtain a coloring using minfO(\Delta 1\Gamma 2=k log 1=2 \Delta log n); O(n 1\Gamma 3=(k+1) log 1=2 n)g colors.  Our results are inspired by the recent work of Goemans and Williamson who used an algorithm for semidefinite optimization problems, which generalize linear programs, to obtain improved approximations for the MAX CUT and MAX 2-SAT problems.  An intriguing outcome of our work is a duality relationship established between the value of the optimum solution to our semidefinite program and the Lov'asz #-function.  We show lower bounds on the gap between the optimum solution of our semidefinite program and the actual chromatic number; by duality this also demonstrates interesting new facts about the #-function. 
Polynomial Time Approximation Schemes for Dense Instances of NP-Hard Problems| Abstract We present a unified framework for designing polynomial time approximation schemes (PTASs) for "dense" instances of many NP-hard optimization problems, including maximum cut, graph bisection, graph separation, minimum k-way cut with and without specified terminals, and maximum 3-satisfiability.  By dense graphs we mean graphs with minimum degree #(n), although our algorithms solve most of these problems so long as the average degree is #(n).  Denseness for non-graph problems is defined similarly.  The unified framework begins with the idea of exhaustive sampling: picking a small random set of vertices, guessing where they go on the optimum solution, and then using their placement to determine the placement of everything else.  The approach then develops into a PTAS for approximating certain smooth integer programs where the objective function and the constraints are "dense" polynomials of constant degree. 
Near-optimal Intraprocedural Branch Alignment| Abstract Branch alignment reorders the basic blocks of a program to minimize pipeline penalties due to control-transfer instructions.  Prior work in branch alignment has produced useful heuristic methods.  We present a branch alignment algorithm that usually achieves the minimum possible pipeline penalty and on our benchmarks averages within 0. 3% of a provable optimum.  We compare the control penalties and running times of our algorithm to an older, greedy approach and observe that both the greedy method and our method are close to the lower bound on control penalties, suggesting that greedy is good enough.  Surprisingly, in actual execution our method produces programs that run noticeably faster than the greedy method.  We also report results from training and testing on different data sets, validating that our results can be achieved in real-world usage.  Training and testing on different data sets slightly reduced the benefits from both branch alignment algorithms, but the ranking of the algorithms does not change, and the bulk of the benefits remain. 
Byzantine Modification Detection in Multicast Networks using Randomized Network Coding| Abstract--- Distributed randomized network coding is a flexible and robust approach to transmitting and compressing information in multi-source multicast networks.  In this paper, we show how the path diversity and distributed randomness of this approach can be exploited to provide for Byzantine modification detection.  This is achieved by incorporating a simple polynomial hash value into each packet, which adds minimal computational and communication overhead.  The effectiveness of our scheme relies only on a Byzantine attacker being unable to design and supply modified packets with complete knowledge of other packets received by other nodes.  The detection probability can be traded off against the overhead (i. e. , the ratio of hash bits to data bits) -- the detection probability increases with the overhead, as well as with the number of unmodified packets obtained at the receiver whose contents are unknown to the attacker. 
Finding the Hidden Path: Time Bounds for All-Pairs Shortest Paths| Abstract.  We investigate the all-pairs shortest paths problem in weighted graphs.  We present an algorithm---
First-Price Path Auctions| ABSTRACT We study first-price auction mechanisms for auctioning flow between given nodes in a graph.  A first-price auction is any auction in which links on winning paths are paid their bid amount; the designer has flexibility in specifying remaining details.  We assume edges are independent agents with fixed capacities and costs, and their objective is to maximize their profit.  We characterize all strong #-Nash equilibria of a first-price auction, and show that the total payment is never significantly more than, and often less than, the well known dominant strategy Vickrey-Clark-Groves mechanism.  We then present a randomized version of the first-price auction for which the equilibrium condition can be relaxed to #-Nash equilibrium.  We next consider a model in which the amount of demand is uncertain, but its probability distribution is known.  For this model, we show that a simple ex ante first-price auction may not have any #-Nash equilibria.  We then present a modified mechanism with 2-parameter bids which does have an #-Nash equilibrium. 
Randomized Approximation Schemes for Cuts and Flows in Capacitated Graphs| Abstract We improve on random sampling techniques for approximately solving problems that involve cuts and flows in graphs.  We give a near-linear-time construction that transforms any graph on n vertices into an O(n log n)-edge graph on the same vertices whose cuts have approximately the same value as the original graph's.  In this new graph, for example, we can run the ~ O(m 3=2 )-time maximum flow algorithm of Goldberg and Rao to find an s--t minimum cut in ~ O(n 3=2 ) time.  This corresponds to a (1 + #)-times minimum s--t cut in the original graph.  In a similar way, we can approximate a sparsest cut to within O(log n) in ~ O(n 2 ) time using a previous ~ O(mn)-time algorithm.  A related approach leads to a randomized divide and conquer algorithm producing an approximately maximum flow in ~ O(m p n) time. 
On the Feasibility of Peer-to-Peer Web Indexing and Search| Abstract This paper discusses the feasibility of peer-to-peer full-text keyword search of the Web.  Two classes of keyword search techniques are in use or have been proposed: flooding of queries over an overlay network (as in Gnutella), and intersection of index lists stored in a distributed hash table.  We present a simple feasibility analysis based on the resource constraints and search workload.  Our study suggests that the peer-to-peer network does not have enough capacity to make naive use of either of search techniques attractive for Web search.  The paper presents a number of existing and novel optimizations for P2P search based on distributed hash tables, estimates their effects on performance, and concludes that in combination these optimizations would bring the problem to within an order of magnitude of feasibility.  The paper suggests a number of compromises that might achieve the last order of magnitude. 
A Randomized Fully Polynomial Time Approximation Scheme for the All-Terminal Network Reliability Problem| Abstract The classic all-terminal network reliability problem posits a graph, each of whose edges fails independently with some given probability.  The goal is to determine the probability that the network becomes disconnected due to edge failures.  This problem has obvious applications in the design of communication networks.  Since the problem is ]P-complete and thus believed hard to solve exactly, a great deal of research has been devoted to estimating the failure probability.  In this paper, we give a fully polynomial randomized approximation scheme that, given any n-vertex graph with specified failure probabilities, computes in time polynomial in n and 1=ffl an estimate for the failure probability that is accurate to within a relative error of 1 \Sigma ffl with high probability.  We also give a deterministic polynomial approximation scheme for the case of small failure probabilities.  Some extensions to evaluating probabilities of k-connectivity, strong connectivity in directed Eulerian graphs, and r-way disconnection, and to evaluating the Tutte polynomial are also described. 
Job Scheduling in Rings| Abstract We give distributed approximation algorithms for job scheduling in a ring architecture.  In contrast to almost all other parallel scheduling models, the model we consider captures the influence of the underlying communications network by specifying that task migration from one processor to another takes time proportional to the distance between those two processors in the network.  As a result, our algorithms must balance both computational load and communication time.  The algorithms are simple, require no global control, and work in a variety of settings.  All come with small constant-factor approximation guarantees; the basic algorithm yields schedules of length at most 4:22 times optimal.  We also give a lower bound on the performance of any distributed algorithm and the results of simulation experiments, which give better results than our worst-case analysis. 
A Polynomial-Time Approximation Scheme for Weighted Planar Graph TSP| Abstract Given a planar graph on n nodes with costs (weights) on its edges, define the distance between nodes i and j as the length of the shortest path between i and j.  Consider this as an instance of metric TSP.  For any '' ? 0, our algorithm finds a salesman tour of total cost at most (1 + '') times optimal in time n O(1=" 2 ) .  We also present a quasi-polynomial time algorithm for the Steiner version of this problem. 
Structural Cohesion and Embeddedness: A Hierarchical Concept of Social Groups0| Abstract Although questions about social cohesion lie at the core of our discipline, definitions are often vague and difficult to operationalize.  Here research on social cohesion and social embeddedness is linked by developing a concept of structural cohesion based on network node connectivity.  Structural cohesion is defined as the minimum number of actors who, if removed from a group, would disconnect the group.  A structural dimension of embeddedness can then be defined through the hierarchical nesting of these cohesive structures.  The empirical applicability of nestedness is demonstrated in two dramatically different substantive settings, and additional theoretical implications with reference to a wide array of substantive fields are discussed.  "[S]ocial solidarity is a wholly moral phenomenon which by itself is not amenable to exact observation and especially not to measurement. " (Durkheim
The Benefits of Coding over Routing in a Randomized Setting| Abstract--- We present a novel randomized coding approach for robust, distributed transmission and compression of information in networks.  We give a lower bound on the success probability of a random network code, based on the form of transfer matrix determinant polynomials, that is tighter than the Schwartz-Zippel bound for general polynomials of the same total degree.  The corresponding upper bound on failure probability is on the order of the inverse of the size of the finite field, showing that it can be made arbitrarily small by coding in a sufficiently large finite field, and that it decreases exponentially with the number of codeword bits.  We demonstrate the advantage of randomized coding over routing for distributed transmission in rectangular grid networks by giving, in terms of the relative grid locations of a source-receiver pair, an upper bound on routing success probability that is exceeded by a corresponding lower bound on coding success probability for sufficiently large finite fields.  We also show that our lower bound on the success probability of randomized coding holds for linearly correlated sources.  This implies that randomized coding effectively compresses linearly correlated information to the capacity of any network cut in a feasible connection problem. 
Random sampling in residual graphs| ABSTRACT Consider an n-vertex, m-edge, undirected graph with maximum flow value v.  We give a new ~ O(m+nv)-time maximum flow
Deterministic Network Coding by Matrix Completion| Abstract We present a new deterministic algorithm to construct network codes for multicast problems, a particular class of network information flow problems.  Our algorithm easily generalizes to several variants of multicast problems.  Our approach is based on a new algorithm for maximum-rank completion of mixed matrices---taking a matrix whose entries are a mixture of numeric values and symbolic variables, and assigning values to the variables so as to maximize the resulting matrix rank.  Our algorithm is faster than existing deterministic algorithms and can operate over a smaller field. 
Chord: A scalable peer-to-peer look-up protocol for internet applications| Content 1 Motivation. 
Haystack: Per-User Information Environments| Abstract Traditional Information Retrieval (IR) systems are designed to provide uniform access to centralized corpora by large numbers of people.  The Haystack project emphasizes the relationship between a particular individual and his corpus.  An individual's own haystack priviliges information with which that user interacts, gathers data about those interactions, and uses this metadata to further personalize the retrieval process.  This paper describes the prototype Haystack system. 
Koorde: A Simple Degree-Optimal Distributed Hash Table| Abstract Koorde 1 is a new distributed hash table (DHT) based on Chord [15] and the de Bruijn graphs [2].  While inheriting the simplicity of Chord, Koorde meets various lower bounds, such as O(log n) hops per lookup request with only 2 neighbors per node (where n is the number of nodes in the DHT), and O(log n/ log log n) hops per lookup request with O(log n) neighbors per node. 
VCG Overpayment in Random Graphs| Abstract Motivated by the increasing need to price networks, we study the prices resulting from of a variant of the VCG mechanism, specifically defined for networks [11].  This VCG mechanism is the unique efficient and strategyproof mechanism, however it is not budget-balanced and in fact it is known to result in arbitrarily bad overpayments for some graphs [1, 8].  In contrast, we study more common types of graphs and show that the VCG overpayment is not too high, so it is still an attractive pricing candidate.  We prove that the average overpayment in ErdosRenyi random graphs with unit costs is p/(2- p) for any n, when the average degree is higher than a given threshold.  Our simulations show that the overpayment is greater than p/(2- p) below this threshold, hence, together with the constant upper bound from Mihail et al.  [12], the overpayment is constant regardless of graph size.  We then find empirically that the complete graph with random edge costs has worse overpayments, higher than #((log n) 1. 5 ).  Again from simulations, we see that the power-law graph with unit costs has overpayments that decrease with graph size and finally, the power-law graph with uniformly random costs has a small constant overpayment. 
RDF Authoring Environments for End Users| Abstract The Semantic Web enables powerful agent-facilitated negotiation and retrieval functionality by enabling the transfer of machine-readable metadata across the Internet.  This next generation Web presupposes the mass availability of metadata such as clothing measurements, busy/free time calendar indications, and dietary restrictions, meaning that users will need to provide more information to their systems to reap the aforementioned benefits.  Current tools for capturing ontology-encoded metadata from users are ill-suited for this task, requiring knowledge of ontologies or the ability to navigate generalized abstract directed graphs.  In this paper we present user interface paradigms based on the idea that an object can be represented on the screen by an extensible family of views.  We present several strategies for allowing users to create RDF metadata by manipulating views, including a drag and drop form-based property editor and a viewbased graph editor.  Users benefit from being able to interact with objects by means of views suited to the context at hand, such as photograph representations or human-readable summary descriptions, rather than text fields with plaintext strings or URIs.  Finally, we discuss a strategy for enabling advanced users to construct ontologies and customized views and to distribute them to users unfamiliar with modeling knowledge, ultimately giving end users intuitive interfaces for entering metadata.  Motivation The Semantic Web provides a foundation upon which machines will be able to perform sophisticated coordination activities automatically, such as scheduling appointments, finding products that match specific criteria, and sharing ratings and opinions of goods and services over the Internet [3].  These activities require that specific pieces of knowledge, ranging from people's calendars to product specifications, be made available in machine-readable form.  The Resource Description Framework (RDF) was designed specifically as a standard means of encoding such information for the Semantic Web [2].  As the amount of information recorded in RDF grows, the activities described above will start to become possible.  Technologies for exposing database information as XML or RDF have been developed and allow existing systems to participate in the growth of the Semantic Web.  However, an important bottleneck to the proliferation of RDF as a platform for conducting everyday activities, such as coordinating schedules and sharing opinions, is the dearth of tools designed to allow end users to express information in RDF.  Today's RDF authoring tools generally come in four flavors: (1) ontology editors such as Protg[6], OilEd [9] and Onto-mat [4]; (2) graph-based representation viewers such as IsaViz [5]; (3) schema-specific user interfaces (the type automatically generated by database applications such as Microsoft Access and FileMaker) including Reggie [10] and Ont-o-mat; and (4) taxonomy editors exposed by tools such as Protgand used by services such as the Open Directory Project [11].  However, existing tool implementations have for the most part focused on maintaining ontological constraints and not on addressing the HCI issues of information collection.  At the same time, a lot of the information that needs to eventually be provided in RDF is already being collected relatively successfully by current software.  In fact, end user software employs many of the same approaches to information collection as those used by RDF authoring tools but is "streamlined" enough to be intuitive to non-technical people.  For example, users know how to drag and drop contacts into the "To" field of an e-mail editor to fill in a message header form (header forms can be presented by many ontology editors).  A simple row of radio buttons allows Web sites that sell books to collect rating information from customers.  Diagramming software such as Microsoft Visio allows users to draw organization charts and relationship diagrams.  Although not often considered as such, the kinds of information collected by the above user interface paradigms are also the kinds of information one wants to record in RDF for machine consumption.  Of course the interfaces associated with these examples are built from ontological constraints, such as the set of possible ratings for a book or the presence of a "To" property in an e-mail message header.  Some users will understand how to put together basic schemas given the right tools, and the system should allow these users to conveniently share their schemas with others in the same community spirit as the current Web.  Users can easily become familiar with incorporating Web content such as clip art, news articles, or tables into their documents and e-mails because of the ease with which Web browsers and word processors (through copy and paste or drag and drop) allow users to take advantage of publicly-posted information.  Semantic Web client-side software must be able to allow users to take advantage of schemas posted on the Web just as easily.  Approach In this paper we present user interface paradigms for allowing users to record many different forms of metadata, i. e. , create and manage properties of resources (the Semantic Web term for object) and relationships between resources.  The basic principle behind this paradigm is the idea of views---user interface components that serve as proxies for resources on the screen.  Several views may be associated with any one resource; for example, a person may be displayed as an icon (an icon view) or as a large key-value pair listing as is the case in an address book (a property listing view).  New views may be introduced into the system, allowing limitless freedom and flexibility in terms of how an object may be presented in a manner most suitable to the current context.  Exposing resources as views enables a number of advantages.  One benefit is that users are made able to work with their information by direct manipulation, an HCI concept that has found to be successful in past research [7].  For example, users can drag and drop a view into a list as a means of indicating to the system that the resource represented by the view should be added to that list.  Another advantage is that the problem of exposing a user interface by which users can manipulate the properties of a resource can then be cast as a problem of designing appropriate views for that resource, meaning that one does not need to rely on a handful of views to handle all possible kinds and types of resources.  Instead, we have developed several fundamental paradigms that correspond to various modalities in which users can specify information.  These paradigms correspond to the four types of tools mentioned above.  The most basic of these is the property editor, which allows users both to specify metadata and to customize the ontology by filling in or rearranging a form (types 1 and 3).  We also describe paradigms for managing lists of resources and for placing resources into taxonomies (type 4).  Finally, we present a graph editing paradigm that allows users to specify the relationships between resources visually when appropriate (type 2).  While these approaches for metadata input have been explored previously, what distinguishes the work presented here is the fact that views that embody the paradigms given here usually embed views of other resources. 
Tackling the Poor Assumptions of Naive Bayes Text Classifiers| Abstract Naive Bayes is often used as a baseline in text classification because it is fast and easy to implement.  Its severe assumptions make such efficiency possible but also adversely affect the quality of its results.  In this paper we propose simple, heuristic solutions to some of the problems with Naive Bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model.  We find that our simple corrections result in a fast algorithm that is competitive with stateof-the-art text classification algorithms such as the Support Vector Machine. 
Structural Cohesion and Embeddedness: A hierarchical conception of social groups| Research leading to new applications of the concept of connectivity to large scale social networks was funded by NSF grants SBR-9310033 and BCS-9978282 to Douglas White, the first entitled
Simple efficient load balancing algorithms for peer-to-peer systems| ABSTRACT Load balancing is a critical issue for the efficient operation of peerto-peer networks.  We give two new load-balancing protocols whose provable performance guarantees are within a constant factor of optimal.  Our protocols refine the consistent hashing data structure that underlies the Chord (and Koorde) P2P network.  Both preserve Chord's logarithmic query time and near-optimal data migration cost.  Consistent hashing is an instance of the distributed hash table (DHT) paradigm for assigning items to nodes in a peer-to-peer system: items and nodes are mapped to a common address space, and nodes have to store all items residing closeby in the address space.  Our first protocol balances the distribution of the key address space to nodes, which yields a load-balanced system when the DHT maps items "randomly" into the address space.  To our knowledge, this yields the first P2P scheme simultaneously achieving O(logn) degree, O(logn) look-up cost, and constant-factor load balance (previous schemes settled for any two of the three).  Our second protocol aims to directly balance the distribution of items among the nodes.  This is useful when the distribution of items in the address space cannot be randomized.  We give a simple protocol that balances load by moving nodes to arbitrary locations "where they are needed. " As an application, we use the last protocol to give an optimal implementation of a distributed data structure for range searches on ordered data. 
On the Dynamic Multicast Problem for Coded Networks| Abstract--- We consider the problem of finding minimum-cost time-varying subgraphs that can deliver continuous service to dynamic multicast groups in coded networks (i. e.  networks that use network coding).  This problem is relevant for applications such as real-time media distribution.  We formulate the problem within the framework of dynamic programming and apply dynamic programming theory to suggest how it may be solved. 
User Interfaces for Supporting Multiple Categorization| Abstract: As the amount of information stored on and accessed by computer has increased over the past twenty years, the tools available for organizing and retrieving such information have become outdated.  The folder paradigm has dominated existing user interfaces as the primary mechanism for organizing information for day-to-day use.  This paradigm encourages many-to-one placement of documents into strictly hierarchical containers.  In this paper we examine an alternative organization and navigation mechanism that promotes membership in multiple overlapping categories (as opposed to storage containment).  In particular, we explore the user interface consequences of multiple categorization support being made conveniently available from within Web browsers.  We have carried out user studies providing evidence that compared to the folder paradigm, multiple categorization not only improves organization and retrieval times but also matches more closely with the way users naturally think about organizing their information. 
Constant Interaction-Time Scatter/Gather Browsing of Very Large Document Collections| Abstract The Scatter/Gather document browsing method uses fast document clustering to produce table-of-contentslike outlines of large document collections.  Previous work [1] developed linear-time document clustering algorithms to establish the feasibility of this method over moderately large collections.  However, even linear-time algorithms are too slow to support interactive browsing of very large collections such as Tipster, the DARPA standard text retrieval evaluation collection.  We present a scheme that supports constant interaction-time Scatter/Gather of arbitrarily large collections after nearlinear time preprocessing.  This involves the construction of a cluster hierarchy.  A modification of Scatter/Gather employing this scheme, and an example of its use over the Tipster collection are presented.  1 Background Our previous work on Scatter/Gather [1] has shown that document clustering can be used as a first-class tool for browsing large text collections.  Browsing is distinguished from search because it is query-free.  We posit situations in which the user has an information need that is either too general or too vague to be usefully expressed as a query is some search language.  For example, the user may not be familiar with the vocabulary appropriate for describing a topic of interest, or may not wish to commit himself to a particular choice of words.  Indeed, the user may not be looking for anything specific at all, but rather may wish to explore the general
(De)randomized Construction of Small Sample Spaces in \calNC| Abstract Koller and Megiddo introduced the paradigm of constructing compact distributions that satisfy a given set of constraints, and showed how it can be used to efficiently derandomize certain types of algorithm.  In this paper, we significantly extend their results in two ways.  First, we show how their approach can be applied to deal with more general expectation constraints.  More importantly, we provide the first
Better Random Sampling Algorithms for Flows in Undirected Graphs| Abstract We present better random sampling algorithms for maximum flows in undirected graphs.  Our algorithms apply to capacitated or uncapacitated graphs, and find a maximum flow of value v in ~ O( p mnv) time.  This improves on a previous bound of ~ O(m 2=3 n 1=3 v) given by the author recently, which in turn improved on the O(mv) time bound for a typical augmenting path algorithm.  In uncapacitated graphs without parallel edges, the bound is no worse than ~ O(n 5=2 ).  We give another algorithm that finds a (1\Gamma ffl) times maximum flow in time ~ O(m p n=ffl), regardless of v. 
Sticky notes for the semantic web| ABSTRACT Computer-based annotation is increasing in popularity as a mechanism for revising documents and sharing comments over the Internet.  One reason behind this surge is that viewpoints, summaries, and notes written by others are often helpful to readers.  In particular, these types of annotations can help users locate or recall relevant documents.  We believe that this model can be applied to the problem of retrieval on the Semantic Web.  In this paper, we propose a generalized annotation environment that supports richer forms of description such as natural language.  We discuss how RDF can be used to model annotations and the connections between annotations and the documents they describe.  Furthermore, we explore the idea of a question answering interface that allows retrieval based both on the text of the annotations and the annotations' associated metadata.  Finally, we speculate on how these features could be pervasively integrated into an information management environment, making Semantic Web annotation a first class player in terms of document management and retrieval. 
Text Bundling: Statistics Based Data-Reduction| Abstract As text corpora become larger, tradeoffs between speed and accuracy become critical: slow but accurate methods may not complete in a practical amount of time.  In order to make the training data a manageable size, a data reduction technique may be necessary.  Subsampling, for example, speeds up a classifier by randomly removing training points.  In this paper, we describe an alternate method for reducing the number of training points by combining training points such that important statistical information is retained.  Our algorithm keeps the same statistics that fast, linear-time text algorithms like Rocchio and Naive Bayes use.  We provide empirical results that show our data reduction technique compares favorably to three other data reduction techniques on four standard text corpora. 
Minimum cuts in near-linear time| Abstract We significantly improve known time bounds for solving the minimum cut problem on undirected graphs.  We use a "semi-duality" between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques.  We give a randomized (Monte Carlo) algorithm that finds a minimum cut in an m-edge, n-vertex graph with high probability in O(m log 3 n) time.  We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O(n 2 log n) time.  This variant has an optimal RNC parallelization.  Both variants improve on the previous best time bound of O(n 2 log 3 n).  Other applications of the tree-packing approach are new, nearly tight bounds on the number of near-minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner. 
An ~| Abstract We show how the results of Karger, Motwani, and Sudan [6] and Blum [3] can be combined in a natural manner to yield a polynomial-time algorithm for ~ O(n 3=14 )-coloring any n-node 3-colorable graph.  This improves on the previous best bound of ~ O(n 1=4 ) colors [6]. 
Grigni: [12] A Polynomial-Time Approximation Scheme for Weighted Planar Graph TSP| Abstract Given a planar graph on n nodes with costs (weights) on its edges, define the distance between nodes i and j as the length of the shortest path between i and j.  Consider this as an instance of metric TSP.  For any '' ? 0, our algorithm finds a salesman tour of total cost at most (1 + '') times optimal in time n O(1=" 2 ) .  We also present a quasi-polynomial time algorithm for the Steiner version of this problem. 
Efficient Algorithms for New Computational Models| Abstract Advances in hardware design and manufacturing often lead to new ways in which problems can be solved computationally.  In this thesis we explore fundamental problems in three computational models that are based on such recent advances.  The first model is based on new chip architectures, where multiple independent processing units are placed on one chip, allowing for an unprecedented parallelism in hardware.  We provide new scheduling algorithms for this computational model.  The second model is motivated by peer-to-peer networks, where countless (often inexpensive) computing devices cooperate in distributed applications without any central control.  We state and analyze new algorithms for load balancing and for locality-aware distributed data storage in peer-to-peer networks.  The last model is based on extensions of the streaming model.  It is an attempt to capture the class of problems that can be efficiently solved on massive data sets.  We give a number of algorithms for this model, and compare it to other models that have been proposed for massive data set computations.  Our algorithms and complexity results for these computational models follow the central thesis that it is an important part of theoretical computer science to model real-world computational structures, and that such effort is richly rewarded by a plethora of interesting and challenging problems. 
User interface continuations| ABSTRACT Dialog boxes that collect parameters for commands often create ephemeral, unnatural interruptions of a program's normal execution flow, encouraging the user to complete the dialog box as quickly as possible in order for the program to process that command.  In this paper we examine the idea of turning the act of collecting parameters from a user into a first class object called a user interface continuation.  Programs can create user interface continuations by specifying what information is to be collected from the user and supplying a callback (i. e. , a continuation) to be notified with the collected information.  A partially completed user interface continuation can be saved as a new command, much as currying and partially evaluating a function with a set of parameters produces a new function.  Furthermore, user interface continuations, like other continuation-passing paradigms, can be used to allow program execution to continue uninterrupted while the user determines a command's parameters at his or her leisure. 
Using Linear Programming to Decode Linear Codes| Abstract --- Given a linear code and observations from a noisy channel, the decoding problem is to determine the most likely (ML) codeword.  We describe a method for approximate ML decoding of an arbitrary binary linear code, based on a linear programming (LP) relaxation that is defined by a factor graph or parity check representation of the code.  The resulting LP decoder, which generalizes our previous work on turbo-like codes [FK02, FWK02], has the ML certificate property: it either outputs the ML codeword with a guarantee of correctness, or acknowledges an error.  We provide a precise characterization of when the LP decoder succeeds, based on the cost of pseudocodewords associated with the factor graph.  We introduce the notion of the fractional distance # of a code, defined with respect to a particular LP relaxation, and prove that the LP decoder will correct up to [#/2]- 1 errors.  For the BEC, we prove that the performance of LP decoding is equivalent to standard iterative decoding. 
Rounding Algorithms for a Geometric Embedding of Minimum Multiway Cut| Abstract Given an undirected graph with edge costs and a subset of k # 3 nodes called terminals, a multiway, or k-way, cut is a subset of the edges whose removal disconnects each terminal from the others.  The multiway cut problem is to find a minimum-cost multiway cut.  This problem is Max-SNP hard.  Recently Calinescu, Karloff, and Rabani (STOC'98) gave a novel geometric relaxation of the problem and a rounding scheme that produced a (3/2
Learning Markov networks: maximum bounded tree-width graphs| Abstract Markov networks are a common class of graphical models used in machine learning.  Such models use an undirected graph to capture dependency information among random variables in a joint probability distribution.  Once one has chosen to use a Markov network model, one aims to choose the model that "best explains" the data that has been observed---this model can then be used to make predictions about future data.  We show that the problem of learning a maximum likelihood Markov network given certain observed data can be reduced to the problem of identifying a maximum weight low-treewidth graph under a given input weight function.  We give the first constant factor approximation algorithm for this problem.  More precisely, for any fixed treewidth objective k, we find a treewidth-k graph with an f(k) fraction of the maximum possible weight of any treewidthk graph. 
Spam-I-am: A Proposal for Spam Control using Distributed Quota Management",|
Looking up data in P2P systems|
Web Caching with Consistent Hashing|
An ~ O(n 2 ) Algorithm for Minimum Cuts|
Random Sampling in Graph Optimization Problems|
Derandomization through approximation: An NC algorithm for minimum cuts|
A randomized fully polynomial approximation scheme for the all terminal network reliability problem|
Approximate s--t min-cuts in ~ O(n 2 ) time,|
Infranet: Circumventing censorship and surveillance|
Approximating s-t minimum cuts in ~ O(n 2 ) time|
Scatter/Gather as a Tool for the Navigation of Retrieval Results|
Approximate graph coloring using semidefinite programming|
Random Sampling in Matroids, with Applications to Graph Connectivity and Minimum Spanning Trees|
The perfect search engine is not enough: a study of orienteering behavior in directed search|
Haystack: A Platform for Authoring End User Semantic Web Applications|
Network coding from a network flow perspective|
A Scalable Peer-to-peer Lookup Service for Internet Applications|
Minimum-cost multicast over coded packet networks," submitted to|
A simple degree-optimal distributed hash table|
Simple Constant-Space Distributed Hash Tables|
Tree pattern inference and matching for wrapper induction on the world wide web|
Decoding Turbo-Like Codes via Linear Programming|
Augmenting Undirected Edge Connectivity in (n) Time|
Improved approximation for graph coloring|
Chord: A Protocol for Heterogeneous Subgroup Formation in Peer-to-Peer Networks,|
Diminished Chord: A Protocol for Heterogeneous Subgroup Formation in Peer-to-Peer Networks|
Approximating, verifying, and constructing minimum spanning forests,|
Kademlia: A peer-to-peer information system based on the xor metric|
Haystack: a user interface for creating, browsing, and organizing arbitrary semistructured information|
What makes a good answer? The role of context in question answering|
Toward a random operation of networks," submitted to|
Using linear programming to decode binary linear codes|
Notes on decoding turbo-like codes via linear programming|

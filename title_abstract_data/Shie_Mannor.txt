The Consistency of Greedy Algorithms for Classification| Abstract.  We consider a class of algorithms for classification, which are based on sequential greedy minimization of a convex upper bound on the 0 1 loss function.  A large class of recently popular algorithms falls within the scope of this approach, including many variants of Boosting algorithms.  The basic question addressed in this paper relates to the statistical consistency of such approaches.  We provide precise conditions which guarantee that sequential greedy procedures are consistent, and establish rates of convergence under the assumption that the Bayes decision boundary belongs to a certain class of smooth functions.  The results are established using a form of regularization which constrains the search space at each iteration of the algorithm.  In addition to providing general consistency results, we provide rates of convergence for smooth decision boundaries.  A particularly interesting conclusion of our work is that Logistic function based Boosting provides faster rates of convergence than Boosting based on the exponential function used in AdaBoost. 
Dynamic abstraction in reinforcement learning via clustering| Abstract We consider a graph theoretic approach for automatic construction of options in a dynamic environment.  A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions.  A clustering algorithm is then used to partition the state space to dierent regions.  Policies for reaching the dierent parts of the space are separately learned and added to the model in a form of options (macro-actions).  The options are used for accelerating the Q-Learning algorithm.  We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial.  Experiments indicate significant speedups, especially in the initial learning phase. 
A Contract-Based Model for Directed Network Formation| Abstract We consider a network game where the nodes of the network wish to form a graph to route traffic between themselves.  We present a model where costs are incurred for routing traffic, as well as for a lack of network connectivity.  We focus on directed links and the link stability equilibrium concept, and characterize connected link stable equilibria.  The structure of connected link stable networks is analyzed for several special cases. 
Error reducing sampling in reinforcement learning| Abstract In reinforcement learning, an agent collects information interacting with an environment and uses it to derive a behavior.  This paper focuses on efficient sampling; that is, the problem of choosing the interaction samples so that the corresponding behavior tends quickly to the optimal behavior.  Our main result is a sensitivity analysis relating the choice of sampling any state-action pair to the decrease of an error bound on the optimal solution.  We derive two new model-based algorithms.  Simulations demonstrate a quicker convergence (in the sense of the number of samples) of the value function to the real optimal value function. 
Bias and variance in value function estimation| Abstract We consider the bias and variance of value function estimation that are caused by using an empirical model instead of the true model.  We analyze these bias and variance for Markov processes from a classical (frequentist) statistical point of view, and in a Bayesian setting.  Using a second order approximation, we provide explicit expressions for the bias and variance in terms of the transition counts and the reward statistics.  We present supporting experiments with artificial Markov chains and with a large transactional database provided by a mail-order catalog firm. 
On the Empirical State-Action Frequencies in Markov Decision Processes Under General Policies| Abstract We consider the empirical state-action frequencies and the empirical reward in weakly communicating finite-state Markov decision processes under general policies.  We define a certain polytope and establish that every element of this polytope is the limit of the empirical frequency vector, under some policy, in a strong sense.  Furthermore, we show that the probability of exceeding a given distance between the empirical frequency vector and the polytope decays exponentially with time, under every policy.  We provide similar results for vector-valued empirical rewards. 
Asymptotics of Efficiency Loss in Competitive Market Mechanisms| Abstract--- We consider the loss of efficiency in competitive market mechanisms that were introduced by Kelly [Kel97].  We model a large heterogenous population by letting each user have a random utility function.  We show that if the utility functions are bounded, the competitive equilibrium will be as efficient as the social optimum with high probability as the number of users increases.  This is the case for both inelastic capacity as well as elastic capacity under some standard assumptions.  We extend this result to a network setup where sources and destinations are picked at random.  If, however, the utility functions are not bounded, the loss of efficiency does not converge to zero.  Collaborating simulation results are also presented. 
The Sample Complexity of Exploration in the Multi-Armed Bandit Problem| Abstract We consider the multi-armed bandit problem under the PAC ("probably approximately correct") model.  It was shown by Even-Dar et al.  (2002) that given n arms, a total of O (n/e 2 )log(1/d) # trials suffices in order to find an e-optimal arm with probability at least 1- d.  We establish a matching lower bound on the expected number of trials under any sampling policy.  We furthermore generalize the lower bound, and show an explicit dependence on the (unknown) statistics of the arms.  We also provide a similar bound within a Bayesian setting.  The case where the statistics of the arms are known but the identities of the arms are not, is also discussed.  For this case, we provide a lower bound of Q (1/e 2 )(n + log(1/d)) # on the expected number of trials, as well as a sampling policy with a matching upper bound.  If instead of the expected number of trials, we consider the maximum (over all sample paths) number of trials, we establish a matching upper and lower bound of the form Q (n/e 2 )log(1/d) # .  Finally, we derive lower bounds on the expected regret, in the spirit of Lai and Robbins. 
Weak Learners and Improved Rates of Convergence in Boosting| Abstract The problem of constructing weak classifiers for boosting algorithms is studied.  We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data.  While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems.  Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds.  Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established. 
The Steering Approach for Multi-Criteria Reinforcement Learning| Abstract We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown.  In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature.  This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function.  The objective of the learning agent is to have its long-term average reward vector belong to a given target set.  We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games.  This algorithm combines, in an appropriate way, a finite set of standard, scalar-reward learning algorithms.  Sucient conditions are given for the convergence of the learning algorithm to a general target set.  The specialization of these results to the single-controller Markov decision problem are discussed as well. 
PAC Bounds for Multi-armed Bandit and Markov Decision Processes| Abstract.  The bandit problem is revisited and considered under the PAC model.  Our main contribution in this part is to show that given n arms, it suffices to pull the arms O( n # 2
Action Elimination and Stopping Conditions for Reinforcement Learning| Abstract We consider incorporating action elimination procedures in reinforcement learning algorithms.  We suggest a framework that is based on learning an upper and a lower estimates of the value function or the Q-function and eliminating actions that are not optimal.  We provide a model-based and a model-free variants of the elimination method.  We further derive stopping conditions that guarantee that the learned policy is approximately optimal with high probability.  Simulations demonstrate a considerable speedup and added robustness. 
Online Calibrated Forecasts: Efficiency versus Universality for Learning in Games| Abstract We provide a simple learning process that enables an agent to forecast a sequence of outcomes.  Our forecasting scheme, termed tracking forecast, is based on tracking the past observations while emphasizing recent outcomes.  As opposed to other forecasting schemes, we sacrifice universality in favor of a significantly reduced computational burden.  We show that if the sequence of outcomes has certain properties---it has some internal (hidden) state that does not change too fast---then the tracking forecast is weakly calibrated so that the forecast appears to be correct most of the time.  For binary outcomes, this result holds without any internal state assumptions.  We consider learning in a repeated strategic game where each player attempts to compute some forecast of the opponent actions and play a best response to it.  We show that if one of the players uses tracking forecast, while the other players uses a standard learning algorithm (such as exponential regret matching or smooth fictitious play), then the player using the tracking forecast obtains the best response to the actual play of the other players.  We further show that if both players use a tracking forecast, then under certain conditions on the game matrix, a convergence to a Nash equilibrium is possible with positive probability for a larger class of games than smooth fictitious play. 
On Finding Good State Aggregation Functions| Abstract We describe a novel algorithm that learns to perform a Heuristic Embedding of Markov Processes (HEMP) into a low dimensional Euclidean space (Engel & Mannor, 2001) Learning is performed online by observing actual state transitions and gradually constructing a map of the Markov state space.  In this map the distance between any two states is related to the transition probabilities between them.  Such maps may be useful as an alternative representation for Markov Decision Processes (MDPs) in cases where the state space is of high dimensionality and the number of states is large.  These are exactly the conditions under which one would have to resort to hierarchical RL schemes.  One of the fundamental questions in the field of hierarchical RL is that of state aggregation and dimensional reduction - namely, how can state information be safely abstracted away.  We motivate the algorithm from the stateaggregation and dimensional reduction perspectives, and demonstrate its operation in a maze test-bed. 
Greedy Algorithms for Classification -- Consistency, Convergence Rates, and Adaptivity| Abstract Many regression and classification algorithms proposed over the years can be described as greedy procedures for the stagewise minimization of an appropriate cost function.  Some examples include additive models, matching pursuit, and boosting.  In this work we focus on the classification
Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning|
On-Line Learning with Imperfect Monitoring|
Bayes Meets Bellman: The Gaussian Process Approach to Temporal Difference Learning|
Learning Embedded Maps of Markov Processes|
Lower Bounds on the Sample Complexity of Exploration in the Multi-armed Bandit Problem|
Geometric Bounds for Generalization in Boosting|

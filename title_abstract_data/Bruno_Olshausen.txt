Learning Sparse Codes with a Mixture-of-Gaussians Prior| Abstract We describe a method for learning an overcomplete set of basis functions for the purpose of modeling sparse structure in images.  The sparsity of the basis function coefficients is modeled with a mixture-of-Gaussians distribution.  One Gaussian captures nonactive coefficients with a small-variance distribution centered at zero, while one or more other Gaussians capture active coefficients with a large-variance distribution.  We show that when the prior is in such a form, there exist efficient methods for learning the basis functions as well as the parameters of the prior.  The performance of the algorithm is demonstrated on a number of test cases and also on natural images.  The basis functions learned on natural images are similar to those obtained with other methods, but the sparse form of the coefficient distribution is much better described.  Also, since the parameters of the prior are adapted to the data, no assumption about sparse structure in the images need be made a priori, rather it is learned from the data. 
Simple-Cell-Like Receptive Fields Maximize Temporal Coherence in Natural Video| Recently, statistical models of natural images have shown the emergence of several properties of the visual cortex.  Most models have considered the nongaussian properties of static image patches, leading to sparse coding or independent component analysis.  Here we consider the basic time dependencies of image sequences instead of their nongaussianity.  We show that simple-cell-type receptive elds emerge when temporal response strength correlation is maximized for natural image sequences.  Thus, temporal response strength correlation, which is a nonlinear measure of temporal coherence, provides an alternative to sparseness in modeling simple-cell receptive eld properties.  Our results also suggest an interpretation of simple cells in terms of invariant coding principles, which have previously been used to explain complex-cell receptive elds. 
Learning Sparse Image Codes using a Wavelet Pyramid Architecture| Abstract We show how a wavelet basis may be adapted to best represent natural images in terms of sparse coefficients.  The wavelet basis, which may be either complete or overcomplete, is specified by a small number of spatial functions which are repeated across space and combined in a recursive fashion so as to be self-similar across scale.  These functions are adapted to minimize the estimated code length under a model that assumes images are composed of a linear superposition of sparse, independent components.  When adapted to natural images, the wavelet bases take on different orientations and they evenly tile the orientation domain, in stark contrast to the standard, non-oriented wavelet bases used in image compression.  When the basis set is allowed to be overcomplete, it also yields higher coding efficiency than standard wavelet bases. 
What is the other 85% of V1 doing?| Abstract This article will pose the following challenge: that despite four decades of research characterizing the response properties of V1 neurons, we still do not have a decent picture of how V1 really operates---i. e. , how a population of its neurons represents natural scenes under realistic viewing conditions.  We identify five problems with the current view that stem largely from biases in the design and execution of experiments, in addition to the contributions of non-linearities in the cortex that are not well understood.  Our purpose is to open the window to new theories, a number of which we describe along with some proposals for testing them. 
Sparse coding with an overcomplete basis set: A strategy employed by V1?| Abstract The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable to the basis functions of wavelet transforms.  Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images (Olshausen and Field, 1996a).  Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding.  Of particular interest is the case when the code is overcomplete---i. e. , when the number of code elements is greater than the effective dimensionality of the input space.  Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear.  These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli. 
Learning Sparse Multiscale Image Representations| Abstract We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coefficients.  The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coefficients to have exact zero values.  Coefficients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler.  The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coefficients.  Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods. 
Spatial Decorrelation in Orientation-Selective Cortical Cells| We propose a model for the lateral connectivity of orientation-selective cells in the visual cortex.  We study the properties of the input signal to the visual cortex and find new statistical structures that have not been processed in the retino-geniculate pathway.  Using the idea that the system performs redundancy reduction of the incoming signals, we derive the lateral connectivity that will achieve this for a set of orientation-selective local circuits, as well as the complete spatial structure of a network composed of such circuits.  We compare the results with various physiological measurements. 
Natural image statistics and efficientcoding| Abstract.  Natural images contain characteristic statistical regularities that set them apart from purely random images.  Understanding what these regularities are can enable natural images to be coded more efficiently.  In this
Convergent evidence for the visual analysis of optic flow through anisotropic attenuation of high spatial frequencies| Photoreceptors strongly attenuate high temporal frequencies.  Hence when an image moves, high spatial frequency components are lost if their direction of modulation coincides with the direction of movement, but not if it is orthogonal.  The power spectra of natural images are remarkably consistent in having a 1/f 2 falloff in power in all directions.  For moving images, the spatial power spectra will be distorted by becoming steeper in the direction corresponding to modulation in the direction of motion, and the contours of equal power will tend to become elliptical.  This study demonstrates that the mammalian visual system is specifically sensitive to such anisotropic changes of the local power spectrum, and it is suggested that these distortions are used to determine patterns of optic flow.  Convergent evidence from work on Glass figures, motion streaks, and sensitivity to non-Cartesian gratings is called on in support of this interpretation, which has been foreshadowed in several recent publications. 
Sparse Components of Images and Optimal Atomic Decompositions| for enabling him to attend the 1997 Workshop on Natural Scene Statistics, which gave the author a chance to meet and speak with some of the principals in this area of research. 
Bilinear Sparse Coding for Invariant Vision| Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images.  However, these approaches do not take image transformations into account.  We describe an unsupervised algorithm for learning both localized features and their transformations directly from images using a sparse bilinear generative model.  We show that from an arbitrary set of natural images, the algorithm produces oriented basis filters that can simultaneously represent features in an image and their transformations.  The learned generative model can be used to translate features to different locations, thereby reducing the need to learn the same feature at multiple locations, a limitation of previous approaches to sparse coding and ICA.  Our results suggest that by explicitly modeling the interaction between local image features and their transformations, the sparse bilinear approach can provide a basis for achieving transformation-invariant vision. 
Natural image statistics and neural representation|
Sparse codes and spikes,|
Emergence of single cell receptive eld properties by learning a sparse code for natural images|
Inferring Sparse, Overcomplete Image Codes Using an Efficient Coding Framework|
Learning linear, sparse, factorial codes|
Sparse coding of natural images produces localized|
Principles of image representation in visual cortex|
Sparse coding with over-complete basis set: A strategy employed by v1?|
Timecourse of neural signatures of object recognition|
editors|
Shape perception reduces activity in human primary visual cortex|
A nonlinear Hebbian network that learns to detect disparity in random- dot stereograms|
\Learning sparse image codes using a wavelet pyramid architecture," Advances in Neural Information Processing Systems,|
Early visual processing of natural images|

Eigenvoice Speaker Adaptation via Composite Kernel PCA| Abstract Eigenvoice speaker adaptation has been shown to be effective when only a small amount of adaptation data is available.  At the heart of the method is principal component analysis (PCA) employed to find the most important eigenvoices.  In this paper, we postulate that nonlinear PCA, in particular kernel PCA, may be even more effective.  One major challenge is to map the feature-space eigenvoices back to the observation space so that the state observation likelihoods can be computed during the estimation of eigenvoice weights and subsequent decoding.  Our solution is to compute kernel PCA using composite kernels, and we will call our new method kernel eigenvoice speaker adaptation.  On the TIDIGITS corpus, we found that compared with a speaker-independent model, our kernel eigenvoice adaptation method can reduce the word error rate by 28--33% while the standard eigenvoice approach can only match the performance of the speaker-independent model. 
Surrogate maximization/minimization algorithms for AdaBoost and the logistic regression model| Abstract Surrogate maximization (or minimization) (SM) algorithms are a family of algorithms that can be regarded as a generalization of expectation-maximization (EM) algorithms.  There are three major approaches to the construction of surrogate functions, all relying on the convexity of some function.  In this paper, we solve the boosting problem by proposing SM algorithms for the corresponding optimization problem.  Specifically, for AdaBoost, we derive an SM algorithm that can be shown to be identical to the algorithm proposed by Collins et al.  (2002) based on Bregman distance.  More importantly, for LogitBoost (or logistic boosting), we use several methods to construct dierent surrogate functions which result in dierent SM algorithms.  By combining multiple methods, we are able to derive an SM algorithm that is also the same as an algorithm derived by Collins et al.  (2002).  Our approach based on SM algorithms is much simpler and convergence results follow naturally. 
A STUDY OF VARIOUS COMPOSITE KERNELS FOR KERNEL EIGENVOICE SPEAKER ADAPTATION| ABSTRACT Eigenvoice-based methods have been shown to be effective for fast speaker adaptation when the amount of adaptation data is small, say, less than 10 seconds.  In traditional eigenvoice (EV) speaker adaptation, linear principal component analysis (PCA) is used to derive the eigenvoices.  Recently, we proposed that eigenvoices found by nonlinear kernel PCA could be more effective, and the eigenvoices thus derived were called kernel eigenvoices (KEV).  One of our novelties is the use of composite kernel that makes it possible to compute state observation likelihoods via kernel functions.  In this paper, we investigate two different composite kernels: direct sum kernel and tensor product kernel for KEV adaptation.  In an evaluation on the TIDIGITS task, it is found that KEV speaker adaptation using both forms of composite kernel are equally effective, and they outperform a speaker-independent model and the adapted models from EV, MAP, or MLLR adaptation using 2. 1s and 4. 1s of speech.  For example, with 2. 1s of adaptation data, KEV adaptation outperforms the speaker-independent model by 27. 5%, whereas EV, MAP, or MLLR adaptation are not effective at all. 
Parametric Distance Metric Learning with Label Information| Abstract Distance-based methods in pattern recognition and machine learning have to rely on a similarity or dissimilarity measure between patterns in the input space.  For many applications, Euclidean distance in the input space is not a good choice and hence more complicated distance metrics have to be used.  In this paper, we propose a parametric method for metric learning based on class label information.  We first define a dissimilarity measure that can be proved to be metric.  It has the favorable property that between-class dissimilarity is always larger than within-class dissimilarity.  We then perform parametric learning to find a regression mapping from the input space to a feature space, such that the dissimilarity between patterns in the input space is approximated by the Euclidean distance between points in the feature space.  Parametric learning is performed using the iterative majorization algorithm.  Experimental results on realworld benchmark data sets show that this approach is promising. 
Mining customer preference ratings for product recommendation using the support vector machine and the latent class model| Abstract As Internet commerce becomes more popular, customers' preferences on various products can now be readily acquired on-line via various e-commerce systems.  Properly mining this extracted data can generate useful knowledge for providing personalized product recommendation services.  In general, recommender systems use two complementary techniques.  Content-based systems match customer interests with products attributes, while collaborative filtering systems utilize preference ratings from other customers.  In this paper, we address some problems faced by these two systems, and study how machine learning techniques, namely the support vector machine and the latent class model, can be used to alleviated them. 
Efficient Hyperkernel Learning Using Second-Order Cone Programming| Abstract.  The kernel function plays a central role in kernel methods.  Most existing methods can only adapt the kernel parameters or the kernel matrix based on empirical data.  Recently, Ong et al.  introduced the method of hyperkernels which can be used to learn the kernel function directly in an inductive setting [12].  However, the associated optimization problem is a semidefinite program (SDP), which is very computationally expensive even with the recent advances in interior point methods.  In this paper, we show that this learning problem can be equivalently reformulated as a second-order cone program (SOCP), which can then be solved more eciently than SDPs.  Experimental results on both toy and real-world data sets show significant speedup.  Moreover, in comparison with the kernel matrix learning method proposed by Lanckriet et al.  [7], our proposed SOCP-based hyperkernel method yields better generalization performance, with a speed that is comparable to their formulation based on quadratically constrained quadratic programming (QCQP). 
Incremental Eigen Decomposition| Abstract--- Eigen decomposition is a central mathematical tool in many pattern recognition and machine learning techniques.  However, it becomes computationally infeasible in the presence of a large set of high-dimensional samples.  Moreover, in highly dynamic domains with a continual supply of new samples, an incremental approach that keeps on updating the eigen decomposition will be more desirable than the traditional approach of simply re-computing the decomposition from scratch.  In this paper, by using a method for updating singular value decompositions, we propose a procedure to obtain an approximate eigen decomposition by processing the samples in chunks or sequentially.  On applying this to principal component analysis for image denoising, experimental results show that this restricted form of updating can still achieve comparable performance as the batch version. 
Bayesian Support Vector Regression| Abstract We show that the Bayesian evidence framework can be applied to both ffl-support vector regression (ffl-SVR) and -support vector regression (-SVR) algorithms.  Standard SVR training can be regarded as performing level one inference of the evidence framework, while levels two and three allow automatic adjustments of the regularization and kernel parameters respectively, without the need of a validation set. 
Texture classification using the support vector machines| Abstract In recent years, support vector machines (SVMs) have demonstrated excellent performance in a variety of pattern recognition problems.  In this paper, we apply SVMs for texture classification, using translation-invariant features generated from the discrete wavelet frame transform.  To alleviate the problem of selecting the right kernel parameter in the SVM, we use a fusion scheme based on multiple SVMs, each with a different setting of the kernel parameter.  Compared to the traditional Bayes classifier and the learning vector quantization algorithm, SVMs, and, in particular, the fused output from multiple SVMs, produce more accurate classification results on the Brodatz texture album. 
The Pre-Image Problem in Kernel Methods| Abstract--- In this paper, we address the problem of finding the pre-image of a feature vector in the feature space induced by a kernel.  This is of central importance in some kernel applications, such as on using kernel principal component analysis (PCA) for image denoising.  Unlike the traditional method in [1] which relies on nonlinear optimization, our proposed method directly finds the location of the pre-image based on distance constraints in the feature space.  It is non-iterative, involves only linear algebra and does not suffer from numerical instability or local minimum problems.  Evaluations on performing kernel PCA and kernel clustering on the USPS data set show much improved performance. 
Rival Penalized Competitive Learning for Model-Based Sequence Clustering| Abstract In this paper, we propose a model-based, competitive learning procedure for the clustering of variable-length sequences.  Hidden Markov models (HMMs) are used as representations for the cluster centers, and rival penalized competitive learning (RPCL), originally developed for domains with static, fixed-dimensional features, is extended.  State merging operations are also incorporated to favor the discovery of smaller HMMs.  Simulation results show that our extended version of RPCL can produce a more accurate cluster structure than k-means clustering. 
Using the discrete wavelet frame transform to merge Landsat TM and SPOT panchromatic images| Abstract In this paper, we propose a pixel level image fusion algorithm for merging Landsat thematic mapper (TM) images and SPOT panchromatic images.  The two source images are first decomposed using the discrete wavelet frame transform (DWFT), which is both aliasing free and translation invariant.  Wavelet coe2cients from TM's approximation subband and SPOT's detail subbands are then combined together, and the fused image is reconstructed by performing the inverse DWFT.  Experimental results show that the proposed approach outperforms methods based on the intensity-hue-saturation transform, principal component analysis and discrete wavelet transform in preserving spectral and spatial information, especially in situations where the source images are not perfectly registered. 
Linear Dependency between epsilon and the Input Noise in epsilon-Support Vector Regression| Abstract.  In using the ffl-support vector regression (ffl-SVR) algorithm, one has to decide on a suitable value of the insensitivity parameter ffl.  Smola et al.  [6] determined its "optimal" choice based on maximizing the statistical efficiency of a location parameter estimator.  While they successfully predicted a linear scaling between the optimal ffl and the noise in the data, the value of the theoretically optimal ffl does not have a close match with its experimentally observed counterpart.  In this paper, we attempt to better explain the experimental results there, by analyzing a toy problem with a closer setting to the ffl-SVR.  Our resultant predicted choice of ffl is much closer to the experimentally observed value, while still demonstrating a linear trend with the data noise. 
Wishart Processes: A Statistical View of Reproducing Kernels and Its Applications to Kernel Learning| Abstract Kernels are playing an increasingly important role in machine learning.  In this paper, we propose a statistical view of kernels and establish a new connection between reproducing kernels and Gaussian processes.  Specifically, we draw equivalence between two notions, that the reproducing kernel is a Wishart process and that the dimensions of the feature vectors in the kernel-induced feature space are mutually independent Gaussian processes.  This leads to a probabilistic generative model of the kernel matrix following the Wishart distribution.  We find that the degree of freedom parameter of the Wishart distribution has a clear physical meaning, which is equal to the dimensionality of the feature space.  Moreover, although the feature vectors follow the Gaussian distributions, they are no longer required to be mutually independent.  Based on the statistical view proposed, we address the kernel matrix learning problem as a specific application of this view.  In particular, we propose two possible methods for kernel matrix learning, both of which are based on maximizing some likelihood function.  The first method learns the hyperparameters of a kernel matrix in an inductive learning setting, using an optimization procedure such as a gradient method.  The second method uses an expectation-maximization (EM) iterative procedure to estimate the eigenvalues and the dimensionality of the feature space in a transductive learning setting. 
Bayesian Inference on Principal Component Analysis Using Reversible Jump Markov Chain Monte Carlo| Abstract Based on the probabilistic reformulation of principal component analysis (PCA), we consider the problem of determining the number of principal components as a model selection problem.  We present a hierarchical model for probabilistic PCA and construct a Bayesian inference method for this model using reversible jump Markov chain Monte Carlo (MCMC).  By regarding each principal component as a point in a one-dimensional space and employing only birthdeath moves in our reversible jump methodology, our proposed method is simple and capable of automatically determining the number of principal components and estimating the parameters simultaneously under the same disciplined framework.  Simulation experiments are performed to demonstrate the effectiveness of our MCMC method. 
Multifocus image fusion using artificial neural networks| Abstract Optical lenses, particularly those with long focal lengths, suffer from the problem of limited depth of field.  Consequently, it is often difficult to obtain good focus for all objects in the picture.  One possible solution is to take several pictures with different focus points, and then combine them together to form a single image.  This paper describes an application of artificial neural networks to this pixel level multifocus image fusion problem based on the use of image blocks.  Experimental results show that the proposed method outperforms the discrete wavelet transform based approach, particularly when there is movement in the objects or mis-registration of the source images. 
Distance Metric Learning with Kernels| Abstract--- In this paper, we propose a feature weighting method that works in both the input space and the kernel-induced feature space.  It assumes only the availability of similarity (dissimilarity) information, and the number of parameters in the transformation does not depend on the number of features.  Besides feature weighting, it can also be regarded as performing nonparametric kernel adaptation.  Experimental results on both toy and real-world datasets show promising results. 
Fusing Images with Different Focuses Using Support Vector Machines| Abstract--- Many vision-related processing tasks, such as edge detection, image segmentation and stereo matching, can be performed more easily when all objects in the scene are in good focus.  However, in practice, this may not be always feasible as optical lenses, especially those with long focal lengths, only have a limited depth of field.  One common approach to recover an everywherein-focus image is to use wavelet-based image fusion.  First, several source images with different focuses of the same scene are taken and processed with the discrete wavelet transform (DWT).  Among these wavelet decompositions, the wavelet coefficient with the largest magnitude is selected at each pixel location.  Finally, the fused image can be recovered by performing the inverse DWT.  In this paper, we improve this fusion procedure by applying the discrete wavelet frame transform (DWFT) and the support vector machines (SVM).  Unlike DWT, DWFT yields a translationinvariant signal representation.  Using features extracted from the DWFT coefficients, a SVM is trained to select the source image that has the best focus at each pixel location, and the corresponding DWFT coefficients are then incorporated into the composite wavelet representation.  Experimental results show that the proposed method outperforms the traditional approach both visually and quantitatively. 
A Novel Distance-based Classifier Using Convolution Kernels and Euclidean Embeddings| Abstract Distance-based classification methods such as the nearest-neighbor and k-nearest-neighbor classifiers have to rely on a metric or distance measure between points in the input space.  For many applications, Euclidean distance in the input space is not a good choice and hence more complicated distance measures have to be used.  In this paper, we propose a novel kernel-based method that achieves Euclidean embedding by ensuring that the feature space is always Euclidean.  Thus, Euclidean distance-based classification methods can be applied in the feature space.  Unlike typical kernels which correspond to a nonlinear mapping from the input space, our kernel function corresponds to a nonlinear mapping from the joint space incorporating both the input and class label spaces.  The kernel function, which can be seen as a convolution kernel formed from the tensor product kernel and the direct sum kernel, tends to increase the separability between classes.  We have applied our new classification method to some face recognition benchmark datasets.  Our method is significantly faster than other face recognition methods and yet it can deliver comparable classification accuracy. 
Scaling up Support Vector Data Description by Using Core-Sets| Abstract--- Support vector data description (SVDD) is a powerful kernel method that has been commonly used for novelty detection.  While its quadratic programming formulation has the important computational advantage of avoiding the problem of local minimum, this has a runtime complexity of O(N 3 ), where N is the number of training patterns.  It thus becomes prohibitive when the data set is large.  Inspired from the use of core-sets in approximating the minimum enclosing ball problem in computational geometry, we propose in this paper an approximation method that allows SVDD to scale better to larger data sets.  Most importantly, the proposed method has a running time that is only linear in N .  Experimental results on two large real-world data sets demonstrate that the proposed method can handle data sets that are much larger than those that can be handled by standard SVDD packages, while its approximate solution still attains equally good, or sometimes even better, novelty detection performance. 
Improving De-Noising by Coefficient De-Noising and Dyadic Wavelet Transform| Abstract Soft thresholding has been a standard wavelet de-noising procedure in many signal and image processing applications.  Theoretically, it is also almost optimal in the sense of nearly achieving the minimax mean-squared error.  Inspired by this property, this paper proposes the addition of coefficient de-noising before soft thresholding.  This extra step serves to reduce noise in the empirical wavelet coefficients at each scale, and can be shown to yield a lower mean-squared error.  Moreover, we advocate the use of the translation-invariant dyadic wavelet transform, together with an approximate self-dual wavelet, instead of the discrete wavelet transform (DWT) in performing denoising.  Experiments show that the proposed method improves the signal-to-noise ratios of the de-noised signals.  Moreover, the de-noised signals do not have artifacts typically associated with DWT-based methods. 
Mining Customer Product Ratings for Personalized Marketing| Abstract With the increasing popularity of Internet commerce, a wealth of information about the customers can now be readily acquired on-line.  An important example is the customers' preference ratings for the various products offered by the company.  Successful mining of these ratings can thus allow the company's direct marketing campaigns to provide automatic, personalized product recommendations.  In general, these recommender systems are based on two complementary techniques.  Content-based systems match customer interests with information about the products, while collaborative systems utilize preference ratings from the other customers.  In this paper, we address some issues faced by these systems, and study how recent machine learning algorithms, namely the support vector machine and the latent class model, can be used to alleviate these problems. 
Bayesian Transductive Learning of the Kernel Matrix| Abstract This paper addresses the problem of transductive learning of the kernel matrix from a Bayesian perspective.  We consider the target kernel matrix as a random matrix following the Wishart distribution with a positive definite parameter matrix.  This parameter matrix, in turn, has the inverted Wishart distribution (with a positive definite hyperparameter matrix) as its conjugate prior.  By regarding kernel matrix learning as a missing data problem, we propose a Bayesian procedure which uses the Kullback-Leibler (KL) divergence to measure the similarity between the learned kernel matrix and the target kernel matrix.  An expectation-maximization (EM) algorithm is devised to infer the missing data and the model parameters in a maximum a posteriori (MAP) manner.  Using different settings for the target kernel and hyperparameter matrices, our model can be applied to different types of learning problems.  In particular, we consider its application in a semi-supervised learning setting and present two Bayesian nonlinear semi-supervised learning methods, one of which is a nonlinear Gaussian process method and the other is a nonlinear transductive discriminant analysis method.  Classification and clustering experiments are performed on benchmark data sets with encouraging results. 
Applying the Bayesian Evidence Framework to \nu -Support Vector Regression| Abstract.  Following previous successes on applying the Bayesian evidence framework to support vector classifiers and the #-support vector regression algorithm, in this paper we extend the evidence framework also to the #-support vector regression (#-SVR) algorithm.  We show that #SVR training implies a prior on the size of the #-tube that is dependent on the number of training patterns.  Besides, this prior has properties that are in line with the error-regulating behavior of #.  Under the evidence framework, standard #-SVR training can then be regarded as performing level one inference, while levels two and three allow automatic adjustments of the regularization and kernel parameters respectively, without the need of a validation set.  Furthermore, this Bayesian extension allows computation of the prediction intervals, taking uncertainties of both the weight parameter and the #-tube width into account.  Performance of this method is illustrated on both synthetic and real-world data sets. 
Learning the Kernel in Mahalanobis One-Class Support Vector Machines| Abstract In this paper, we show that one-class SVMs can also utilize data covariance in a robust manner to improve performance.  Furthermore, by constraining the desired kernel function as a convex combination of base kernels, we show that the weighting coefficients can be learned via quadratically constrained quadratic programming (QCQP) or second order cone programming (SOCP) methods.  Performance on both toy and real-world data sets show promising results.  This paper thus offers another demonstration of the synergy between convex optimization and kernel methods. 
Speedup of Kernel Eigenvoice Speaker Adaptation by Embedded Kernel PCA| Abstract Recently, we proposed an improvement to the eigenvoice (EV) speaker adaptation called kernel eigenvoice (KEV) speaker adaptation.  In KEV adaptation, eigenvoices are computed using kernel PCA, and a new speaker's adapted model is implicitly computed in the kernel-induced feature space.  Due to many online kernel evaluations, both adaptation and subsequent recognition of KEV adaptation are slower than EV adaptation.  In this paper, we eliminate all online kernel computations by finding an approximate pre-image of the implicit adapted model found by KEV adaptation.  Furthermore, the two steps of finding the implicit adapted model and its approximate pre-image are integrated by embedding the kernel PCA procedure in our new embedded kernel eigenvoice (eKEV) speaker adaptation method.  When tested in an TIDIGITS task with less than 10s of adaptation speech, eKEV adaptation obtained a speedup of 6--14 times in adaptation and 136 times in recognition over KEV adaptation with 12--13% relative improvement in recognition accuracy. 
Moderating the outputs of support vector machine classifiers|
Integrating the evidence framework and the support vector machine|
Learning with Idealized Kernels|
Automated text categorization using support vector machine|
Finding the pre images in kernel principal component analysis|
An Extended Genetic Rule Induction Algorithm,|
The evidence framework applied to support vector machines,|
Learning with idealized kernel,"|
Support vector mixture for classification and regression problems|

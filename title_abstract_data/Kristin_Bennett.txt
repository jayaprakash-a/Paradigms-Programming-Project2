Dimensionality Reduction via Sparse Support Vector Machines| Abstract We describe a methodology for performing variable ranking and selection using support vector machines (SVMs).  The method constructs a series of sparse linear SVMs to generate linear models that can generalize well, and uses a subset of nonzero weighted variables found by the linear models to produce a final nonlinear model.  The method exploits the fact that a linear SVM (no kernels) with ` 1 -norm regularization inherently performs variable selection as a side-effect of minimizing capacity of the SVM model.  The distribution of the linear model weights provides a mechanism for ranking and interpreting the effects of variables.  Starplots are used to visualize the magnitude and variance of the weights for each variable.  We illustrate the effectiveness of the methodology on synthetic data, benchmark problems, and challenging regression problems in drug design.  This method can dramatically reduce the number of variables and outperforms SVMs trained using all attributes and using the attributes selected according to correlation coefficients.  The visualization of the resulting models is useful for understanding the role of underlying variables. 
A Parametric Optimization Method for Machine Learning| Abstract The classification problem of constructing a plane to separate the members of two sets can be formulated as a parametric bilinear program.  This approach was originally created to minimize the number of points misclassified.  However, a novel interpretation of the algorithm is that the subproblems represent alternative error functions of the misclassified points.  Each subproblem identifies a specified number of outliers and minimizes the magnitude of the errors on the remaining points.  A tuning set is used to select the best result among the subproblems.  A parametric Frank-Wolfe method was used to solve the bilinear subproblems.  Computational results on a number of datasets indicate that the results compare very favorably with linear programming and heuristic search approaches.  The algorithm can be used as part of a decision tree algorithm to create nonlinear classifiers. 
Geometry in Learning| Abstract One of the fundamental problems in learning is identifying members of two different classes.  For example, to diagnose cancer, one must learn to discriminate between benign and malignant tumors.  Through examination of tumors with previously determined diagnosis, one learns some function for distinguishing the benign and malignant tumors.  Then the acquired knowledge is used to diagnose new tumors.  The perceptron is a simple biologically inspired model for this two-class learning problem.  The perceptron is trained or constructed using examples from the two classes.  Then the perceptron is used to classify new examples.  We describe geometrically what a perceptron is capable of learning.  Using duality, we develop a framework for investigating different methods of training a perceptron.  Depending on how we define the "best" perceptron, different minimization problems are developed for training the perceptron.  The effectiveness of these methods is evaluated empirically on four practical applications: breast cancer diagnosis, detection of heart disease, political voting habits, and sonar recognition.  This paper does not assume prior knowledge of machine learning or pattern recognition. 
Exploiting unlabeled data in ensemble methods| ABSTRACT An adaptive semi-supervised ensemble method, ASSEMBLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data.  ASSEMBLE alternates between assigning "pseudo-classes" to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data.  Mathematically, this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled data.  Unlike alternative approaches, ASSEMBLE does not require a semi-supervised learning method for the base classifier.  ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems.  ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition.  In addition, strong results on several benchmark datasets using both decision trees and neural networks support the proposed method. 
Hybrid Extreme Point Tabu Search| Abstract We develop a new hybrid tabu search method for optimizing a continuous differentiable function over the extreme points of a polyhedron.  The method combines extreme point tabu search with traditional descent algorithms based on linear programming.  The tabu search algorithm utilizes both recency-based and frequency-based memory and oscillates between local improvement and diversification phases.  The hybrid algorithm iterates between using the descent algorithm to find a local minimum and using tabu search to improve locally and then move to a new area of the search space.  This algorithm can be used on many important classes of problems in global optimization including bilinear programming, multilinear programming, multiplicative programming, concave minimization, and complementarity problems.  The algorithm is applied to two practical problems: the quasistatic multi-rigid-body contact problem in robotics and the global tree optimization problem in machine learning.  Computational results show that the hybrid algorithm outperforms the descent and tabu search algorithms used alone. 
Duality, Geometry, and Support Vector Regression| Abstract We develop an intuitive geometric framework for support vector regression (SVR).  By examining when #-tubes exist, we show that SVR can be regarded a classification problem in the dual space.  Hard and soft #-tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by #.  A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets.  In the proposed approach the effects of the choices of all parameters become clear geometrically. 
Bilinear Separation of Two Sets in n-Space| Abstract The NP-complete problem of determining whether two disjoint point sets in the n-dimensional real space R
Constrained K-Means Clustering| Abstract We consider practical methods for adding constraints to the K-Means clustering algorithm in order to avoid local solutions with empty clusters or clusters having very few points.  We often observe this phenomena when applying K-Means to datasets where the number of dimensions is n 10 and the number of desired clusters is k 20.  We propose explicitly adding k constraints to the underlying clustering optimization problem requiring that each cluster have at least a minimum number of points in it.  We then investigate the resulting cluster assignment step.  Preliminary numerical tests on real datasets indicate the constrained approach is less prone to poor local solutions, producing a better summary of the underlying data. 
Support Vector Machines: Hype or Hallelujah?| ABSTRACT Support Vector Machines (SVMs) and related kernel methods have become increasingly popular tools for data mining tasks such as classification, regression, and novelty detection.  The goal of this tutorial is to provide an intuitive explanation of SVMs from a geometric perspective.  The classification problem is used to investigate the basic concepts behind SVMs and to examine their strengths and weaknesses from a data mining perspective.  While this overview is not comprehensive, it does provide resources for those interested in further exploring SVMs. 
A Support Vector Machine Approach to Decision Trees| Abstract Key ideas from statistical learning theory and support vector machines are generalized to decision trees.  A support vector machine is used for each decision in the tree.  The "optimal" decision tree is characterized, and both a primal and dual space formulation for constructing the tree are proposed.  The result is a method for generating logically simple decision trees with multivariate linear or nonlinear decisions.  The preliminary results indicate that the method produces simple trees that generalize well with respect to other decision tree algorithms and single support vector machines. 
A Pattern Search Method for Model Selection of Support Vector Regression| Abstract We develop a fully-automatic pattern search methodology for model selection of support vector machines (SVMs) for regression and classification.  Pattern search (PS) is a derivative-free optimization method suitable for low-dimensional optimization problems for which it is difficult or impossible to calculate derivatives.  The methodology was motivated by an application in drug design in which regression models are constructed based on a few high-dimensional examplars.  Automatic model selection in such underdetermined problems is essential to avoid overfitting and to avoid overestimates of generalization capability caused by selecting parameters based on testing results.  We focus on SVM model selection for regression based on leave-one-out (LOO) and cross-validated estimates of mean squared error, but the search strategy is applicable to any model criterion.  Because the resulting error surface produces an extremely noisy map of the model quality with many local minima, the resulting generalization capacity of any single local optimal model illustrates high variance.  Thus several locally optimal SVM models are generated and then bagged or averaged to produce the final SVM.  This strategy of pattern search combined with model averaging has proven to be very effective on benchmark tests and in high-variance drug design domains with high potential of overfitting. 
Support Vector Machine Regression in Chemometrics| Abstract Predicting the biological activity of a compound from its chemical structure is a fundamental problem in drug design.  The ability
A Column Generation Algorithm For Boosting| Abstract We examine linear program (LP) approaches to boosting and demonstrate their efficient solution using LPBoost, a column generation simplex method.  We prove that minimizing the soft margin error function (equivalent to solving an LP) directly optimizes a generalization error bound.  LPBoost can be used to solve any boosting LP by iteratively optimizing the dual classification costs in a restricted LP and dynamically generating weak learners to make new LP columns.  Unlike gradient boosting algorithms, LPBoost converges finitely to a global solution using well defined stopping criteria.  Computationally, LPBoost finds very sparse solutions as good as or better than those found by ADABoost using comparable computation. 
An Extreme Point Tabu Search Method for Data Mining| Abstract We propose an Extreme Point Tabu Search (EPTS) algorithm that constructs globally optimal decision trees for classification problems.  Typically, decision tree algorithms are greedy.  They optimize the misclassification error of each decision sequentially.  Our non-greedy approach minimizes the misclassification error of all the decisions in the tree concurrently.  Decision trees are ideal for data-mining because their logical structure makes them easily understandable.  Using Global Tree Optimization (GTO), we can optimize existing decision trees.  This capability can be used in data mining for avoiding overfitting, transferring knowledge, incorporating domain knowledge, and maintaining existing decision trees.  Our method works by fixing the structure of the decision tree and then representing it as a set of disjunctive linear inequalities.  An optimization problem is constructed that minimizes the errors within the disjunctive linear inequalities.  To reduce the misclassification error, a nonlinear error function is minimized over a polyhedral region.  A new EPTS algorithm is used to search the extreme points of the polyhedral region for an optimal solution.  Promising computational results are given for both randomly generated and real-world problems. 
On Support Vector Decision Trees for Database Marketing| Abstract We introduce a support vector decision tree method for customer targeting in the framework of large databases (database marketing).  The goal is to provide a tool to identify the best customers based on historical data.  Then this tool is used to forecast the best potential customers among a pool of prospects.  We begin by recursively constructing a decision tree.  Each decision consists of a linear combination of independent attributes.  A linear program motivated by the support vector machine method from Vapnik's Statistical Learning Theory is used to construct each decision.  This linear program automatically selects the relevant subset of attributes for each decision.  Each customer is scored based on the decision tree.  A gainschart table is used to verify the goodness of fit of the targeting, to determine the likely prospects and the expected utility or profit.  Successful results are given for three industrial problems.  The method consistently produced trees with a very small number of decision nodes.  Each decision consisted of a relatively small number of attributes, which evinces the method's power of dimensionality reduction.  The largest training dataset tested contained 15,700 points with 866 attributes.  The commercial optimization package used, CPLEX, is capable of solving even larger problems. 
Optimal Decision Trees| Abstract We propose an Extreme Point Tabu Search (EPTS) algorithm that constructs globally optimal decision trees for classification problems.  Typically, decision tree algorithms are greedy.  They optimize the misclassification error of each decision sequentially.  Our non-greedy approach minimizes the misclassification error of all the decisions in the tree concurrently.  Using Global Tree Optimization (GTO), we can optimize existing decision trees.  This capability can be used in classification and data mining applications to avoid overfitting, transfer knowledge, incorporate domain knowledge, and maintain existing decision trees.  Our method works by fixing the structure of the decision tree and then representing it as a set of disjunctive linear inequalities.  An optimization problem is constructed that minimizes the errors within the disjunctive linear inequalities.  To reduce the misclassification error, a nonlinear error function is minimized over a polyhedral region.  We show that it is sufficient to restrict our search to the extreme points of the polyhedral region.  A new EPTS algorithm is used to search the extreme points of the polyhedral region for an optimal solution.  Promising computational results are given for both randomly generated and real-world problems. 
Duality and Geometry in SVM Classifiers| Abstract We develop an intuitive geometric interpretation of the standard support vector machine (SVM) for classification of both linearly separable and inseparable data and provide a rigorous derivation of the concepts behind the geometry.  For the separable case finding the maximum margin between the two sets is equivalent to finding the closest points in the smallest convex sets that contain each class (the convex hulls).  We now extend this argument to the inseparable case by using a reduced convex hull reduced away from outliers.  We prove that solving the reduced convex hull formulation is exactly equivalent to solving the standard inseparable SVM for appropriate choices of parameters.  Some additional advantages of the new formulation are that the effect of the choice of parameters becomes geometrically clear and that the formulation may be solved by fast nearest point algorithms.  By changing norms these arguments hold for both the standard 2-norm and 1-norm SVM. 
A Genetic Algorithm Approach for Semi-Supervised Clustering| Abstract A novel semi-supervised clustering algorithm is proposed that synergizes the benefits of supervised and unsupervised learning methods.  Data are clustered
Global Tree Optimization: A Non-greedy Decision Tree Algorithm| Abstract A non-greedy approach for constructing globally optimal multivariate decision trees with fixed structure is proposed.  Previous greedy tree construction algorithms are locally optimal in that they optimize some splitting criterion at each decision node, typically one node at a time.  In contrast, global tree optimization explicitly considers all decisions in the tree concurrently.  An iterative linear programming algorithm is used to minimize the classification error of the entire tree.  Global tree optimization can be used both to construct decision trees initially and to update existing decision trees.  Encouraging computational experience is reported. 
Semi-Supervised Support Vector Machines| Abstract We introduce a semi-supervised support vector machine (S 3 VM) method.  Given a training set of labeled data and a working set of unlabeled data, S 3 VM constructs a support vector machine using both the training and working sets.  We use S 3 VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik.  The transduction problem is to estimate the value of a classification function at the given points in the working set.  This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data.  We propose a general S 3 VM model that minimizes both the misclassification error and the function capacity based on all the available data.  We show how the S 3 VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming.  Results of S 3 VM and the standard 1-norm support vector machine approach are compared on eleven data sets.  Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insu#cient training information is available.  In every case, S 3 VM either improved or showed no significant difference in generalization compared to the traditional approach. 
Sparse Regression Ensembles in Infinite and Finite Hypothesis Spaces| Abstract We examine methods for constructing regression ensembles based on a linear program (LP).  The ensemble regression function consists of linear combinations of base hypotheses generated by some boosting-type base learning algorithm.  Unlike the classification case as in AdaBoost, for regression the set of possible hypotheses producible by the base learning algorithm may be infinite.  We explicitly tackle the issue of how to define and solve ensemble regression when the hypothesis space is infinite.  Our approach is based on a semi-infinite linear program that has an infinite number of constraints and a finite number of variables.  We show that the regression problem is well posed for infinite hypothesis spaces in both the primal and dual spaces.  Most importantly, we prove there exists an optimal solution to the infinite hypothesis space problem consisting of a finite number of hypothesis.  We propose two algorithms for solving the infinite and finite hypothesis problems.  One uses column generation simplex-type algorithm and the other adopts an exponential barrier approach.  Furthermore, we give sucient conditions on the base learning algoritm and the hypothesis set to be used for infinite regression ensembles.  Computational results show that these methods are extremely promising. 
Enlarging the Margins in Perceptron Decision Trees| Abstract Capacity control in perceptron decision trees is typically performed by controlling their size.  We prove that other quantities can be as relevant to reduce their flexibility and combat overfitting.  In particular, we provide an upper bound on the generalization error which depends both on the size of the tree and on the margin of the decision nodes.  So enlarging the margin in perceptron decision trees will reduce the upper bound on generalization error.  Based on this analysis, we introduce three new algorithms, which can induce large margin perceptron decision trees.  To assess the effect of the large-margin bias, OC1 [18] of Murthy, Kasif and Salzberg, a well-known system for inducing perceptron decision tree, is used as the baseline algorithm.  An extensive experimental study on real world data showed that all three new algorithms perform better or at least not significantly worse than OC1 on almost every dataset with only one exception.  OC1 did worse than the best margin-based method on every dataset. 
Multicategory Discrimination via Linear Programming| Abstract A single linear program is proposed for discriminating between the elements of k disjoint point sets in the n-dimensional real space R n : When the conical hulls of the k sets are (k\Gamma 1)-point disjoint in R n+1 , a k-piece piecewise-linear surface generated by the linear program completely separates the k sets.  This improves on a previous linear programming approach which required that each set be linearly separable from the remaining k \Gamma 1 sets.  When the conical hulls of the k sets are not (k \Gamma 1)-point disjoint, the proposed linear program generates an error-minimizing piecewise-linear separator for the k sets.  For this case it is shown that the null solution is never a unique solver of the linear program and occurs only under the rather rare condition when the mean of each point set equals the mean of the means of the other k \Gamma 1 sets.  This makes the proposed linear computational programming formulation useful for approximately discriminating between k sets that are not piecewise-linear separable.  Computational results are reported for three previously available databases. 
Multicategory Classification by Support Vector Machines| Abstract We examine the problem of how to discriminate between objects of three or more classes.  Specifically, we investigate how two-class discrimination methods can be extended to the multiclass case.  We show how the linear programming (LP) approaches based on the work of Mangasarian and quadratic programming (QP) approaches based on Vapnik's Support Vector Machines (SVM) can be combined to yield two new approaches to the multiclass problem.  In LP multiclass
To appear in Optimization Methods and Software ROBUST LINEAR PROGRAMMING DISCRIMINATION OF TWO LINEARLY INSEPARABLE SETS| A single linear programming formulation is proposed which generates a plane that minimizes an average sum of misclassified points belonging to two disjoint points sets in n-dimensional real space.  When the convex hulls of the two sets are also disjoint, the plane completely separates the two sets.  When the convex hulls intersect, our linear program, unlike all previously proposed linear programs, is guaranteed to generate some error-minimizing plane, without the imposition of extraneous normalization constraints that inevitably fail to handle certain cases.  The effectiveness of the proposed linear program has been demonstrated by successfully testing it on a number of databases.  In addition, it has been used in conjunction with the multisurface method of piecewiselinear separation to train a feed-forward neural network with a single hidden layer. 
Feature Minimization within Decision Trees| Abstract Decision trees for classification can be constructed using mathematical programming.  Within decision tree algorithms, the feature minimization problem is to construct accurate decisions using as few features or attributes within each decision as possible.  Feature minimization is an important aspect of data mining since it helps identify what attributes are important and helps produce accurate and interpretable decision trees.  In feature minimization with bounded accuracy, we minimize the number of features using a given misclassification error tolerance.  This problem can be formulated as a parametric bilinear program and is shown to be NP-complete.  A parametric FrankWolfe method is used to solve the bilinear subproblems.  The resulting minimization algorithm produces more compact, accurate, and interpretable trees.  This procedure can be applied to many different error functions.  Formulations and results for two error functions are given.  One method, FM RLP-P, dramatically reduced the number of features of one dataset from 147 to 2 while maintaining an 83. 6% testing accuracy.  Computational results compare favorably with the standard univariate decision tree method, C4. 5, as well as with linear programming methods of tree construction. 
Semi-Supervised Clustering Using Genetic Algorithms| Abstract A semi-supervised clustering algorithm is proposed that combines the benefits of supervised and unsupervised learning methods.  Data are
Linear Programming Boosting via Column Generation| Abstract.  We examine linear program (LP) approaches to boosting and demonstrate their ecient solution using LPBoost, a column generation based simplex method.  We formulate the problem as if all possible weak hypotheses had already been generated.  The labels produced by the weak hypotheses become the new feature space of the problem.  The boosting task becomes to construct a learning function in the label space that minimizes misclassification error and maximizes the soft margin.  We prove that for classification, minimizing the 1-norm soft margin error function directly optimizes a generalization error bound.  The equivalent linear program can be eciently solved using column generation techniques developed for large-scale optimization problems.  The resulting LPBoost algorithm can be used to solve any LP boosting formulation by iteratively optimizing the dual misclassification costs in a restricted LP and dynamically generating weak hypotheses to make new LP columns.  We provide algorithms for soft margin classification,
in Infinite and Finite Hypothesis Spaces| Abstract.  We examine methods for constructing regression ensembles based on a linear program (LP).  The ensemble
Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING| Abstract We examine mathematical models for semi-supervised support vector machines (S 3 VM).  Given a training set of labeled data and a working set of unlabeled data, S 3 VM constructs a support vector machine using both the training and working sets.  We use S 3 VM to solve the transductive inference problem posed by Vapnik. 
Large Margin Trees for Induction and Transduction| Abstract The problem of controlling the capacity of decision trees is considered for the case where the decision nodes implement linear threshold functions.  In addition to the standard early stopping and pruning procedures, we implement a strategy based on the margins of the decision boundaries at the nodes.  The approach is motivated by bounds on generalization error obtained in terms of the margins of the individual classifiers.  Experimental results are given which demonstrate that considerable advantage can be derived from using the margin information.  The same strategy is applied to the problem of transduction, where the positions of the testing points are revealed to the training algorithm.  This information is used to generate an alternative training criterion motivated by transductive theory.  In the transductive case, the results are not as encouraging, suggesting that little, if any, consistent advantage is culled from using the unlabelled data in the proposed fashion.  This conclusion does not contradict theoretical results, but leaves open the theoretical and practical question of whether more effective use can be made of the additional information. 
Robust linear programming discrimination of two linearly inseparable sets|
Combining support vector and mathematical programming methods for classification|
Density-based indexing for nearest neighbor queries|
Neural network training via linear programming|
Neural-network training via linear programming," Dept|
in press)| Geometry in learning. 
Feature selection for insilico drug design using genetic algorithms and neural networks|
Decision tree construction via linear programming|
Optimal decision trees through multilinear programming|
Density-Based Indexing for Approximate Nearest-Neighbor Queries|
A Linear Programming Approach to Novelty Detection|
A Genetic Algorithm for Database Query Optimization|
Sparse Kernel Partial Least Squares Regression|
Large margin decision trees for induction and transduction|
Multicategory separation via linear programming|
Unlabeled data supervised learning competition,|
Breast cancer diagnosis and prognostic determination from cell analysis| Manuscript, Departments of Surgery and Human Oncology and Computer Sciences,. 
An optimization perspective on kernel partial least squares regression|
Linear Programming Boosting via Column Generation|
Support vector machines approaches to pharmaceutical modeling|
Wavelet representations of molecular electronic properties: Applications in adme, qspr, and qsar| Presentation, QSAR in Cells Symposium of the Computers in Chemistry Division's 220th. 
MARK: a boosting algorithm for heterogeneous kernel models|
Optimization approaches to semisupervised learning|
Prediction of Protein Retention Times in Anion-Exchange Chromatography Systems Using Support Vector Regression|
Geometry in learning| Web manuscript, Rensselaer Polytechnic Institute,. 
Global tree optimizaiton:a non-greedy decision tree algorithm|
Bilinear separation in n-space|
A column generation approach to boosting|

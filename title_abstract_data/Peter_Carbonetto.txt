Why can't Jose read? The problem of learning semantic associations in a robot environment| Abstract We study the problem of learning to recognise objects in the context of autonomous agents.  We cast object recognition as the process of attaching meaningful concepts to specific regions of an image.  In other words, given a set of images and their captions, the goal is to segment the image, in either an intelligent or naive fashion, then to find the proper mapping between words and regions.  In this paper, we demonstrate that a model that learns spatial relationships between individual words not only provides accurate annotations, but also allows one to perform recognition that respects the real-time constraints of an autonomous, mobile robot. 
A Statistical Model for General Contextual Object Recognition| Abstract.  We consider object recognition as the process of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects.  Given a set of images and their associated text (e. g.  keywords, captions, descriptions), the objective is to segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions.  Previous models are limited by the scope of the representation.  In particular, they fail to exploit spatial context in the images and words.  We develop a more expressive model that takes this into account.  We formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens.  By learning both word-to-region associations and object relations, the proposed model augments scene segmentations due to smoothing implicit in spatial consistency.  Context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the EM algorithm for estimating the model parameters and densities of the unknown alignment variables.  Instead, we develop an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step.  The experiments indicate that our approximate inference and learning algorithm converges to good local solutions.  Experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition.  Most significantly, spatial context combined with a nonlinear discrete object representation allows our models to cope well with over-segmented scenes. 
Unsupervised Statistical Models for General Object Recognition| Abstract We approach the object recognition problem as the process of attaching meaningful labels to specific regions of an image.  Given a set of images and their captions, we segment the images, in either a crude or sophisticated fashion, then learn the proper associations between words and regions.  Previous models are limited by the scope of the representation, and performance is constrained by noise from poor initial clusterings of the image features.  We propose three improvements that address these issues.  First, we describe a model that incorporates clustering into the learning step using a basic mixture model.  Second, we propose Bayesian priors on the mixture model to stabilise learning and automatically weight features.  Third, we develop a more expressive model that learns spatial relations between regions of a scene.  Using the analogy of building a lexicon via an aligned bitext, we formulate a probabilistic mapping between the image feature vectors and the supplied word tokens.  To find the best hypothesis, we hill-climb the log-posterior using the EM algorithm.  Spatial context introduces cycles to our probabilistic graphical model, so we use loopy belief propagation to compute the expectation of the complete log-posterior, and iterative scaling and iterative proportional fitting on the pseudo-likelihood approximation to render parameter estimation tractable.  The EM algorithm is no longer guaranteed to converge with an intractable posterior, but experiments show the approximate E and M Steps consistently converge to a local solution.  Empirical results on a diverse array of images show that learning image feature clusters using a standard mixture model, feature weighting using Bayesian shrinkage priors and spatial context potentials considerably improve the accuracy of general object recognition.  Moreover, results suggest that good performance can be obtained without expensive image segmentations. 
Bayesian learning for weakly supervised object classification| Abstract We explore the extent to which we can exploit interest point detectors for representing and recognising classes of objects.  Detectors propose sparse sets of candidate regions based on local salience and stability criteria.  However, local selection does not take into account discrimination reliability across instances in the same object class, so we realise selection by learning from weakly supervised data in the form of images paired with their captions.  Through experiments on a wide variety of object classes and detectors, we show that modeling object recognition as a constrained data association problem and learning the Bayesian way by integrating over multiple hypotheses leads to sparse classifiers that outperform contemporary methods.  Moreover, our learned representations based on local features leave little room for improvement on standard image databases, so we propose new data sets to corroborate models for general object recognition. 
Bayesian feature weighting for unsupervised learning, with application to object recognition|
Why can't Josread? the problem of learning semantic associations in a robot environment|
A constrained semi-supervised learning approach to data association|

Multi-Protocol Visualization A Tool Demonstration| ABSTRACT This paper describes a system for the visualization of multiple protocols.  The visualizer makes possible the identification of both intra and inter-protocol behaviour.  This tool has become a critical resource in the development of our multi-protocol monitoring system; allowing the verification of the monitoring system, identification of new modes of behaviour and the easy visualization of potentially overwhelming quantities of information 1 . 
Ecient algorithms for non-parametric clustering with clutter| Abstract Detecting and counting overdensities in data is a common problem in the physical and geographic sciences.  One of the most successful of recent algorithms for the counting version of the problem was introduced by Cuevas, Febrero and Fraiman [Cuevas et al. , 2000], which will be referred to as the CFF algorithm.  This algorithm first determines the subset of data points that are in high density regions using a non-parametric density estimator.  A clustering step follows where such high density points are agglomerated.  While this algorithm was originally intended to estimate the number of clusters, it can also be used to perform non-parametric clustering against a noisy background.  However, the algorithm proposed by CFF is too computationally expensive to work on large datasets with greater than two dimensions.  We propose an alternative implementation of the CFF algorithm producing exactly the same results but addressing the computational problems in both the density estimation and in the agglomeration step.  We will then illustrate the effectiveness of our approach on large multi-dimensional astrophysics datasets. 
Efficient Value of Information for Graphical Models --DRAFT-| Abstract Value of information (VOI) calculations are vital to many operations on in graphical models, e. g. , active learning and sensitivity analysis.  Previously, all-pairs VOI calculation has involved a cost quadratic in network size.  In this work, we show how to perform the same computation with cost linear in network size.  The loss function that allows this is of a form amenable to computation by dynamic programming.  The messagepassing algorithm that results is described and empirical results demonstrate large speedups without decrease in accuracy.  In the cost-sensitive domains examined, superior accuracy is achieved. 
Optimal Reinsertion: A New Search Operator for Accelerated and More Accurate Bayesian Network Structure Learning| Abstract We show how a conceptually simple search operator called Optimal Reinsertion can be applied to learning Bayesian Network structure from data.  On each step we pick a node called the target.  We delete all arcs entering or exiting the target.  We then find, subject to some constraints, the globally optimal combination of in-arcs and out-arcs with which to reinsert it.  The heart of the paper is a new algorithm called ORSearch which allows each optimal reinsertion step to be computed efficiently on large datasets.  Our empirical results compare Optimal Reinsertion against a highly tuned implementation of multi-restart hill climbing.  The results typically show one to two orders of magnitude speed-up on a variety of datasets.  They usually show better final results, both in terms of BDEU score and in modeling of future data drawn from the same distribution.  1.  Bayesian Network Structure Search Given a dataset of R records and m categorical attributes, how can we find a Bayesian network structure that provides a good model of the data? Happily, the formulation of this question into a well-defined optimization problem is now fairly well understood (Heckerman et al. , 1995; Cooper & Herskovits, 1992).  However, finding the optimal solution is an NP-complete problem (Chickering, 1996a).  The computational issues in performing heuristic search in this space are also severe, even taking into account the numerous ingenious and effective innovations in recent years (e. g.  (Chickering, 1996b; Friedman & Goldszmidt, 1997;
A Nonparametric Approach to Noisy and Costly Optimization| Abstract This paper describes Pairwise Bisection: a nonparametric approach to optimizing a noisy function with few function evaluations.  The algorithm uses nonparametric reasoning about simple geometric relationships to find minima efficiently.  Two factors often frustrate optimization: noise and cost.  Output can contain significant quantities of noise or error, while time or money allows for only a handful of experiments.  Pairwise bisection is used here to attempt to automate the process of robust and efficient experiment design.  Real world functions also tend to violate traditional assumptions of continuousness and Gaussian noise.  Since nonparametric statistics do not depend on these assumptions, this algorithm can optimize a wide variety of phenomena with fewer restrictions placed on noise.  The algorithm's performance is compared to that of three competing algorithms, Amoeba, PMAX, and Q2 on several different test functions.  Results on these functions indicate competitive performance and superior resistance to noise.  1.  Problem The problem of optimizing a function f : ! n ! ! will be discussed here as finding a local minimum for the function, i. e.  a point x \Lambda such that there exists a neighborhood B of x \Lambda with f(x \Lambda ) f(x) 8x 2 B: (1) In the noisy case, the observed output y is a combination of the underlying function f and some amount of error or noise.  Assuming noise has a mean of zero, y(x) = f(x) + noise (2) For example, during the optimization of a chemical process, if the underlying function f(x) is the yield of a chemical reaction using parameters x (say, temperature and pH,) then the noisy output y(x) might represent the observed yield.  The task of the optimizer is to find x \Lambda , the combination of temperature and pH with the greatest expected yield.  Many algorithms exist to perform this task, especially in the numerical analysis literature (Press et al. , 1992), but they typically require many iterations of the experimental cycle.  When experiments are costly or time-consuming, these algorithms are inappropriate. 
An Investigation of Practical Approximate Nearest Neighbor Algorithms| Abstract This paper concerns approximate nearest neighbor searching algorithms, which have become increasingly important, especially in high dimensional perception areas such as computer vision, with dozens of publications in recent years.  Much of this enthusiasm is due to a successful new approximate nearest neighbor approach called Locality Sensitive Hashing (LSH).  In this paper we ask the question: can earlier spatial data structure approaches to exact nearest neighbor, such as metric trees, be altered to provide approximate answers to proximity queries and if so, how? We introduce a new kind of metric tree that allows overlap: certain datapoints may appear in both the children of a parent.  We also introduce new approximate k-NN search algorithms on this structure.  We show why these structures should be able to exploit the same randomprojection-based approximations that LSH enjoys, but with a simpler algorithm and perhaps with greater efficiency.  We then provide a detailed empirical evaluation on five large, high dimensional datasets which show accelerations one to three orders of magnitude over LSH.  This result holds true throughout the spectrum of approximation levels. 
Security and Survivability Reasoning Frameworks and Architectural Design Tactics| KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL.  CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.  Use of any trademarks in this report is not intended in any way to infringe on the rights of the trademark holder. 
Accelerating Exact k- 2911 Algorithms with Geometric Reasoning| Abstract We present new algorithms for the k- 3391 clustering problem.  They use the kd-d 22 data structure to reduce the large number of nearest-33594 bor queries issued by the traditional algorithm.  Sufficient statistics are stored in the nodes of the kd-d 343 Then, an analysis of the geometry of the current cluster centers results in great reduction of the work needed to update the centers.  Our algorithms behave exactly as the traditional k- 2743 algorithm.  Proofs of correctness are included.  The kd-d 24 can also be used to initialize the k- 4370 starting centers efficiently.  Our algorithms can be easily extended to provide fast ways of computing the error of a given cluster assignment, regardless of the method in which those clusters were obtained.  We also showhow to use them in a setting whichallows approximate clustering results, with the benefit of running faster.  Wehave implemented and tested our algorithms on both real and simulated data.  Results show a speedup factor of up to 170 on real astrophysical data, and superiorityover the naive algorithm onsimulated data in up to 5 dimensions.  Our algorithms scale well with respect to the number of points and number of centers, allowing for clustering with tens of thousands of centers. 
Accelerating Exact k-means Algorithms with Geometric Reasoning| Abstract We present new algorithms for the k-means clustering problem.  They use the kd-tree data structure to reduce the large number of nearest-neighbor queries issued by the traditional algorithm.  Sufficient statistics are stored in the nodes of the kd-tree.  Then, an analysis of the geometry of the current cluster centers results in great reduction of the work needed to update the centers.  Our algorithms behave exactly as the traditional k-means algorithm.  Proofs of correctness are included.  The kd-tree can also be used to initialize the k-means starting centers efficiently.  Our algorithms can be easily extended to provide fast ways of computing the error of a given cluster assignment, regardless of the method in which those clusters were obtained.  We also show how to use them in a setting which allows approximate clustering results, with the benefit of running faster.  We have implemented and tested our algorithms on both real and simulated data.  Results show a speedup factor of up to 170 on real astrophysical data, and superiority over the naive algorithm on simulated data in up to 5 dimensions.  Our algorithms scale well with respect to the number of points and number of centers, allowing for clustering with tens of thousands of centers. 
Robust Value Function Approximation by Working Backwards| Abstract In this paper, we examine the intuition that TD( ) is meant to operate by approximating asynchronous value iteration.  We note that on the important class of discrete acyclic stochastic tasks, value iteration is inefficient compared with the DAG-SP algorithm, which essentially performs only one sweep instead of many by working backwards from the goal.  The question we address in this paper is whether there is an analogous algorithm that can be used in large stochastic state spaces requiring function approximation.  We present such an algorithm, analyze it, and give comparative results to TD on several domains.  LEARNING CONTROL BACKWARDS Computing an accurate value function is the key to dynamic-programming-based algorithms for optimal sequential control in Markov Decision Processes.  The optimal value function V \Lambda (x) specifies, for each state x in the state space X, the expected cumulative reward when starting in state x and acting optimally thereafter.  It is also the unique solution to the Bellman equations (using the notation of [11]): 8x 2 X, V (x) = n R(x) if x is a terminal state max a2A(x) \Gamma R(x; a) + fl P y2X Prob(x a ! y)V (y) \Delta otherwise (1) The Bellman equation at x also reveals the optimal control from x: any action which instantiates the max is an optimal choice [2].  For small discrete problems, the value function can be stored in a lookup table and computed by iterative algorithms such as value iteration (VI) [2].  VI computes V \Lambda by repeatedly sweeping over the state space, applying Equation 1 as an assignment statement (this is called a "one-step backup") at each state in parallel.  If the lookup table is initialized with all 0's, then after i sweeps of VI, the table will represent the maximum expected return of a path of length i from each state.  For certain goal-oriented domains, this corresponds to the intuition that VI works by propagating correct V \Lambda values backwards, by one step per iteration, from the terminal states.  More precisely, there are two classes of MDPs for which correct V \Lambda values can be assigned by working strictly backwards from terminal states: 1.  deterministic domains with no positive-
Beyond Gigabit Ethernet: Physical Layer Issues in Future Optical Networks| Abstract: This paper presents a study of the errors observed on an optical Gigabit Ethernet link in a state of low receiver power.  This condition is increasingly likely as networks become more complex, with longer fibre lengths, optical switching systems and higher data rates.  We discover that some octets and sequences of octets have a far higher probability of being received in error than others.  This non-uniformity of error in the physical layer may severely affect network performance at higher levels, and should be carefully considered as the next generation of optical networks is developed. 
Very Fast Outlier Detection in Large Multidimensional Data Sets| Abstract.  Outliers are objects that do not comply with the general behavior of the data.  Applications such as exploration in science databases need fast interactive tools for outlier detection in data sets that have unknown distributions, are large in size, and are in high dimensional space.  Existing algorithms for outlier detection are too slow for such applications.  We present an algorithm based on an innovative use of k-d trees that doesn't assume any probability model and is linear in the number of objects and in the number of dimensions.  We also provide experimental results that show that this is indeed a practical solution to the above problem. 
Active Learning in Discrete Input Spaces| Abstract Traditional design of experiments (DOE) from the statistics literature focuses on optimizing an output parameter over a space of continuous input parameters.  Here we consider DOE, or active learning, for discrete input spaces.  A trivial example of this is the k-armed bandit problem, which is the case of having a single input attribute of arity k.  We address the full problem of many attributes where it is impossible to test every combination of attribute-value pairs even once within the given number of experiments, but we expect to be able to generalize on the results of experiments.  We further pose the problem of active learning on fixed experiment sets where we can not choose any possible setting of the input variables, but instead must choose from a fixed set of available experiments.  We discuss discrete DOE and fixed experiment sets in marketing and pharmaceutical domains.  We propose several active learning algorithms based on the idea of building a function approximator for the experiments taken so far and using its predictions and confidence intervals to select future experiments.  The algorithms are tested using commonly available data sets.  We conclude with our ideas for extending these algorithms. 
First Year Report Computer Laboratory| My research interests are in the field of network security infrastructures, where the focus of my work is currently on intrusion detection & prevention, honeypot technology, traffic monitoring & analysis and behavioural modelling.  This is reflected in the projects I have worked on during my time in the Systems Research Group so far: Nprobe is one of the major research efforts conducted in the Systems Research Group, addressing several open issues in the field of network monitoring such as scalability problems when monitoring fast networks, space-efficient storage of traffic data, distributed monitoring, online and offline multi-protocol analysis, connection modelling and others.  Section 2 describes my involvement in the Nprobe project.  Honeycomb is a system for automated generation of intrusion detection signatures using honeypots; this project evolved out of my interest in intrusion detection, honeypots and contributions to the open source honeypot honeyd.  This work also touches on several of the current problems of security infrastructures: telling benign from malicious behaviour; distribution, aggregation and correlation of security-relevant information such as alerts, attack signatures and system configuration; reacting to intrusions, integration of different kinds of security-relevant computing devices; just to name a few.  Section 3 describes the Honeycomb project in detail.  Netdude is a framework for inspection and manipulation of network traffic captured in libpcap trace files and the topic of Section 4.  This project was started during my work on traffic normalisation as an intern at ACIRI, when we recognised the need for a tool that allows us to make specific modifications to trace files in order to test our normalisations.  Since then, a large amount of effort has gone into enabling not only editing in-the-small, but also scalable and robust handling of arbitrary large traces; a feature that currently no other toolset for trace file manipulation provides. 
Preliminary System Dynamics Maps of the Insider Cyber-threat Problem| Abstract Twenty five researchers from eight institutions and a variety of disciplines, viz. 
cGraph: A Fast Graph-Based Method for Link Analysis and Queries| Abstract Many techniques in the social sciences and graph theory deal with the problem of examining and analyzing patterns found in the underlying structure and associations of a group of entities.  However, much of this work assumes that this underlying structure is known or can easily be inferred from data, which may often be an unrealistic assumption for many real-world problems.  Below we consider the problem of learning and querying a graph-based model of this underlying structure.  The model is learned from noisy observations linking sets of entities.  We explicitly allow different types of links (representing different types of relations) and temporal information indicating when a link was observed.  We quantitatively compare this representation and learning method against other algorithms on the task of predicting future links and new "friendships" in a variety of real world data sets. 
Structured Errors in Optical Gigabit Ethernet Packets| PAM DISCLAIMER: THIS DOCUMENT IS PROVIDED TO YOU "AS IS" WITH NO WARRANTIES WHATSOEVER, INCLUDING ANY WARRANTY OF MERCHANTABILITY NON-INFRINGEMENT,
Memory-based Stochastic Optimization| Abstract In this paper we introduce new algorithms for optimizing noisy plants in which each experiment is very expensive.  The algorithms build a global non-linear model of the expected output at the same time as using Bayesian linear regression analysis of locally weighted polynomial models.  The local model answers queries about confidence, noise, gradient and Hessians, and use them to make automated decisions similar to those made by a practitioner of Response Surface Methodology.  The global and local models are combined naturally as a locally weighted regression.  We examine the question of whether the global model can really help optimization, and we extend it to the case of time-varying functions.  We compare the new algorithms with a highly tuned higher-order stochastic optimization algorithm on randomly-generated functions and a simulated manufacturing task.  We note significant improvements in total regret, time to converge, and final solution quality. 
Finding Underlying Connections: A Fast Graph-Based Method for Link Analysis and Collaboration Queries| Abstract Many techniques in the social sciences and graph theory deal with the problem of examining and analyzing patterns found in the underlying structure and associations of a group of entities.  However, much of this work assumes that this underlying structure is known or can easily be inferred from data, which may often be an unrealistic assumption for many real-world problems.  Below we consider the problem of learning and querying a graph-based model of this underlying structure.  The model is learned from noisy observations linking sets of entities.  We explicitly allow different types of links (representing different types of relations) and temporal information indicating when a link was observed.  We quantitatively compare this representation and learning method against other algorithms on the task of predicting future links and new "friendships" in a variety of real world data sets. 
Locally Weighted Learning for Control| Abstract Lazy learning methods provide useful representations and training algorithms for learning about complex phenomena during autonomous adaptivecontrol of complex systems.  This paper surveys ways in whichlocallyweighted learning,
Very FastEM- 2162 Mixture Model Clustering using Multiresolution kd-47424| Abstract Clustering is importantinmany fi elds including manufacturing, biology,fi 16800 and astronomy.  Mixture models are a popular approach due to their statistical foundations, and EM is a very popular method forfior 15 mixture models.  EM, however, requires many accesses of the data, and thus has been dismissed as impractical (e. g.  (Zhang, Ramakrishnan, & Livny, 1996)) for data mining of enormous datasets.  We present a new algorithm, based on the multiresolution kd-trees of (Moore, Schneider, & Deng, 1997), which dramatically reduces the cost of EM-based clustering, with savings rising linearly with the number of datapoints.  Although presented here for maximum likelihood estimation of Gaussian mixture models, it is also applicable to nonGaussian models (provided class densities are monotonic in Mahalanobis distance), mixed categorical/numeric clusters, and Bayesian methods such as Autoclass (Cheeseman & Oldford, 1994). 
A Comparison of System Monitoring Methods, Passive Network Monitoring and Kernel Instrumentation| Abstract This paper presents the comparison of two methods of system monitoring, passive network monitoring and kernel instrumentation.  The comparison is made on the basis of passive network monitoring being used as a replacement for kernel instrumentation in some situations.  Despite the fact that the passive network monitoring technique is shown to perform poorly as a direct replacement for kernel instrumentation, this paper indicates the areas where passive network monitoring could be used to the greatest advantage and presents methods by which the discrepancies between results of the two techniques could be minimised. 
Packet error rate and bit error rate non-deterministic relationship in optical network applications| Abstract: The non-deterministic relationship between Bit Error Rate and Packet Error Rate is demonstrated for an optical media access layer in common use.  We show that frequency components of coded, non-random data can cause this relationship. 
Real-valued All-Dimensions Search: Low-overhead Rapid Searching over Subsets of Attributes| Abstract This paper is about searching the combinatorial space of contingency tables during the inner loop of a nonlinear statistical optimization.  Examples of this operation in various data analytic communities include searching for nonlinear combinations of attributes that contribute significantly to a regression (Statistics), searching for items to include in a decision list (machine learning) and association rule hunting (Data Mining).  This paper investigates a new, ecient approach to this class of problems, called RADSEARCH (Real-valued All-Dimensions-tree Search).  RADSEARCH finds the global optimum, and this gives us the opportunity to empirically evaluate the question: apart from algorithmic elegance what does this attention to optimality buy us? We compare RADSEARCH with other recent successful search algorithms such as CN2, PRIM, APriori, OPUS and DenseMiner.  Finally, we introduce RADREG, a new regression algorithm for learning real-valued outputs based on RADSEARCHing for highorder interactions. 
Value Function Based Production Scheduling| Abstract Production scheduling, the problem of sequentially configuring a factory to meet forecasted demands, is a critical problem throughout the manufacturing industry.  The requirement of maintaining product inventories in the face of unpredictable demand and stochastic factory output makes standard scheduling models, such as job-shop, inadequate.  Currently applied algorithms, such as simulated annealing and constraint propagation, must employ ad-hoc methods such as frequent replanning to cope with uncertainty.  In this paper, we describe a Markov Decision Process (MDP) formulation of production scheduling which captures stochasticity in both production and demands.  The solution to this MDP is a value function which can be used to generate optimal scheduling decisions online.  A simple example illustrates the theoretical superiority of this approach over replanning-based methods.  We then describe an industrial application and two reinforcement learning methods for generating an approximate value function on this domain.  Our results demonstrate that in both deterministic and noisy scenarios, value function approximation is an effective technique. 
Optical Network Packet Error-Rate due to Physical Layer Coding| Abstract--- A physical layer coding scheme is designed to make optimal use of the available physical link, providing functionality to higher components in the network stack.  This paper presents results of an exploration of the errors observed when an optical Gigabit Ethernet link is subject to attenuation.  The results show that some data symbols suffer from a far higher probability of error than others.  This effect is caused by an interaction between the physical layer and the 8B/10B block coding scheme.  We illustrate how the application of a scrambler, performing datawhitening, restores content-independent uniformity of packetloss.  We also note the implications of our work for other (N,K) block-coded systems and discuss how this effect will manifest itself in a scrambler-based system.  A conjecture is made that there is a need to build converged systems, with the combinations of physical, data-link, and network layers optimised to interact correctly.  In the mean time, what will become increasingly necessary is both an identification of the potential for failure and the need to plan around it. 
Chasing Errors through the Network Stack -A Testbed for Investigating Errors in Real Traffic on Optical Networks| Abstract--- A testbed is described which allows both physical layer errors to be observed and analysed, as well as monitoring network performance via frame loss.  Real network traffic loads can be used for testing, so that all measurements taken are representative of what would be seen in a deployed system.  We illustrate our testbed with an examination of the behaviour of a well-known networking standard, Gigabit Ethernet, in conditions of reduced receiver power on optical fibre.  Our testbed results show that the line codes used to represent the data in the network affect the bit error rate for that data.  Along with the previously reported result that bit error rate and packet error rate have only a weakly deterministic relationship, this highlights the need for testing of all network layers within a complete system carrying real world traffic. 
A Bayesian Spatial Scan Statistic| Abstract We propose a new Bayesian method for spatial cluster detection, the "Bayesian spatial scan statistic," and compare this method to the standard (frequentist) scan statistic approach.  We demonstrate that the Bayesian statistic has several advantages over the frequentist approach, including increased power to detect clusters and (since randomization testing is unnecessary) much faster runtime.  We evaluate the Bayesian and frequentist methods on the task of prospective disease surveillance: detecting spatial clusters of disease cases resulting from emerging disease outbreaks.  We demonstrate that our Bayesian methods are successful in rapidly detecting outbreaks while keeping number of false positives low. 
Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks with Mixed Continuous And Discrete Variables| Abstract Recently developed techniques have made it possible to quickly learn accurate probability density functions from data in low-dimensional continuous spaces.  In particular, mixtures of Gaussians can be fitted to data very quickly using an accelerated EM algorithm that employs multiresolution kdtrees (Moore, 1999).  In this paper, we propose a kind of Bayesian network in which low-dimensional mixtures of Gaussians over different subsets of the domain's variables are combined into a coherent joint probability model over the entire domain.  The network is also capable of modelling complex dependencies between discrete variables and continuous variables without requiring discretization of the continuous variables.  We present ecient heuristic algorithms for automatically learning these networks from data, and perform comparative experiments illustrating how well these networks model real scienti#c data and synthetic data.  We also briefly discuss some possible improvements to the networks, as well as their possible application to anomaly detection, classification, probabilistic inference, and compression. 
A Delay-Line Based Motion Detection Chip| Abstract Inspired by a visual motion detection model for the rabbit retina and by a computational architecture used for early audition in the barn owl, we have designed a chip that employs a correlation model to report the one-dimensional field motion of a scene in real time.  Using subthreshold analog VLSI techniques, we have fabricated and successfully tested a 8000 transistor chip using a standard MOSIS process. 
Explaining Structured Errors in Gigabit Ethernet| PURPOSE, MERCHANTABILITY, OR INFRINGEMENT OF ANY PATENT, COPYRIGHT OR OTHER INTELLECTUAL PROPERTY RIGHT.  Intel products are not intended for use in medical, life saving, life sustaining applications.  Intel may make changes to specifications and product descriptions at any time, without notice.  Copyright Intel Corporation 2003 * Other names and brands may be claimed as the property of others. 
Q2: Memory-Based Active Learning for Optimizing Noisy Continuous Functions| Abstract This paper introduces a new algorithm, Q2, for optimizing the expected output of a multiinput noisy continuous function.  Q2 is designed to need only a few experiments, it avoids strong assumptions on the form of the function, and it is autonomous in that it requires little problem-specific tweaking.  These capabilities are directly applicable to industrial processes, and may become increasingly valuable elsewhere as the machine learning field expands beyond prediction and function identification, and into embedded active learning subsystems in robots, vehicles and consumer products.  Four existing approaches to this problem (response surface methods, numerical optimization, supervised learning, and evolutionary methods) all have inadequacies when the requirement of "black box" behavior is combined with the need for few experiments.  Q2 uses instance-based determination of a convex region of interest for performing experiments.  In conventional instance-based approaches to learning, a neighborhood was defined by proximity to a query point.  In contrast, Q2 defines the neighborhood by a new geometric procedure that captures the size and shape of the zone of possible optimum locations.  Q2 also optimizes weighted combinations of outputs, and finds inputs to produce target outputs.  We compare Q2 with other optimizers of noisy functions on several problems, including a simulated noisy process with both non-linear continuous dynamics and discreteevent queueing components.  Results are encouraging in terms of both speed and autonomy.  1 ACTIVE LEARNING FOR OPTIMIZATION The apparently humble task of parameter tweaking for noisy systems is of great importance whether the parameters being tweaked are for an algorithm, a real manufacturing process, a simulation, or a scientific experiment.  The purpose of this paper is two-fold.  First, we wish to highlight the potential importance of machine learning as an as-yet underexploited tool in this domain.  Second, we will introduce Q2, a new algorithm designed for this domain.  We consider a generalized noisy optimization task in which a vector x of real-valued inputs produces a scalar output y that is a noisy function of x: y = g(x) + noise (1) Given a constrained space of legal inputs, the task is to find the input vector x opt that maximizes g, using only a small number of experiments.  In both industrial settings and in algorithm-tuning, this task often demands considerable human intervention and insight.  A factory manager who wants to optimize a process can: ffl Buy a computer, statistics software, and hire a professional statistician to solve the problem using insight and experiment design.  ffl Save money and try to "wing it" by manually tuning the parameters.  For highly expensive or safety-critical processes, the first option is always preferable, leaving only the question of which are the best analysis and experiment design tools for the statistician to use.  This area is heavily investigated by the academic statistics community.  But there are also many situations in which it is impractical to enlist human-aided analysis during optimization, for example if a vehicle engine self-tunes during driving.  And there are many other situations in which the potential benefit from optimization is too small to justify paying for expert professional analysis.  In such cases, it is tempting to ask: Can "black box" automated methods optimize noisy systems? If practical black box methods are found, they could be widely used.  Somewhat fancifully, this could lead to the eventual inclusion of Black Box Optimizer chips within a huge range of consumer products, from vehicle engines and industrial equipment down to refrigerators, toasters, and toys.  In the next section we discuss variants of the Black Box Noisy Optimization task.  Then in Section 3 we discuss existing approaches.  After that we present and evaluate Q2, a new algorithm.  2 VARIANTS OF NOISY OPTIMIZATION The generalized noisy optimization task summarized by Equation 1 has many variants.  For instance, in some domains each experiment is a lengthy procedure, and so there is ample computation time between experiments.  In other domains, experiments are very quick, leaving an optimizer little time to make its recommendations.  The specifics of the domain determine which methods are appropriate.  The following factors need to be considered: ffl Minimize regret or the number of experiments? Do we pay a constant cost per experiment, or do experiments with poor results cost us more? In scenarios such as tuning the parameters for an algorithm, or optimizing a test plant in which all products will be discarded, the cost per experiment may be constant.  But in a task such as minimizing the fuel consumption of a running engine, some experiments cost more than others.  Here, we focus on simply minimizing the number of experiments.  Note that this presumes that we are not risk-averse: there is no penalty for performing highly unpredictable experiments.  ffl How much computer time is available to choose experiments? If experiments are very cheap and very quick, then an algorithm that needs extensive CPU time to select the ideal next experiment could still be inferior to one that requires only a fraction of a second to suggest a reasonable-but-less-than-ideal experiment.  Here, we assume that experiments are costly enough (in time or money) that it pays to choose them carefully.  But the Q2 algorithm can be adjusted to satisfy any desired tradeoff between the speed and the quality of proposed experiments.  ffl Are we doing local or global optimization? Unless we have strong prior knowledge, global optimization of a function of more than a couple of inputs requires a very large number of experiments.  Q2 is only designed to find a local optimum, though empirically it appears to be good at discovering the global optimum.  ffl Can we re-use old data? Many algorithms have a "current location" or "current set of k recent evaluations" but otherwise disregard earlier evaluations.  Q2, however, can exploit any existing data, including previous evaluations obtained by other experimental methods.  In this paper we also assume that there are no long term dynamics, i. e.  the output of the n'th experiment depends only on the n'th chosen x, not on previous x values or the time.  Unlike [2, 6] we only try to find the optimum, not to model the g function.  3 POSSIBLE APPROACHES Many disciplines have methods that are relevant to noisy optimization.  Space permits only a brief survey.  Numerical analysis: Numerical methods such as Newton-Raphson or Levenberg-Marquardt [11] have fast convergence properties, but they must be applied carefully to prevent oscillations or divergence to infinity, which violates our desire for black box autonomy.  Furthermore, current numerical methods cannot survive noise.  Stochastic approximation: The algorithm of [12] finds roots without the use of derivative estimates.  Keifer-Wolfowitz (KW) [5] is a related algorithm for noisy optimization.  It estimates the gradient by performing experiments in both directions along each dimension of the input space.  Based on the estimate, it moves its experiment center and repeats.  It uses decreasing step sizes to ensure convergence.  KW's strengths are its aggressive exploration, its simplicity, and that it comes with convergence guarantees.  However, it can attempt wild experiments if there is noise, and discards the data it collects after each gradient estimate is made.  Amoeba (see below) is a similar approach, but in our experience is superior to KW.  Amoeba search: Amoeba [11] searches k-d space using a simplex (i. e.  a k-dimensional tetrahedron).  The function is evaluated at each vertex.  The worstperforming vertex is reflected through the hyperplane defined by the remaining vertices to produce a new simplex that has moved up the estimated gradient.  Ingenious simplex transformations let the simplex shrink near the optimum, grow in large linear zones, and ooze along ridges. 
Cached Suffi1622 t Statistics for Effi 333 tMachine Learning with Large Datasets| Abstract This paper introduces new algorithms and data structures for quick counting for machine learning datasets.  We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries.  Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingencytable.  Weprovide a very sparse data structure, the ADtree, to minimize memoryuse.  We provide analytical worst-case bounds for this structure for several models of data distribution.  We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves.  We showhowtheADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and weprovide a number of empirical results comparing ADtree methods against traditional direct counting approaches.  We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and FrequentSets. 
Probabilistic Noise Identification and Data Cleaning| Abstract Real world data is never as perfect as we would like it to be and can often suffer from corruptions that may impact interpretations of the data, models created from the data, and decisions made based on the data.  One approach to this problem is to identify and remove records that contain corruptions.  Unfortunately, if only certain fields in a record have been corrupted then usable, uncorrupted data will be lost.  In this paper we present LENS, an approach for identifying corrupted fields and using the remaining noncorrupted fields for subsequent modeling and analysis.  Our approach uses the data to learn a probabilistic model containing three components: a generative model of the clean records, a generative model of the noise values, and a probabilistic model of the corruption process.  We provide an algorithm for the unsupervised discovery of such models and empirically evaluate both its performance at detecting corrupted fields and, as one example application, the resulting improvement this gives to a classifier. 
Mixtures of Rectangles: Interpretable Soft Clustering| Abstract To be effective, data-mining has to conclude with a succinct description of the data.  To this end, we explore a clustering technique that finds dense regions in data.  By constraining our model in a specific way, we are able to represent the interesting regions as an intersection of intervals.  This has the advantage of being easily read and understood by humans.  Specifically, we fit the data to a mixture model in which each component is a hyperrectangle in M-dimensional space.  Hyperrectangles may overlap, meaning some points can have soft membership of several components.  Each component is simply described by, for each attribute, lower and upper bounds of points in the cluster.  The computational problem of finding a locally maximum-likelihood collection of k rectangles is made practical by allowing the rectangles to have soft "tails" in the early stages of an EM-like optimization scheme.  Our method requires no user-supplied parameters except for the desired number of clusters.  These advantages make it highly attractive for "turn-key" data-mining application.  We demonstrate the usefulness of the method in subspace clustering for synthetic data, and in real-life datasets.  We also show its effectiveness in a classification setting. 
An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators| Abstract The generalization error of a
Direct Policy Search using Paired Statistical Tests| Abstract Direct policy search is a practical way to solve reinforcement learning problems involving continuous state and action spaces.  The goal becomes finding policy parameters that maximize a noisy objective function.  The Pegasus method converts this stochastic optimization problem into a deterministic one, by using fixed start states and fixed random number sequences for comparing policies (Ng & Jordan, 1999).  We evaluate Pegasus, and other paired comparison methods, using the mountain car problem, and a difficult pursuer-evader problem.  We conclude that: (i) Paired tests can improve performance of deterministic and stochastic optimization procedures.  (ii) Our proposed alternatives to Pegasus can generalize better, by using a different test statistic, or changing the scenarios during learning.  (iii) Adapting the number of trials used for each policy comparison yields fast and robust learning. 
Tractable learning of large Bayes net structures from sparse data| Abstract This paper addresses three questions.  Is it useful to attempt to learn a Bayesian network structure with hundreds of thousands of nodes? How should such structure search proceed practically? The third question arises out of our approach to the second: how can Frequent Sets (Agrawal et al. , 1993), which are extremely popular in the area of descriptive data mining, be turned into a probabilistic model? Large sparse datasets with hundreds of thousands of records and attributes appear in social networks, warehousing, supermarket transactions and web logs.  The complexity of structural search made learning of factored probabilistic models on such datasets unfeasible.  We propose to use Frequent Sets to significantly speed up the structural search.  Unlike previous approaches, we not only cache n-way sufficient statistics, but also exploit their local structure.  We also present an empirical evaluation of our algorithm applied to several massive datasets. 
Multi- alue- unctions: Efficient Automatic Action Hierarchies for Multiple Goal MDPs| Abstract If you have planned to achieve one particular goal in a stochastic delayed rewards problem and then someone asks about a diff1740 t goal what should you do? What if you need to be ready to quickly supply an answer for any possible goal? This paper shows that by using a new kind of automatically generated abstract action hierarchy that with N states, preparing for all of N possible goals can be muchmuchcheaper than N times the work of preparing for one goal.  In goal-based MarkovDecision Problems, it is usual to generate a policy(x), mapping states to actions, and a value function J(x), mapping states to an estimate of minimum expected cost-to-goal, starting at x.  In this paper we will use the terminology that a multi-1 olicy ? (x# y) (for all state-pairs (x# y)) maps a state x to the first action it should take in order to reach y with expected minimum cost and a multi-6 alue- function J ? (x# y) is a definition of this minimum cost.  Building these objects quickly and with little memory is the main purpose of this paper, but a secondary result is a natural, automatic, wayto create a set of parsimonious yet powerful abstract actions for MDPs.  The paper concludes with a set of empirical results on increasingly large MDPs. 
Fast Robust Logistic Regression for Large Sparse Datasets with Binary Outputs| Abstract Although popular and extremely well established in mainstream statistical data analysis, logistic regression is strangely absent in the field of data mining.  There are two possible explanations of this phenomenon.  First, there might be an assumption that any tool which can only produce linear classification boundaries is likely to be trumped by more modern nonlinear tools.  Second, there is a legitimate fear that logistic regression cannot practically scale up to the massive dataset sizes to which modern data mining tools are applied.  This paper consists of an empirical examination of the first assumption, and surveys, implements and compares techniques by which logistic regression can be scaled to data with millions of attributes and records.  Our results, on a large life sciences dataset, indicate that logistic regression can perform surprisingly well, both statistically and computationally, when compared with an array of more recent classification algorithms. 
Network Monitoring with Nprobe| Abstract--- This paper presents an architecture for monitoring 10 Gbps networks, drawing on experience from a current 1 Gbps implementation.  The architecture performs full line-rate capture and implements on-line analysis and compression to record interesting data without loss.  The use of the monitoring and analysis system is demonstrated along with the presentation of several initial findings made possible with the tool. 
Reinforcement Learning: A Survey| Abstract This paper surveys the field of reinforcement learning from a computer-science
Efficient Algorithms for Minimizing Cross Validation Error| Abstract Model selection is important in many areas of supervised learning.  Given a dataset and a set of models for predicting with that dataset, we must choose the model which is expected to best predict future data.  In some situations, such as online learning for control of robots or factories, data is cheap and human expertise costly.  Cross validation can then be a highly effective method for automatic model selection.  Large scale cross validation search can, however, be computationally expensive.  This paper introduces new algorithms to reduce the computational burden of such searches.  We show how experimental design methods can achieve this, using a technique similar to a Bayesian version of Kaelbling's Interval Estimation.  Several improvements are then given, including (1) the use of blocking to quickly spot near-identical models, and (2) schemata search: a new method for quickly finding families of relevant features.  Experiments are presented for robot data and noisy synthetic datasets.  The new algorithms speed up computation without sacrificing reliability, and in some cases are more reliable than conventional techniques. 
Distributed Value Functions| Abstract Many interesting problems, such as power grids, network switches, and traffic flow, that are candidates for solving with reinforcement learning (RL), also have properties that make distributed solutions desirable.  We propose an algorithm for distributed reinforcement learning based on distributing the representation of the value function across nodes.  Each node in the system only has the ability to sense state locally, choose actions locally, and receive reward locally (the goal of the system is to maximize the sum of the rewards over all nodes and over all time).  However each node is allowed to give its neighbors the current estimate of its value function for the states it passes through.  We present a value function learning rule, using that information, that allows each node to learn a value function that is an estimate of a weighted sum of future rewards for all the nodes in the network.  With this representation, each node can choose actions to improve the performance of the overall system.  We demonstrate our algorithm on the distributed control of a simulated power grid.  We compare it against other methods including: use of a global reward signal, nodes that act locally with no communication, and nodes that share rewards (but not value function) information with each other.  Our results show that the distributed value function algorithm outperforms the others, and we conclude with an analysis of what problems are best suited for distributed value functions and the new research directions opened up by this work. 
Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets| Abstract This paper introduces new algorithms and data structures for quick counting for machine learning datasets.  We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries.  Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table.  We provide a very sparse data structure, the ADtree, to minimize memory use.  We provide analytical worst-case bounds for this structure for several models of data distribution.  We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves.  We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches.  We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets. 
Using Tarjan's Red Rule for Fast Dependency Tree Construction| Abstract We focus on the problem of efficient learning of dependency trees.  It is well-known that given the pairwise mutual information coefficients, a minimum-weight spanning tree algorithm solves this problem exactly and in polynomial time.  However, for large data-sets it is the construction of the correlation matrix that dominates the running time.  We have developed a new spanning-tree algorithm which is capable of exploiting partial knowledge about edge weights.  The partial knowledge we maintain is a probabilistic confidence interval on the coefficients, which we derive by examining just a small sample of the data.  The algorithm is able to flag the need to shrink an interval, which translates to inspection of more data for the particular attribute pair.  Experimental results show significant improvement in running time, without loss in accuracy of the generated trees.  Interestingly, our spanning-tree algorithm is based solely on Tarjan's red-edge rule, which is generally considered a guaranteed recipe for bad performance. 
Gradient Descent for General Reinforcement Learning| Abstract A simple learning rule is derived, the VAPS algorithm, which can be instantiated to generate a wide range of new reinforcementlearning algorithms.  These algorithms solve a number of open problems, define several new approaches to reinforcement learning, and unify different approaches to reinforcement learning under a single theory.  These algorithms all have guaranteed convergence, and include modifications of several existing algorithms that were known to fail to converge on simple MDPs.  These include Qlearning, SARSA, and advantage learning.  In addition to these value-based algorithms it also generates pure policy-search reinforcement-learning algorithms, which learn optimal policies without learning a value function.  In addition, it allows policysearch and value-based algorithms to be combined, thus unifying two very different approaches to reinforcement learning into a single Value and Policy Search (VAPS) algorithm.  And these algorithms converge for POMDPs without requiring a proper belief state.  Simulations results are given, and several areas for future research are discussed. 
Very Fast EM-Based Mixture Model Clustering Using Multiresolution Kd-Trees| Abstract Clustering is important in many fields including manufacturing, biology, finance, and astronomy.  Mixture models are a popular approach due to their statistical foundations, and EM is a very popular method for finding mixture models.  EM, however, requires many accesses of the data, and thus has been dismissed as impractical (e. g.  (Zhang, Ramakrishnan, & Livny, 1996)) for data mining of enormous datasets.  We present a new algorithm, based on the multiresolution kd-trees of (Moore, Schneider, & Deng, 1997), which dramatically reduces the cost of EM-based clustering, with savings rising linearly with the number of datapoints.  Although presented here for maximum likelihood estimation of Gaussian mixture models, it is also applicable to nonGaussian models (provided class densities are monotonic in Mahalanobis distance), mixed categorical/numeric clusters, and Bayesian methods such as Autoclass (Cheeseman & Oldford, 1994). 
Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time| Abstract We present a new algorithm,
Empirical Bayes Screening for Link Analysis| Abstract The domain of link analysis has recently re-ignited interest among researchers due to its applicability to new areas such as intelligence analysis (for example, identifying cliques of suspicious people), large scale social network analysis and genomics.  The area of link analysis is not new and comprise a number of techniques developed by different communities.  In this paper we propose a statistical approach to answering questions such as: what would be the "interesting" k-tuples of entities (that can be people, ingredients in a recipe, etc - depending on the application), given a dataset of observed ntuples of entities.  A typical example of an n-tuple might be a set of people observed to be having a meeting, or observed traveling to the same destination.  Currently, it is common to work with pairwise count matrices.  Empirical Bayes Screening (EBS) has several advantages over existing methods, one of them being the ability to take advantage of the interactions of higher order (for example, a group of three people significantly working together even though no two of them have significantly atypical pairwise interaction).  EBS has the additional advantage of being insensitive to the small sample size of co-occurrences.  We discuss advantages and disadvantages of the algorithm and provide performance analysis based on several datasets. 
An experimental configuration for the evaluation of CAC algorithms| Abstract Interest in Connection Admission Control (CAC) algorithms stems from the need for a network user and a network provider to forge an agreement on the Quality of Service (QoS) for a new network connection.  Traditional evaluation of CAC algorithms has been through simulation studies.  We present an alternative approach: an evaluation environment for CAC algorithms that is based around an experimental test-rig.  This paper presents the architecture of the test-rig and an evaluation of its performance. 
Learning Evaluation Functions for Global Optimization and Boolean Satisfiability| Abstract This paper describes Stage, a learning approach to automatically improving search performance on optimization problems.  Stage learns an evaluation function which predicts the outcome of a local search algorithm, such as hillclimbing or Walksat, as a function of state features along its search trajectories.  The learned evaluation function is used to bias future search trajectories toward better optima.  We present positive results on six large-scale optimization domains. 
Repairing Faulty Mixture Models using Density Estimation| Abstract Previous work in mixture model clustering has focused primarily on the issue of model selection.  Model scoring functions (including penalized likelihood and Bayesian approximations) can guide a search of the model parameter and structure space.  Relatively little research has addressed the issue of how to move through this space.  Local optimization techniques, such as expectation maximization, solve only part of the problem; we still need to move between different local optima.  The traditional approach, restarting the search from different random configurations, is inefficient.  We describe a more directed and controlled way of moving between local maxima.  Using multi-resolution kdtrees for fast density estimation, we search by modifying models within regions where they fail to predict the datapoint density.  We compare this algorithm with a canonical clustering method, finding favorable results on a variety of large, low-dimensional datasets. 
Tractable Group Detection on Large Link Data Sets| Abstract Discovering underlying structure from co-occurrence data is an important task in a variety of fields, including: insurance, intelligence, criminal investigation, epidemiology, human resources, and marketing.  Previously Kubica et.  al.  presented the group detection algorithm (GDA) - an algorithm for finding underlying groupings of entities from co-occurrence data.  This algorithm is based on a probabilistic generative model and produces coherent groups that are consistent with prior knowledge.  Unfortunately, the optimization used in GDA is slow, potentially making it infeasible for many large data sets.  To this end, we present k-groups - an algorithm that uses an approach similar to that of k-means to significantly accelerate the discovery of groups while retaining GDA's probabilistic model.  We compare the performance of GDA and k-groups on a variety of data, showing that k-groups' sacrifice in solution quality is significantly offset by its increase in speed. 
Locally Weighted Learning for Control| Abstract This paper surveys locally weighted learning, a form of lazy learning and memorybased learning, and focuses on locally weighted linear regression.  The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, interference between old and new data, implementing locally weighted learning efficiently, and applications of locally weighted learning.  A companion paper surveys how locally weighted learning can be used in robot learning and control. 
Multi-Value-Functions: Efficient Automatic Action Hierarchies for Multiple Goal MDPs| Abstract If you have planned to achieve one particular goal in a stochastic delayed rewards problem and then someone asks about a different goal what should you do? What if you need to be ready to quickly supply an answer for any possible goal? This paper shows that by using a new kind of automatically generated abstract action hierarchy that with N states, preparing for all of N possible goals can be much much cheaper than N times the work of preparing for one goal.  In goal-based Markov Decision Problems, it is usual to generate a policy (x), mapping states to actions, and a value function J(x), mapping states to an estimate of minimum expected cost-to-goal, starting at x.  In this paper we will use the terminology that a multi-policy ? (x; y) (for all state-pairs (x; y)) maps a state x to the first action it should take in order to reach y with expected minimum cost and a multi-valuefunction J ? (x; y) is a definition of this minimum cost.  Building these objects quickly and with little memory is the main purpose of this paper, but a secondary result is a natural, automatic, way to create a set of parsimonious yet powerful abstract actions for MDPs.  The paper concludes with a set of empirical results on increasingly large MDPs. 
Rule-Based Anomaly Pattern Detection for Detecting Disease Outbreaks| Abstract Searching for anomalies in multidimensional data with a temporal component is a dicult task especially when the exact features of the anomalies are unknown.  A standard but simplistic algorithm would be to obtain counts of certain events over a time interval such as a day and mark that interval to contain anomalies if this count exceeds a threshold.  This naive approach misses anomalies that aggregate in feature space but do not occur frequently enough to skew the count of monitored events over the time interval.  A desired solution should find these anomalous patterns rather than individual anomalies.  In order to approach this problem, we propose using a rule-based anomaly detection algorithm that characterizes each anomalous pattern with a rule.  The significance of each rule is carefully evaluated using Fisher's Exact Test and a randomization test.  The performance of our algorithm is compared against the standard algorithm by measuring the number of false positives and the timeliness of detection.  Simulated data is used in the evaluation phase.  This data was produced by a simulator that simulates the effects of a disease outbreak on a city.  The results indicate that our algorithm has significantly better detection times for common significance thresholds while having a slightly higher false positive rate. 
ADtrees for Fast Counting and for Fast Learning of Association Rules| Abstract: The problem of discovering association rules in large databases has received considerable research attention.  Much research has examined the exhaustive discovery of all association rules involving positive binary literals (e. g.  Agrawal et al.  1996).  Other research has concerned finding complex association rules for high-arity attributes such as CN2 (Clark and Niblett 1989).  Complex association rules are capable of representing concepts such as "PurchasedChips=True and PurchasedSoda=False and Area=NorthEast and CustomerType=Occasional AgeRange=Young", but their generality comes with severe computational penalties (intractable numbers of preconditions can have large support).  Here, we introduce new algorithms by which a sparse data structure called the ADtree, introduced in (Moore and Lee 1997), can accelerate the finding of complex association rules from large datasets.  The ADtree uses the algebra of probability tables to cache a dataset's sufficient statistics within a tractable amount of memory.  We first introduce a new ADtree algorithm for quickly counting the number of records that match a precondition.  We then show how this can be used in accelerating exhaustive search for rules, and for accelerating CN2-type algorithms.  Results are presented on a variety of datasets involving many records and attributes. 
X-means: Extending K-means with Efficient Estimation of the Number of Clusters| Abstract Despite its popularity for general clustering, K-means suffers three major shortcomings; it scales poorly computationally, the number of clusters K has to be supplied by the user, and the search is prone to local minima.  We propose solutions for the first two problems, and a partial remedy for the third.  Building on prior work for algorithmic acceleration that is not based on approximation, we introduce a new algorithm that efficiently, searches the space of cluster locations and number of clusters to optimize the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC) measure.  The innovations include two new ways of exploiting cached sufficient statistics and a new very efficient test that in one K-means sweep selects the most promising subset of classes for refinement.  This gives rise to a fast, statistically founded algorithm that outputs both the number of classes and their parameters.  Experiments show this technique reveals the true number of classes in the underlying distribution, and that it is much faster than repeatedly using accelerated K-means for different values of K. 
Architecture of a Network Monitor| Abstract--- This paper describes a system for simultaneously monitoring multiple protocols.  It performs full linerate capture and implements on-line analysis and compression to record interesting data without loss of information.  We accept that the balance must be maintained in such a system between disk-bandwidth, CPU-capacity and datareduction in order to perform monitoring at full line-rate.  We present the architecture in detail and measure the performance of our sample implementation, Nprobe. 
Detecting Significant Multidimensional Spatial Clusters| Abstract Assume a uniform, multidimensional grid of bivariate data, where each cell of the grid has a count c i and a baseline b i .  Our goal is to find spatial regions (d-dimensional rectangles) where the c i are significantly higher than expected given b i .  We focus on two applications: detection of clusters of disease cases from epidemiological data (emergency department visits, over-the-counter drug sales), and discovery of regions of increased brain activity corresponding to given cognitive tasks (from fMRI data).  Each of these problems can be solved using a spatial scan statistic (Kulldorff, 1997), where we compute the maximum of a likelihood ratio statistic over all spatial regions, and find the significance of this region by randomization.  However, computing the scan statistic for all spatial regions is generally computationally infeasible, so we introduce a novel fast spatial scan algorithm, generalizing the 2D scan algorithm of (Neill and Moore, 2004) to arbitrary dimensions.  Our new multidimensional multiresolution algorithm allows us to find spatial clusters up to 1400x faster than the naive spatial scan, without any loss of accuracy. 
Gradient Descent Approaches to Neural-Net-Based Solutions of the Hamilton-Jacobi-Bellman Equation| Abstract In this paper weinvestigate new approaches to dynamic-programming-based optimal control of continuous time-and-space systems.  We use neural networks to approximate the solution to the Hamilton-Jacobi-Bellman (HJB) equation which is, in the deterministic case studied here, a first-order, non-linear, partial differential equation.  We derive the gradient descent rule for integrating this equation inside the domain, given the conditions on the boundary. We apply this approach to the \Caron-the-hill" whichisatwo-dimensional highly non-linear control problem.  We discuss the results obtained and pointoutalow quality of approximation of the value function and of the derived control.  We attribute this bad approximation to the fact that the HJB equation has many generalized solutions (i. e.  di#erentiable almost everywhere) other than the value function, and our gradient descent method converges to one among these functions, thus possibly failing to find the correct value function.  We illustrate this limitation on a simple onedimensional control problem. 
The CoMo White Paper| DISCLAIMER: THIS DOCUMENT IS PROVIDED TO YOU "AS IS" WITH NO WARRANTIES WHATSOEVER, INCLUDING ANY WARRANTY OF MERCHANTABILITY NON-INFRINGEMENT, OR FITNESS FOR ANY PARTICULAR PURPOSE.  INTEL AND THE AUTHORS OF THIS DOCUMENT DISCLAIM ALL LIABILITY, INCLUDING LIABILITY FOR INFRINGEMENT OF ANY PROPRIETARY RIGHTS,
Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation| Abstract Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high.  Techniques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models reduce the applicability of these search methods.  Hoeffding Races is a technique for finding a good model for the data by quickly discarding bad models, and concentrating the computational effort at differentiating between the better ones.  This paper focuses on the special case of leave-one-out cross validation applied to memorybased learning algorithms, but we also argue that it is applicable to any class of model selection problems. 
Rapid detection of significant spatial clusters| ABSTRACT Given an N N grid of squares, where each square has a count c i j and an underlying population p i j , our goal is to find the rectangular region with the highest density, and to calculate its significance by randomization.  An arbitrary density function D, dependent on a region's total count C and total population P, can be used.  For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff's spatial scan statistic D K to find the most significant spatial disease cluster.  A naive approach to finding the maximum density region requires O # N 4 # time, and is generally computationally infeasible.  We present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region.  For sufficiently dense regions, this method finds the maximum density region in O ### N logN # 2 # time, in practice resulting in significant (20-2000x) speedups on both real and simulated datasets. 
Trustworthy Refinement Through Intrusion-Aware Design (TRIAD)| TECHNICAL REPORT CMU/SEI-2003-TR-002 ESC-TR-2003-002 Pittsburgh, PA 15213-3890
Efficient Multi-Object Dynamic Query Histograms| Abstract Dynamic Queries offer continuous feedback during range queries, and have been shown to be effective and satisfying.  Recent work has extended them to datasets of 100,000 objects and, separately, to queries involving relations among multiple objects.  The latter work enables filtering houses by properties of their owners, for instance.  Our primary concern is providing feedback from histograms during Dynamic Query.  The height of each histogram bar shows the count of selected objects whose attribute value falls into a given range.  Unfortunately, previous efficient algorithms for single object queries overcount in the case of multiple objects if, for instance, a house has multiple owners.  This paper presents an efficient algorithm that with high probability closely approximates the true counts.  1.  Previous Dynamic Query work 1. 1.  Single Object Interface Figure 1 shows a Dynamic Query (DQ) interface as implemented in VQE, a Visual Query Environment for exploring data from a database [1].  VQE is built on top of Visage [2], an interactive data visualization system developed by Carnegie Mellon and Maya Design Group.  The subset of the database being explored at any given time by a VQE query is called the active subset.  The top row in the upper box of Figure 1 indicates that the query is being applied to an active subset of 195 people, and that 72 of these people satisfy the constraints imposed by the sliders on the remaining two rows.  Namely, they have a birthdate between 8/1935 and 8/1968 and a salary between $15,760 and $66,729.  The rectangular sliders are superimposed on histograms where the dark bars show the distribution of the attribute values for the 72 selected people.  The dark bars partially cover the light bars, which show the distribution for all 195 people.  When the user drags either end of the sliders to change a query's selection range, the counts, histograms, and visibility of the points in the chart are updated in real time.  In Figure 1, the attributes shown on the axes in the chart are the same ones being filtered, so data only appears in a small rectangular area.  1. 2.  Efficient Algorithms We will only describe algorithms for updating the histograms.  The counts in the top row of the upper box in Figure 1 can be implemented as a zero dimensional histogram.  The chart can be implemented as a 2D histogram where the pixel at xy is turned on if the count for that histogram bucket is greater than zero.  For the following algorithms let a number of sliders p slider width in pixels r number of objects in active subset The sliders' edges can be positioned at any of the p pixel positions.  Even though the histogram bars as displayed are several pixels wide, the histograms are computed with p buckets internally.  Several of these buckets are then added when computing the bar heights for display.  Figure 1 Restrictions on salary and birthdate have filtered the active subset of 195 people down to 72 visible in the chart. 
Eigengalaxies for Fast Galaxy Morphology| Abstract We investigate the following nonlinear regression problem from astronomy: given a noisy, distorted image of an unknown galaxy, find the best-#tting parameters for a galaxy-image model quickly .  Solving this problem is of importance to the field of astronomy, since the distribution of galactic morphologies tests cosmological theories which are under-constrained.  This is an area of active research and there are tens of millions of images which have not been analyzed due to computational costs.  The algorithm follows the nonparametric paradigm by populating image space with several million model galaxies ahead of time.  During run-time, it just finds the model image nearest to the unknown image and returns that as the answer.  Using "eigengalaxies" allows the algorithm to achieve significant improvements in speed over previously published techniques. 
Nonparametric Density Estimation: Toward Computational Tractability| Abstract Density estimation is a core operation of virtually all
Efficient Exact k-NN and Nonparametric Classification in High Dimensions| Abstract This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classifiers and the prediction phase of Support Vector Machine classifiers.  We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly find the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints.  This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited.  For clarity, this paper concentrates on pure k-NN classification and the prediction phase of SVMs.  We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN.  These results include datasets with up to 10 6 dimensions and 10 5 records, and show non-trivial speedups while giving exact answers. 
The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces|
Gradient Descent for General Reinforcement Learning|
Learning Evaluation Functions to Improve Optimization by Local Search|
Wearable Sensor Badge and Sensor Jacket for Context Awareness|
Stochastic Link and Group Detection|
Structured Errors in Optical Gigabit Ethernet,|
Acquisition of Dynamic Control Knowledge for a Robotic Manipulator|
The parti-game algorithm for variable resolution reinforcement learning in multi-demensional state spaces|
Generalization in Reinforcement Learning: Safely Approximating the Value Function|
Using Prediction to Improve Combinatorial Optimization Search|
Design and Assurance Strategy for the NRL Pump|
A single locus mass-action model of assortative mating, with comments on the process of speciation|
Assortative mating genes selected along a gradient|
Variable Resolution Discretization in Optimal Control|
The Anchors Hierarchy: Using the Triangle Inequality to Survive High Dimensional Data|
Efficient Memory-Based Learning for Robot Control|
Variable Resolution Dynamic Programming|
Variable Resolution Discretization for High-Accuracy Solutions of Optimal Control Problems|
Multiresolution Instance-Based Learning|
Accelerating Exact -means Algorithms with Geometric Reasoning|
On recursions connected with symmetric groups I,|
Fast, Robust Adaptive Control by Learning only Forward Models|
Memory-Based Reinforcement Learning: Efficient Computation with Prioritized Sweeping|
Polynomial Regression Predictions|
Radsearch: A new approach for finding optimal rules eciently from dense datasets|
Rapid Evaluation of Multiple Density Models|
Memory-based stochastic optimization, Advances in Neural Information Processing Systems (|
`N-Body' Problems in Statistical Learning|
A locally weighted learning tutorial using Vizier 1|0,. 
Efficient Locally Weighted Polynomial Regression Predictions|
Memory-based reinforcement learning: Converging with less data and less real time|
Bayesian Network Anomaly Pattern Detection for Disease Outbreaks|
Sedimentation rate as determined by 226 Ra activity in marine barite,|
Multi-value functions: Efficient automatic hierarchies for multiple-goals mdps|
Reinforcement Learning for Cooperating and Communicating Reactive Agents in Electrical Power Grids|
Generalization in Renforcemen Learning: Savely Approximating the Value Function,|
The Racing Algorithm: Model Selection for Lazy Learners|
Influence and variance of a markov chain : Application to adaptive discretizations in optimal control|
Rates of Convergence for Variable Resolution Schemes in Optimal Control|
Fast Structure Search for Gaussian Mixture Models|
Bayesian Networks for Lossless Dataset Compression|
A Security Policy and Formal Top Level Specification for a Multi-Level Secure Local Area Network|
Memory-based learning for control|
Efficient algorithms for minimizing cross validation error, in:|
Memory-based Function Approximators for Learning Control|
Operating System and File System Monitoring: a Comparison of Passive Network Monitoring with Full Kernel Instrumentation Techniques|
The Specification and Verified Decomposition of System Requirements Using CSP|
Lessons Learned During Requirements Acquisition for COTS Systems|
Gradient descent reinforcement learning|
Policy Search using Paired Comparisons|
A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters|
An Introduction to Reinforcement Learning|
Active learning for hidden markov models: Objective functions and algorithms|
Efficient locally weighted polynomial regression prediction,|
Tractable structural learning of large bayesian networks from sparse data (|
Learning Evaluation Functions for Large Acyclic Domains|
Locally weighted regression|
Lazy and sequential ADtree construction|
Knowledge of knowledge and intelligent experimentation for learning control|
New Algorithms for efficient high-dimensional nonparametric classification|
An introductory tutorial on kd-trees|
Evaluation of a Practical Connection Admission Control for ATM Networks Based on On-line Measurements,|
An investigation of memory-based function approximators for learning control|
Detection of emerging space-time clusters|
Mitochondrial gene trees versus nuclear-gene trees: A reply to Hoelzer|
Multiresolutioninstance-lutio learning|
Very fastEM-t 273 mixture model clustering using multiresolutionkd-resolu|
Fast algorithms and efficient statistics: Density estimation in large astronomical datasets|
in press)| Locally weighted learning. 
Ecient kernel density algorithms|
Computational astrostatistics: Fast and efficient tools for analysing huge astronomical data sources|
Anchors Hierarchy: Using the Triangle Inequality to Survive High Dimensional Data|
Stochastic link and group detection|
Fast Algorithms and Efficient Statistics: Npoint Correlation Functions,|
Personal Communication,|
Doctoral dissertation: Efficient Memory-based Learning for Robot Control|
A Dynamic Adaptation of AD-trees for Efficient Machine Learning on Large Data Sets|
Experimental results from a practical implementation of a Measurement Based CAC algorithm|
Median Filtering for Removal of Low-Frequency Background Drift|
Sweeping: Reinforcement Learning with Less Data and Less Real Time|
Rule-Based Anomaly Pattern Detection for Detecting Disease Outbreaks",|
Lazily Cached Sufficient Statistics: New data structures and theory,|
Locally weighted training|
Alias detection in link data sets|
Reinforcement Laerning: A Survey'|
Memory-based reinforcement learning: Converging ith less data and less time",|
Foundations for survivable systems engineering,|
Memory-Based Methods for Regression and Classification|
A racing algorithm: Model selection for memory based learners|
Memory-based stochastic optimization| To appear in the proceedings of NIPS-95,. 
A locally weighted learning tutorial using vizier|

A NEW VIEW OF ICA| ABSTRACT We present a new way of interpreting ICA as a probability density model and a new way of fitting this model to data.  The advantage of our approach is that it suggests simple, novel extensions to overcomplete, undercomplete and multilayer non-linear versions of ICA.  1.  ICA AS A CAUSAL GENERATIVE MODEL Factor analysis is based on a causal generative model in which an observation vector is generated in three stages.  First, the activities of the factors (also known as latent or hidden variables) are chosen independently from one dimensional Gaussian priors.  Next, these hidden activities are multiplied by a matrix of weights (the "factor loading" matrix) to produce a noise-free observation vector.  Finally, independent Gaussian "sensor noise" is added to each component of the noise-free observation vector.  Given an observation vector and a factor loading matrix, it is tractable to compute the posterior distribution of the hidden activities because this distribution is a Gaussian, though it generally has off-diagonal terms in the covariance matrix so it is not as simple as the prior distribution over hidden activities.  ICA can also be viewed as a causal generative model [1, 2] that differs from factor analysis in two ways.  First, the priors over the hidden activities remain independent but they are non-Gaussian.  By itself, this modification would make it intractable to compute the posterior distribution over hidden activities.  Tractability is restored by eliminating sensor noise and by using the same number of factors as input dimensions.  This ensures that the posterior distribution over hidden activities collapses to a point.  Interpreting ICA as a type of causal generative model suggests a number of ways in which it might be generalized, for instance to deal with more hidden units than input dimensions.  Most of these generalizations retain marginal independence of the hidden activities and add sensor noise, but fail to preserve the property that the posterior distribution collapses to a point.  As # Funded by the Wellcome Trust and the Gatsby Charitable Foundation.  a result inference is intractable and crude approximations are needed to model the posterior distribution, e. g. , a MAP estimate in [3], a Laplace approximation in [4, 5] or more sophisticated variational approximations in [6]. 
Learning Causally Linked Markov Random Fields| Abstract We describe a learning procedure for a generative model that contains a hidden Markov Random Field (MRF) which has directed connections to the observable variables.  The learning procedure uses a variational approximation for the posterior distribution over the hidden variables.  Despite the intractable partition function of the MRF, the weights on the directed connections and the variational approximation itself can be learned by maximizing a lower bound on the log probability of the observed data.  The parameters of the MRF are learned by using the mean field version of contrastive divergence [1].  We show that this hybrid model simultaneously learns parts of objects and their inter-relationships from intensity images.  We discuss the extension to multiple MRF's linked into in a chain graph by directed connections. 
Energy-Based Models for Sparse Overcomplete Representations| Abstract We present a new way of extending independent components analysis (ICA) to overcomplete representations.  In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs.  This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs.  By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution.  Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002).  When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data.  In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces. 
Learning Sparse Topographic Representations with Products of Student-t Distributions| Abstract We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs.  We encourage the system to find sparse features by using a Studentt distribution to model each filter output.  If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map.  Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters.  Once the model has been learned it can be used as a prior to derive the "iterated Wiener filter" for the purpose of denoising images. 
An Alternative Infinite Mixture Of Gaussian Process Experts| Abstract We present an infinite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space.  Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals.  The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model.  This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian specification of the effective `gating network' for the different experts. 
Energy-based models for sparse overcomplete representations| Technical report, University of Toronto, Department of Computer Science. 

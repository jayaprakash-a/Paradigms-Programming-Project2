Dynamical Modeling with Kernels for Nonlinear Time Series Prediction| Abstract We consider the question of predicting nonlinear time series.  Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models.  The kernel trick is used twice: first, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression.  Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space.  Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions. 
Boosting Mixture Models for Semi-supervised Learning| Abstract.  This paper introduces MixtBoost, a variant of AdaBoost dedicated to solve problems in which both labeled and unlabeled data are available.  We propose several definitions of loss for unlabeled data, from which margins are defined.  The resulting boosting schemes implement mixture models as base classifiers.  Preliminary experiments are analyzed and the relevance of loss choices is discussed.  MixtBoost improves on both mixture models and AdaBoost provided classes are structured, and is otherwise similar to AdaBoost. 
Optimal Linear Regression on Classifier Outputs| Abstract.  We consider the combination of the outputs of several classifiers trained independently for the same discrimination task.  We introduce new results which provide optimal solutions in the case of linear combinations.  We compare our solutions to existing ensemble methods and characterize situations where our approach should be preferred. 
Support Vector Machines Based on a Semantic Kernel for Text Categorization|
Trio learning: a new strategy for building hybrid neural trees|
Rule extraction with fuzzy neural network",|
Learning Fuzzy Control rules with a Fuzzy Neural|
Incremental Support Vector Machine Learning: A Local Approach|

A non-negative sparse coding network learns contour coding and integration from natural images| Abstract An important approach in visual neuroscience considers how the function of
A multi-layer sparse coding network learns contour coding from natural images| Abstract An important approach in visual neuroscience considers how the function of the early visual system relates to the statistics of its natural input.  Previous studies have shown how many basic properties of the primary visual cortex, such as the receptive fields of simple and complex cells and the spatial organization (topography) of the cells, can be understood as efficient coding of natural images.  Here we extend the framework by considering how the responses of complex cells could be sparsely represented by a higher-order neural layer.  This leads to contour coding and end-stopped receptive fields.  In addition, contour integration could be interpreted as top-down inference in the presented model. 
Estimation of Non-Normalized Statistical Models by Score Matching| Abstract One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant.  Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant.  Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data.  While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function.  The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model.  The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data. 
A unifying model for blind separation of independent sources| Abstract Many algorithms have been proposed for blind separation of statistically independent sources.  Most of the
INDEPENDENT COMPONENT ANALYSIS FOR BINARY DATA: AN EXPERIMENTAL STUDY| ABSTRACT We consider a mixing model where independent binary components are mixed using binary OR operations.  Using extensive simulations, we investigate whether the model can be estimated using ordinary cumulant-based ICA algorithms.  We show that the model can indeed be estimated if the data is sparse enough.  We also compare the 3rd and 4th order cumulants.  In the no-noise and low-noise cases, the 3rd order cumulant performs better, but in the presence of strong noise, the 4th-order cumulant, somewhat surprisingly, performs better for very sparse data. 
Blind separation of sources that have spatiotemporal variance dependencies| Abstract In blind source separation methods, the sources are typically assumed to be independent.  Some methods are
A two-layer sparse coding model learns simple and complex cell receptive fields and topography from natural images| Abstract The classical receptive fields of
Imposing sparsity on the mixing matrix in independent component analysis| In independent component analysis, prior information on the distributions of the independent components is often used; some weak information is in fact necessary for succesful estimation.  In contrast, prior information on the mixing matrix is usually not used.  This is because it is considered that the estimation should be completely blind as to the form of the mixing matrix.  Nevertheless, it could be possible to find forms of prior information that are sufficiently general to be useful in a wide range of applications.  In this paper, we argue that prior information on the sparsity of the mixing matrix could be a constraint general enough to merit attention.  Moreover, we show that the computational implementation of such sparsifying priors on the mixing matrix is very simple since in many cases they can be expressed as conjugate priors.  The property of being conjugate priors means that essentially the same algorithm can be used as in ordinary ICA. 
for Non-Normal Factor Analysis| Summary.  Independent component analysis (ICA) was developed in the signal processing and neural computation communities.  Its original purpose was to solve what is called the blind source separation problem: when linear mixtures of some original source signals are observed, the goal is to recover the source signals, using minimum assumptions on the mixing matrix (i. e.  blindly).  This leads to a linear model that is very similar to the one used in factor analysis.  What makes ICA fundamentally different from conventional factor analysis is that the source signals are assumed to be non-Gaussian, in addition to the basic assumption of their independence.  In fact, this implies that the model can be uniquely estimated from the data, using supplementary information that is not contained in the covariance matrix.  Interestingly, a very close connection can be found with projection pursuit: The basic form of the ICA model can be estimated by finding the projections that are maximally non-Gaussian, which is the goal of projection pursuit as well.  On the other hand, the dimension of the observed data vector is often first reduced by principal component analysis, in which case ICA can be viewed as a method of determining the factor rotation using the non-Gaussianity of the factors. 
STATISTICAL MODELS OF IMAGES AND EARLY VISION| ABSTRACT A fundamental question in visual neuroscience is: Why are the receptive fields and response properties of visual neurons as they are? A modern approach to this problem emphasizes the importance of adaptation to ecologically valid input.  In this paper, we will review work on modelling statistical regularities in ecologically valid visual input ("natural images") and the obtained functional explanation of the properties of visual neurons.  A seminal statistical model for natural images was linear sparse coding which is equivalent to the model called independent component analysis (ICA).  Linear features estimated by ICA resemble wavelets or Gabor functions, and provide a very good description of the properties of simple cells in the primary visual cortex.  We have introduced extensions of ICA that are based on modelling dependencies of the "independent" components estimated by basic ICA.  The dependencies of the components are used to define either a grouping or a topographic order between the components.  With natural image data, these models lead to emergence of further properties of visual neurons: the topographic organization and complex cell receptive fields.  We have also modelled the temporal structure of natural image sequences, which provides an alternative approach to the sparseness used in most models.  These models can be combined in a unifying framework that we call bubble coding.  Finally, we will discuss a promising new direction of research: predictive visual neuroscience.  There, the goal is to try to predict response properties of neurons in areas that are poorly understood, still based on statistical modelling of natural input. 
The Fixed-Point Algorithm and Maximum Likelihood Estimation for Independent Component Analysis| Abstract The author introduced previously a fast
Discovery of non-gaussian linear causal models using ICA| Abstract In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data (Spirtes et al.  2000; Pearl 2000).  Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data.  Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-gaussian distributions of non-zero variances.  The solution relies on the use of the statistical method known as independent component analysis (ICA), and does not require any pre-specified time-ordering of the variables.  We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data. 
Validating the independent components of neuroimaging time-series via clustering and visualization| Abstract Recently, independent component analysis (ICA) has been widely used in the
ICASSO: SOFTWARE FOR INVESTIGATING THE RELIABILITY OF ICA ESTIMATES BY CLUSTERING AND VISUALIZATION| Abstract.  A major problem in application of independent component analysis (ICA) is that the reliability of the estimated independent components is not known.  Firstly, the finite sample size induces statistical errors in the estimation.  Secondly, as real data never exactly follows the ICA model, the contrast function used in the estimation may have many local minima which are all equally good, or the practical algorithm may not always perform properly, for example getting stuck in local minima with strongly suboptimal values of the contrast function.  We present an explorative visualization method for investigating the relations between estimates from FastICA.  The algorithmic and statistical reliability is investigated by running the algorithm many times with different initial values or with differently bootstrapped data sets, respectively.  Resulting estimates are compared by visualizing their clustering according to a suitable similarity measure.  Reliable estimates correspond to tight clusters, and unreliable ones to points which do not belong to any such cluster.  We have developed a software package called Icasso to implement these operations.  We also present results of this method when applying Icasso on biomedical data. 
Connection between multilayer perceptrons and regression using independent component analysis| Abstract The data model of independent component analysis (ICA) gives a multivariate probability density that describes many kinds of sensory data better than classical models like gaussian densities or gaussian mixtures.  When only a subset of the random variables is observed, ICA can be used for regression, i. e.  to predict the missing observations.  In this paper, we show that the resulting regression is closely related to regression by a multi-layer perceptron (MLP).  In fact, if linear dependencies are first removed from the data, regression by ICA is, as a first-order approximation, equivalent to regression by MLP.  This theoretical result gives a new interpretation of the elements of the MLP: The outputs of the hidden layer neurons are related to estimates of the values of the independent components, and the sigmoid nonlinearities are obtained from the probability densities of the independent components. 
Chapter 9 Adaptive cognitive systems| 123 9. 1 Introduction Our research on cognitive systems focuses on modeling and applying methods of unsupervised and reinforcement learning.  The general aim is to provide a methodological framework for theories of conceptual development, symbol grounding, communication among autonomous agents, and constructive learning.  We also work in close collaboration with other groups in our laboratory, e. g. , related to multimodal environments and sensory fusion. 
A Novel Temporal Generative Model of Natural Video as an Internal Model in Early Vision| Abstract--- In computational neuroscience one application of generative models is to examine how the statistics sensory input data are related to the properties corresponding sensory neural networks.  In this approach it is assumed that neural networks are tuned to the properties of input data, that is, that they have learned ecient internal representations of their environment.  In this paper we present a hypothetical internal representation for natural video at the level of early vision, more precisely, the level of simple and complex cells.  We define two-layer generative model for natural video, based on temporal relationships between simple cell outputs.  Preliminary results of estimating the parameters of the model from natural data suggest that the learned temporal interactions between cell outputs are similar to complex cell pooling of simple cell outputs.  This unsupervised learning pooling separates our experimental results from empirical work based on other advanced self-organizing models early vision. 
Blind source separation by nonstationarity of variance: A cumulant-based approach| Abstract Blind separation of source signals usually relies either on the nongaussianity
Estimating overcomplete independent component bases for image windows| Abstract Estimating overcomplete ICA bases for image windows is a dicult problem.  Most algorithms require the estimation of values of the independent components which leads to computationally heavy procedures.  Here we first review the existing methods, and then introduce two new algorithms that estimate an approximate overcomplete basis quite fast in a high-dimensional space.  The first algorithm is based on the prior assumption that the basis vectors are randomly distributed in the space, and therefore close to orthogonal.  The second replaces the conventional orthogonalization procedure by a transformation of the marginal density to gaussian. 
Survey on Independent Component Analysis, Neural|
ICA fixed-point algorithm in extraction of artifacts from EEG|

FRAMEWISE PHONE CLASSIFICATION USING SUPPORT VECTOR MACHINES| ABSTRACT We describe the use of Support Vector Machines for phonetic classification on the TIMIT corpus.  Unlike previous work, in which entire phonemes are classified, our system operates in a framewise manner and is intended for use as the front-end of a hybrid system similar to ABBOT.  We therefore avoid the problems of classifying variable-length vectors.  Our frame-level phone classification accuracy on the complete TIMIT test set is competitive with other results from the literature.  In addition, we address the serious problem of scaling Support Vector Machines by using the Kernel Fisher Discriminant. 
Kalman-filter based Join Cost for Unit-selection Speech Synthesis| Abstract We introduce a new method for computing join cost in unitselection speech synthesis which uses a linear dynamical model (also known as a Kalman filter) to model line spectral frequency trajectories.  The model uses an underlying subspace in which it makes smooth, continuous trajectories.  This subspace can be seen as an analogy for underlying articulator movement.  Once trained, the model can be used to measure how well concatenated speech segments join together.  The objective join cost is based on the error between model predictions and actual observations.  We report correlations between this measure and mean listener scores obtained from a perceptual listening experiment.  Our experiments use a state-of-the art unit-selection text-to-speech system: rVoice from Rhetorical Systems Ltd. 
The Keyword Lexicon - An accent-independent lexicon for automatic speech recognition| ABSTRACT Recent work at the Centre for Speech Technology Research (CSTR) at the University of Edinburgh has developed an accent-independent lexicon for speech synthesis (the Unisyn project).  The main purpose of this lexicon is to avoid the problems and cost of writing a new lexicon for every new accent needed for synthesis.  Only recently [1], a first attempt has been made to use the Keyword Lexicon for automatic speech recognition. 
ASR - Articulatory Speech Recognition| We describe a speech recognition system which uses a combination of acoustic and articulatory features as input.  Linear dynamic models capture the trajectories which characterise each segment type.  We describe classification and recognition tasks for systems based on acoustic data in conjunction with both real and automatically recovered articulatory parameters. 
Named Entity Extraction from Word Lattices| Abstract We present a method for named entity extraction from word lattices produced by a speech recogniser.  Previous work by others on named entity extraction from speech has used either a manual transcript or 1-best recogniser output.  We describe how a single Viterbi search can recover both the named entity sequence and the corresponding word sequence from a word lattice, and further that it is possible to trade off an increase in word error rate for improved named entity extraction. 
Transforming F0 Contours| Abstract Voice transformation is the process of transforming the characteristics of speech uttered by a source speaker, such that a listener would believe the speech was uttered by a target speaker.  Training F0 contour generation models for speech synthesis requires a large corpus of speech.  If it were possible to adapt the F0 contour of one speaker to sound like that of another speaker, using a small, easily obtainable parameter set, this would be extremely valuable.  We present a new method for the transformation of F0 contours from one speaker to another based on a small linguistically motivated parameter set.  The system performs a piecewise linear mapping using these parameters.  A perceptual experiment clearly demonstrates that the presented system is at least as good as an existing technique for all speaker pairs, and that in many cases it is much better and almost as good as using the target F0 contour. 
AN AUTOMATIC SPEECH RECOGNITION SYSTEM USING NEURAL NETWORKS AND LINEAR DYNAMIC MODELS TO RECOVER AND MODEL ARTICULATORY TRACES| ABSTRACT We describe a speech recognition system which uses articulatory parameters as basic features and phone-dependent linear dynamic models.  The system first estimates articulatory trajectories from the speech signal.  Estimations of x and y coordinates of 7 actual articulator positions in the midsagittal plane are produced every 2 milliseconds by a recurrent neural network, trained on real articulatory data.  The output of this network is then passed to a set of linear dynamic models, which perform phone recognition.  1.  MOTIVATION Hidden Markov Models (HMMs) have dominated automatic speech recognition for at least the last decade.  The model's success lies in its mathematical simplicity; efficient and robust algorithms have been developed to facilitate its practical implementation.  However, there is nothing uniquely speech-oriented about acoustic-based HMMs.  Standard HMMs model speech as a series of stationary regions in some representation of the acoustic signal.  Speech is a continuous process though, and ideally should be modelled as such.  Furthermore, HMMs assume that state and phone boundaries are strictly synchronized with events in the parameter space, whereas in fact different acoustic and articulatory parameters do not necessarily change value simultaneously at boundaries.  We propose that modelling speech in the articulatory domain will inherently account for for the underlying processes of speech production, such as coarticulation, and will therefore offer improvements in recognition performance.  Because the trajectories evolve smoothly over time, we chose to model them using a Linear Dynamic Model (LDM), extending the approach used in [2].  We have had access to real articulatory data, which has been used to train a neural network mapping from acoustic to articulatory domains.  Both original and recovered data have been modelled.  1. 1.  Data The data consisted of a corpus of 460 TIMIT sentences for which parallel acoustic-articulatory information was recorded using a Carstens Electromagnetic Articulograph (EMA) system (this
Structural Representation of Speech for Phonetic Classification| Abstract This paper explores the issues involved in using symbolic metric algorithms for automatic speech recognition (ASR), via a structural representation of speech.  This representation is based on a set of phonological distinctive features which is a linguistically well-motivated alternative to the "beads-on-a-string" view of speech that is standard in current ASR systems.  We report the promising results of phoneme classification experiments conducted on a standard continuous speech task. 
Multisyn Voices from ARCTIC Data for the Blizzard Challenge| Abstract This paper describes the process of building unit selection voices for the Festival multisyn engine using four ARCTIC datasets, as part of the Blizzard evaluation challenge.  The build procedure is almost entirely automatic, with very little need for human intervention.  We discuss the difference in the evaluation results for each voice and evaluate the suitability of the ARCTIC datasets for building this type of voice. 
A Hybrid ANN/DBN Approach to Articulatory Feature Recognition| Abstract Artificial neural networks (ANN) have proven to be well suited to the task of articulatory feature (AF) recognition.  Previous studies have taken a cascaded approach where separate ANNs are trained for each feature group, making the assumption that features are statistically independent.  We address this by using ANNs to provide virtual evidence to a dynamic Bayesian network (DBN).  This gives a hybrid ANN/DBN model and allows modelling of inter-feature dependencies.  We demonstrate significant increases in AF recognition accuracy from modelling dependencies between features, and present the results of embedded training experiments in which a set of asynchronous feature changes are learned.  Furthermore, we report on the application of a Viterbi training scheme in which we alternate between realigning the AF training labels and retraining the ANNs. 
DETECTION OF SYMBOLIC GESTURAL EVENTS IN ARTICULATORY DATA FOR USE IN STRUCTURAL REPRESENTATIONS OF CONTINUOUS SPEECH| ABSTRACT One of the crucial issues which often needs to be addressed in structural approaches to speech representation is the choice of fundamental symbolic units of representation.  In this paper, a physiologically inspired methodology for defining these symbolic atomic units in terms of primitive articulatory events is proposed.  It is shown how the atomic articulatory events (gestures) can be detected directly in the articulatory data.  An algorithm for evaluating the reliability of the articulatory events is described and promising results of the experiments conducted on MOCHA articulatory database are presented. 
Communicating Ada Tasks| Abstract.  Ada has proven successful in the area of high integrity systems development, but its tasking model is hard to reason about.  Thus, Ravenscar has been defined as a restricted subset of the Ada tasking model, which meets the requirements of producing analysable and deterministic code.  A central feature of Ravenscar is the use of protected objects to ensure mutually exclusive access to shared data.  In this paper, we use Ravenscar protected objects to implement CSP channels in Ada.  This allows us to transform the data-oriented asynchronous tasking model of Ravenscar into the cleaner, and better understood, messagepassing synchronous model of CSP. Thus, formal proofs and techniques for model-checking CSP specifications can be applied to Ravenscar programs.  In turn, this raises our confidence in these programs and their reliability. We will formally verify our implementation of the CSP channel using the Circus model of Ravenscar protected objects providedin[3]. 
Estimating Detailed Spectral Envelopes Using Articulatory Clustering| Abstract This paper presents an articulatory-acoustic mapping where detailed spectral envelopes are estimated.  During the estimation, the harmonics of a range of F0 values are derived from the spectra of multiple voiced speech signals vocalized with similar articulator settings.  The envelope formed by these harmonics is represented by a cepstrum, which is computed by fitting the peaks of all the harmonics based on the weighted least square method in the frequency domain.  The experimental result shows that the spectral envelopes are estimated with the highest accuracy when the cepstral order is 48-64 for a female speaker, which suggests that representing the real response of the vocal tract requires high-quefrency elements that conventional speech synthesis methods are forced to discard in order to eliminate the pitch component of speech. 
Asynchronous Articulatory Feature Recognition Using Dynamic Bayesian Networks|
Estimating the Spectral Envelope of Voiced Speech Using Multi-frame Analysis| Abstract This paper proposes a novel approach for estimating the spectral envelope of voiced speech independently of its harmonic structure.  Because of the quasi-periodicity of voiced speech, its spectrum indicates harmonic structure and only has energy at frequencies corresponding to integral multiples of F0 .  It is hence impossible to identify transfer characteristics between the adjacent harmonics.  In order to resolve this problem, Multiframe Analysis (MFA) is introduced.  The MFA estimates a spectral envelope using many portions of speech which are vocalised using the same vocal-tract shape.  Since each of the portions usually has a different F0 and ensuing different harmonic structure, a number of harmonics can be obtained at various frequencies to form a spectral envelope.  The method thereby gives a closer approximation to the vocal-tract transfer function. 
Articulatory Feature Recognition Using Dynamic Bayesian Networks| Abstract This paper describes the use of dynamic Bayesian networks for the task of articulatory feature recognition.  We show that by modeling the dependencies between a set of 6 multi-leveled articulatory features, recognition accuracy is increased over an equivalent system in which features are considered independent.  Results are compared to those found using artificial neural networks on an identical task. 
Phone Classification in Pseudo-Euclidean Vector Spaces| Abstract Recently we have proposed a structural framework for modelling speech, which is based on patterns of phonological distinctive features, a linguistically well-motivated alternative to standard vector-space acoustic models like HMMs.  This framework gives considerable representational freedom by working with features that have explicit linguistic interpretation, but at the expense of the ability to apply the wide range of analytical decision algorithms available in vector spaces, restricting oneself to more computationally expensive and less-developed symbolic metric tools.  In this paper we show that a dissimilarity-based distance-preserving transition from the original structural representation to a corresponding pseudo-Euclidean vector space is possible.  Promising results of phone classification experiments conducted on the TIMIT database are reported. 
FESTIVAL 2 -- BUILD YOUR OWN GENERAL PURPOSE UNIT SELECTION SPEECH SYNTHESISER| ABSTRACT This paper describes version 2 of the Festival speech synthesis system.  Festival 2 provides a development environment for concatenative speech synthesis, and now includes a general purpose unit selection speech synthesis engine.  We discuss various aspects of unit selection speech synthesis, focusing on the research issues that relate to voice design and the automation of the voice development process. 
Using Prosodic Information to Constrain Language Models for Spoken Dialogue| ABSTRACT We presentwork intended to improve speech recognition performance for computer dialogue by taking into account the way that dialogue context and intonational tune interact to limit the possibilities for what an utterance might be.  We report here on the extra constraintachieved in a bigram language model, expressed in terms of entropy,by using separate submodels for different sorts of dialogue acts, and trying to predict which submodel to apply by analysis of the intonation of the sentence being recognised. 
SPEECH SYNTHESIS USING NON-UNIFORM UNITS IN THE VERBMOBIL PROJECT| ABSTRACT We describe a concatenative speech synthesiser for British English which uses the HADIFIX [8] inventory structure originally developed for German by Portele.  An inventory of non-uniform units was investigated with the aim of improving segmental quality compared to diphones.  A combination of soft (diphone) and hard concatenation was used, which allowed a dramatic reduction in inventory size.  We also present a unit selection algorithm which selects an optimum sequence of units from this inventory for a given phoneme sequence.  The work described is part of the concept-to-speech synthesiser for the language and speech project Verbmobil [12] which is funded by the German Ministry of Science (BMBF). 
Modelling the uncertainty in recovering articulation from acoustics| Abstract This paper presents an experimental comparison of the performance of the multilayer perceptron (MLP) with that of the mixture density network (MDN) for an acoustic-to-articulatory mapping task.  A corpus of acoustic-articulatory data recorded by electromagnetic articulography (EMA) for a single speaker was used as training and test data for this purpose.  In theory, the MDN is able to provide a richer, more flexible description of the target variables in response to a given input vector than the least-squares trained MLP.  Our results show that the mean likelihoods of the target articulatory parameters for an unseen test set were indeed consistently higher with the MDN than with the MLP.  The increase ranged from approximately 3% to 22%, depending on the articulatory channel in question.  On the basis of these results, we argue that using a more flexible description of the target domain, such as that o6ered by the MDN, can prove beneficial when modelling the acoustic-to-articulatory mapping. 
Subjective Evaluation of Join Cost Functions Used in Unit Selection Speech Synthesis| Abstract.  In our previous papers, we have proposed join cost functions derived from spectral distances, which have good correlations with perceptual scores obtained for a range of concatenation discontinuities.  To further validate their ability to predict concatenation discontinuities, we have chosen the best three spectral distances and evaluated them subjectively in a listening test.  The unit sequences for synthesis stimuli are obtained from a state-of-the-art unit selection text-to-speech system: rVoice from Rhetorical Systems Ltd.  In this paper, we report listeners' preferences for each of the three join cost functions. 
Inductive String Template-Based Learning of Spoken Language| representation.  We also present the results of the phoneme classification experiments conducted on the TIMIT corpus of continuous speech. 
Source-Filter Separation for Articulation-to-Speech Synthesis| Abstract In this paper we examine a method for separating out the vocal-tract filter response from the voice source characteristic using a large articulatory database.  The method realises such separation for voiced speech using an iterative approximation procedure under the assumption that the speech production process is a linear system composed of a voice source and a vocal-tract filter, and that each of the components is controlled independently by different sets of factors.  Experimental results show that the spectral variation is evidently influenced by the fundamental frequency or the power of speech, and that the tendency of the variation may be related closely to speaker identity.  The method enables independent control over the voice source characteristic in our articulation-to-speech synthesis. 
Discriminative Methods for Improving Named Entity Extraction on Speech Data| Abstract In this paper we present a method of discriminatively training language models for spoken language understanding; we show improvements in named entity F-scores on speech data using these improved language models.  A comparison between theoretical probabilities associated with manual markup and the actual probabilities of output markup is used to identify probabilities requiring adjustment.  We present results which support our hypothesis that improvements in F-scores are possible by using either previously used training data or held out development data to improve discrimination amongst a set of N-gram language models. 
Genetic Triangulation of Graphical Models for Speech and Language Processing| Abstract Graphical models are an increasingly popular approach for speech and language processing.  As researchers design ever more complex models it becomes crucial to find triangulations that make inference problems tractable.  This paper presents a genetic algorithm for triangulation search that is well-suited for speech and language graphical models.  It is unique in two ways: First, it can find triangulations appropriate for graphs with a mix of stochastic and deterministic dependencies.  Second, the search is guided by optimizing the inference speed (CPU runtime) on real data.  We show results on 10 real-world speech and language graphs and demonstrate inference speed-ups over standard triangulation methods. 
Estimation of Voice Source and Vocal Tract Characteristics Based on Multi-frame Analysis| Abstract This paper presents a new approach for estimating voice source and vocal tract filter characteristics of voiced speech.  When it is required to know the transfer function of a system in signal processing, the input and output of the system are experimentally observed and used to calculate the function.  However, in the case of source-filter separation we deal with in this paper, only the output (speech) is observed and the characteristics of the system (vocal tract) and the input (voice source) must simultaneously be estimated.  Hence the estimate becomes extremely difficult, and it is usually solved approximately using oversimplified models.  We demonstrate that these characteristics are separable under the assumption that they are independently controlled by different factors.  The separation is realised using an iterative approximation along with the Multi-frame Analysis method, which we have proposed to find spectral envelopes of voiced speech with minimum interference of the harmonic structure. 
Predicting Consonant Duration with Bayesian Belief Networks| Abstract Consonant duration is influenced by a number of linguistic factors such as the consonant's identity, within-word position, stress level of the previous and following vowels, phrasal position of the word containing the target consonant, its syllabic position, identity of the previous and following segments.  In our work, consonant duration is predicted from a Bayesian belief network (BN) consisting of discrete nodes for the linguistic factors and a single continuous node for the consonant's duration.  Interactions between factors are represented as conditional dependency arcs in this graphical model.  Given the parameters of the belief network, the duration of each consonant in the test set is then predicted as the value with the maximum probability.  We compare the results of the belief network model with those of sums-of-products (SoP) and classification and regression tree (CART) models using the same data.  In terms of RMS error, our BN model performs better than both CART and SoP models.  In terms of the correlation coefficient, our BN model performs better than SoP model, and no worse than CART model.  In addition, the Bayesian model reliably predicts consonant duration in cases of missing or hidden linguistic factors. 
SPEECH RECOGNITION VIA PHONETICALLY-FEATURED SYLLABLES| Abstract We describe recent work on two new automatic speech recognition systems.  The first part of this paper describes the components of a system based on phonological features (which we call Espresso-P) in which the values of these features are estimated from the speech signal before being used as the basis for recognition.  In the second part of the paper, another system (which we call Espresso-A) is described in which articulatory parameters are used instead of phonological features and a linear dynamical system model is used to perform recognition from automatically estimated values of these articulatory parameters.  1.  Phonological feature-based system: Espresso-P The first 5 sections of this paper report work on the components of a two stage recognition architecture based on phonological features rather than phones.  While phonological features have been proposed before as the basis of a speech recognition system (see section 1. 2 for a review), the use of features has been out of favour until recently because there had been little success in extracting them from speech waveforms, and a lack of suitable models with which to perform actual recognition.  This paper reports a set of experiments which show that phonological features can be accurately and robustly extracted from speech; furthermore, we have shown that this is possible for speaker independent continuous speech.  1. 1.  The theoretical basis of phonological features Most speech recognisers today are based on phones (or phonemes) which, in our opinion, are often given undue legitimacy in the speech community, particularly with respect to the assumption that a sequence of acoustic observations can be synchronised with a sequence of phones.  Often phones are seen as being the "atoms" of speech in that they are the set of units from which all else (that is, word sequences) can be built.  But just as with atoms in physics, it is now widely accepted in phonology that phones are decomposable into smaller, more fundamental units.  There is no consensus as to what these units are, but the most popular view is that phones can be constructed from a set of phonological distinctive features.  Phones are a useful representation because words can easily be re-written as phones using a lexicon.  We argue here however that it is inappropriate to directly link acoustic observations to HMM states and phones: the HMM paradigm is not valid.  The principle of distinctive features was first proposed in the classic work of Jakobson, Fant and Halle (1952).  Although this work gained much attention when published, many (e. g.  (Jones, 1957)) regarded features as no-more than a useful classification scheme, whereby one could refer to the class of "nasal phones" or "voiced phones".  The power of features became evident with the publication of The Sound Pattern of English by Chomsky and Halle (1968) (hereafter SPE), where the authors showed that what were otherwise complex phonological rules could be written concisely if features were used rather than phones.  The goal of feature theory in phonology has been to discover the most basic set of fundamental underlying units (the features) from which surface forms (e. g.  phones) can be derived; a small number of simple features can be combined to give rise to the larger number of phones, whose behaviour is more complex.  1. 2.  Related work on Phonological Features The idea of using phonological features for speech recognition is not new, as many others have seen the basic theoretical advantages laid out above.  Among others, the CMU Hearsay-II system (Goldberg & Reddy, 1976) made some use of features, as did the CSTR Alvey recogniser (Harrington, 1987).  Often these these systems used knowledge based techniques to extract their features and in the end the performance of these systems was poor on speaker independent continuous speech.  Some more recent work has continued in this vein. 
Transforming Voice Quality| Abstract Voice transformation is the process of transforming the characteristics of speech uttered by a source speaker, such that a listener would believe the speech was uttered by a target speaker.  In this paper we address the problem of transforming voice quality.  We do not attempt to transform prosody.  Our system has two main parts corresponding to the two components of the source-filter model of speech production.  The first component transforms the spectral envelope as represented by a linear prediction model.  The transformation is achieved using a Gaussian mixture model, which is trained on aligned speech from source and target speakers.  The second part of the system predicts the spectral detail from the transformed linear prediction coefficients.  A novel approach is proposed, which is based on a classifier and residual codebooks.  On the basis of a number of performance metrics it outperforms existing systems. 
USING INTONATION TO CONSTRAIN LANGUAGE MODELS IN SPEECH RECOGNITION| ABSTRACT This paper describes a method for using intonation to reduce word error rate in a speech recognition system designed to recognise spontaneous dialogue speech.  We use a form of dialogue analysis based on the theory of conversational games.  Different move types under this analysis conform to different language models.  Different move types are also characterised by different intonational tunes.  Our overall recognition strategy is first to predict from intonation the type of game move that a test utterance represents, and then to use a bigram language model for that type of move during recognition. 
The size of triangulations supporting a given link| Abstract Let T be a triangulation of S 3 containing a link L in its 1--skeleton.  We give an explicit lower bound for the number of tetrahedra of T in terms of the bridge number of L .  Our proof is based on the theory of almost normal surfaces. 
Subjective Evaluation of Join Cost Functions Used in Unit Selection Speech| Abstract.  In our previous papers, we have proposed join cost functions derived from spectral distances, which have good correlations with perceptual scores obtained for a range of concatenation discontinuities.  To further validate their ability to predict concatenation discontinuities, we have chosen the best three spectral distances and evaluated them subjectively in a listening test.  The unit sequences for synthesis stimuli are obtained from a state-of-the-art unit selection text-to-speech system: rVoice from Rhetorical Systems Ltd.  In this paper, we report listeners' preferences for each of the three join cost functions. 
SVitchboard 1: Small Vocabulary Tasks from Switchboard 1| Abstract We present a conversational telephone speech data set designed to support research on novel acoustic models.  Small vocabulary tasks from 10 words up to 500 words are defined using subsets of the Switchboard-1 corpus; each task has a completely closed vocabulary (an OOV rate of 0%).  We justify the need for these tasks, describe the algorithm for selecting them from a large corpus, give a statistical analysis of the data and present baseline whole-word hidden Markov model recognition results.  The goal of the paper is to define a common data set and to encourage other researchers to use it. 
DYNAMICAL SYSTEM MODELLING OF ARTICULATOR MOVEMENT| ABSTRACT We describe the modelling of articulatory movements using (hidden) dynamical system models trained on Electro-Magnetic Articulograph (EMA) data.  These models can be used for automatic speech recognition and to give insights into articulatory behaviour.  They belong to a class of continuous-state Markov models, which we believe can offer improved performance over conventional Hidden Markov Models (HMMs) by better accounting for the continuous nature of the underlying speech production process -- that is, the movements of the articulators.  To assess the performance of our models, a simple speech recognition task was used, on which the models show promising results. 
Using Information Above the Word Level for Automatic Speech Recognition|
Festival 2 - Build Your Own General Purpose Unit Selection Speech Synthesiser| 5th ISCA Speech Synthesis. 
Tax evasion and equity theory: An investigative approach|
Case tools and organizational action|
Z: Grammar and concrete and abstract syntaxes|
The CICS application programming interface: Program control|
Specification and design of a library system|
Detection of Phonological Features in Continuous Speech using Neural Networks|
Helpmate autonomous mobile robot navigation system|
Intonation and Dialog Context as Constraints for Speech Recognition|
The Edinburgh Speech Tools Library Version 1|2. 0",. 
Understanding interactive dynamic situations|
"A visual surveillance system for incidence detection|
CSTR Speech Tools, 1996/7| email fpault,. 
Final report for Verbmobil 1 Teilprojekt 4|4. 
Mathematics for specification and design: The problem with lifts|. 
Edinburgh Speech Tools Library, System Documentation Edition 1|2. 
` On relative performance contracts and fund managers incentives,|
#The case for IPv6|# Internet-Draft,. 
"Submersible Holocamera for Detection of Particle Characteristics and Motions in the Ocean" Deep-Sea Research I 46|
From Context to Content: Leveraging Context to Infer Media Metadata|
Mesoscale investigation of the deformation field of an aluminum bicrystal,|
Using prosodic information to constrain language modesl for spoken dialogue|
"CSTR software,"|
Formal specification and development in z and b|
How to make a triangulation of S 3 polytopal, preprint, arXiv:|
A compliance notation for verifying concurrent systems|
CICS Project Report: Experiences and Results from The Use Of Z In IBM|
Monetary Transmission: Through Bank Loans or Bank Liabilities?"|
MMM2: Mobile Media Metadata for Media Sharing,|
Progress in SANS studies of polymer systems,|
CASE 2000: The future of CASE technology|
Cognitive abilities and school performance of extremely low birth weight children and matched term control children at age 8 years: a regional study|
editors|
Personal Communication| UNC-Chapel Hill Institute of Marine Science,. 
A Topological Representation Theorem for Oriented Matroids,|
Private or Public? A Taxonomy of Optimal Ownership and Management Regimes|
Organizational capabilities and competitive advantage: Senior managers' perceptions of past use, past payoff, and future use|
The new MMT,"|
Fast Animation of Amorphous and Gaseous Volumes|
The self-reported depressive symptoms in inner-city adolescents seeking routine health care|

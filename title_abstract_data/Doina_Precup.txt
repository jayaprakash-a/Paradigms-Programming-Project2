Hierarchical Optimal Control of MDPs| Abstract Fundamental to reinforcement learning, as well as to the
Learning to Schedule Straight-Line Code| Abstract Program execution speed on modern computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor.  To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling.  Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming.  We show how to cast the instruction scheduling problem as a learning task, obtaining the heuristic scheduling algorithm automatically.  Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions).  Our empirical results show that just a few features are adequate for quite good performance at this task for a real modern processor, and that any of several supervised learning methods perform nearly optimally with respect to the features used. 
Planning with Closed-Loop Macro Actions| Abstract Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence.  In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning.  Conventional model-based reinforcement learning uses primitive actions that last one time step and that can be modeled independently of the learning agent.  These can be generalized to macro actions, multi-step actions specified by an arbitrary policy and a way of completing.  Macro actions generalize the classical notion of a macro operator in that they are closed loop, uncertain, and of variable duration.  Macro actions are needed to represent common-sense higher-level actions such as going to lunch, grasping an object, or traveling to a distant city.  This paper generalizes prior work on temporally abstract models (Sutton 1995) and extends it from the prediction setting to include actions, control, and planning.  We define a semantics of models of macro actions that guarantees the validity of planning using such models.  This paper present new results in the theory of planning with macro actions and illustrates its potential advantages in a gridworld task. 
RedAgent-2003: An Autonomous Market-Based Supply-Chain Management Agent| Abstract The Supply Chain Management track of the international Trading Agents Competition (TAC SCM) was introduced in 2003 as a test-bed for researchers interested in building autonomous agents that act in dynamic supply chains.  TAC SCM provides a challenging scenario for existing AI decisionmaking algorithms, due to the high dimensionality and the non-determinism of the environment, as well as the combinatorial nature of the problem.  In this paper we present RedAgent, the winner of the first TAC SCM competition.  RedAgent is based on a multi-agent design, in which many simple, heuristic agents manage tasks such as fulfilling customer orders or procuring particular resources.  The key idea is to use internal markets as the main decision mechanism, in order to determine what products to focus on and how to allocate the existing resources.  The internal markets ensure the coordination of the individual agents, but at the same time provide price estimates for the goods that RedAgent has to sell and purchase, a key feature in this domain.  We describe RedAgent's architecture and analyze its behavior based on data from the competition. 
Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning| Abstract Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI.  this paper we consider how these challenges can be addressed within the mathematical framework reinforcement learning and Markov decision processes (MDPs).  We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over period of time.  Examples of options include picking up an object, going to lunch, and traveling a distant city, as well as primitive actions such as muscle twitches and joint torques.  Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in natural and general way.  In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning.  Formally, a set of options defined over MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory options.  However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory.  We present results for three such cases: (1) we show that the results planning with options can be used during execution interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able learn about an option from fragments of execution, and (3) we propose notion subgoal that can be used to improve the options themselves.  All of these results have precursors in the existing literature; the contribution of this paper is to establish them in simpler and more general setting with fewer changes to the existing reinforcement learning framework.  In particular, we show that these results can be obtained without committing (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro Corresponding author. 
Off-policy Learning with Recognizers| Abstract We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods.  We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections.  We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance.  This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence.  Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators.  Off-policy learning is learning about one way of behaving while actually behaving in another way.  For example, Q-learning is an off- policy learning method because it learns about the optimal policy while taking actions in a more exploratory fashion, e. g. , according to an e-greedy policy.  Off-policy learning is of interest because only one way of selecting actions can be used at any time, but we would like to learn about many different ways of behaving from the single resultant stream of experience.  For example, the options framework for temporal abstraction involves considering a variety of different ways of selecting actions.  For each such option one would like to learn a model of its possible outcomes suitable for planning and other uses.  Such option models have been proposed as fundamental building blocks of grounded world knowledge (Sutton, Precup & Singh, 1999; Sutton, Rafols & Koop, 2005).  Using off-policy learning, one would be able to learn predictive models for many options at the same time from a single stream of experience.  Unfortunately, off-policy learning using temporal-difference methods has proven problematic when used in conjunction with function approximation.  Function approximation is essential in order to handle the large state spaces that are inherent in many problem domains.  Q-learning, for example, has been proven to converge to an optimal policy in the tabular case, but is unsound and may diverge in the case of linear function approximation (Baird, 1996).  Precup, Sutton, and Dasgupta (2001) introduced and proved convergence for the first off-policy learning algorithm with linear function approximation.  They addressed the problem of learning the expected value of a target policy based on experience generated using a different behavior policy.  They used importance sampling techniques to reduce the off-policy case to the on-policy case, where existing convergence theorems apply (Tsitsiklis & Van Roy, 1997; Tadic, 2000).  There are two important difficulties with that approach.  First, the behavior policy needs to be stationary and known, because it is needed to compute the importance sampling corrections.  Second, the importance sampling weights are often ill-conditioned.  In the worst case, the variance could be infinite and convergence would not occur.  The conditions required to prevent this were somewhat awkward and, even when they applied and asymptotic convergence was assured, the variance could still be high and convergence could be slow.  In this paper we address both of these problems in the context of off-policy learning for options.  We introduce the notion of a recognizer.  Rather than specifying an explicit target policy (for instance, the policy of an option), about which we want to make predictions, a recognizer specifies a condition on the actions that are selected.  For example, a recognizer for the temporally extended action of picking up a cup would not specify which hand is to be used, or what the motion should be at all different positions of the cup.  The recognizer would recognize a whole variety of directions of motion and poses as part of picking the cup.  The advantage of this strategy is not that one might prefer a multitude of different behaviors, but that the behavior may be based on a variety of different strategies, all of which are relevant, and we would like to learn from any of them.  In general, a recognizer is a function that recognizes or accepts a space of different ways of behaving and thus, can learn from a wider range of data.  Recognizers have two advantages over direct specification of a target policy: 1) they are a natural and easy way to specify a target policy for which importance sampling will be well conditioned, and 2) they do not require the behavior policy to be known.  The latter is important because in many cases we may have little knowledge of the behavior policy, or a stationary behavior policy may not even exist.  We show that for the case of state aggregation, even if the behavior policy is unknown, convergence to a good model is achieved.  1 Non-sequential example The benefits of using recognizers in off-policy learning can be most easily seen in a nonsequential context with a single continuous action.  Suppose you are given a sequence of sample actions a i 2 [0, 1], selected i. i. d.  according to probability density b : [0, 1] 7! + (the behavior density).  For example, suppose the behavior density is of the oscillatory form shown as a red line in Figure 1.  For each each action, a i , we observe a corresponding outcome, z i 2 , a random variable whose distribution depends only on a i .  Thus the behavior density induces an outcome density.  The on-policy problem is to estimate the mean m b of the outcome density.  This problem can be solved simply by averaging the sample outcomes: ^ m b = i z i .  The off-policy problem is to use this same data to learn what the mean would be if actions were selected in some way other than b.  For example, if the actions were restricted to a designated range, such as between 0. 7 and 0. 9.  There are two natural ways to pose this off-policy problem.  The most straightforward way is to be equally interested in all actions within the designated region.  One professes to be interested in actions selected according to a target density p : [0, 1] 7! + , which in the example would be 5. 0 between 0. 7 and 0. 9, and zero elsewhere, as in the dashed line in Figure 1 (left).  The importance- sampling estimate of the mean outcome is ^ m p = i p(a i ) b(a i ) z i .  (1)
Special issue: TOOLS USA 2003 Improving Rule Set Based Software Quality Prediction: A Genetic Algorithm-based Approach| Abstract The object-oriented (OO) paradigm has now reached maturity.  OO software products are becoming more complex which makes their evolution effort and time consuming.  In this respect, it has become important to develop tools that allow assessing the stability of OO software (i. e. , the ease with which a software item can evolve while preserving its design).  In general, predicting the quality of OO software is a complex task.  Although many predictive models are proposed in the literature, we remain far from having reliable tools that can be applied to real industrial systems.  The main obstacle for building reliable predictive tools for real industrial systems is the lackof representative samples.  Unlike other domains where such samples can be drawn from available large repositories of data, in OO software the lack of such repositories makes it hard to generalize, to validate and to reuse existing models.  Since universal models do not exist, selecting an appropriate quality model is a difficult, non-trivial decision for a company.  In this paper, we propose two general approaches to solve this problem.  They consist of combining/adapting a set of existing models.  The process is driven by the context of the target company.  These approaches are applied to OO software stability prediction. 
Multi-Time Models for Reinforcement Learning| Abstract Reinforcement learning can be used not only to predict rewards, but also to predict states, i. e.  to learn a model of the world's dynamics.  Models can be defined at different levels of temporal abstraction.  Multi-time models are models that focus on predicting what will happen, rather than when a certain event will take place.  Based on multi-time models, we can define abstract actions, which enable planning (presumably in a more efficient way) at various levels of abstraction.  1 Action models Model-based reinforcement learning offers a possible solution to the problem of integrating planning in a real-time learning agent.  Models are used to make predictions about the environment.  The input of a model is a state and an action.  The model should output a distribution of possible future states, as well as the expected value of the reward along the way.  Since models provide action-dependent predictions, they also define policies for achieving certain states, or for achieving maximum expected rewards.  Reinforcement learning algorithms have traditionally been concerned with 1-step models, which assume that the agent interacts with its environment at some discrete, lowest-level time scale.  We extend this framework by defining multi-time models, which describe the environment at different time scales.  Multi-time models are a formalism for describing abstract actions. 
MIT Press| Improved Switching among Temporally Abstract Actions.  Abstract In robotics and other control applications it is commonplace to have a
Exponentiated Gradient Methods for Reinforcement Learning| Abstract This paper introduces and evaluates a natural extension of linear exponentiated gradient methods that makes them applicable to reinforcement learning problems.  Just as these methods speed up supervised learning, we find that they can also increase the efficiency of reinforcement learning.  Comparisons are made with conventional reinforcement learning methods on two test problems using CMAC function approximators and replacing traces.  On a small prediction task, exponentiated gradient methods showed no improvement, but on a larger control task (Mountain Car) they improved the learning speed by approximately 25%.  A more detailed analysis suggests that the difference may be due to the distribution of irrelevant features. 
Learning to Schedule Straight-Line Code| Abstract Execution speed of programs on modern computer architectures is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor.  To realize potential execution efficiency, it is now customary for an optimizing compiler to employ a heuristic algorithm for instruction scheduling.  These algorithms are painstakingly hand-crafted, which is expenseive and time-consuming.  We show how to cast the instruction scheduling problem as a learning task, so that one obtains the heuristic scheduling algorithm automatically.  Our focus is the narrower problem of scheduling straight-line code,
Ph|D.  Thesis Proposal.  Study of Reinforcement Learning Methods with Generalization Capabilities.  Abstract My Ph. D.  research will contribute to the area of reinforcement learning (RL) - a framework for learning by computational agents that are situated in a dynamic environment and strive to achieve some long-term performance goals.  In order to be applicable to problems with large state spaces, RL methods have to make use of function approximation (FA) techniques.  However, such a combination appears to be very difficult for theoretical analysis and theoretical results are limited.  Moreover, there is not much practical experience with these systems.  In my thesis, I plan to study the effect of FA and problem characteristics on RL algorithms.  As the first stage of my work, I will conduct a large-scale empirical study of a number of RL methods combined with FA techniques.  This will be the first attempt to date to compare and study the algorithms' performance in an extensive, unified test environment.  I will define attributes characterizing learning problems and develop a collection of benchmark problems based on these attributes.  In the second stage of my work, I will develop algorithms and recommendations to partially automate the design of RL systems by automatic parameter settings and selection of appropriate RL/FA methods based on domain properties.  I will perform an in-depth analysis of the experimental results in order to gain understanding of the various outstanding questions about the properties and relative merits of the algorithms.  The goal is to elucidate the effect of domain and FA dynamics on RL algorithms.  My thesis will contribute to both application and theoretical aspects of RL. 
Improved Switching among Temporally Abstract Actions| Abstract In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps hand-crafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible.  In this paper we present a framework based on Markov decision processes and semi-Markov decision processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem.  In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers, with negligible additional cost and no re-planning.  In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.  In many applications, solutions to parts of a task are known, either because they were handcrafted by people or because they were previously learned or planned.  For example, in robotics applications, there may exist controllers for moving joints to positions, picking up objects, controlling eye movements, or navigating along hallways.  More generally, an intelligent system may have available to it several temporally extended courses of action to choose from.  In such cases, a key challenge is to take full advantage of the existing temporally extended actions, to choose or switch among them effectively, and to plan at their level rather than at the level of individual actions.  Recently, several researchers have begun to address these challenges within the framework of reinforcement learning and Markov decision processes (e. g. , Singh, 1992; Kaelbling, 1993; Dayan & Hinton, 1993; Thrun and Schwartz, 1995; Sutton, 1995; Dietterich, 1998; Parr & Russell, 1998; McGovern, Sutton & Fagg, 1997).  Common to much of this recent work is the modeling of a temporally extended action as a policy (controller) and a condition for terminating, which we together refer to as an option (Sutton, Precup & Singh, 1998).  In this paper we consider the problem of effectively combining given options into one overall policy, generalizing prior work by Kaelbling (1993).  Sections 1--3 introduce the framework; our new results are in Sections 4 and 5.  1 Reinforcement Learning (MDP) Framework In a Markov decision process (MDP), an agent interacts with an environment at some discrete, lowest-level time scale t = 0; 1; 2; : : : On each time step, the agent perceives the state of the environment, s t 2 S, and on that basis chooses a primitive action, a t 2 A.  In response to each action, a t , the environment produces one step later a numerical reward, r t+1 , and a next state, s t+1 .  The one-step model of the environment consists of the one-step statetransition probabilities and the one-step expected rewards, p a ss 0 = Prfs t+1 = s 0 j s t = s; a t = ag and r a s = Efr t+1 j s t = s; a t = ag; for all s; s 0 2 S and a 2 A.  The agent's objective is to learn an optimal Markov policy, a mapping from states to probabilities of taking each available primitive action, : S \Theta A ! [0; 1], that maximizes the expected discounted future reward from each state s: V (s) = E n r t+1 + flr t+2 + \Delta \Delta \Delta fi fi fi s t = s; o = X a2As (s; a)[r a s + fl X s 0 p a ss 0 V (s 0 )]; where (s; a) is the probability with which the policy chooses action a 2 A s in state s, and fl 2 [0; 1] is a discount-rate parameter.  V (s) is called the value of state s under policy , and V is called the state-value function for .  The optimal state-value function gives the value of a state under an optimal policy: V \Lambda (s) = max V (s) = max a2As [r a s + fl P s 0 p a ss 0 V \Lambda (s 0 )].  Given V \Lambda , an optimal policy is easily formed by choosing in each state s any action that achieves the maximum in this equation.  A parallel set of value functions, denoted Q and Q \Lambda , and Bellman equations can be defined for state-action pairs, rather than for states.  Planning in reinforcement learning refers to the use of models of the environment to compute value functions and thereby to optimize or improve policies.  2 Options We use the term options for our generalization of primitive actions to include temporally extended courses of action.  Let h t;T = s t ; a t ; r t+1 ; s t+1 ; a t+1 ; : : : ; r T ; s T be the history sequence from time t T to time T , and let \Omega denote the set of all possible histories in the given MDP.  Options consist of three components: an initiation set I ` S, a policy : \Omega \Theta A ! [0; 1], and a termination condition fi : \Omega ! [0; 1].  An option o = hI; ; fii can be taken in state s if and only if s 2 I.  If o is taken in state s t , the next action a t is selected according to (s t ; \Delta).  The environment then makes a transition to s t+1 , where o terminates with probability fi(h t;t+1 ), or else continues, determining a t+1 according to (h t;t+1 ; \Delta), and transitioning to state s t+2 , where o terminates with probability fi(h t;t+2 ) etc.  We call the general options defined above semi-Markov because and fi depend on the history sequence; in Markov options and fi depend only on the current state.  Semi-Markov options allow "timeouts", i. e. , termination after some period of time has elapsed, and other extensions which cannot be handled by Markov options.  The initiation set and termination condition of an option together limit the states over which the option's policy must be defined.  For example, a hand-crafted policy for a mobile robot to dock with its battery charger might be defined only for states I in which the battery charger is within sight.  The termination condition fi would be defined to be 1 outside of I and when the robot is successfully docked.  We can now define policies over options.  Let the set of options available in state s be denoted O s ; the set of all options is denoted O = S s2S O s .  When initiated in a state s t , the Markov policy over options : S \Theta O ! [0; 1] selects an option o 2 O s t according to the probability distribution (s t ; \Delta).  The option o is then taken in s t , determining actions until it terminates in s t+k , at which point a new option is selected, according to (s t+k ; \Delta), and so on.  In this way a policy over options, , determines a (non-stationary) policy over actions, or flat policy, = f().  We define the value of a state s under a general flat policy as the expected return
A Convergent Form of Approximate Policy Iteration| Abstract We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a "policy improvement operator" to generate a new policy based on the learned state-action values.  We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy.  To our knowledge, this is the first convergence result for any form of approximate policy iteration under similar computational-resource assumptions. 
Classification Using Phi-Machines and Constructive Function Approximation| Abstract The new classification algorithm CLEF combines a version of a linear machine known as a \Phi-machine with a non-linear function approximator that constructs its own features.  The algorithm finds non-linear decision boundaries by constructing features that are needed to learn the necessary discriminant functions.  The CLEF algorithm is proven to separate all consistently labelled training instances, even when they are not linearly separable in the input variables.  The algorithm is illustrated on a variety of tasks. 
Sparse Distributed Memories for On-Line Value-Based Reinforcement Learning| Abstract.  In this paper, we advocate the use of Sparse Distributed Memories (SDMs) for on-line, value-based reinforcement learning (RL).  SDMs provide a linear, local function approximation scheme, designed to work when a very large/ high-dimensional input (address) space has to be mapped into a much smaller physical memory.  We present an implementation of the SDM architecture for on-line, value-based RL in continuous state spaces.  An important contribution of this paper is an algorithm for dynamic on-line allocation and adjustment of memory resources for SDMs, which eliminates the need for choosing the memory size and structure a priori.  In our experiments, this algorithm provides very good performance while efficiently managing the memory resources. 
Characterizing Markov Decision Processes| proposed MDP attributes can be used to facilitate the design of reinforcement learning systems. 
Model minimization by linear PSR| Abstract Predictive state representation (PSR), proposed by [Littman et al. , 2002; Singh et al. , 2004] , are a general representation for controlled dynamical systems.  We present a sufficient condition under which a linear PSR compresses a POMDP representation. 
Combining TD-learning with Cascade-correlation Networks| Abstract Using neural networks to represent value functions in reinforcement learning algorithms often involves a lot of work in hand-crafting the network structure, and tuning the learning parameters.  In this paper, we explore the potential of using constructive neural networks in reinforcement learning.  Constructive neural network methods are appealing because they can build the network structure based on the data that needs to be represented.  To our knowledge, such algorithms have not been used in reinforcement learning.  A major issue is that constructive algorithms often work in batch mode, while many reinforcement learning algorithms work on-line.  We use a cache to accumulate data, then use a variant of cascade correlation to update the value function.  Preliminary results on the game of Tic-Tac-Toe show the potential of this new algorithm, compared to using static feed-forward neural networks trained with backpropagation. 
Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales| Abstract Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key challenges for AI.  In this paper we develop an approach to these problems based on the mathematical framework of reinforcement learning and Markov decision processes (MDPs).  We extend the usual notion of action to include options---whole courses of behavior that may be temporally extended, stochastic, and contingent on events.  Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques.  Options may be given a priori, learned by experience, or both.  They may be used interchangeably with actions in a variety of planning and learning methods.  The theory of semi-Markov decision processes (SMDPs) can be applied to model the consequences of options and as a basis for planning and learning methods using them.  In this paper we develop these connections, building on prior work by Bradtke and Duff (1995), Parr (1998) and others.  Our main novel results concern the interface between the MDP and SMDP levels of analysis.  We show how a set of options can be altered by changing only their termination conditions to improve over SMDP methods with no additional cost.  We also introduce intra-option temporal-difference methods that are able to learn from fragments of an option's execution.  Finally, we propose a notion of subgoal which can be used to improve the options themselves.  Overall, we argue that options and their models provide hitherto missing aspects of a powerful, clear, and expressive framework for representing and organizing knowledge.  1.  Temporal Abstraction To make everyday decisions, people must foresee the consequences of their possible courses of action at multiple levels of temporal abstraction.  Consider a traveler deciding to undertake a journey to a distant city.  To decide whether or not to go, the benefits of the trip must be weighed against the expense.  Having decided to go, choices must be made at each leg, e. g. , whether to fly or to drive, whether to take a taxi or to arrange a ride.  Each of these steps involves foresight and decision, all the way down to the smallest of actions.  For example, just to call a taxi may involve finding a telephone, dialing each digit, and the individual muscle contractions to lift the receiver to the ear.  Human decision making routinely
Inference of Gene Regulatory Networks from Large Scale Gene Expression Data| Abstract With the advent of the age of genomics, an increasing number of genes have been identified and their functions documented.  However, not as much is known of specific regulatory relations among genes (e. g.  gene A up-regulates gene B).  At the same time, there is an increasing number of large-scale gene expression datasets, in which the mRNA transcript levels of tens of thousands of genes are measured at a number of time points, or under a number of different conditions.  A number of studies have proposed to find gene regulatory networks from such datasets.  Our method is a modification of the continuous-time neural network method of Wahde & Hertz [25, 26].  The genetic algorithm used to update weights was replaced with Levenberg-Marquardt optimization.  We tested our method on artificial data as well as Spellman's yeast cell cycle data [22].  Results indicated that this method was able to detect salient regulatory relations between genes.  Abstract Avec l'arrivee de la genomique, un nombre croissant de gnes ont ete identifies et leurs fonctions documentees.  Toutefois, les relations regulatoires specifiques (par exemple, l'e#et regulatoire du gne A sur le gne B) sont moins bien connues.  En m^eme temps, nous disposons d'une quantite de donnees genetique `a grande echelle de plus en plus vaste, ou nous pouvons trouver l'evolution de dizaines de milliers de g`enes echantillonnes `a plusieurs instants ou sous di#erentes conditions.  Des etudes ont proposes d'inferer des reseaux regulatoires genetiques `a partir de ces donnees.  Notre methode est une modification des reseaux neuronaux a temps continu de Wahde et Hertz [25, 26].  L'algorithme genetique utilise pour la mise `a jours des poids est remplace par la methode Levenberg-Marquardt.  Nous avons teste notre methode avec des donnees artificielles ainsi que les donnes de cycle de levure de Spellman.  Nos resultats indiquent que cette methode est capable de detecter les relations regulatoires saillantes entre les gnes. 
Category: Reinforcement Learning and Control; ORAL presentation Improved Switching among Temporally Abstract Actions| Abstract In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps handcrafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible.  In this paper we present a framework based on Markov Decision Processes and Semi-Markov Decision Processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem.  In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers with negligible additional cost and no replanning.  In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.  In many applications, possible solutions to parts of a task are known, either because they were handcrafted by people or because they were previously learned or planned.  For example, in robotics applications, there may exist controllers for moving joints to positions, picking up objects, controlling eye movements, or navigating along hallways.  More generally, an intelligent system may have available to it several temporally extended courses of action to choose from.  In such cases, a key challenge is to take full advantage of the existing temporally extended actions, to choose or switch among them effectively and to plan at their level rather than at the level of individual actions.  Recently, several researchers have begun to address these challenges within the framework of reinforcement learning and Markov decision processes (e. g. , Singh, 1992; Kaelbling, 1993; Dayan & Hinton, 1993; Thrun and Schwartz, 1995; Sutton, 1995; Dietterich, 1998; Parr & Russell, 1998; McGovern, Sutton & Fagg, 1997).  Common to much of this recent work is the modeling of a temporally extended action as a policy (controller) and a condition for terminating, which we together refer to as an option (Sutton, Precup & Singh, 1998, in preparation).  In this paper we consider the problem of effectively combining given options into one overall policy.  1 Reinforcement Learning (MDP) Framework In the Markov Decision Process (MDP) framework an agent interacts with an environment at some discrete, lowest-level time scale t = 0; 1;
Theoretical Results on Reinforcement Learning with Temporally Abstract Options| Abstract.  We present new theoretical results on planning within the framework of temporally abstract reinforcement learning (Precup & Sutton, 1997; Sutton, 1995).  Temporal abstraction is a key step in any decision making system that involves planning and prediction.  In temporally abstract reinforcement learning, the agent is allowed to choose among "options", whole courses of action that may be temporally extended, stochastic, and contingent on previous events.  Examples of options include closed-loop policies such as picking up an object, as well as primitive actions such as joint torques.  Knowledge about the consequences of options is represented by special structures called multi-time models.  In this paper we focus on the theory of planning with multi-time models.  We define new Bellman equations that are satisfied for sets of multi-time models.  As a consequence, multi-time models can be used interchangeably with models of primitive actions in a variety of well-known planning methods including value iteration, policy improvement and policy iteration. 
Intra-Option Learning about Temporally Abstract Actions| Abstract Several researchers have proposed modeling temporally abstract actions in reinforcement learning by the combination of a policy and a termination condition, which we refer to as an option.  Value functions over options and models of options can be learned using methods designed for semi-Markov decision processes (SMDPs).  However, all these methods require an option to be executed to termination.  In this paper we explore methods that learn about an option from small fragments of experience consistent with that option, even if the option itself is not executed.  We call these methods intra-option learning methods because they learn from experience within an option.  Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporaldifference mechanisms to learn simultaneously about all the options consistent with an experience, not just the few that were actually executed.  In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the consequences of options.  We present computational examples in which these new methods learn much faster than SMDP methods and learn effectively when SMDP methods cannot learn at all.  We also sketch a convergence proof for intraoption value learning. 
Using core beliefs for point-based value iteration| Abstract Recent research on point-based approximation algorithms for POMDPs demonstrated that good solutions to POMDP problems can be obtained without considering the entire belief simplex.  For instance, the Point Based Value Iteration (PBVI) algorithm [Pineau et al. , 2003] computes the value function only for a small set of belief states and iteratively adds more points to the set as needed.  A key component of the algorithm is the strategy for selecting belief points, such that the space of reachable beliefs is well covered.  This paper presents a new method for selecting an initial set of representative belief points, which relies on finding first the basis for the reachable belief simplex.  Our approach has better worst-case performance than the original PBVI heuristic, and performs well in several standard POMDP tasks. 
Eligibility Traces for Off-Policy Policy Evaluation| Abstract Eligibility traces have been shown to speed reinforcement learning, to make it more robust to hidden states, and to provide a link between Monte Carlo and temporal-difference methods.  Here we generalize eligibility traces to off-policy learning, in which one learns about a policy different from the policy that generates the data.  Off-policy methods can greatly multiply learning, as many policies can be learned about from the same data stream, and have been identified as particularly useful for learning about subgoals and temporally extended macro-actions.  In this paper we consider the off-policy version of the policy evaluation problem, for which only one eligibility trace algorithm is known, a Monte Carlo method.  We analyze and compare this and four new eligibility trace algorithms, emphasizing their relationships to the classical statistical technique known as importance sampling.  Our main results are 1) to establish the consistency and bias properties of the new methods and 2) to empirically rank the new methods, showing improvement over one-step and Monte Carlo methods.  Our results are restricted to model-free, table-lookup methods and to offline updating (at the end of each episode) although several of the algorithms could be applied more generally.  In reinforcement learning, we generally learn from experience, that is, from the sequence of states, actions, and rewards generated by the agent interacting with its environment.  This data is affected by the decision-making policy used by the agent to select its actions, and thus we often end up learning something that is a function of the agent's policy.  For example, the common subproblem of policy evaluation is to learn the value function for the agent's policy (the function giving the expected future reward available from each state--action pair).  In general, however, we might want to learn about policies other than that currently followed by the agent, a process known as off-policy learning.  For example, 1-step Q-learning is often used in an off-policy manner, learning about the greedy policy while the data is generated by a slightly randomized policy that ensures exploration.  Off-policy learning is especially important for research on the use of temporally extended actions in reinforcement learning (
Multi-time Models for Temporally Abstract Planning| Abstract Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence.  In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning.  Current model-based reinforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver.  This paper generalizes prior work on temporally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning.  We introduce a more general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship to the Bellman equations.  This paper summarizes the theoretical framework of multi-time models and illustrates their potential advantages in a gridworld planning task.  The need for hierarchical and abstract planning is a fundamental problem in AI (see, e. g. , Sacerdoti, 1977;
Using Options for Knowledge Transfer in Reinforcement Learning| Abstract One of the original motivations for the use of temporally extended
Off-Policy Temporal Difference Learning with Function Approximation|
Constructive function approximation|
Eligibility traces for o8-polcy policy evaluation|
An autonomous, market-based supply-chain management agent|
RedAgent - Winner of TAC SCM 2003|
in preparation| Between MDPs and Semi-MDPs: learning, planning, and representing knowledge at multiple temporal scales. 
Empirical comparison of gradient descent and exponentiated gradient descent in supervised and reinforcement learning,|
Eligibilty trace methods for o#-policy evaluation|
Sparse distribute memories as function approximators in value-based reinforcement learning: Case studies|
Proceedings of the 1994 Conference (|
A convergent form of approxinate policy iteration|
planning, and representing knowledge at multiple temporal scales|
A Planning Algorithm for Predictive State Representations|
How to Find Big-Oh in Your Data Set (and How Not to)|
Learning Policies for Local Instruction Scheduling|
Searching for Big-Oh in the data: Inferring asymptotic complexity from experiments|
Multi-Time Models for Reinforcement Learning, University of Massachusetts Amherst|

Learning, Regularization and Ill-Posed Inverse Problems| Abstract Many works have recently shown that strong connections relate learning from examples and kernel methods to regularization techniques for ill-posed inverse problems.  Nevertheless by now there was no clear evidence neither that learning from examples could be seen as an inverse problem nor that theoretical results (like consistency) in learning theory could be independently derived using tools from regularization theory for ill-posed inverse problems.  In this paper we provide a positive answer to both the above questions.  First, considering the square loss, we define a linear direct problem and the corresponding linear inverse problem translating the learning problem in the language of regularization theory.  Second, considering the discretization problem of a possibly ill-posed linear inverse problem we show that consistency results and optimal regularization parameter choice can be easily derived. 
Are Loss Functions All the Same?| Abstract In this paper we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory.  We introduce a convexity assumption - which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss.  We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification.  The main outcome of our analysis is that, for classification, the hinge loss appears to be the loss of choice.  Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate.  Furthermore, if the hypothesis space is suciently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage. 
Discretization error analysis for tikhonov regularization|
Some properties of regularized kernel methods|
Model selection for regularized leastsquares algorithm in learning theory|
Support vectors algorithms as regularization networks|
Notes on the use of different loss functions|

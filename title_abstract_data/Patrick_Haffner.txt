High Quality Document Image Compression with DjVu| Abstract We present a new image compression technique called "DjVu " that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color.  This enables fast transmission of document images over low-speed connections, while faithfully reproducing the visual aspect of the document, including color, fonts, pictures, and paper texture.  The DjVu compressor separates the text and drawings, which needs a high spatial resolution, from the pictures and backgrounds, which are smoother and can be coded at a lower spatial resolution.  Then, several novel techniques are used to maximize the compression ratio: the bi-level foreground image is encoded with AT&T's proposal to the new JBIG2 fax standard, and a new wavelet-based compression method is used for the backgrounds and pictures.  Both techniques use a new adaptive binary arithmetic coder called the Z-coder.  A typical magazine page in color at 300dpi can be compressed down to between40to60KB,approximately 5 to 10 times better than JPEG for a similar level of subjective quality.  A real-time, memory efficientversion of the decoder was implemented, and is available as a plug-in for popular web browsers. 
Max-Planck-Institut fr biologische Kybernetik| Abstract We describe a new method for performing a nonlinear form of Principal Component Analysis.  By the use of integral operator kernel functions, we can efficiently compute principal components in high--dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible 5--pixel products in 16\Theta16 images.  We give the derivation of the method, along with a discussion of other techniques which can be made nonlinear with the kernel approach; and present first experimental results on nonlinear feature extraction for pattern recognition.  AS and KRM are with GMD First (Forschungszentrum Informationstechnik)
Escaping the Convex Hull with Extrapolated Vector Machines| Abstract Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls.  We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls.  XVMs improve SVM generalization very significantly on the MNIST [7] OCR data.  They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 
OPTIMIZING SVMS FOR COMPLEX CALL CLASSIFICATION| ABSTRACT Large margin classifiers such as Support Vector Machines (SVM) or Adaboost are obvious choices for natural language document or call routing.  However, how to combine several binary classifiers to optimize the whole routing process and how this process scales when it involves many different decisions (or classes) is a complex problem that has only received partial answers [1, 2].  We propose a global optimization process based on an optimal channel communication model that allows a combination of possibly heterogeneous binary classifiers.  As in Markov modeling, computational feasibility is achieved through simplifications and independence assumptions that are easy to interpret.  Using this approach, we have managed to decrease the call-type classification error rate for AT&T's How May I Help You (HMIHY( sm) ) natural dialog system by 50%. 
Weighted Automata Kernels -- General Framework and Algorithms| Abstract Kernel methods have found in recent years wide use in statistical learning techniques due to their good performance and their computational efficiency in high-dimensional feature space.  However, text or speech data cannot always be represented by the fixed-length vectors that the traditional kernels handle.  We recently introduced a general kernel framework based on weighted transducers, rational kernels, to extend kernel methods to the analysis of variable-length sequences and weighted automata [5] and described their application to spoken-dialog applications.  We presented a constructive algorithm for ensuring that rational kernels are positive definite symmetric, a property which guarantees the convergence of discriminant classification algorithms such as Support Vector Machines, and showed that many string kernels previously introduced in the computational biology literature are special instances of such positive definite symmetric rational kernels [4].  This paper reviews the essential results given in [5, 3, 4] and presents them in the form of a short tutorial. 
Rational Kernels: Theory and Algorithms| Abstract Many classification algorithms were originally designed for fixed-size vectors.  Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and more generally weighted automata.  An approach widely used in statistical learning techniques such as Support Vector Machines (SVMs) is that of kernel methods, due to their computational efficiency in high-dimensional feature spaces.  We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that extend kernel methods to the analysis of variable-length sequences or more generally weighted automata.  We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm.  Not all rational kernels are positive definite and symmetric (PDS), or equivalently verify the Mercer condition, a condition that guarantees the convergence of training for discriminant classification algorithms such as SVMs.  We present several theoretical results related to PDS rational kernels.  We show that under some general conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We give the proof of several characterization results that can be used to guide the design of PDS rational kernels.  We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels.  Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  Rational kernels can be combined with SVMs to form efficient and powerful techniques for a variety of classification tasks in text and speech processing, or computational biology.  We describe examples of general families of PDS rational kernels that are useful in many of these applications and report the result of our experiments illustrating the use of
Rational Kernels| Abstract We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that can be used for analysis of variable-length sequences or more generally weighted automata, in applications such as computational biology or speech recognition.  We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm.  We also describe several general families of positive definite symmetric rational kernels.  These general kernels can be combined with Support Vector Machines to form efficient and powerful techniques for spoken-dialog classification: highly complex kernels become easy to design and implement and lead to substantial improvements in the classification accuracy.  We also show that the string kernels considered in applications to computational biology are all specific instances of rational kernels. 
A GENERAL SEGMENTATION SCHEME FOR DJVU DOCUMENT COMPRESSION| Abstract We describe the "DjVu" (Dej
Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks| Abstract Signal processing and pattern recognition algorithms make extensive use of convolution.  In many cases, computational accuracy is not as important as computational speed.  In feature extraction, for instance, the features of interest in a signal are usually quite distorted.  This form of noise justifies some level of quantization in order to achieve faster feature extraction.  Our approach consists of approximating regions of the signal with low degree polynomials, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions).  With this representation, convolution becomes extremely simple and can be implemented quite effectively.  The true convolution can be recovered byintegrating the result of the convolution.  This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks. 
Gradient-Based Learning Applied to Document Recognition| Abstract--Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique.  Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing.  This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task.  Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. 
Positive Definite Rational Kernels| Abstract.  Kernel methods are widely used in statistical learning techniques.  We recently introduced a general kernel framework based on weighted transducers or rational relations, rational kernels, to extend kernel methods to the analysis of variable-length sequences or more generally weighted automata.  These kernels are ecient to compute and have been successfully used in applications such as spoken-dialog classi#cation.  Not all rational kernels are positive definite and symmetric (PDS) however, a sucient property for guaranteeing the convergence of discriminant classification algorithms such as Support Vector Machines.  We present several theoretical results related to PDS rational kernels.  We show in particular that under some conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels.  Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  1 Motivation Many classification algorithms were originally designed for fixed-length vectors.  Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and even more generally weighted automata.  Indeed, the output of a large-vocabulary speech recognizer for a particular input speech utterance, or that of a complex information extraction system combining several information sources for a specific input query, is typically a weighted automaton compactly representing a large set of alternative sequences.  The weights assigned by the system to each sequence are used to rank different alternatives according to the models the system is based on.  The error rate of such complex systems is still too high in many tasks to rely only on their one-best output, thus it is preferable instead to use the full output weighted automata which contain the correct result in most cases.  Kernel methods [13] are widely used in statistical learning techniques such as Support Vector Machines (SVMs) [2, 4, 14] due to their computational eciency in high-dimensional feature spaces.  Recently, a general kernel framework Semiring Set # # 0 1 Boolean f0; 1g _ ^ 0 1 Probability R+ + # 0 1 Log R [f2 ; +1g # log + +1 0 Tropical R [f2 ; +1g min + +1 0 Table 1.  Semiring examples.  # log is defined by: x # log y = log(e x + e y ).  based on weighted transducers or rational relations, rational kernels, was introduced to extend kernel methods to the analysis of variable-length sequences or more generally weighted automata [3].  It was shown that there are general and ecient algorithms for computing rational kernels.  Rational kernels have been successfully used for applications such as spoken-dialog classification.  Not all rational kernels are positive definite and symmetric (PDS), or equivalently verify the Mercer condition [1], a condition that guarantees the convergence of discriminant classification algorithms such as SVMs.  This motivates the study undertaken in this paper.  We present several theoretical results related to PDS rational kernels.  In particular, we show that under some conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We also study the relationship between rational kernels and some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler [6], and some string kernels used in the context of computational biology [8].  We show that these kernels are all specific instances of rational kernels.  In each case, we explicitly describe the corresponding weighted transducer.  These transducers are often simple and ecient for computing kernels.  Their diagram often provides more insight into the definition of kernels and can guide the design of new kernels.  Our results also include the proof of the fact that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  2 Preliminaries In this section, we present the algebraic definitions and notation necessary to introduce rational kernels.  Definition 1 ([7]).  A system (K ; #; #; 0; 1) is a semiring if: (K ; #; 0) is a commutative monoid with identity element 0; (K ; #; 1) is a monoid with identity element 1; # distributes over #; and 0 is an annihilator for #: for all a 2 K ; a # 0 = 0 # a = 0.  Thus, a semiring is a ring that may lack negation.  Table 1 lists some familiar semirings.  Definition 2.  A weighted finite-state transducer T over a semiring K is an 8-tuple T = (#;
Browsing through High Quality Document Images with DjVu| Abstract We present a new image compression technique called "DjVu " that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color.  With DjVu , any screen connected to the Internet can access and display images of scannedpages while faithfully reproducing the font, color, drawing, pictures, and paper texture.  A typical magazine page in color at 300dpi can becompressed down to between 40 to 60 KB, approximately 5 to 10 times better than JPEG for a similar level of subjective quality.  B&W documents are typically 15 to 30 KBytes at 300dpi, or 4 to 8 times better than CCITT-G4.  Areal-time, memory efficient version of the decoder was implemented, and is available as a plug-in for popular web browsers. 
Integrating Time Alignment and Neural Networks for High Performance Continuous Speech Recognition|
SVMs for histogrambased image classification|
DjVu: Analyzing and Compressing Scanned Documents for Internet Distribution|
Rational kernels| Neural Information Processing Systems 16,. 
Discriminant learning with minimum memory loss for improved nonvocabulary rejection,|
Multi-State Time Delay Networks for Continuous Speech Recognition|
Chemical kinetic and proton magnetic resonance studies of 59-adenosine monophosphate binding to ribonuclease A|
Fast backpropagation learning methods for neural networks in speech",|
Investigation on bolt-cathodes with floating zone melted polycrystalline and monocrystalline LaB 6 emitters",|
Implement "Fuzzy" Connectionist Time-Alignment in Speech Recognition Proceedings ICANN,|
Time-delay neural networks embedding time alignment: a performance analysis,|
Connectionist Word-Level Classification in Speech Recognition Proceedings ICASSP 92,|
Connectionist speech recognition with a global MMI algorithm,|
Rational kernels| In Becker et al.  [5]. 
Gradient-based learning applied to document recognition|
Color Documents on the Web with DJVU|
1999 "SVMs for histogram based image classification,|

A Maximum Likelihood Prosody Recognizer| Abstract Automatic prosody recognition (APR) is of fundamental importance for automatic speech understanding.  In this paper, we propose a maximum likelihood prosody recognizer consisting of a GMM-based acoustic model that models the distribution of the phone-level acoustic-prosodic observations (pitch, duration and energy) and an ANN-based language model that models the word-level stochastic dependence between prosody and syntax.  Our experiments on the Radio News Corpus show that our recognizer is able to achieve 84% pitch accent recognition accuracy and 93% intonational phrase boundary (IPB) recognition accuracy in a leave-one-speaker-out task which has exceeded previous reported results on the same corpus.  The same recognizer is tested on a subset of Switchboard corpus.  The accuracies are degraded but still significantly better than the chance levels. 
Gaussian Mixture Models of Phonetic Boundaries For Speech Recognition| models.  Stop consonant recognition accuracy is increased by 40%. 
AVICAR: Audio-Visual Speech Corpus in a Car Environment| Abstract We describe a large audio-visual speech corpus recorded in a car environment, as well as the equipment and procedures used to build this corpus.  Data are collected through a multi-sensory array consisting of eight microphones on the sun visor and four video cameras on the dashboard.  The script for the corpus consists of four categories: isolated digits, isolated letters, phone numbers, and sentences, all in English.  Speakers from various language backgrounds are included, 50 male and 50 female.  In order to vary the signal-to-noise ratio, each script has five different noise conditions: idling, driving at 35 mph with windows open and closed, and driving at 55 mph with windows open and closed.  The corpus is available through
Speaker-Independent Automatic Detection of Pitch Accent| Abstract This paper presents a novel approach to the automatic detection of pitch accent in spoken English.  The approach that we propose is based on a time-delay recursive neural network (TDRNN), which takes into account contextual information in two ways: (1) a delayed version of prosodic and spectral features serve as inputs which represent an explicit trajectory along time; and (2) recursions from the output layer and some hidden layers provide the contextual labeling information that reflects characteristics of pitch accentuation in spoken English.  We apply the TDRNN to pitch accent detection in two forms.  In the normal TDRNN, all of the prosodic and spectral features are used as an entire set in a single TDRNN.  In the distributed TDRNN, the network consists of several TDRNNs each treating each prosodic feature as a single input.  In addition, we propose a feature called spectral balance-based cepstral coefficient (SBCC) to capture the spectral characteristic of pitch accentuation.  We used the Boston Radio News Corpus (BRNC) to conduct experiments on the speakerindependent detection of pitch accent.  The experimental results showed that the automatic labels of pitch accent exhibited an average of 83. 6% agreement with the hand labels. 
BAYESIAN LEARNING FOR MODELS OF HUMAN SPEECH PERCEPTION| ABSTRACT Humans speech recognition error rates are 30 times lower than machine error rates.  Psychophysical experiments have pinpointed a number of specific human behaviors that may contribute to accurate speech recognition, but previous attempts to incorporate such behaviors into automatic speech recognition have often failed because the resulting models could not be easily trained from data.  This paper describes Bayesian learning methods for computational models of human speech perception.  Specifically, the linked computational models proposed in this paper seek to imitate the following human behaviors: independence of distinctive feature errors, perceptual magnet effect, the vowel sequence illusion, sensitivity to energy onsets and offsets, and redundant use of asynchronous acoustic correlates.  The proposed models differ from many previous computational psychological models in that the desired behavior is learned from data, using a constrained optimization algorithm (the EM algorithm), rather than being coded into the model as a series of fixed rules.  1.  INDEPENDENT FEATURE ERRORS This paper has two goals.  First, this paper seeks to introduce a large variety of recent speech psychology results to the statistical signal processing community.  Second, this paper proposes computational models of the human behaviors evidenced in these recent results.  It is, perhaps, necessary to discuss the reasons why an engineer might be interested in psychology.  There is no a priori need for automatic speech recognizers to imitate the processes of human speech perception: the experience of the past 30 years indicates that simple but trainable mathematical models consistently achieve lower error rates than psychologically motivated but untrainable models.  Despite this progress, however, automatic speech recognition error rates are typically 30 to 300 times worse than human speech recognition error rates [1].  In order to close the gap between human and machine performance, it may be useful to evaluate the performance advantages conferred by specific human speech perceptual behaviors, and to try to imitate the most apparently useful behaviors using trainable machine learning models.  In 1952, Jakobson, Fant and Halle suggested encoding each phoneme as a vector of binary "distinctive features:" voiced vs.  unvoiced, lowpass vs.  highpass, spectrally compact vs.  spectrally diffuse [2].  The idea that a phoneme This work was supported by NSF award number 0132900.  Statements in this paper reflect the opinions and conclusions of the authors, and are not endorsed by the NSF.  can be decomposed into independently manipulable dimensions is quite old: classical Greek, Hebrew, Arabic, and Japanese, for example, mark secondary distinctions such as voicing and aspiration by means of diacritics.  Jakobson's binary notation was important in part because, within three years after Jakobson's paper, Miller and Nicely were able to prove the psychological reality of a nearly binary distinctive feature notation similar to Jakobson's [3].  Miller and Nicely [3] asked listeners to transcribe noisy recordings of consonant-vowel syllables.  Human listeners rarely misunderstand nonsense syllables under quiet listening conditions, but with enough noise, it is possible to get listeners to make mistakes, and the mistakes they make are revealing.  First, some distinctive features are more susceptible to noise than others: place of articulation is reliably communicated only at SNR above -6dB, while sonorancy is reliably communicated even at -12dB SNR.  Second, errors in the perception of distinctive features are approximately independent, in the following sense: given that the true values of the N distinctive features are F = [f1 , .  .  .  , fN ] T , the SNR-dependent probability that a listener will perceive the vector ^ F = [ ^ f1 , .  .  .  , ^ fN ] T is given by p( ^ F |F, SNR) # N Y i=1 p( ^ f i |f i , SNR) (1) Eq.  (1) does not specify the dependence of distinctive feature errors on any particular acoustic signal.  Several authors have suggested an implementation of Eq.  (1) that makes signal-dependence explicit in the following way, where X is the particular acoustic signal used to transmit feature vector F : p( ^ F |X) = N Y i=1 p( ^ f i |X) (2) Eq.  (2) is motivated by training considerations.  Each feature has two possible settings (f i = 1 and f i = - 1), thus the feature vector F has 2 N possible settings.  A classifier trained to represent p( ^ F |X) must distinguish 2 N different labels, while a classifier trained to represent p( ^ f i |X) only distinguishes two labels; the former therefore typically requires 2 N- 1 times as much training data as the latter.  Unfortunately, Eq.  (2) is incorrect in three ways.  First, it is neither a necessary nor sufficient condition for Eq.  (1).  Second, it is suboptimal as an engineering system: a classifier trained to model p( ^ F |X) directly, without factoring as shown in Eq.  (2), usually results in fewer errors than a bank of classifiers trained as in Eq.  (2).  Third, it is not a correct model of human speech perception.  Volaitis and Miller [4], for example, have demonstrated that a voice onset time (VOT) of 40ms is sufficient to turn a synthesized /b/ into /p/, but that /g/ only becomes /k/ when the VOT passes 50ms, i. e.  p(voiced|X, labial) 6= p(voiced|X, velar).  2.  PERCEPTUAL MAGNET EFFECT A somewhat better approximation of Eq.  (1) may be created by assuming that the perceived feature vector ^ F is a deterministic function of the signal X; that is, assume that any given listener will always hear the same sequence of phonemes in response to a given acoustic signal.  Specifically, choose any continuous function G(X) = [g1 (X), .  .  .  , gN (X)] T that specifies the response pattern of listeners by the constraint ^ f i = sgn(g i (X)).  If G(X) is assumed to be a deterministic function, then Eq.  (1) is equivalent to p( ^ F |F, SNR) # N Y i=1 Z ^ f i g i (X)}0 p(X|f i , SNR)dX (3) The function G(X) is, thus far, completely unconstrained, except that ^ f i = sgn(g i (X)) and Eq.  (3) holds.  Given these constraints, it is possible to choose G(X) such that the dimensions of G(X) are conditionally independent, i. e. , Z ^ f i g i (X)}0 p(X|f i , SNR)dX = Z 1 0 p(g i (X)|f i , SNR)dg i (4) where the limits of the right-hand integral are (0, 1) as shown if ^ f i = 1, and (- , 0) if ^ f i = - 1.  By combining Eq.  (3) and (4), a parsimonious speech sound classifier is produced.  The classifier consists of two functions: a class-independent multidimensional transform G(X), and a set of class-dependent scalar PDFs ^ p(g i (X)|f i ).  The task of a human learner, or of a mathematical model of human speech perception, is to learn functions G(X) and ^ p(g i (X)|f i ) that optimally approximate the unknown PDF p(X, F ).  Human learners rely primarily on unsupervised learning [5], but methods of optimal learning are only clearly defined for a supervised learner.  Specifically, suppose that the learner is given M training tokens of the form (Xm , Fm ), 1 # m # M , drawn from the unknown PDF p(X, F ), where Xm is a waveform and Fm = [f1m , .  .  .  , fNm ] T is a vector of distinctive features.  The optimal supervised learning algorithm (in the minimum expected K-L divergence sense) is the algorithm that chooses ^ p(g i (X)|f i ) and g i (X) in order to minimize the empirical cross-entropy, Hemp (^p; p) =M M X m=1 N X i=1 log ^ p(g i (X)|f i ) (5) Cross-entropy measures mismatch between p(X, F ) and the modeled PDF ^ p(g i (X)|f i ), but cross-entropy does not measure the classification performance of the resulting model.  Recall that the classifier output is ^ f i = sgn(g i (X)), thus classification errors occur whenever f i 6= sgn(g i (X)).  Classification error may be improved by subtracting, from the cross-entropy, a regularization term monotonically dependent on the probability of misclassification.  For example, if ^ p(g i |f i ) is modeled using a mixture Gaussian model, ^ p(g i |f i = f) = K X k=1 c ifk N (g i ; ifk , # 2 ifk ) (6) then the simultaneous goals of minimum K-L divergence and minimum classification error may be achieved by maximizing L(^p, G) for some regularization constant #: L(^p, G) = # M X m=1 N X i=1 fimifk # ifk - Hemp (^p; p) (7) L(^p, G) may be maximized using the EM algorithm.  Given #, an initial set of parameters including the parameters of both G(X) and ^ p(g i (X)|f i ), the EM iteration chooses ^ # to maximize Q(#, ^ #) = M X m=1 N X i=1 K X k=1 p(k, g i (Xm )|fim , #) # log p(g i (Xm )|k, fim , ^ #) + # fimifk # ifk # (8) where k is the mixture index selected to model the ith distinctive feature of the mth training token.  Differentiating Q(#, ^ #) with respect to mixture Gaussian parameters results in formulas similar to those in [6].  Differentiating with respect to the parameters of G(X) results in formulas similar to [7].  The acoustic-to-perceptual transform G(X) can be constructed mathematically, but that does not necessarily imply its psychological reality.  In order to determine whether or not the space G(X) corresponds to any psychologically real phenomenon, it is necessary once again to examine the psycholinguistic literature.  The ability of listeners to discriminate two nearly identical synthesized speech waveforms (e. g. , identical except for a 50Hz difference in the second formant) is highest if the two waveforms straddle a phoneme boundary (e. g. , if one waveform is classified as /i/ while the other is classified as /I/).  Kuhl and her colleagues [8] have demonstrated that the phoneme boundary does not need to lie between the two waveforms in order to increase their discriminability: two waveforms that are both classified as /i/, but that are both close to the /i/-/I/ boundary, are more discriminable than are two waveforms that are both close to the center of the /i/ region in acoustic space.  They explain their results by positing a continuous-valued "perceptual space" computed by the listener as a nonlinear transformation of the acoustic space, G(X) = [g1 (X), .  .  .  , gN (X)], such that the magnitude of the Jacobian of the transform is smaller near the center of a phoneme region than it is near the border between phoneme regions [5].  These variations in the value of the Jacobian they term the "perceptual magnet effect. " The proposed perceptual space G(X) is controversial, but continues to serve as an organizing paradigm for new experiments, e. g. , [9].  3.  REDUNDANT ACOUSTIC CORRELATES Listeners do not need to hear all of the acoustic evidence for a distinctive feature in order to correctly recognize the
NON-LINEAR INDEPENDENT COMPONENT ANALYSIS FOR SPEECH RECOGNITION| ABSTRACT This paper addresses the problem of representing the speech signal using a set of features that are approximately statistically independent.  This statistical independence simplifies building probabilistic models based on these features that can be used in applications like speech recognition.  Since there is no evidence that the speech signal is a linear combination of separate factors or sources, we use a more general non-linear transformation of the speech signal to achieve our approximately statistically independent feature set.  We choose the transformation to be symplectic to maximize the likelihood of the generated feature set.  In this paper, we describe applying this nonlinear transformation to the speech time-domain data directly and to the Melfrequency cepstrum coefficients (MFCC).  The features resulted from this transformation are used in phoneme recognition experiments.  The best results achieved show about 2% improvement in recognition accuracy compared to results based on MFCC features. 
PROSODY DEPENDENT SPEECH RECOGNITION WITH EXPLICIT DURATION MODELLING AT INTONATIONAL PHRASE BOUNDARIES| Abstract Does prosody help word recognition? In this paper, we propose a novel probabilistic framework in which word and phoneme are dependent on prosody in a way that improves word recognition.  The prosody attribute that we investigate in this study is the duration lengthening effects of the speech segments in the vicinity of intonational phrase boundaries.  Explicit Duration Hidden Markov Model (EDHMM) is implemented to provide an accurate phoneme duration model.  This study is conducted on Boston University Radio New Corpus with prosodic boundaries marked using ToBI labelling system.  We found that lengthening of the phrase final rhymes can be reliably modelled by EDHMM, which significantly improves the prosody dependent acoustic modelling.  Conversely, no systematic duration variation is found at phrase initial position.  With prosody dependence implemented in acoustic model, pronunciation model and language model, both word recognition accuracy and boundary recognition accuracy are improved by 1% over systems without prosody dependence. 
FORMANT TRACKING BY MIXTURE STATE PARTICLE FILTER| ABSTRACT This paper presents a mixture state particle filter method for formant tracking during both vowels and consonants.  We show that mixture state particle filter model is able to incorporate prior information about phoneme class into the system, which helps the system to find global optimal solutions.  Formant frequencies are defined as eigenfrequencies of the vocal tract in this paper, and by exploring this fact using spectral estimation techniques, the observation PDF of the particle filter can be simplified.  We show that by using this likelihood function in the importance weights, the system is able to track the formants using a small number of particles. 
AN AUTOMATIC PROSODY LABELING SYSTEM USING ANN-BASED SYNTACTIC-PROSODIC MODEL AND GMM-BASED ACOUSTIC-PROSODIC MODEL| ABSTRACT Automatic prosody labeling is important for both speech synthesis and automatic speech understanding.  Humans use both syntactic cues and acoustic cues to develop their prediction of prosody for a given utterance.  This process can be effectively modeled by an ANN-based syntactic-prosodic model that predicts prosody from syntax and a GMM-based acoustic-prosodic model that predicts prosody from acoustic-prosodic observations.  Our experiments on the Radio News Corpus show that ANN is effective in learning the stochastic mapping from the syntactic representation of word strings to prosody labels, with an accuracy of 82. 7% for pitch accent labeling and 90. 5% for intonational phrase boundary (IPB) labeling.  When acoustic observations and reasonably accurate phoneme transcriptions are given, a GMM-based acousticprosodic model, coupled with the syntactial-prosodic model, can achieve 84% pitch accent recognition accuracy and 93% IPB recognition accuracy.  These results are obtained using different speakers for training and testing and have considerably exceeded all previously reported results on the same corpus, especially for the task of IPB detection. 
RECOGNITION OF PROSODIC FACTORS AND DETECTION OF LANDMARKS FOR IMPROVEMENTS TO CONTINUOUS SPEECH RECOGNITION SYSTEMS| This thesis examines the usefulness of including prosodic and phonetic context information in the phoneme model of a speech recognizer.  This is done creating a series of prosodic and phonetic models and then comparing the log likelihoods of each model.  The comparison of log likelihoods shows that both prosodic and phonetic context information improve the phoneme model for most phonemes.  The prosodic and phonetic context information is then modeled in two separate recognizers to show that while phonetic context modeling does provide some useful information for the recognizer, prosody provides much more.  This thesis also experiments with the detection of landmarks, the transitions between two phonemes, using binary classification.  The experiments presented show that landmarks can be detected with high accuracy and are useful for phone classification. 
A FACTORIAL HMM APPROACH TO SIMULTANEOUS RECOGNITION OF ISOLATED DIGITS SPOKEN BY MULTIPLE TALKERS ON ONE AUDIO CHANNEL| ABSTRACT This paper addresses the novel problem of recognizing digits spoken simultaneously by two different talkers.  A Factorial Hidden Markov Model architecture is proposed to accurately model the simultaneous utterance of two digits.  Nadas' MIXMAX approximation is extended to a mixture of Gaussians observation PDF which enables the implementation of the proposed system.  The multiple digit recognizer is found to successfully recognize pairs of simultaneous utterances of digits at 0db SNR with up to 89% accuracy. 
PROSODY AS A CONDITIONING VARIABLE IN SPEECH RECOGNITION| ABSTRACT In this paper, we demonstrate two different methods for improving the accuracy and correctness of the standard HMM speech recognizer.  The first method involves incorporating prosody into the acoustic model.  The second method modifies the HMM so that it can make use of explicit duration probability densities to improve recognition rates.  We prove that both of these methods are effective by running two different experiments using the Hidden Markov Toolkit (HTK).  The first experiment determines the correctness and accuracy of a model by performing word recognition.  The second experiment determines how well a model picks up on the prosody. 
Automatic recognition of pitch movements using timedelay recursive neural network|
The effect of accent on the acoustic cues to stop voicing in Radio News speech|
"Nonlinear maximum likelihood feature transformation for speech recognition,|

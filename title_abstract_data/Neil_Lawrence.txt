A Comparison of State-of-the-Art Classification Techniques with Application to Cytogenetics| by the naive Bayesian classifier and high classification performance represented by the other techniques. 
Probabilistic Modelling of Replica Divergence| Abstract It is common in distributed systems to replicate data.  In many cases this data evolves in a consistent fashion, and this evolution can be modelled.  A probabilistic model of the evolution allows us to estimate the divergence of the replicas and can be used by the application to alter its behaviour, for example to control synchronisation times, to determine the propagation of writes, and to convey to the user information about how much the data may have evolved.  In this paper, we describe how the evolution of the data may be modelled and outline how the probabilistic model may be utilised in various applications, concentrating on a news database example. 
Node Relevance Determination| Abstract Hierarchical Bayesian inference in parameterised models offers an approach for controlling complexity.  In this paper we utilise a novel prior for the leaning of a model's structure.  We call the prior node relevance determination.  It is applicable in a range of models including sigmoid belief networks and Boltzmann machines.  We demonstrate how the approach may be applied to determine structure in a multi-layer perceptron. 
Approximating Posterior Distributions in Belief Networks Using Mixtures| Abstract Exact inference in densely connected Bayesian networks is computationally intractable, and so there is considerable interest in developing effective approximation schemes.  One approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution.  While this leads to a tractable algorithm, the mean field distribution is assumed to be factorial and hence unimodal.  In this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions.  We derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks.  Our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased. 
ACOUSTIC SPACE DIMENSIONALITY SELECTION AND COMBINATION USING THE MAXIMUM ENTROPY PRINCIPLE| ABSTRACT In this paper we propose a discriminative approach to acoustic space dimensionality selection based on maximum entropy modelling.  We form a set of constraints by composing the acoustic space with the space of phone classes, and use a continuous feature formulation of maximum entropy modelling to select an optimal feature set.  The suggested approach has two steps: (1) the selection of the best acoustic space that efficiently and economically represents the acoustic data and its variability; (2) the combination of selected acoustic features in the maximum entropy framework to estimate the posterior probabilities over the phonetic labels given the acoustic input.  Specific contributions of this paper include a parameter estimation algorithm (generalized improved iterative scaling) that enables the use of negative features, the parameterization of constraint functions using Gaussian mixture models, and experimental results using the TIMIT database. 
Variational Bayesian Independent Component Analysis| Abstract Blind separation of signals through the info-max algorithm may be viewed as maximum likelihood learning in a latent variable model.  In this paper we present an alternative approach to maximum likelihood learning in these models, namely Bayesian inference.  It has already been shown how Bayesian inference can be applied to determine latent dimensionality in principal component analysis models (Bishop, 1999a).  In this paper we derive a similar approach for removing unecessary source dimensions in an independent component analysis model.  We present results on a toy data-set and on some artificially mixed images. 
Semi-supervised Learning via Gaussian Processes| Abstract We present a probabilistic approach to learning a Gaussian Process classifier in the presence of unlabeled data.  Our approach involves a #null category noise model# (NCNM) inspired by ordered categorical noise models.  The noise model reflects an assumption that the data density is lower between the class-conditional densities.  We illustrate our approach on a toy problem and present comparative results for the semi-supervised classification of handwritten digits. 
Variational Inference for Visual Tracking| Abstract The likelihood models used in probabilistic visual tracking applications are often complex non-linear and/or nonGaussian functions, leading to analytically intractable inference.  Solutions then require numerical approximation techniques, of which the particle filter is a popular choice.  Particle filters, however, degrade in performance as the dimensionality of the state space increases and the support of the likelihood decreases.  As an alternative to particle filters this paper introduces a variational approximation to the tracking recursion.  The variational inference is intractable in itself, and is combined with an efficient importance sampling procedure to obtain the required estimates.  The algorithm is shown to compare favourably with particle filtering techniques on a synthetic example and two real tracking problems.  The first involves the tracking of a designated object in a video sequence based on its colour properties, whereas the second involves contour extraction in a single image. 
Variational Learning for Multi-Layer Networks of Linear Threshold Units| Abstract Linear threshold units (LTUs) were originally proposed as models of biological neurons.  They were widely studied in the context of the perceptron (Rosenblatt, 1962).  Due to the diculties of finding a general algorithm for networks with hidden nodes, they never passed into general use.  In this work we derive an algorithm in the context of a probabilistic models and show how it may be applied in multi-layer networks of linear threshold units.  We demonstrate the performance of the algorithm on three data-sets. 
Estimating a Kernel Fisher Discriminant in the Presence of Label Noise| Abstract Data noise is present in many machine learning problems domains, some of these are well studied but others have received less attention.  In this paper we propose an algorithm for constructing a kernel Fisher discriminant (KFD) from training examples with noisy labels.  The approach allows to associate with each example a probability of the label being flipped.  We utilise an expectation maximization (EM) algorithm for updating the probabilities.  The E-step uses class conditional probabilities estimated as a by-product of the KFD algorithm.  The M-step updates the flip probabilities and determines the parameters of the discriminant.  We demonstrate the feasibility of the approach on two real-world data-sets. 
Reducing the variability in cDNA microarray image processing by Bayesian inference| Abstract Motivation: Gene expression levels are obtained from microarray experiments through the extraction of pixel intensities from a scanned image of the slide.  It is widely acknowledged that variabilities can occur in expression levels extracted from the same images by different users with the same software packages.  These inconsistencies arise due to differences in the refinement of the placement of the microarray `grids'.  We introduce a novel automated approach to the refinement of grid placements that is based upon the use of Bayesian inference for determining the size, shape and positioning of the microarray `spots', capturing uncertainty that can be passed to downstream analysis.  Results: Our experiments demonstrate that variability between users can be significantly reduced using the approach.  The automated nature of the approach also saves hours of researchers' time normally spent in refining the grid placement.  Availability: A MATLAB implementation of the algorithm and tiff images of the slides used in our experiments, as well as the code necessary to recreate them are available for noncommercial use from
Fast Sparse Gaussian Process Methods: The Informative Vector Machine| Abstract We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning.  Our goal is not only to learn d--sparse predictors (which can be evaluated in O(d) rather than O(n), d # n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements.  The scaling of our method is at most O(n d 2 ), and in large real-world classification experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be significantly faster in training.  In contrast to the SVM, our approximation produces estimates of predictive probabilities (rror bars'), allows for Bayesian model selection and is less complex in implementation. 
Mixture Representations for Inference and Learning in Boltzmann Machines| Abstract Boltzmann machines are undirected graphical models with two-state stochastic variables, in which the logarithms of the clique potentials are quadratic functions of the node states.  They have been widely studied in the neural computing literature, although their practical applicability has been limited by the difficulty of finding an effective learning algorithm.  One well-established approach, known as mean field theory, represents the stochastic distribution using a factorized approximation.  However, the corresponding learning algorithm often fails to find a good solution.  We conjecture that this is due to the implicit uni-modality of the mean field approximation which is therefore unable to capture multi-modality in the true distribution.  In this paper we use variational methods to approximate the stochastic distribution using multi-modal mixtures of factorized distributions.  We present results for both inference and learning to demonstrate the effectiveness of this approach. 
Fast Forward Selection to Speed Up Sparse Gaussian Process Regression| Abstract We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection.  Our method is essentially as fast as an equivalent one which selects the \support" patterns at random, yet it can outperform random selection on hard curve fitting tasks.  More importantly, it leads to a suciently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically.  We demonstrate the model selection capabilities of the algorithm in a range of experiments.  In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods. 
Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data| Abstract In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA).  Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space.  We show that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings.  This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets.  Additionally our non-linear algorithm can be further kernelised leading to `twin kernel PCA' in which a mapping between feature spaces occurs. 
Algorithms and architectures Presentation preference: Oral Not previously submitted elsewhere Fast Sparse Gaussian Process Methods: The Informative Vector Machine| Abstract We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretical principles, previously suggested for active learning.  In contrast to most previous work on sparse GPs, our goal is not only to learn sparse predictors (which can be evaluated in O(d) rather than O(n), d # n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements.  The scaling of our method is at most O(n # d 2 ), and in large real-world classification experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet it requires only a fraction of the training time.  In contrast to the SVM, our approximation produces estimates of predictive probabilities (rror bars'), allows for Bayesian model selection and is less complex in implementation. 
Optimising Synchronisation Times for Mobile Devices| Abstract With the increasing number of users of mobile computing devices (e. g.  personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important.  Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail.  In this paper we explore the question of the optimal strategy for synchronising such replicas.  We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour.  We then formulate objective functions which can be minimised with respect to the synchronisation timings.  We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 
Learning to learn with the informative vector machine| Abstract This paper describes an efficient method for learning the parameters of a Gaussian process (GP).  The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior.  An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case.  The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks.  The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task. 
A Hybrid MaxEnt/HMM based ASR System| Abstract The aim of this work is to develop a practical framework, which extends the classical Hidden Markov Models (HMM) for continuous speech recognition based on the Maximum Entropy (MaxEnt) principle.  The MaxEnt models can estimate the posterior probabilities directly as with Hybrid NN/HMM connectionist speech recognition systems.  In particular, a new acoustic modelling based on discriminative MaxEnt models is formulated and is being developed to replace the generative Gaussian Mixture Models (GMM) commonly used to model acoustic variability.  Initial experimental results using the TIMIT phone task are reported. 
A sparse Bayesian compression scheme - the informative vector machine|
A variational Bayesian committee of neural networks|
Variational inference in probabilistic models|
The role of pitx3 in mouse eye development dissected using microarray analysis|
Gaussian process models for visualization of high dimensional data|
Sparse Bayesian learning: The informative vector machine|
Sparse representation for Gaussian process models|

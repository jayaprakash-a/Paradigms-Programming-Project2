Transferring Prior Information Between Models Using Imaginary Data| Abstract.  Bayesian modeling is limited by our ability to formulate prior distributions that adequately represent our actual prior beliefs --- a task that is especially difficult for realistic models with many interacting parameters.  I show here how a prior distribution formulated for a simpler, more easily understood model can be used to modify the prior distribution of a more complex model.  This is done by generating imaginary data from the simpler "donor" model, which is conditioned on in the more complex "recipient" model, effectively transferring the donor model's well-specified prior information to the recipient model.  Such prior information transfers are also useful when comparing two complex models for the same data.  Bayesian model comparison based on the Bayes factor is very sensitive to the prior distributions for each model's parameters, with the result that the wrong model may be favoured simply because the prior for the right model was not carefully formulated.  This problem can be alleviated by modifying each model's prior to potentially incorporate prior information transferred from the other model.  I discuss how these techniques can be implemented by simple Monte Carlo and by Markov chain Monte Carlo with annealed importance sampling.  Demonstrations on models for two-way contingency tables and on graphical models for categorical data show that prior information transfer can indeed overcome deficiencies in prior specification for complex models. 
Technical Report No| 9805, Department of Statistics, University of Toronto Annealed Importance Sampling.  Abstract.  Simulated annealing --- moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions --- has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers.  Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler.  The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases.  This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling.  It is also related to thermodynamic integration methods for estimating ratios of normalizing constants.  Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers. 
Near Shannon Limit Performance of Low Density Parity Check Codes| A linear code may be described in terms of a generator matrix G or in terms of a parity check matrix H, which satisfies Hx = 0 for all codewords x.  In 1962, Gallager reported work on binary codes defined in terms of low density parity check matrices (abbreviated `GL codes') [5, 6].  The matrix H was defined in a non-systematic form; each column of H had a small weight (e. g. , 3) and the weight per row was also uniform; the matrix H was constructed at random subject to these constraints.  Gallager proved distance properties of these codes and described a probability-based decoding algorithm with promising empirical performance.  However it appears that GL codes have been generally forgotten, the assumption perhaps being that concatenated codes [4] were superior for practical purposes (R. G.  Gallager, personal communication).  During our work on MN codes [8] we realised that it is possible to create `good' codes from very sparse random matrices, and to decode them (even beyond their minimum distance) using approximate probabilistic algorithms.  We eventually reinvented Gallager's decoding algorithm and GL codes.  In this paper we report the empirical performance of these codes on Gaussian channels.  We have proved theoretical properties of GL codes (essentially, that the channel coding theorem holds for them) elsewhere [9].  GL codes can also be defined over GF (q).  We are currently implementing this generalization.  We created sparse random parity check matrices in the following ways.  Construction 1A.  An M by N matrix (M rows, N columns) is created at random with weight per column t (e. g. , t = 3), and weight per row as uniform as possible, and overlap between any two columns no greater than 1.  (The weight of a column is the number of non-zero elements; the overlap between two columns is their inner product. ) Construction 2A.  Up to M/2 of the columns are designated weight 2 columns, and these are constructed such that there is zero overlap between any pair of columns.  The remaining columns are made at random with weight 3, with the weight per row as uniform as possible, and overlap between any two columns of the entire matrix no greater than 1.  Constructions 1B and 2B.  A small number of columns are deleted from a matrix produced by constructions 1A and 2A, respectively, so that the bipartite graph corresponding to the matrix has no short cycles of length less than some length l.  The above constructions do not ensure that all the rows of the matrix are linearly independent, so the M N matrix created is the parity matrix of a linear code with rate at least R # K/N , where K = N - M .  We report results on the assumption that the rate is R.  The generator matrix of the code can be created by Gaussian elimination.  We simulated a Gaussian channel with binary input a and additive noise of variance # 2 = 1.  If one communicates using a code of rate R then it is conventional to describe the signal to noise ratio by E b N0 = a 2 2R# 2 and to report this number in decibels as 10 log 10 E b /N 0 .  Decoding.  The decoding problem is to find the most probable vector x such that Hxmod 2 = 0, with the likelihood of x given by Q n f xn n where f 1 n = 1/(1 + exp(- 2ay n /# 2 )) and f 0 n = 1 - f 1 n , and y n is the channel's output at time n.  Gallager's algorithm (reviewed in detail in [9]) may be viewed as an approximate belief propagation algorithm [10].  (The Turbo decoding algorithm may also be viewed as a belief propagation algorithm (R. J. McEliece and D. J. C. MacKay, unpublished). ) We refer to the elements of x as bits and to the rows of H as checks.  We denote the set of bits n that participate in check m by N (m) # {n : Hmn = 1}.  Similarly we define the set of checks in which bit n participates, M(n) # {m : Hmn = 1}.  We denote a set N (m) with bit n excluded by N (m)\n.  The algorithm has two alternating parts, in which quantities q mn and r mn associated with each non-zero element in the H matrix are iteratively updated.  The quantity q x mn is meant to be the probability that bit n of x is x, given the information obtained via checks other than check m.  The quantity r x mn is meant to be the probability of check m being satisfied if bit n of x is considered fixed at x and the other bits have a separable distribution given by the probabilities {q mn 0 : n 0 2 N (m)\n}.  The algorithm would produce the exact posterior probabilities of all the bits if the bipartite graph defined by the matrix H contained no cycles [10].  Initialization.  The variables q 0 mn and q 1 mn are initialized to the values f 0 n and f 1 n respectively.  Horizontal step.  We define #q mn # q 0 mn- q 1 mn and compute for each m, n: fir mn = Y n 0 2N (m)\n #q mn 0 (1) then set r 0 mn = 1 2 (1 + fir mn ) and r 1 mn = 1 2 (1- fir mn ).  Vertical step.  For each n and m and for x = 0, 1 we update: q x mn = #mn f x n Y m 0 2M(n)\m r x m 0 n (2) where #mn is chosen such that q 0 mn +q 1 mn = 1.  We can also update the `pseudoposterior probabilities' q 0 n and q 1 n , given by: q x n = # n f x n Y m2M(n) r x mn . 
Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo Method| Abstract.  It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the "Hybrid Monte Carlo" method.  This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian.  In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space.  The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions.  Appropriate weight scaling factors are found automatically.  By applying known techniques for calculation of "free energy" differences, it should also be possible to compare the merits of different network architectures.  The work described here should also be applicable to a wide variety of statistical models other than neural networks. 
Defining Priors for Distributions Using Dirichlet Diffusion Trees| Abstract.  I introduce a family of prior distributions over univariate or multivariate distributions, based on the use of a "Dirichlet diffusion tree" to generate exchangeable data sets.  These priors can be viewed as generalizations of Dirichlet processes and of Dirichlet process mixtures.  They are potentially of general use for modeling unknown distributions, either of observed data or of latent values.  Unlike simple mixture models, Dirichlet diffusion tree priors can capture the hierarchical structure that is present in many distributions.  Depending on the "divergence function" employed, a Dirichlet diffusion tree prior can produce discrete or continuous distributions.  Empirical evidence is presented that some divergence functions produce distributions that are absolutely continuous, while others produce distributions that are continuous but not absolutely continuous.  Although Dirichlet diffusion trees are defined in terms of a continuous-time stochastic process, inference for finite data sets can be expressed in terms of finite-dimensional quantities, which should allow computations to be performed by reasonably efficient Markov chain Monte Carlo methods. 
Likelihood--based Boosting| Abstract We present a probabilistic interpretation of `boosting' in terms of a mixture of experts model, and give a boosting algorithm that is a crippled maximum likelihood algorithm.  Boosting is a method for training multiple classifiers on a dataset and combining their outputs to make predictions for new cases (
Monte Carlo Inference for Belief Networks Using Coupling From the Past| 1999 A common method of inference for belief networks is Gibbs sampling, in which a Markov chain converging to the desired distribution is simulated.  In practice, however, the distribution obtained with Gibbs sampling differs from the desired distribution by an unknown error, since the simulation time is finite.  Coupling from the past selects states from exactly the desired distribution by starting chains in every state at a time far enough back in the past that they reach the same state at time t = 0.  To track every chain is an intractable procedure for large state spaces.  The method proposed in this thesis uses a summary chain to approximate the set of chains.  Transitions of the summary chain are efficient for noisy-or belief networks, provided that sibling variables of the network are not directly connected, but often require more simulation time steps than would be needed if chains were tracked exactly.  Testing shows that the method is a potential alternative to ordinary Gibbs sampling, especially for networks that have poor Gibbs sampling convergence, and when the user has a low error tolerance. 
Assessing Relevance Determination Methods Using DELVE| Abstract.  Empirically assessing the predictive performance of learning methods is an essential component of research in machine learning.  The DELVE environment was developed to support such assessments.  It provides a collection of datasets, a standard approach to conducting experiments with these datasets, and software for the statistical analysis of experimental results.  In this paper, DELVE is used to assess the performance of neural network methods when the inputs available to the network have varying degrees of relevance.  The results confirm that the Bayesian method of "Automatic Relevance Determination" (ARD) is often (but not always) helpful, and show that a variation on "early stopping" inspired by ARD is also beneficial.  The experiments also reveal some other interesting characteristics of the methods tested.  This example illustrates the essential role of empirical testing, and shows the strengths and weaknesses of the DELVE environment. 
Multiple Alignment of Continuous Time Series| Abstract Multiple realizations of continuous-valued time series from a stochastic process often contain systematic variations in rate and amplitude.  To leverage the information contained in such noisy replicate sets, we need to align them in an appropriate way (for example, to allow the data to be properly combined by adaptive averaging).  We present the Continuous Profile Model (CPM), a generative model in which each observed time series is a non-uniformly subsampled version of a single latent trace, to which local rescaling and additive noise are applied.  After unsupervised training, the learned trace represents a canonical, high resolution fusion of all the replicates.  As well, an alignment in time and scale of each observation to this trace can be found by inference in the model.  We apply CPM to successfully align speech signals from multiple speakers and sets of Liquid Chromatography-Mass Spectrometry proteomic data. 
The wake-sleep algorithm for unsupervised neural networks| Abstract We describe an unsupervised learning algorithm for a multilayer network of stochastic neurons.  Bottom-up "recognition" connections convert the input into representations in successive hidden layers and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above.  In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below.  In the "sleep" phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.  Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections.  The wake-sleep algorithm finesses both these problems.  When there is no teaching signal to be matched, some other goal is required to force the hidden units to extract underlying structure.  In the wake-sleep algorithm the goal is to learn representations that are economical to describe but allow the input to be reconstructed accurately.  Each input vector could be communicated to a receiver by first sending its hidden representation and then sending the difference between the input vector and its top-down reconstruction from the hidden representation.  The aim of learning is to minimize the "description length" which is the total number of bits that would be required to communicate the input vectors in this way [1].  No communication actually takes place, but minimizing the description length that would be required forces the network to learn economical representations that capture the underlying regularities in the data [2].  The neural network has two quite different sets of connections.  The bottom-up "recognition" connections are used to convert the input vector into a representation in one or more layers of hidden units.  The top-down "generative" connections are then used to reconstruct an approximation to the input vector from its underlying representation.  The training algorithm for these two sets of connections can be used with many different types of stochastic neuron, but for simplicity we use only stochastic binary units that have states of or # .  The state of unit # is ### and the probability that it is on is: ### # ### fiff # 25510 - ##### # ##### # ##### # # (1) where # # is the bias of the unit and # # # is the weight on a connection from unit # .  Sometimes the units are driven by the generative weights and other times by the recognition weights, but the same equation is used in both cases.  In the "wake" phase the units are driven bottom-up using the recognition weights, producing a representation of the input vector in the first hidden layer, a representation of this representation in the second hidden layer and so on.  All of these layers of representation combined are called the "total representation" of the input, and the binary state of each hidden unit, , in total representation ! is fiff" # .  This total representation could be used to communicate the input vector, $ , to a receiver.  According to Shannon's coding theorem, it requires #&%('*),+ bits to communicate an event that has probability + under a distribution agreed by the sender and receiver.  We assume that the receiver knows the top-down generative weights [3] so these can be used to create the agreed probability distributions required for communication.  First, the activity of each unit, - , in the top hidden layer is communicated using the distribution #. # "/10 ### "/ # which is obtained by applying Eq.  1 to the single generative bias weight of unit - .  Then the activities of the units in each lower layer are communicated using the distribution #(# " # 0 #2# " # # obtained by applying Eq.  1 to the already communicated activities in the layer above, # " / , and the generative weights, # / # .  The description length of the binary state of unit is: 3 # # " # # # # # " # %('*)4# " # #5# # # " # # %('*)6# #7# " # # (2) The description length for input vector $ using the total representation ! is simply the cost of describing all the hidden states in all the hidden layers plus the cost of describing the input vector given the hidden states 3 # ! 0 $ # # 3 # ! #8# 3 # $#9 ! # #;:<}=@?A: # =B< 3 # # " # #8# :DC 3 # #DE C 9F! # (3)
Density Modeling and Clustering Using Dirichlet Diffusion Trees| SUMMARY I introduce a family of prior distributions over multivariate distributions, based on the use of a "Dirichlet diffusion tree" to generate exchangeable data sets.  These priors can be viewed as generalizations of Dirichlet processes and of Dirichlet process mixtures, but unlike simple mixtures, they can capture the hierarchical structure present in many distributions, by means of the latent diffusion tree underlying the data.  This latent tree also provides a hierarchical clustering of the data, which, unlike ad hoc clustering methods, comes with probabilistic indications of uncertainty.  The relevance of each variable to the clustering can also be determined.  Although Dirichlet diffusion trees are defined in terms of a continuoustime process, posterior inference involves only finite-dimensional quantities, allowing computation to be performed by reasonably efficient Markov chain Monte Carlo methods.  The methods are demonstrated on problems of modeling a two-dimensional density and of clustering gene expression data. 
Sampling from Multimodal Distributions Using Tempered Transitions| Abstract.  I present a new Markov chain sampling method appropriate for distributions with isolated modes.  Like the recently-developed method of "simulated tempering", the ``tempered transition" method uses a series of distributions that interpolate between the distribution of interest and a distribution for which sampling is easier.  The new method has the advantage that it does not require approximate values for the normalizing constants of these distributions, which are needed for simulated tempering, and can be tedious to estimate.  Simulated tempering performs a random walk along the series of distributions used.  In contrast, the tempered transitions of the new method move systematically from the desired distribution, to the easily-sampled distribution, and back to the desired distribution.  This systematic movement avoids the inefficiency of a random walk, an advantage that unfortunately is cancelled by an increase in the number of interpolating distributions required.  Because of this, the sampling efficiency of the tempered transition method in simple problems is similar to that of simulated tempering.  On more complex distributions, however, simulated tempering and tempered transitions may perform differently.  Which is better depends on the ways in which the interpolating distributions are ``deceptive". 
On Deducing Conditional Independence from d-Separation in Causal Graphs with Feedback| Abstract Pearl and Dechter (1996) claimed that the
A Split-Merge Markov Chain Monte Carlo Procedure for the Dirichlet Process Mixture Model| Abstract.  We propose a split-merge Markov chain algorithm to address the problem of inefficient sampling for conjugate Dirichlet process mixture models.  Traditional Markov chain Monte Carlo methods for Bayesian mixture models, such as Gibbs sampling, can become trapped in isolated modes corresponding to an inappropriate clustering of data points.  This article describes a Metropolis-Hastings procedure that can escape such local modes by splitting or merging mixture components.  Our Metropolis-Hastings algorithm employs a new technique in which an appropriate proposal for splitting or merging components is obtained by using a restricted Gibbs sampling scan.  We demonstrate empirically that our method outperforms the Gibbs sampler in situations where two or more components are similar in structure. 
Markov Chain Sampling Methods for Dirichlet Process Mixture Models| Abstract.  Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model are reviewed, and two new classes of methods are presented.  One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling.  The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters.  These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors. 
Bayesian Learning via Stochastic Dynamics| Abstract The attempt to find a single "optimal" weight vector in conventional network training can lead to overfitting and poor generalization.  Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data.  This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution.  1 CONVENTIONAL AND BAYESIAN LEARNING I view neural networks as probabilistic models, and learning as statistical inference.  Conventional network learning finds a single "optimal" set of network parameter values, corresponding to maximum likelihood or maximum penalized likelihood inference.  Bayesian inference instead integrates the predictions of the network over all possible values of the network parameters, weighting each parameter set by its posterior probability in light of the training data.  1. 1 NEURAL NETWORKS AS PROBABILISTIC MODELS Consider a network taking a vector of real-valued inputs, x, and producing a vector of real-valued outputs, y, perhaps computed using hidden units.  Such a network architecture corresponds to a function, f , with y = f(x; w), where w is a vector of connection weights.  If we assume the observed outputs, y, are equal to y plus Gaussian noise of standard deviation oe, the network defines the conditional probability for an observed output vector given an input vector as follows: P (y j x; oe) / exp\Gamma \Gamma jy \Gamma f(x; w)j 2 ffi 2oe 2 \Delta (1) The probability of the outputs in a training set (x 1 ; y 1 ); .  .  .  ; (xn ; yn ) given this fixed noise level is therefore P (y 1 ; .  .  .  ; yn j x 1 ; .  .  .  ; xn ; oe) / exp\Gamma \Gamma P c jy c \Gamma f(x c ; w)j 2 ffi 2oe 2 \Delta (2) Often oe is unknown.  A Bayesian approach to handling this is to assign oe a vague prior distribution and then integrating it away, giving the following probability for the training set (see (Buntine and Weigend, 1991) or (Neal, 1992) for details):
Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification| Abstract.  Gaussian processes are a natural way of defining prior distributions over functions of one or more input variables.  In a simple nonparametric regression problem, where such a function gives the mean of a Gaussian distribution for an observed response, a Gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases.  Hyperparameters that define the covariance function of the Gaussian process can be sampled using Markov chain methods.  Regression models where the noise has a t distribution and logistic or probit models for classification applications can be implemented by sampling as well for latent values underlying the observations.  Software is now available that implements these methods using covariance functions with hierarchical parameterizations.  Models defined in this way can discover high-level properties of the data, such as which inputs are relevant to predicting the response. 
Arithmetic Coding Revisited| Over the last decade, arithmetic coding has emerged as an important compression tool.  It is now the method of choice for adaptive coding on multisymbol alphabets because of its speed, low storage requirements, and effectiveness of compression.  This article describes a new implementation of arithmetic coding that incorporates several improvements over a widely used earlier version by Witten, Neal, and Cleary, which has become a de facto standard.  These improvements include fewer multiplicative operations, greatly extended range of alphabet sizes and symbol probabilities, and the use of low-precision arithmetic, permitting implementation by fast shift/add operations.  We also describe a modular structure that separates the coding, modeling, and probability estimation components of a compression system.  To motivate the improved coder, we consider the needs of a word-based text compression program.  We report a range of experimental results using this and other models.  Complete source code is available. 
Improving Asymptotic Variance of MCMC Estimators: Non-reversible Chains are Better| Abstract.  I show how any reversible Markov chain on a finite state space that is irreducible, and hence suitable for estimating expectations with respect to its invariant distribution, can be used to construct a non-reversible Markov chain on a related state space that can also be used to estimate these expectations, with asymptotic variance at least as small as that using the reversible chain (typically smaller).  The non-reversible chain achieves this improvement by avoiding (to the extent possible) transitions that backtrack to the state from which the chain just came.  The proof that this modification cannot increase the asymptotic variance of an MCMC estimator uses a new technique that can also be used to prove Peskun's (1973) theorem that modifying a reversible chain to reduce the probability of staying in the same state cannot increase asymptotic variance.  A non-reversible chain that avoids backtracking will often take little or no more computation time per transition than the original reversible chain, and can sometime produce a large reduction in asymptotic variance, though for other chains the improvement is slight.  In addition to being of some practical interest, this construction demonstrates that non-reversible chains have a fundamental advantage over reversible chains for MCMC estimation.  Research into better MCMC methods may therefore best be focused on non-reversible chains. 
Splitting and Merging Components of a Nonconjugate Dirichlet Process Mixture Model| Abstract.  The inferential problem of associating data to mixture components is dicult when components are nearby or overlapping.  We introduce a new split-merge Markov chain Monte Carlo technique that eciently classifies observations by splitting and merging mixture components of a nonconjugate Dirichlet process mixture model.  Our method, which is a Metropolis-Hastings procedure with split-merge proposals, samples clusters of observations simultaneously rather than incrementally assigning observations to mixture components.  Split-merge moves are produced by exploiting properties of a restricted Gibbs sampling scan.  A simulation study compares the new split-merge technique to a nonconjugate version of Gibbs sampling and an incremental MetropolisHastings technique.  The results demonstrate the improved performance of the new sampler.  We illustrate the utility of our technique as an unsupervised clustering method using real data. 
Markov Chain Monte Carlo Methods Based on `Slicing' the Density Function| Abstract.  One way to sample from a distribution is to sample uniformly from the region under the plot of its density function.  A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal `slice' defined by the current vertical position.  Variations on such `slice sampling' methods can easily be implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn.  This approach is often easier to implement than Gibbs sampling, and may be more efficient than easily-constructed versions of the Metropolis algorithm.  Slice sampling is therefore attractive in routine Markov chain Monte Carlo applications, and for use by software that automatically generates a Markov chain sampler from a model specification.  One can also easily devise overrelaxed versions of slice sampling, which sometimes greatly improve sampling efficiency by suppressing random walk behaviour.  Random walks can also be avoided in some slice sampling schemes that simultaneously update all variables. 
Circularly-Coupled Markov Chain Sampling| Abstract.  I show how to run an N-time-step Markov chain simulation in a circular fashion, so that the state at time 0 follows the state at time N\Gamma 1 in the same way as states at times t follow those at times t \Gamma 1 for 0 ! t !N .  This wrap-around of the chain is achieved using a coupling procedure, and produces states that all have close to the equilibrium distribution of the Markov chain, under the assumption that coupled chains are likely to coalesce in less than N=2 iterations.  This procedure therefore automatically eliminates the initial portion of the chain that would otherwise need to be discarded to get good estimates of equilibrium averages.  The assumption of rapid coalescence can be tested using auxiliary chains started at times spaced between 0 and N .  When multiple processors are available, such auxiliary chains can be simulated in parallel, and pieced together to give the circularly-coupled chain, in less time than a sequential simulation would have taken, provided that coalescence is indeed rapid.  The practical utility of these procedures is dependent on the development of good coupling schemes, but in contrast to exact sampling techniques such as coupling from the past, there is no need to also devise a way of keeping track of large sets of states.  On the other hand, although the assumptions behind circular coupling can be tested empirically, the results will not provide an absolute guarantee that the points obtained are from the equilibrium distribution. 
SUPPRESSING RANDOM WALKS IN MARKOV CHAIN MONTE CARLO USING ORDERED OVERRELAXATION| Abstract.  Markov chain Monte Carlo methods such as Gibbs sampling and simple forms of the Metropolis algorithm typically move about the distribution being sampled via a random walk.  For the complex, high-dimensional distributions commonly encountered in Bayesian inference and statistical physics, the distance moved in each iteration of these algorithms will usually be small, because it is difficult or impossible to transform the problem to eliminate dependencies between variables.  The inefficiency inherent in taking such small steps is greatly exacerbated when the algorithm operates via a random walk, as in such a case moving to a point n steps away will typically take around n 2 iterations.  Such random walks can sometimes be suppressed using "overrelaxed" variants of Gibbs sampling (a. k. a.  the heatbath algorithm), but such methods have hitherto been largely restricted to problems where all the full conditional distributions are Gaussian.  I present an overrelaxed Markov chain Monte Carlo algorithm based on order statistics that is more widely applicable.  In particular, the algorithm can be applied whenever the full conditional distributions are such that their cumulative distribution functions and inverse cumulative distribution functions can be efficiently computed.  The method is demonstrated on an inference problem for a simple hierarchical Bayesian model. 
The Helmholtz machine| Abstract Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning.  One fruitful approach is to build a parameterised stochastic generative model, independent draws from which are likely to produce the patterns.  For all but the simplest generative models, each pattern can be generated in exponentially many ways.  It is thus intractable to adjust the parameters to maximize the probability of the observed patterns, We describe a way of finessing this combinatorial explosion by maximising an easily computed lower bound on the probability of the observations.  Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways. 
Markov Chain Monte Carlo in Practice: A Roundtable Discussion| Markov chain Monte Carlo (MCMC) methods make possible the use of flexible Bayesian models that would otherwise be computationally infeasible.  In recent years, a great variety of such applications have been described in the literature.  Applied statisticians who are new to these methods may have several questions and concerns, however: How much effort and expertise are needed to design and use a Markov chain sampler? How much confidence can one have in the answers that MCMC produces? How does the use of MCMC affect the rest of the model-building process? At the Joint Statistical Meetings in August, 1996, a panel of experienced MCMC users discussed these and other issues, as well as various "tricks of the trade. " This article is an edited recreation of that discussion.  Its purpose is to offer advice and guidance to novice users of MCMC---and to notso-novice users as well.  Topics include building confidence in simulation results, methods for speeding and assessing convergence, estimating standard errors, identification of models for which good MCMC algorithms exist, and the current state of software development. 
Bayesian Mixture Modeling by Monte Carlo Simulation| Abstract.  It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation.  This method exhibits the true Bayesian predictive distribution, implicitly integrating over the entire underlying parameter space.  An infinite number of mixture components can be accommodated without difficulty, using a prior distribution for mixing proportions that selects a reasonable subset of components to explain any finite training set.  The need to decide on a "correct" number of components is thereby avoided.  The feasibility of the method is shown empirically for a simple classification task. 
Taking Bigger Metropolis Steps by Dragging Fast Variables| Abstract.  I show how Markov chain sampling with the Metropolis-Hasting algorithm can be modified so as to take bigger steps when the distribution being sampled from has the characteristic that its density can be quickly recomputed for a new point if this point differs from a previous point only with respect to a subset of "fast" variables.  I show empirically that when using this method, the efficiency of sampling for the remaining "slow" variables can approach what would be possible using Metropolis updates based on the marginal distribution for the slow variables. 
Factor Analysis Using Delta-Rule Wake-Sleep Learning| We describe a linear network that models correlations between real-valued visible variables using one or more real-valued hidden variables --- a factor analysis model.  This model can be seen as a linear version of the "Helmholtz machine", and its parameters can be learned using the "wake-sleep" method, in which learning of the primary "generative" model is assisted by a "recognition" model, whose role is to fill in the values of hidden variables based on the values of visible variables.  The generative and recognition models are jointly learned in "wake" and "sleep" phases, using just the delta rule.  This learning procedure is comparable in simplicity to Oja's version of Hebbian learning, which produces a somewhat different representation of correlations in terms of principal components.  We argue that the simplicity of wake-sleep learning makes factor analysis a plausible alternative to Hebbian learning as a model of activity-dependent cortical plasticity. 
Priors for Infinite Networks| Abstract Bayesian inference begins with a prior distribution for model parameters that is meant to capture prior beliefs about the relationship being modeled.  For multilayer perceptron networks, where the parameters are the connection weights, the prior lacks any direct meaning --- what matters is the prior over functions computed by the network that is implied by this prior over weights.  In this paper, I show that priors over weights can be defined in such a way that the corresponding priors over functions reach reasonable limits as the number of hidden units in the network goes to infinity.  When using such priors, there is thus no need to limit the size of the network in order to avoid "overfitting".  The infinite network limit also provides insight into the properties of different priors.  A Gaussian prior for hiddento-output weights results in a Gaussian process prior for functions, which can be smooth, Brownian, or fractional Brownian, depending on the hidden unit activation function and the prior for input-to-hidden weights.  Quite different effects can be obtained using priors based on non-Gaussian stable distributions.  In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting. 
Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models| Abstract We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations.  This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM).  An update begins with the creation of "pools" of candidate states at each time.  We then define an embedded HMM whose states are indexes within these pools.  Using a forward-backward dynamic programming algorithm, we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools.  We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error.  We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 
Probabilistic inference using Markov chain Monte Carlo methods|
Connectionist Learning of Belief Networks|
Arithmetic Coding for Data Compression|
The wake-sleep algorithm for self-organizing neural networks|
Markov chain sampling for non-linear state space models using embedded hidden Markov models",|
Good codes based on very sparse matrices|
MCMC in Practice: A Roundtable Discussion|
An improved acceptance procedure for the hybrid Monte Carlo algorithm",|
Bayesian Learning via Stochastic Dynamics|
Source code for arithmetic coding,|
Annealed importance sampling|
Zemel RS 1995 The Helmholtz machine Neural Comput|
Using Peano curves for bilevel display of continuous-tone images"|
Bayesian Learning for Neural Networks|
Probabilistic inference for artificial intelligence using Monte Carlo methods based on Markov chains,|
Corrigendum: Adaptive rejection Metropolis sampling",|
"Delve,"|
Near Shannon Limit Performance of Low Density Parity Check Codes,|
Good codes based on very sparse matrices|" Submitted to. 
M and Zemel R S 1995 The helmholtz machine Neural Computation 7|
Bayesian mixture modelling by Monte Carlo simulation, Technical Report|
Bayesian mixture modelling|
Asymmetric parallel boltzmann machines are belief networks|
Helmholtz machines|
Automatic relevance determination for neural networks|
Bayesian training of backpropagation networks,|
Assessing learning procedures using DELVE|
The DELVE manual|
Probabilistic Inference Using Markov Chain Monte Carlo Methods," technical report|
Arithmetic coding|
Fast arithmetic coding using lowprecision division|
Erroneous results in "marginal likelihood from the Gibbs output"|
Learning stochastic feedforward networks, University of Toronto, Department of Computer Science, Connectionist Research Group,|
Connectionist learning of bayesian networks|
Sampling from multimodal distribtutions using tempered transitions|
Annealed inportance sampling|
Good Error-Correction Codes Based on Very Sparse Matrices,|
The DELVE Manual| DELVE can be found at. 
Probabilistic inference using Markov chain Monte Carlo Methods, University of Toronto Technical Report 1993|
Multiple alignment of continuous time series,|
Error correcting codes using free energy minimization,|
Authors' response to "Compress and Compact discussed further"|

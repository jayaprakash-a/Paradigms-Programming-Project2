Convergence Properties of Some Spike-Triggered Analysis Techniques| Abstract We analyze the convergence properties of three spike-triggered data analysis techniques.  All of our results are obtained in the setting of a (possibly multidimensional) linear-nonlinear (LN) cascade model for stimulus-driven neural activity.  We start by giving exact rate of convergence results for the common spike-triggered average (STA) technique.  Next, we analyze a spike-triggered covariance method, variants of which have been recently exploited successfully by Bialek, Simoncelli, and colleagues.  These first two methods suffer from extraneous conditions on their convergence; therefore, we introduce an estimator for the LN model parameters which is designed to be consistent under general conditions.  We provide an algorithm for the computation of this estimator and derive its rate of convergence.  We close with a brief discussion of the efficiency of these estimators and an application to data recorded from the primary motor cortex of awake, behaving primates. 
# This work was supported by NIH| Abstract Neuroscientists have long been interested in how e$ iently we solve probabilisti sensory problems.  In order to explore analogous questions in the motor domain, we observed the eye movements of human subje ts attempting to tra k a visual target whi h moved sto hasti ally a ross a omputer s reen.  The subje ts' behavior was then ompared to a mathemati allyderived bound on the best performan e possible in su h a task.  The subje ts were able to perform surprisingly near the optimum under the onditions examined.  These results onstitute an important step in determining the e$ ien y of the nervous system in the ontext of ongoing behavior. 
Variational minimax estimation of discrete distributions under KL loss| Abstract We develop a family of upper and lower bounds on the worst-case expected KL loss for estimating a discrete distribution on a finite number m of points, given N i. i. d.  samples.  Our upper bounds are approximationtheoretic, similar to recent bounds for estimating discrete entropy; the lower bounds are based on Bayesian averages of the KL loss under Dirichlet distributions.  The upper bounds are convex in their parameters and thus can be minimized by descent methods to provide estimators with low worst-case error; the lower bounds are indexed by a one-dimensional parameter and are thus easily maximized.  Asymptotic analysis of the bounds demonstrates the uniform KL-consistency of a wide class of estimators as c = N/m ! 1 (no matter how slowly), and shows that no estimator is consistent for c bounded (in contrast to entropy estimation).  Moreover, the bounds are asymptotically tight as c ! 0 or 1, and are shown numerically to be tight within a factor of two for all c.  Finally, in the sparse-data limit c ! 0, we find that the Dirichlet-Bayes (add-constant) estimator with parameter scaling like - c log(c) optimizes both the upper and lower bounds, suggesting an optimal choice of the "add-constant" parameter in this regime. 
The most likely voltage path and large deviations approximations for integrate-and-fire neurons| Abstract We develop theory and numerical methods for computing the most likely subthreshold voltage path of a noisy integrate-and-fire (IF) neuron, given observations of the neuron's superthreshold spiking activity.  This optimal voltage path satisfies a second-order ordinary differential (Euler-Lagrange) equation which may be solved analytically in a number of special cases, and which may be solved numerically in general via a simple "shooting" algorithm.  Our results are applicable for both linear and nonlinear subthreshold dynamics, and in certain cases may be extended to correlated subthreshold noise sources.  We also show how this optimal voltage may be used to: 1) help sample efficiently from the conditional subthreshold noise distribution; 2) obtain approximations to the likelihood that an IF cell with a given set of parameters was responsible for the observed spike train; and 3) obtain approximations to the instantaneous firing rate and interspike interval distribution of a given noisy IF cell.  The latter probability approximations are based on the classical Freidlin-Wentzell theory of large deviations principles for stochastic differential equations.  We close by comparing this most likely voltage path to the true observed 1 subthreshold voltage trace in a case when intracellular voltage recordings are available in vitro. 
Behavioral/Systems/Cognitive Superlinear Population Encoding of Dynamic Hand Trajectory in Primary Motor Cortex| Neural activity in primary motor cortex (MI) is known to correlate with hand position and velocity.  Previous descriptions of this tuning have (1) been linear in position or velocity, (2) depended only instantaneously on these signals, and/or (3) not incorporated the effects of interneuronal dependencies on firing rate.  We show here that many MI cells encode a superlinear function of the full time-varying hand trajectory.  Approximately 20% of MI cells carry information in the hand trajectory beyond just the position, velocity, and acceleration at a single time lag.  Moreover, approximately one-third of MI cells encode the trajectory in a significantly superlinear manner; as one consequence, even small position changes can dramatically modulate the gain of the velocity tuning of MI cells, in agreement with recent psychophysical evidence.  We introduce a compact nonlinear "preferred trajectory" model that predicts the complex structure of the spatiotemporal tuning functions described in previous work.  Finally, observing the activity of neighboring cells in the MI network significantly increases the predictability of the firing rate of a single MI cell; however, we find interneuronal dependencies in MI to be much more locked to external kinematic parameters than those described recently in the hippocampus.  Nevertheless, this neighbor activity is approximately as informative as the hand velocity, supporting the view that neural encoding in MI is best understood at a population level. 
Comparing integrate-and-fire models estimated using intracellular and extracellular data| Abstract We have recently developed a maximum-likelihood (ML) method for estimating integrateand-fire-based stimulus encoding models that can be used even when only extracellular spike train data is available.  Here we derive the MLestimator given the full intracellular voltage trace and apply both the extracellular-only and intracellular method to responses recorded in vitro, allowing a direct comparison of the model fits within a unified statistical framework.  Both models are able to capture the behavior of these cells under dynamic stimulus conditions to a high degree of temporal precision, although we observe significant differences in the stochastic behavior of the two models. 
Log-concavity results on Gaussian process methods for supervised and unsupervised learning| Abstract Log-concavity is an important property in the context of optimization, Laplace approximation, and sampling; Gaussian process methods have become quite popular recently for classification, regression, density estimation, and point process intensity estimation.  Here we prove that the predictive densities corresponding to each of these applications are logconcave, given any observed data.  We also prove that the likelihood is log-concave in the hyperparameters controlling the mean function of the Gaussian prior in the density and point process intensity estimation cases, and the mean, covariance, and observation noise parameters in the classification and regression cases; the proof leads to a useful parameterization of these hyperparameters, indicating a suitably large class of priors for which the corresponding maximum a posteriori problem is log-concave.  Finally, we discuss a modification of the Gaussian process idea which leads to the log-concavity property in somewhat more generality for the density and point process estimation cases. 
Spatiotemporal Tuning of Motor Cortical Neurons for Hand Position and Velocity| A pursuit-tracking task (PTT) and multielectrode recordings were used to investigate the spatiotemporal encoding of hand position and velocity in primate primary motor cortex (MI).  Continuous tracking of a randomly moving visual stimulus provided a broad sample of velocity and position space, reduced statistical dependencies between kinematic variables, and minimized the nonstationarities that are found in typical "step-tracking" tasks.  These statistical features permitted the application of signal-processing and information-theoretic tools for the analysis of neural encoding.  The multielectrode method allowed for the comparison of tuning functions among simultaneously recorded cells.  During tracking, MI neurons showed heterogeneity of position and velocity coding, with markedly different temporal dynamics for each.  Velocity-tuned neurons were approximately sinusoidally tuned for direction, with linear speed scaling; other cells showed sinusoidal tuning for position, with linear scaling by distance.  Velocity encoding led behavior by about 100 ms for most cells, whereas position tuning was more broadly distributed, with leads and lags suggestive of both feedforward and feedback coding.  Individual cells encoded velocity and position weakly, with comparable amounts of information about each.  Linear regression methods confirmed that random, 2-D hand trajectories can be reconstructed from the firing of small ensembles of randomly selected neurons (3--19 cells) within the MI arm area.  These findings demonstrate that MI carries information about evolving hand trajectory during visually guided pursuit tracking, including information about arm position both during and after its specification.  However, the reconstruction methods used here capture only the low-frequency components of movement during the PTT.  Hand motion signals appear to be represented as a distributed code in which diverse information about position and velocity is available within small regions of MI. 
Asymptotic theory of information-theoretic experimental design| Abstract We discuss an idea for collecting data in a relatively efficient manner.  Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials).  We prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design.  In particular, we calculate the asymptotic efficiency of the informationmaximization strategy and demonstrate that this method is in a welldefined sense never less efficient --- and is generically more efficient --than the nonadaptive strategy.  For example, we are able to explicitly calculate the asymptotic relative efficiency of the "staircase method" widely employed in psychophysics research, and to demonstrate the dependence of this efficiency on the form of the "psychometric function" underlying the output responses. 
Maximum likelihood estimation of a stochastic integrate-and-fire neural encoding model| Abstract We examine a cascade encoding model for neural response in which a linear filtering stage is followed by a noisy, leaky, integrate-andfire spike generation mechanism.  This model provides a biophysically more realistic alternative to models based on Poisson (memoryless) spike generation, and can effectively reproduce a variety of spiking behaviors seen in vivo.  We describe the maximum likelihood estimator for the model parameters, given only extracellular spike train responses (not intracellular voltage data).  Specifically, we prove that the log likelihood function is concave and thus has an essentially unique global maximum that can be found using gradient ascent techniques.  We develop an efficient algorithm for computing the maximum likelihood solution, demonstrate the effectiveness of the resulting estimator with numerical simulations, and discuss a method of testing the model's validity using time-rescaling and density evolution techniques. 
Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model| Abstract Recent work has examined the estimation of models of stimulus-driven neural activity in which some linear filtering process is followed by a nonlinear, probabilistic spiking stage.  We analyze the estimation of one such model for which this nonlinear step is implemented by a noisy, leaky, integrate-and-fire mechanism with a spike-dependent aftercurrent.  This model is a biophysically plausible alternative to models with Poisson (memory-less) spiking, and has been shown to effectively reproduce various spiking statistics of neurons in vivo.  However, the problem of estimating the model from extracellular spike train data has not been examined in depth.  We formulate the problem in terms of maximum likelihood estimation, and show that the computational problem of maximizing the likelihood is tractable.  Our main contribution is an algorithm and a proof that this algorithm is guaranteed to find the global optimum with reasonable speed.  We demonstrate the effectiveness of our estimator with numerical simulations.  A central issue in computational neuroscience is the characterization of the functional relationship between sensory stimuli and neural spike trains.  A common model for this relationship consists of linear filtering of the stimulus, followed by a nonlinear, probabilistic spike generation process.  The linear filter is typically interpreted as the neuron's "receptive field," while the spiking mechanism accounts for simple nonlinearities like rectification and response saturation.  Given a set of stimuli and (extracellularly) recorded spike times, the characterization problem consists of estimating both the linear filter and the parameters governing the spiking mechanism.  One widely used model of this type is the Linear-Nonlinear-Poisson (LNP) cascade model, in which spikes are generated according to an inhomogeneous Poisson process, with rate determined by an instantaneous ("memoryless") nonlinear function of the filtered input.  This model has a number of desirable features, including conceptual simplicity and computational tractability.  Additionally, reverse correlation analysis provides a simple unbiased estimator for the linear filter [5], and the properties of estimators (for both the linear filter and static nonlinearity) have been thoroughly analyzed, even for the case of highly non-symmetric or "naturalistic" stimuli [12].  One important drawback of the LNP model,
Inferring prior probabilities from Bayes-optimal behavior| Abstract We discuss a method for obtaining a subject's a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective.  The method is nonparametric in the sense that we do not assume that the prior belongs to any fixed class of distributions (e. g. , Gaussian).  Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a concave maximization problem (with no non-global local maxima) in many important cases.  We explore the variability of the methods and emphasize the importance of regularization of the problem, via mathematical analysis and numerical examples.  We close by briefly discussing an interesting connection to recent models of neural population coding. 
Noise-driven adaptation:in vitro and mathematical analysis| Abstract Variance adaptation processes have recently been examined in cells of
Design of Experiments via Information Theory| Abstract We discuss an idea for collecting data in a relatively efficient manner.  Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials).  We prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design.  For example, we are able to explicitly calculate the asymptotic relative efficiency of the "staircase method" widely employed in psychophysics research, and to demonstrate the dependence of this efficiency on the form of the "psychometric function" underlying the output responses. 
"On the correlation structure of the wavelet coefficients of fractional Brownian motion," IEEE Trans|
Nonparametric inference of prior probabilities from Bayes-optimal behavior| Abstract We discuss a method for obtaining a subject's a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective.  The method is nonparametric in the sense that we do not assume that the prior belongs to any fixed class of distributions (e. g. , Gaussian).  Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a convex optimization problem (with no non-global local maxima) in many important cases.  In addition, we develop methods for analyzing the uncertainty of these estimates.  We demonstrate the accuracy of the method in a simple simulated coin-flipping setting; in particular, the method is able to precisely track the evolution of the subject's posterior distribution as more and more data are observed.  We close by briefly discussing an interesting connection to recent models of neural population coding. 
Temporal tuning properties for hand position and velocity in motor cortical neurons|
Information about movement direction obtained from synchronous activity of motor cortical neurons|
Robustness of neuroprosthetic decoding algorithms,|
Instant neural control of a movement signal|
Estimation of Entropy and Mutual Information|
Advances in Neural Information Processing Systems 17|
Brain-machine interface: Instant neural control of a movement signal,|
Spatiotemporal tuning properties for hand position and velocity in motor cortical neurons|
NIPS 17|
Nonlinear population models for the encoding of dynamic hand position signals in primary motor cortex|
interaction of signals from the receptive field center and surround|

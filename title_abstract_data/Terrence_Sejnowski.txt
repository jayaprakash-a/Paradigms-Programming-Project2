Filter Selection Model for Generating Visual Motion Signals| Abstract Neurons in area MT of primate visual cortex encode the velocity of moving objects.  We present a model of how MT cells aggregate responses from V1 to form such a velocity representation.  Two different sets of units, with local receptive fields, receive inputs from motion energy filters.  One set of units forms estimates of local motion, while the second set computes the utility of these estimates.  Outputs from this second set of units "gate" the outputs from the first set through a gain control mechanism.  This active process for selecting only a subset of local motion responses to integrate into more global responses distinguishes our model from previous models of velocity estimation.  The model yields accurate velocity estimates in synthetic images containing multiple moving targets of varying size, luminance, and spatial frequency profile and deals well with a number of transparency phenomena. 
Selectivity and sparseness in the responses of striate complex cells| Abstract Probability distributions of macaque complex cell responses to a large set of images were determined.  Measures of selectivity were based on the overall shape of the response probability distribution, as quantified by either kurtosis or entropy.  We call this non-parametric selectivity, in contrast to parametric selectivity, which measures tuning curve bandwidths.  To examine how receptive field properties a0ected
Spatiochromatic Receptive Field Properties Derived from Information-Theoretic Analyses of Cone Mosaic Responses to Natural Scenes| Neurons in the early stages of processing in the primate visual system efficiently encode natural scenes.  In previous studies of the chromatic properties of natural images, the inputs were sampled on a regular array, with complete color information at every location.  However, in the retina cone photoreceptors with different spectral sensitivities are arranged in a mosaic.  We used an unsupervised neural network model to analyze the statistical structure of retinal cone mosaic responses to calibrated color natural images.  The second-order statistical dependencies derived from the covariance matrix of the sensory signals wereremoved in the first stage Neural Computation 15, 397--417 (2003) c # 2002 Massachusetts Institute of Technology of processing.  These decorrelating filters were similar to type I receptive fields in parvo- or konio-cellular LGN in both spatial and chromatic characteristics.  In the subsequent stage, the decorrelated signals were linearly transformed to make the output as statistically independent as possible, using independent component analysis.  The independent component filters showed luminance selectivity with simple-cell-like receptive fields, or had strong color selectivity with large, often double-opponent, receptive fields, both of which were found in the primary visual cortex (V1).  These results show that the "form" and "color" channels of the early visual system can be derived from the statistics of sensory signals. 
Optimal Smoothing in Visual Motion Perception| When a ash is aligned with a moving object, subjects perceive the ash to lag behind the moving object.  Two different models have been proposed to explain this " ash-lag" effect.  In the motion extrapolation model, the visual system extrapolates the location of the moving object to counteract neural propagation delays , whereas in the latency difference model, it is hypothesized that moving objects are processed and perceived more quickly than ashed objects.  However, recent psychophysical experiments suggest that neither of these interpretations is feasible (Eagleman & Sejnowski, 2000a , 2000b , 2000c), hypothesizing instead that the visual system uses data from the future of an event before committing to an interpretation .  We formalize this idea in terms of the statistical framework of optimal smoothing and show that a model based on smoothing accounts for the shape of psychometric curves from a ash-lag experiment involving random reversals of motion direction.  The smoothing model demonstrates how the visual system may enhance perceptual accuracy by relying not only on data from the past but also on data collected from the immediate future of an event. 
TD( ) Converges with Probability| Abstract The methods of temporal differences (Samuel, 1959; Sutton 1984, 1988) allow agents to learn accurate predictions about stationary stochastic future outcomes.  The learning is effectively stochastic approximation based on samples extracted from the process generating the agent's future.  Sutton (1988) proved that for a special case of temporal differences, the expected values of the predictions converge to their correct values, as larger samples are taken, and Dayan (1992) extended his proof to the general case.  This paper proves the stronger result that the predictions of a slightly modified form of temporal difference learning converge with probability one, andshows how to quantify the rate of convergence. 
Empirical Entropy Manipulation for Real-World Problems| Abstract No finite sample is sufficient to determine the density, and therefore the entropy, of a signal directly. 
Learning the higher-order structure of a natural sound| Abstract.  Unsupervised learning algorithms paying attention only to second-order statistics ignore the phase structure (higher-order statistics) of signals, which contains all the informative temporal and spatial coincidences which we think of as `features'.  Here we discuss how an Independent Component Analysis (ICA) algorithm may be used to elucidate the higher-order structure of natural signals, yielding their independent basis functions.  This is illustrated with the ICA transform of the sound of a fingernail tapping musically on a tooth.  The resulting independent basis functions look like the sounds themselves, having the same temporal envelopes and the same musical pitches.  Thus they reflect both the phase and frequency information inherent in the data.  Short title: Learning higher order structure.  February 2, 1996 1.  The poverty of second-order statistics.  Natural signals have characteristic statistical dependencies across space and time.  One view of sensory systems is that they must uncover these dependencies by processing them with filters whose form depends on the characteristic statistics of the ensemble of signals to which they are exposed (Barlow 1989, Atick & Redlich 1990).  A considerable effort has gone into finding unsupervised learning algorithms able to self-organise to produce such filters (Linsker 1988, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990, Intrator 1992, Atick & Redlich 1993 and many others).  These efforts have been criticised by David Field 1987, 1994.  A major component of Field's argument is that the above methods are sensitive only to second-order statistics, since they all use correlation-based learning rules (ie: Hebbian and/or antiHebbian rules. ) Most of the methods bear some relation to Principal Components Analysis (the Karhunen-Loeve Transform), a second-order statistical technique.  The most informative features of natural signals, however, require higher-order statistics for their characterisation.  An edge in an image, or the transient attack or decay of a sound waveform, are examples of `features' which involve relationships between not just two, but many tens or even hundreds of pixels or time-points.  The failure of correlation-based learning is most clearly shown by the filters they produce when trained on stationary ensembles of signals.  The filters are typically global (see Figure 2a-c), sensitive to different spatio- or temporal frequencies, but with non-zero weights extending throughout the filter.  They reflect only the amplitude spectrum of the signal and ignore the phase spectrum where most of the suspicious local coincidences in natural signals take place.  An edge in an image, for example, is a coincidence in the phase spectrum, since if we were to Fourier analyse it, we would see many sine waves of different frequencies, all aligned in phase where the edge occurred.  Correlation-based methods cannot learn edge-detectors, though they often may seem to be doing so by local-windowing of the learnt Fourier components, turning them into Gabor-like filters (see Daugman 1985 for an analysis of the pertinence of Gabor filters to vision. ).  To illustrate formally that second order statistics only carry information about the amplitude spectrum, consider the autocorrelation function of a signal, which contains all its second order structure.  The Fourier transform of this is the power spectrum, which is the square of the amplitude spectrum.  Thus the two carry identical information.  To demonstrate intuitively that what we consider as the informative part of a natural signal is captured in the phase spectrum, Fourier transform the signal, remove the phase information, and transform it back to the space or time domain.  It will then look or sound like noise, typically with a 1/f amplitude spectrum.  All the visual features or sounds that our perceptual system thinks of as of as `signal' will be lost.  On the other hand, if we remove the amplitude information, and preserve the phase, the signal will be distorted but remain recognisable.  This points to a curious paradox: correlation-based learning algorithms are sensitive to exactly the part of natural signals which we regard as least meaningful (amplitude), and ignore the part of the signal which we find most meaningful (phase).  To encode the phase of signals, we need an algorithm sensitive to higher-order staistics.  The problem with higher-order statistics is that there are an infinite number of them.  Deciding which to measure a priori would be a difficult task.  Looking for horizontal bars in an image, for example, we may decide to estimate the average
Nonlocal interactions in color perception: Nonlinear processing of chromatic signals from remote inducers| Abstract The perceived color of an object depends on the chromaticity of its immediate background.  But color appearance is also influenced by remote chromaticities.  To quantify these influences, the eects of remote color fields on the appearance of a fixated 2test field were measured using a forced-choice method.  Changes in the appearance of the test field were induced by chromaticity changes of the background and of 2color fields not adjacent to the test field.  The appearance changes induced by the color of the background corresponded to a fraction of between 0. 5 and 0. 95 of the cone contrast of the background change, depending on observer.  The magnitude of induction by the background color was modulated on average by 7. 6 percent by chromaticity changes in the remote color fields.  Chromaticity changes in the remote fields had virtually no inducing eect when they occured without a change in background color.  The spatial range of these chromatic interactions extended over at least 10 degrees from the fovea.  They were established within the first few hundred milliseconds after the change of background color, and depended only weakly on the number of inducing fields.  These results may be interpreted as reflecting rapid chromatic interactions that support robustness of color vision under changing viewing conditions. 
A Comparison of Image Processing Techniques for Visual Speech Recognition Applications| Abstract We examine eight different techniques for developing visual representations in machine vision tasks.  In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection.  We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images.  This result is consistent with previous work on emotion and facial expression recognition.  In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 
Independent Component Analysis of Electroencephalographic Data| Abstract Because of the distance between the skull and brain and their different resistivities, electroencephalographic (EEG) data collected from any point on the human scalp includes activity generated within a large brain area.  This spatial smearing of EEG data by volume conduction does not involve significant time delays, however, suggesting that the Independent Component Analysis (ICA) algorithm of Bell and Sejnowski [1] is suitable for performing blind source separation on EEG data.  The ICA algorithm separates the problem of source identification from that of source localization.  First results of applying the ICA algorithm to EEG and event-related potential (ERP) data collected during a sustained auditory detection task show: (1) ICA training is insensitive to different random seeds.  (2) ICA may be used to segregate obvious artifactual EEG components (line and muscle noise, eye movements) from other sources.  (3) ICA is capable of isolating overlapping EEG phenomena, including alpha and theta bursts and spatially-separable ERP components, to separate ICA channels.  (4) Nonstationarities in EEG and behavioral state can be tracked using ICA via changes in the amount of residual correlation between ICA-filtered output channels. 
A Prototype for Automatic Recognition of Spontaneous Facial Actions| Abstract We present ongoing work on a project for automatic recognition of spontaneous facial actions.  Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command.  Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera.  Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations.  Here we explore an approach based on 3-D warping of images into canonical views.  We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models.  This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement.  The system was tested for recognition of a set of facial actions defined by the Facial Action Coding System (FACS).  We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition.  One exciting aspect of the approach presented here is that information about movement dynamics emerged out of filters which were derived from the statistics of images. 
Neuronal Tuning: To Sharpen or Broaden?| Sensory and motor variables are typically represented by a population of broadly tuned neurons.  A coarser representation with broader tuning can often improve coding accuracy, but sometimes the accuracy may also improve with sharper tuning.  The theoretical analysis here shows that the relationship between tuning width and accuracy depends crucially on the dimension of the encoded variable.  A general rule is derived for how the Fisher information scales with the tuning width, regardless of the exact shape of the tuning function, the probability distribution of spikes, and allowing some correlated noise between neurons.  These results demonstrate a universal dimensionality effect in neural population coding. 
Printed in the United States of America| Abstract: How do minds emerge from developing brains? According to "neural constructivism," the representational features of cortex are built from the dynamic interaction between neural growth mechanisms and environmentally derived neural activity.  Contrary to popular selectionist models that emphasize regressive mechanisms, the neurobiological evidence suggests that this growth is a progressive increase in the representational properties of cortex.  The interaction between the environment and neural growth results in a flexible type of learning: "constructive learning" minimizes the need for prespecification in accordance with recent neurobiological evidence that the developing cerebral cortex is largely free of domain-specific structure.  Instead, the representational properties of cortex are built by the nature of the problem domain confronting it.  This uniquely powerful and general learning strategy undermines the central assumption of classical learnability theory, that the learning properties of a system can be deduced from a fixed computational architecture.  Neural constructivism suggests that the evolutionary emergence of neocortex in mammals is a progression toward more flexible representational structures, in contrast to the popular view of cortical evolution as an increase in innate, specialized circuits.  Human cortical postnatal development is also more extensive and protracted than generally supposed, suggesting that cortex has evolved so as to maximize the capacity of environmental structure to shape its structure and function through constructive learning. 
Representations for Face Recognition by Independent Component Analysis| Abstract A number of current face recognition algorithms find useful face representations using unsupervised statistical methods.  Typically these methods find a set of basis images and represent faces as a linear combination of those images.  Principal component analysis (PCA) is a popular example of such methods.  The basis images found by PCA depend only on on pairwise relationships between pixels in the image database.  In a task such as face recognition, in which important information may be contained in the high-order relationships among pixels, it seems reasonable to expect that better basis images may be found by methods sensitive to these high order statistics.  Independent component analysis (ICA), a generalization of PCA, is one such method.  We used a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons [9].  ICA was performed on face images in the FERET database under two different architectures, one which treated the images as variables and the pixels as outcomes, and a second which treated the pixels as variables and the images as outcomes.  The first architecture found spatially local basis images for the faces.  The second architecture produced a factorial code for the faces.  Both ICA representations were superior to representations based on principal component analysis for recognizing faces across days and changes in expression.  A classifier that combined the two ICA representations gave the best performance. 
A Computational Model of How the Basal Ganglia Produce Sequences| Abstract We propose a systems-level computational model of the basal ganglia based closely on known anatomy and physiology.  First, we assume that the thalamic targets, which relay ascending information to cortical action and planning areas, are tonically inhibited by the basal ganglia.  Second, we assume that the output stage of the basal ganglia, the internal segment of the globus pallidus (GPi), selects a single action from several competing actions via lateral interactions.  Third, we propose that a form of local working memory exists in the form of reciprocal connections between the external globus pallidus (GPe) and the subthalamic nucleus (STN).  As a test of the model, the system was trained to learn a sequence of states that required the context of previous actions.  The striatum, which was assumed to represent a conjunction of cortical states, directly selected the action in the GP during training.  The STN-to-GP connection strengths were modi ed by an associative learning rule and came to encode the sequence after 20 to 40 iterations through the sequence.  Subsequently, the system automatically reproduced the sequence when cued to the rst action.  The behavior of the model was found to be sensitive to the ratio of the striatal-nigral learning rate to the STN-GP learning rate.  Additionally, the degree of striatal inhibition of the globus pallidus had a signi cant in uence on both learning and the ability to select an action.  Low learning rates, which would be hypothesized to re ect low levels of dopamine, as in Parkinson's disease, led to slow acquisition of contextual information.  However, this could be partially offset by modeling a lesion of the globus pallidus that resulted in an increase in the gain of the STN units.  The parameter sensitivity of the model is discussed within the framework of existing behavioral and lesion data. 
Temporal Difference Learning of Position Evaluation in the Game of Go| Abstract The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult.  Development of conventional Go programs is hampered by their knowledge-intensive nature.  We demonstrate a viable alternative by training networks to evaluate Go positions via temporal difference (TD) learning.  Our approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play.  These techniques yield far better performance than undifferentiated networks trained by selfplay alone.  A network with less than 500 weights learned within 3,000 games of 9x9 Go a position evaluation function that enables a primitive one-ply search to defeat a commercial Go program at a low playing level. 
Assignment of Multiplicative Mixtures in Natural Images| Abstract In the analysis of natural images, Gaussian scale mixtures (GSM) have been used to account for the statistics of filter responses, and to inspire hierarchical cortical representational learning schemes.  GSMs pose a critical assignment problem, working out which filter responses were generated by a common multiplicative factor.  We present a new approach to solving this assignment problem through a probabilistic extension to the basic GSM, and show how to perform inference in the model using Gibbs sampling.  We demonstrate the efficacy of the approach on both synthetic and image data. 
LEARNING TO EVALUATE GO POSITIONS VIA TEMPORAL DIFFERENCE METHODS| The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult.  Development of conventional Go programs is hampered by their knowledge-intensive nature.  We demonstrate a viable alternative by training neural networks to evaluate Go positions via temporal difference (TD) learning.  Our approach is based on neural network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play.  These techniques yield far better performance than undifferentiated networks trained by self-play alone.  A network with less than 500 weights learned within 3 000 games of 9x9 Go a position evaluation function
Behavioral/Systems/Cognitive Discovering Spike Patterns in Neuronal Responses| When a cortical neuron is repeatedly injected with the same fluctuating current stimulus (frozen noise) the timing of the spikes is highly precise from trial to trial and the spike pattern appears to be unique.  We show here that the same repeated stimulus can produce more than one reliable temporal pattern of spikes.  A new method is introduced to find these patterns in raw multitrial data and is tested on surrogate data sets.  Using it, multiple coexisting spike patterns were discovered in pyramidal cells recorded from rat prefrontal cortex in vitro, in data obtained in vivo from the middle temporal area of the monkey (Buracas et al. , 1998) and from the cat lateral geniculate nucleus (Reinagel and Reid, 2002).  The spike patterns lasted from a few tens of milliseconds in vitro to several seconds in vivo. We conclude that the prestimulus history of a neuron may influence the precise timing of the spikes in response to a stimulus over a wide range of time scales. 
Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Sub-Gaussian and Super-Gaussian Sources| An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions.  This was achieved by using a simple type of learning rule first derived by Girolami (1997) by choosing negentropy as a projection pursuit index.  Parameterized probability distributions that have sub- and supergaussian regimes were used to derive a general learning rule that preserves the simple architecture proposed by Bell and Sejnowski (1995), is optimized using the natural gradient by Amari (1998), and uses the stability analysis of Cardoso and Laheld (1996) to switch between sub- and supergaussian regimes.  We demonstrate that the extended infomax algorithm is able to separate 20 sources with a variety of source distributions easily.  Applied to high-dimensional data from electroencephalographic recordings, it is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain. 
Competitive Anti-Hebbian Learning of Invariants \Lambda| Abstract Although the detection of invariant structure in a given set of input patterns is vital to many recognition tasks, connectionist learning rules tend to focus on directions of high variance (principal components).  The prediction paradigm is often used to reconcile this dichotomy; here we suggest a more direct approach to invariant learning based on an anti-Hebbian learning rule.  An unsupervised twolayer network implementing this method in a competitive setting learns to extract coherent depth information from random-dot stereograms. 
Bayesian Unsupervised Learning of Higher Order Structure| Abstract Multilayer architectures such as those used in Bayesian belief networks and Helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs.  Because exact probability calculations with these models are often intractable, there is much interest in finding approximate algorithms.  We present an algorithm that efficiently discovers higher order structure using EM and Gibbs sampling.  The model can be interpreted as a stochastic recurrent network in which ambiguity in lower-level states is resolved through feedback from higher levels.  We demonstrate the performance of the algorithm on benchmark problems. 
Independent component representations for face recognition \Lambda| ABSTRACT In a task such as face recognition, much of the important information may be contained in the high-order relationships among the image pixels.  A number of face recognition algorithms employ principal component analysis (PCA), which is based on the second-order statistics of the image set, and does not address high-order statistical dependencies such as the relationships among three or more pixels.  Independent component analysis (ICA) is a generalization of PCA which separates the high-order moments of the input in addition to the second-order moments.  ICA was performed on a set of face images by an unsupervised learning algorithm derived from the principle of optimal information transfer through sigmoidal neurons.  1 The algorithm maximizes the mutual information between the input and the output, which produces statistically independent outputs under certain conditions.  ICA was performed on the face images under two different architectures.  The first architecture provided a statistically independent basis set for the face images that can be viewed as a set of independent facial features.  The second architecture provided a factorial code, in which the probability of any combination of features can be obtained from the product of their individual probabilities.  Both ICA representations were superior to representations based on principal components analysis for recognizing faces across sessions and changes in expression. 
A biologically inspired computational model of the Block Copying Task| Abstract We present in this paper a biologically inspired model of the Basal Ganglia which deals with Block Copying as a sequence learning task.  By breaking a relatively complex task into simpler operations with well-defined skills, an approach which is termed as a skill-based machine design is used in the device of computational models to complete such tasks.  Basal Ganglia are critically involved in sensorimotor control.  From the learning aspects, Actor-Critic architectures have been proposed to model the Basal Ganglia and Temporal Difference has been proposed as a learning algorithm.  The model is implemented and simulation results are presented to shows the capability of our model to successfully complete the task. 
The `Independent Components' of Natural Scenes are Edge Filters| Abstract Field (1994) has suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representaton of natural scenes, and Barlow (1989) has reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features.  We show here that a new unsupervised learning algorithm that is based on information maximisation, a non-linear `infomax' network (Bell and Sejnowski, 1995) when applied to an ensemble of natural scenes, produces sets of visual filters that are localised and oriented.  Some of these filters are Gabor-like and resemble those produced by the sparseness-maximisation network of Olshausen & Field (1996).  In addition, the outputs of these filters are as independent as possible, since the infomax network is able to perform Independent Components Analysis (ICA).  We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA).  The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes.  They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form a natural, informationtheoretic co-ordinate system for natural images.  \Lambda Please send comments to tony@salk. edu.  This paper is submitted to Vision Research. 
A Bayesian Framework for Tilt Perception and Confidence| Abstract The misjudgement of tilt in images lies at the heart of entertaining visual illusions and rigorous perceptual psychophysics.  A wealth of findings has attracted many mechanistic models, but few clear computational principles.  We adopt a Bayesian approach to perceptual tilt estimation, showing how a smoothness prior offers a powerful way of addressing much confusing data.  In particular, we faithfully model recent results showing that confidence in estimation can be systematically affected by the same aspects of images that affect bias.  Confidence is central to Bayesian modeling approaches, and is applicable in many other perceptual domains.  Perceptual anomalies and illusions, such as the misjudgements of motion and tilt evident in so many psychophysical experiments, have intrigued researchers for decades.  1--3 A Bayesian view 4--8 has been particularly influential in models of motion processing, treating such anomalies as the normative product of prior information (often statistically codifying Gestalt laws) with likelihood information from the actual scenes presented.  Here, we expand the range of statistically normative accounts to tilt estimation, for which there are classes of results (on estimation confidence) that are so far not available for motion.  The tilt illusion arises when the perceived tilt of a center target is misjudged (ie bias) in the presence of flankers.  Another phenomenon, called Crowding, refers to a loss in the confidence (ie sensitivity) of perceived target tilt in the presence of flankers.  Attempts have been made to formalize these phenomena quantitatively.  Crowding has been modeled as compulsory feature pooling (ie averaging of orientations), ignoring spatial positions.  9, 10 The tilt illusion has been explained by lateral interactions 11, 12 in populations of orientationtuned units; and by calibration.  13 However, most models of this form cannot explain a number of crucial aspects of the data.  First, the geometry of the positional arrangement of the stimuli affects attraction versus repulsion in bias, as emphasized by Kapadia et al 14 (figure 1A), and others.  15, 16 Second, Solomon et al.  recently measured bias and sensitivity simultaneously.  11 The rich and surprising range of sensitivities, far from flat as a function of flanker angles (figure 1B), are outside the reach of standard models.  Moreover, current explanations do not offer a computational account of tilt perception as the outcome of a normative inference process.  Here, we demonstrate that a Bayesian framework for orientation estimation, with a prior favoring smoothness, can naturally explain a range of seemingly puzzling tilt data.  We explicitly consider both the geometry of the stimuli, and the issue of confidence in the esti(A)
An information-maximisation approach to blind separation and blind deconvolution| Abstract We derive a new self-organising learning algorithm which maximises the information transferred in a network of non-linear units.  The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit.  Under these conditions, information maximisation has extra properties not found in the linear case (Linsker 1989).  The non-linearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation.  This enables the network to separate statistically independent components in the inputs: a higher-order generalisation of Principal Components Analysis.  We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to ten speakers.  We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal).  Finally, we derive dependencies of information transfer on time delays.  We suggest that information maximisation provides a unifying framework for problems in `blind' signal processing. 
CONSISTENCY OF INFOMAX ICA DECOMPOSITION OF FUNCTIONAL BRAIN IMAGING DATA| ABSTRACT Ten spatial infomax ICA decompositions were performed on two fMRI data sets collected from the same subject.  The maximallyindependent spatial components were then tested across decompositions for one-to-one correspondences.  Matching independent component maps by mutual information alone proved ineffective.  Matching component map pairs by correlating their z-transformed voxel map weights demonstrated that the top 100 components were stably reproduced in each of the ten decompositions.  Infomax ICA therefore provided a stable decomposition of fMRI data into spatially independent components. 
Using Aperiodic Reinforcement for Directed Self-Organization During Development| Abstract We present a local learning rule in which Hebbian learning is conditional on an incorrect prediction of a reinforcement signal.  We propose a biological interpretation of such a framework and display its utility through examples in which the reinforcement signal is cast as the delivery of a neuromodulator to its target.  Three examples are presented which illustrate how this framework can be applied to the development of the oculomotor system. 
Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes| Abstract The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion.  This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals.  We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy.  Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i. e.  least redundant.  The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction.  Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors.  Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 
Handling Missing Data with Variational Bayesian Learning of ICA| Abstract Missing data is common in real-world datasets and is a problem for many estimation techniques.  We have developed a variational Bayesian method to perform Independent Component Analysis (ICA) on high-dimensional data containing missing entries.  Missing data are handled naturally in the Bayesian framework by integrating the generative density model.  Modeling the distributions of the independent sources with mixture of Gaussians allows sources to be estimated with different kurtosis and skewness.  The variational Bayesian method automatically determines the dimensionality of the data and yields an accurate density model for the observed data without overfitting problems.  This allows direct probability estimation of missing values in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data. 
Regulation of Persistent Activity by Background Inhibition in an In Vitro Model of a Cortical Microcircuit| We combined in vitro intracellular recording from prefrontal cortical neurons with simulated synaptic activity of a layer 5 prefrontal microcircuit using a dynamic clamp.  During simulated in vivo background conditions, the cell responded to a brief depolarization with a sequence of spikes that outlasted the depolarization, mimicking the activity of a cell recorded during the delay period of a working memory task in the behaving monkey.  The onset of sustained activity depended on the number of action potentials elicited by the cue-like depolarization.  Too few spikes failed to provide enough NMDA drive to elicit sustained reverberations; too many spikes activated a slow intrinsic hyperpolarization current that prevented spiking; an intermediate number of spikes produced sustained activity.  When high dopamine levels were simulated by depolarizing the cell and by increasing the amount of NMDA current, the cell exhibited spontaneous `up-states' that terminated by the activation of a slow intrinsic hyperpolarizing current.  The firing rate during the delay period could be effectively modulated by the standard deviation of the inhibitory background synaptic noise without significant changes in the background firing rate before cue onset.  These results suggest that the balance between fast feedback inhibition and slower AMPA and NMDA feedback excitation is critical in initiating persistent activity and that the maintenance of persistent activity may be regulated by the amount of correlated background inhibition. 
Fast blind separation based on information theory| ABSTRACT Blind separation is an information theoretic problem, and we have proposed an information theoretic `sigmoid-based' solution [2].  Here we elaborate on several aspects of that solution.  Firstly, we argue that the separation matrix may be exactly found by maximising the joint entropy of the random vector resulting from a linear transformation of the mixtures followed by sigmoidal non-linearities which are the cumulative density functions of the `unknown' sources.  Secondly, we present the learning rule for performing this maximisation.  Thirdly, we discuss the role of prior knowledge of the c. d. f. 's of the sources in customising the learning rule.  We argue that sigmoid-based methods are better able to make use of this prior knowledge than cumulant-based methods, because the optimal non-linearity they should use is just an estimate of the source c. d. f.  We also suggest that they may have the edge in terms of robustness and speed of convergence.  Improvements in convergence speed have been facilitated by the introduction of pre-whitening of the mixture data.  An example result demonstrating this is the perfect separation of ten artificially mixed audio signals in 10 seconds of workstation computing time (4 to prewhiten and 6 to separate).  I.  Blind signal processing .  Statistically independent sources propagating in a medium are subject to several forms of distortion and interference.  They may be (1) mixed with other sources (2) mixed with time delayed versions of themselves, and (3) time-delayed.  The mixing may be linear or non-linear.  The inversion of these three forms of scrambling without any knowledge of their form may be called blind signal processing, or blind identification.  When the mixing is linear, we usually refer to (1) as the problem of blind separation [4], (2) as the problem of blind deconvolution, and (3) as the problem of blind time alignment.  These problems are information theoretic problems in the sense that we are dealing with the removal of statistical dependencies introduced by the medium, and the correct measure of statistical dependency is mutual information (see below).  In the most general information theoretic formalism, no special status is given to noise introduced by the medium or the sensors.  It is regarded as another `source' to be separated out.  It cannot be assumed to be characterised only by secondorder statistics (gaussian).  In fact, if we are lucky (and we usually are), it will not be gaussian, for it is the higher-order statistics which characterise a signal as independent and enable it to be separated out from others.  In [2], an information theoretic approach was outlined to all three of the above problems.  This paper is really a series of footnotes to [2], and should be read in conjunction with it if fuller details, or material of an introductory or tutorial nature are needed.  Here we will concentrate on the blind separation problem in order to show more clearly how it is solved by information theory.  II.  Separation through information theory.  A vector of sources s(t) = [s 1 (t); : : : ; s N (t)] propagates in a medium and mixtures of them, x(t) = [x 1 (t); : : : ; xN (t)] = As(t) 1 , are picked up by sensors.  The mixing is linear and static, there are no time delays and there are the same number (N ) of sensors as sources so that the mixing matrix, A, is square.  The important fact that distinguishes a source, s i , from a mixture, x i , is that it is statistically indepen1 henceforth, for convenience, the time index will be considered as implicit.  dent from the other sources, s j .  Their joint probability density function (p. d. f. ), measured across the time ensemble, factorises: f s (s) = N Y i=1 f s i (s i ) (1) Another way of saying this is that the mutual information between any two sources, i and j, is zero: I(s i ; s j ) = E '' ln f s (s) Q N i=1 f s i (s i ) # = 0 (2) where E[:] denotes expected value across the time ensemble.  Mixtures of sources will be statistically dependent on each other and the mutual information between them, I(x i ; x j ) will in general be positive.  Blind separation then consists in finding a matrix, W, so that the linear transformation u = Wx = WAs reestablishes the condition I(u i ; u j ) = 0, for all i 6= j.  This is the problem of Independent Component Analysis (ICA) [4, 3] One solution to this problem is that W is the inverse of A so that WA=I, the identity matrix.  Any other solution matrix, W, can be shown to be a permutation and rescaling of this one.  See Comon [3] for a fuller discussion of these matters.  To make the u i independent, we need to operate on non-linearly transformed output variables, y i = g(u i ), g() being a sigmoidal function.  2 The sigmoidal function provides, through its Taylor series expansion, all the higher-order statistics necessary to establish independence.  This assertion is justified through the following theorem: Theorem.  3 Independent Component Analysis (blind separation) can be performed exactly, by finding the maximum, with respect to W, of the joint entropy, H(y), of an output vector, y, which is the vector u, except that each element is transformed by a sigmoidal function which is a c. d. f.  of a sources which we are looking for.  In practice, we will often assume that all the sources have the same c. d. f.  and use the same sigmoidal function for each element of u.  To prove this theorem, we develop the following six points: Point 1.  Independent variables cannot become dependent by passing each one through a sigmoid.  Thus 2 a sigmoidal function is defined somewhat generally here as an invertible twice-differentiable function mapping the real line into some interval, often the unit interval: R! [0; 1].  3 Subsequent to the publication of this paper in the NOLTA proceedings, it was pointed out to us by Prof.  Nadal that this theorem is subsumed in the argument given in reference [7].  We would like to apologise for the fact that it appears here as an apparently original result and refer the reader to [7] for a fuller development.  if I(u i ; u j ) = 0 and y=g(u), g() being invertible, then I(y i ; y j ) = 0.  Since g\Gamma 1 is also invertible, the converse also holds.  Point 2.  The entropy, H(y), of a sigmoidally transformed variable has its maximum value (of zero) when the sigmoid function is the cumulative density function (c. d. f. ) of the u-variable.  Proof: H(y) is maximum when f y (y)=1 (the uniform distribution).  Thus by the relation: f y (y) = f u (u) dy=du (3) we have dy=du = fu (u) which means y = Fu (u), the cumulative density.  Point 3.  The joint entropy, H(y 1 ; y 2 ), of two sigmoidally transformed variables has its maximum value (of zero) when y 1 and y 2 are independent and the sigmoid function in each is the c. d. f.  of u 1 and u 2 respectively.  This is a clear consequence of Point 2 and the relation: H(y 1 ; y 2 ) = H(y 1 ) + H(y 2 ) \Gamma I(y 1 ; y 2 ) (4) The N-variable joint entropy, H(y), is similarly maximal when each f y i (y i ) term is maximum and all the I(y i ; y j ) are zero.  Point 4.  When two independent non-gaussian variables, u i and u j are linearly combined, the p. d. f.  of the resulting variable has a different shape from either of f u i (u i ) or f u j (u j ).  In general, the p. d. f.  becomes more gaussian, a trend ultimately enshrined in the Central Limit Theorem.  Gaussian variables are the only ones which retain the form of their p. d. f.  under linear combination.  Point 5.  Consider the joint entropy, H(y), of N sigmoidally transformed variables, where the sigmoid functions are the c. d. f. 's of N independent nongaussian sources (ie: y i = F s i (u i )).  This has its maximal value when u i = s i , in other words when the sources are separated! Any mixing of sources, u i = P j s j , will both: ffl introduce statistical dependencies between the u's, moving I(u i ; u j ) away from zero (and hence also I(y i ; y j ) --- see Point 1), and ffl decrease the individual entropy terms, H(y i ), through deviation of f y i (y i ) from 1.  This latter fact is born out by Points 2 and 4 above.  Taken together, this shows that under the special condition that y i = F s i (u i ), the joint entropy H(y) is maximal when the individual entropies, H(y i ), are
Spike-Timing-Dependent Hebbian Plasticity as Temporal Difference Learning| A spike-timing-dependent Hebbian mechanism governs the plasticity of recurrent excitatory synapses in the neocortex: synapses that are activated a few milliseconds before a postsynaptic spike are potentiated, while those that are activated a few milliseconds after are depressed .  We show that such a mechanism can implement a form of temporal difference learning for prediction of input sequences.  Using a biophysical model of a cortical neuron, we show that a temporal difference rule used in conjunction with dendritic backpropagating action potentials reproduces the temporally asymmetric window of Hebbian plasticity observed physiologically.  Furthermore, the size and shape of the window vary with the distance of the synapse from the soma.  Using a simple example , we show how a spike-timing-based temporal difference learning rule can allow a network of neocortical neurons to predict an input a few milliseconds before the input's expected arrival. 
Exploration Bonuses and Dual Control| Abstract Finding the Bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable.  This paper shows how to compute suboptimal estimates based on a certainty equivalence approximation arising from a form of dual control.  This systematizes and extends existing uses of exploration bonuses in reinforcement learning (Sutton, 1990).  The approach has two components: a statistical model of uncertainty in the world and a way of turning this into exploratory behaviour.  It is applied to two-dimensional mazes with moveable barriers. 
The initiation of bursts in thalamic neurons and the cortical control of thalamic sensitivity| Thalamic neurons generate high-frequency bursts of action potentials when a low-threshold (T-type) calcium current, located in soma and dendrites, becomes activated.  Computational models were used to investigate the bursting properties of thalamic relay and reticular neurons.  These two types of thalamic cells differ fundamentally in their ability to generate bursts following either excitatory or inhibitory events.  Bursts generated with excitatory inputs in relay cells required a high degree of convergence from excitatory inputs, whereas moderate excitation drove burst discharges in reticular neurons from hyperpolarized levels.  The opposite holds for inhibitory rebound bursts, which are more dif cult to evoke in reticular neurons than in relay cells.  The differences between the reticular neurons and thalamocortical neurons were due to different kinetics of the T-current, different electrotonic properties and different distribution patterns of the T-current in the two cell types.  These properties enable the cortex to control the sensitivity of the thalamus to inputs and are also important for understanding states such as absence seizures. 
GABAA AND GABAB RECEPTORS| Abstract Since the introduction of the alpha function by Rall in 1967 [12], there has been signi#cant progress in our understanding of the molecular events underlying synaptic transmission.  Particular receptor types have been identified and their activation kinetics characterized.  It is now possible to develop models of these receptors, using a formalism similar to that introduced by Hodgkin and Huxley [9].  In this paper, we present recently-introduced models obtained by simplifying more detailed biophysical models of postsynaptic receptors [7].  The simplified models are fully compatible with the Hodgkin-Huxley formalism, are very e#cient to simulate, and account for important phenomena such as synaptic summation and desensitization.  These models should be useful in large-scale network simulations.  Fast kinetic models In a previous paper [7], we developed a model of synaptic transmission that incorporated the kinetics of voltage-dependentchannels, the kinetics of exocytosis of neurotransmitter in the synapse, diffusion of the neurotransmitter and binding to postsynaptic receptors, and finally the kinetics of activation of these receptors.  It was shown that introducing simplifying assumptions leads to much simpler kinetic models for a variety of receptor types, including fast transmission and neuromodulation.  Synaptic receptors activate or deactivate ion channels located in the postsynaptic cell.  In the case of \fast" synaptic transmission, the receptor and the channel are part of the same protein complex.  In these so-called ionotropic receptors, the ligand is a neurotransmitter and its binding to the complex leads to the opening of the associated ionophore.  Ionotropic receptors include the glutamate AMPA/kainate and NMDAtypes, the fast GABAergic receptors 9 (GABAA ) and nicotinic acetylcholine (ACh) receptors.  However, for a variety of neurotransmitters, the channel is independent of the receptor and the gating occurs through the production of an intracellular second messenger.  These so-called metabotropic receptors include the slow GABAergic type (GABAB ), muscarinic ACh receptors, noradrenergic receptors, serotonergic receptors, as well as other types.  Wehave considered relatively detailed kinetic schemes for both ionotropic and metabotropic types of synaptic receptors.  For both types, we derived simple kinetic schemes that account for most of their properties.  The simplified models assume that the time course of the ligand, L, occurs as a pulse, triggered by the presynaptic spike.  The ligand then gates the opening of a ion channel, according to the following possible schemes: O C + L 1 r 1 r 2 r 1 r 2 C + L 1 C 2 r r 3 4 r r 5 6 O D r 1 r 2 C + L 1 r 3 r 4 r 5 O D 6 r (1) (2) (3) In these schemes, C 1 and C 2 represent the closed states of the channel, O is the open state, D represents the desensitized state and r 1 . . . r 6 are the associated rate constants.  The synaptic current I syn is obtained from the relation: I syn =fig syn m (V, E syn ) where # g syn is the maximal synaptic conductance, m is the fraction of channels in open state, V is the postsynaptic membrane potential and E syn is the reversal potential.  The time course of the ligand L and the kinetic constants distinguish ionotropic from metabotropic receptors.  In the former, L represents the neurotransmitter and occurs as a pulse of 1 mM amplitude and 1 ms duration, as estimated from patch-clamp recordings [4].  In the latter, L is a second messenger and occurs as a pulse of 1 #M amplitude and 50-100 ms duration.  These values were estimated from kinetic data of second-messenger transduction.  The computational advantage of simple schemes such as (1-2) is that the time course of the current can be obtained analytically [6, 7].  The analytic expressions make these models extremely powerful because they do not require differential equations to be solved numerically (see ref.  [7] for more details).  Another advantage is that it is easy to fit model to experimental data, as described in the next section. 
An information-maximization approach to blind separation and blind deconvolution|
A Learning Algorithm for Boltzmann Machines|
Edges are the Independent Components of Natural Scenes|
Rapid Temporal Modulation of Synchrony by Competition in Cortical Interneuron Networks|
Objective Functions for Neural Map Formation|
Learning Nonlinear Overcomplete Representations for Efficient Coding|
Classifying Facial Actions|
A Unifying Objective Function for Topographic Mappings|
A perceptron reveals the face of sex|
Neuronal tuning: To Sharpen or Broaden?|
A 'Neural' Network that Learns to Play Backgammon|
A Parallel Network that Learns to Play Backgammon|
Tempering Backpropagation Networks: Not All Weights are Created Equal|
Unsupervised Classification with Non-Gaussian Mixture Models Using ICA|
Extended ICA Removes Artifacts from Electroencephalographic Recordings|
Learning Overcomplete Representations|
The Spectral Independent Components of Natural Scenes|
Slow Feature Analysis: Unsupervised Learning of Invariances|
Using Feedforward Neural Networks to Monitor Alertness from Changes in EEG Correlation and Coherence|
Viewpoint Invariant Face Recognition using Independent Component Analysis and Attractor Networks|
Unsupervised Discrimination of Clustered Data via Optimization of Binary Information Gain|
Learning to solve random-dot stereograms of dense and transparent surfaces with recurrent backpropagation|
Image Representations for Facial Expression Coding|
Dynamic Features for Visual Speechreading: A Systematic Comparison|
Constrained Optimization for Neural Map Formation: A Unifying Framework for Weight Growth and Normalization|
Towards a computational model of the superior colliculus and its pathways"|
Predictive Sequence Learning in Recurrent Neocortical Circuits|
Egocentric spatial representation in early vision|
Spatial Representations in the Parietal Cortex May Use Basis Functions|
Combining Visual and Acoustic Speech Signals with a Neural Network Improves Intelligibility|
A Mixture Model System for Medical and Machine Diagnosis|
Spatiochromatic Receptive Field Properties Derived from Information-Theoretic Analyses of Cone Mosaic Responses to Natural Scenes|
Using the td(lambda) algorithm to learn an evaluation function for the game of go|
Variational Learning of Clusters of Undercomplete Nonsymmetric Independent Components|
Dictionary Learning Algorithms for Sparse Representation|
Foraging in an Uncertain Environment Using Predictive Hebbian Learning|
A Learning Algorith, for Bolzmann Machines|
Coding Time-Varying Signals Using Sparse, Shift-Invariant Representations|
A Unifying InformationTheoretic Framework For Independent Component Analysis,|
Optimal Smoothing in Visual Motion Perception|
Massively Parallel Architectures for AI: NETL, Thistle, and Boltzmann Machines|
Cortical and thalamic components of augmenting responses: A modeling study|
Competitive Anti-Hebbian Learning of Invariants|
A Non-linear Information Maximisation Algorithm that Performs Blind Separation|
Slow feature analyUnsupervised learning invariances,|
A Model of Spatial Representations in Parietal Cortex Explains Hemineglect|
Graphical Models: Foundations of Neural Computation|
Seeing White: Qualia in the Context of Decoding Population Codes|
Independent Component Representation for Face Recognition|
Neural Network Analysis of Distributed Representations of Dynamical Sensory-Motor Transormations in the Leech|
Faster learning for dynamic recurrent backpropagation|
Learning to evaluate Go positions via temporal difference methods|
Attractor Reliability Reveals Deterministic Structure in Neuronal Spike Trains|

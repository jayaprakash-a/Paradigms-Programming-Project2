Contextual models for object detection using boosted random fields| Abstract We seek to both detect and segment objects in images.  To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which use boosting to learn the graph structure and local evidence of a conditional random field (CRF).  The graph structure is learned by assembling graph fragments in an additive model.  The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efficient inference.  We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade.  We apply our system to detect stuff and things in office and street scenes. 
Learning Hierarchical Models of Scenes, Objects, and Parts| Abstract We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes.  The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator.  Each object category then has its own distribution over these parts, which are shared between objects.  We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters.  Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available.  We also extend this hierarchical framework to scenes containing multiple objects. 
Semantic Organization of Scenes using Discriminant Structural Templates| Abstract In this paper, we present a procedure for organizing real world scenes along semantic axes.  The approach is based on the output energies of linear discriminant filters that take into account, or not, spatial information.  We introduce three semantic axes along which pictures are ordered.  The main semantic axis computes the degree of naturalness of a scene.  Then, urban pictures are evaluated according to their degree of verticalness and natural scenes, according to their degree of openness.  We observe the emergence of typical scene categories such as beach, mountain, skyscrapers, city center, etc. , along the axes. 
Context-based vision system for place and object recognition| 2003 massachusetts institute of technology, cambridge, ma 02139 usa --- www. ai. mit. edu massachusetts institute of technology --- artificial intelligence laboratory @ MIT Abstract While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are.  In this paper we present a context-based vision system for place and object recognition.  The goal is to identify familiar locations (e. g. , office 610, conference room 941, Main Street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e. g. , table, chair, car, computer).  We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and how such contextual information introduces strong priors that simplify object recognition.  We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types.  The algorithm has been integrated into a mobile system that provides real-time feedback to the user. 
Sharing Features: Efficient Boosting Procedures for Multiclass Object Detection| Abstract We consider the problem of detecting a large number of different object classes in cluttered scenes.  Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data.  We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes.  The detectors for each class are trained jointly, rather than independently.  For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes.  In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges. 
TOP-DOWN CONTROL OF VISUAL ATTENTION IN OBJECT DETECTION| ABSTRACT Current computational models of visual attention focus on bottom-up information and ignore scene context.  However, studies in visual cognition show that humans use context to facilitate object detection in natural scenes by directing their attention or eyes to diagnostic regions.  Here we propose a model of attention guidance based on global scene configuration.  We show that the statistics of low-level features across the scene image determine where a specific object (e. g.  a person) should be located.  Human eye movements show that regions chosen by the top-down model agree with regions scrutinized by human observers performing a visual search task for people.  The results validate the proposition that top-down information from visual context modulates the saliency of image regions during the task of object detection.  Contextual information provides a shortcut for efficient object detection systems. 
Describing Visual Scenes using Transformed Dirichlet Processes| Abstract Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes.  In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image.  Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data.  For visual scenes, mixture components describe the spatial structure of visual features in an object--centered coordinate frame, while transformations model the object positions in a particular image.  Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler.  Applied to a dataset of partially labeled street scenes, we show that the TDP's inclusion of spatial structure improves detection performance, flexibly exploiting partially labeled training images. 
Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes| Abstract Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not.  We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities.  We present a conditional random field for jointly solving the tasks of object detection and scene classification. 
Global Semantic Classification of Scenes using Power Spectrum Templates| Challenge of Image Retrieval (CIR99),
Statistics of natural image categories| Abstract In this paper we study the statistical properties of natural images belonging to different categories and their relevance for scene and object categorization tasks.  We discuss how second-order statistics are correlated with image categories, scene scale and objects.  We propose how scene categorization could be computed in a feedforward manner in order to provide top-down and contextual information very early in the visual processing chain.  Results show how visual categorization based directly on low-level features, without grouping or segmentation stages, can benefit object localization and identification.  We show how simple image statistics can be used to predict the presence and absence of objects in the scene before exploring the image.  (Some figures in this article are in colour only in the electronic version)
Shape Recipes: Scene Representations that Refer to the Image| Abstract The goal of low-level vision is to estimate an underlying scene, given an observed image.  Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store.  We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene configurations.  Shape recipes are an example: these are the regression coefficients that predict the bandpassed shape from image data.  We describe the benefits of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation. 
Statistical Context Priming for Object Detection| Abstract There is general consensus that context can be a rich source of information about an object's identity, location and scale.  However, the issue of how to formalize contextual influences is still largely open.  Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties.  We represent global context information in terms of the spatial layout of spectral components.  The resulting scheme serves as an effective procedure for context driven focus of attention and scale-selection on real-world scenes.  Based on a simple holistic analysis of an image, the scheme is able to accurately predict object locations and sizes. 
Properties and Applications of Shape Recipes| In low-level vision, the representation of scene properties such as shape, albedo, etc. , are very high dimensional as they have to describe complicated structures.  The approach proposed here is to let the image itself bear as much of the representational burden as possible.  In many situations, scene and image are closely related and it is possible to find a functional relationship between them.  The scene information can be represented in reference to the image where the functional specifies how to translate the image into the associated scene.  We illustrate the use of this representation for encoding shape information.  We show how this representation has appealing properties such as locality and slow variation across space and scale.  These properties provide a way of improving shape estimates coming from other sources of information like stereo. 
Asymmetrical Filters for Vision Chips: a Basis for the Design of Large Sets of Spatial and Spatiotemporal Filters| Abstract The problem of actual vision machines on VLSI is the implementation of a large set of linear filters selective to features such as edges, corners, orientations, motion.  This procedurerequires large arrays of filters with complex structures.  In order to reducethecomplexity, we propose to implement a simple basis of filters able to generate morecomplex filters such as oriented quadratureband-pass filters, quadraturewedge filters, velocity tuned filters.  The current basis consists of asymmetrical filters implemented with Cellular Neural Networks having only one layer and interactions with four neighbors.  We show that this basis generates relevant filters for vision applications and show how the particular structure of the filters basis allows the direct implementation of spatio-temporal filters with a low aditional cost. 
Contextual Priming for Object Detection|
Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope|
Depth Estimation from Image Structure|
From retinal circuits to motion processing: a neuromorphic approachtovelocity estimation|
An efficient neuromorphic analog network for motion estimation|
Global semantic classification of scenes using power spectrum templates," in Proceedings of Challenge of Image Retrieval Electronic Workshops in Computing Series|
Scene-Centered Description from Spatial Envelope Properties|
Scene Semantic Spaces from Global and Color Templates|
How image statistics drive shape-from-texture and shape-from-specularities|

Image Reconstruction by Linear Programming| Abstract A common way of image denoising is to project a noisy image to the subspace of admissible images made for instance by PCA.  However, a major drawback of this method is that all pixels are updated by the projection, even when only a few pixels are corrupted by noise or occlusion.  We propose a new method to identify the noisy pixels by # 1 -norm penalization and update the identified pixels only.  The identification and updating of noisy pixels are formulated as one linear program which can be solved efficiently.  Especially, one can apply the #-trick to directly specify the fraction of pixels to be reconstructed.  Moreover, we extend the linear program to be able to exploit prior knowledge that occlusions often appear in contiguous blocks (e. g.  sunglasses on faces).  The basic idea is to penalize boundary points and interior points of the occluded area differently.  We are able to show the #-property also for this extended LP leading a method which is easy to use.  Experimental results impressively demonstrate the power of our approach. 
Pairwise Coupling for Machine Recognition of Hand-Printed Japanese Characters| Abstract Machine recognition of hand-printed Japanese characters has been an area of great interest for many years.  The major problem with this classification task is the huge number of different characters.  Applying standard "state-ofthe-art" techniques, such as the SVM, to multi-class problems of this kind imposes severe problems, both of a conceptual and a technical nature: (i) separating one class from all others may be an unnecessarily hard problem; (ii) solving these subproblems can impose unacceptably high computational costs.  In this paper, a new approach to Japanese character recognition is presented that successfully overcomes these shortcomings.  It is based on a pairwise coupling procedure for probabilistic two-class kernel classifiers.  Experimental results for Hiragana recognition effectively demonstrate that our method attains an excellent level of prediction accuracy while imposing very low computational costs. 
The Leave-One-Out Kernel| Abstract.  Recently, several attempts have been made for deriving datadependent kernels from distribution estimates with parametric models (e. g.  the Fisher kernel).  In this paper, we propose a new kernel derived from any distribution estimators, parametric or nonparametric.  This kernel is called the Leave-one-out kernel (i. e.  LOO kernel), because the leave-one-out process plays an important role to compute this kernel.  We will show that, when applied to a parametric model, the LOO kernel converges to the Fisher kernel asymptotically as the number of samples goes to infinity. 
Marginalized Kernels Between Labeled Graphs| Abstract A new kernel function between two labeled graphs is presented.  Feature vectors are defined as the counts of label paths produced by random walks on graphs.  The kernel computation finally boils down to obtaining the stationary state of a discrete-time linear system, thus is eciently performed by solving simultaneous linear equations.  Our kernel is based on an infinite dimensional feature space, so it is fundamentally dierent from other string or tree kernels based on dynamic programming.  We will present promising empirical results in classification of chemical compounds.  1
Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection| Abstract We address the problem of learning a symmetric positive definite matrix.  The central issue is to design parameter updates that preserve positive definiteness.  Our updates are motivated with the von Neumann divergence.  Rather than treating the most general case, we focus on two key applications that exemplify our methods: on-line learning with a simple square loss, and finding a symmetric positive definite matrix subject to linear constraints.  The updates generalize the Exponentiated Gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive definite matrix of trace one instead of a probability vector (which in this context is a diagonal positive definite matrix with trace one).  The generalized updates use matrix logarithms and exponentials to preserve positive definiteness.  Most importantly, we show how the analyzes of the original EG update and AdaBoost generalize to the non-diagonal case.  We apply the new versions of both, called the Matrix Exponentiated Gradient (MEG) update and DefiniteBoost, to learn a kernel matrix from distance measurements. 
Asymptotic Properties of the Fisher Kernel| Abstract This paper analyses the Fisher kernel from a statistical point of view.  The Fisher
An Introduction to Kernel-Based Learning Algorithms| Abstract---This paper provides an introduction to support vector machines (SVMs), kernel Fisher discriminant analysis, and kernel principal component analysis (PCA), as examples for successful kernel-based learning methods.  We first give a short background about Vapnik--Chervonenkis (VC) theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations.  We illustrate the usefulness of kernel algorithms by finally discussing applications such as optical character recognition (OCR) and DNA analysis. 
Marginalized Kernels for RNA Sequence Data Analysis| Abstract We present novel kernels that measure similarity of two RNA sequences, taking account of their secondary structures.  Two types of kernels are presented.  One is for RNA sequences with-, the other for those without known secondary structures.  The latter employs stochastic context-free grammar (SCFG) for estimating the secondary structure.  We call the latter the marginalized count kernel (MCK).  We show computational experiments for MCK using 74 sets of human tRNA sequence data: (i) kernel principal component analysis (PCA) for visualizing tRNA similarities, (ii) supervised classification with support vector machines (SVMs).  Both types of experiment show promising results for MCKs. 
Marginalized kernels for biological sequences| ABSTRACT Motivation: Kernel methods such as support vector machines require a kernel function between objects to be defined a priori.  Several works have been done to derive kernels from probability distributions, e. g.  the Fisher kernel.  However, a general methodology to design a kernel is not fully developed.  Results: We propose a reasonable way of designing a kernel when objects are generated from latent variable models (e. g.  HMM).  First of all, a joint kernel is designed for complete data which include both visible and hidden variables.  Then a marginalized kernel for visible data is obtained by taking the expectation with respect to hidden variables.  We will show that the Fisher kernel is a special case of marginalized kernels, which gives another viewpoint to the Fisher kernel theory.  Although our approach can be applied to any object, we particularly derive several marginalized kernels useful for biological sequences (e. g.  DNA and proteins).  The effectiveness of marginalized kernels is illustrated in the task of classifying bacterial gyrase subunit B (gyrB) amino acid sequences. 
Clustering with the Fisher Score| Abstract Recently the Fisher score (or the Fisher kernel) is increasingly used as a feature extractor for classification problems.  The Fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model.  This paper gives a theoretical analysis about how class information is preserved in the space of the Fisher score, which turns out that the Fisher score consists of a few important dimensions with class information and many nuisance dimensions.  When we perform clustering with the Fisher score, K-Means type methods are obviously inappropriate because they make use of all dimensions.  So we will develop a novel but simple clustering algorithm specialized for the Fisher score, which can exploit important dimensions.  This algorithm is successfully tested in experiments with artificial data and real data (amino acid sequences). 
An Introduction to Kernel-Based Learning Algorithms|
Clustering OCR-ed texts for browsing document image database|
Support vector classifier with asymmetric kernel function|
and KlausRobert Muller| A new discriminative kernel from probabilistic models. 
and Klaus-Robert Muller| Asymptotic properties of the Fisher kernel. 
Finger-pressure waveforms measured on clynes' sentograph distinguished among emotions,"|
Optimal hyperplane classifier based on entropy number bound,|
A New Discriminative Kernel from Probabilistic Models|
Membrane permeability change during inhibitory transmitter action in crayfish receptor cell,"|
The em Algorithm for Kernel Matrix Completion with Auxiliary Data|
Reconfiguration strategies for hybrid systems|
Soren Sonnenburg, and Klaus-Robert Muller|
The MH1 domains of smad2 and smad3 are involved in the regulation of the ALK7 signals|
Fractional dimension function,|
Support vector classifiers with asymetric kernel functions|
Subspace classifier in the Hilbert space|

Factored A Search for Models over Sequences and Trees| Abstract We investigate the calculation of A* bounds for sequence and tree models which are the explicit intersection of a set of simpler models or can be bounded by such an intersection.  We provide a natural viewpoint which unifies various instances of factored A* models for trees and sequences, some previously known and others novel, including multiple sequence alignment, weighted finitestate transducer composition, and lexicalized statistical parsing.  The specific case of parsing with a product of syntactic (PCFG) and semantic (lexical dependency) components is then considered in detail.  We show that this factorization gives a modular lexicalized parser which is simpler than comparably accurate non-factored models, and which allows efficient exact inference with large treebank grammars. 
Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the Penn Treebank| Abstract This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar.  We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs.  bottom-up strategies.  We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations. 
Named Entity Recognition with Character-Level Models| Abstract We discuss two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation.  The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.  Our best model achieves an overall F 1 of 86. 07% on the English test data (92. 31% on the development data).  This number represents a 25% error reduction over the same model without word-internal (substring) features. 
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL,| Abstract In the fall term of 2004, I taught a new statistical NLP course focusing on core tools and machine-learning algorithms.  The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system.  Using provided scaffolding, students built realistic tools with nearly state-of-theart performance in most cases.  This paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way. 
Distributional Phrase Structure Induction| Abstract Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data.  Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts.  We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features.  The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.  1 Overview While early work showed that small, artificial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective.  For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality.  Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000).  It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text.  However, there are compelling motivations for unsupervised grammar induction.  Building supervised training data requires considerable resources, including time and linguistic expertise.  Furthermore, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within a supervised parser's supervisory information, and, therefore, often not explicitly modeled in such systems.  For example, our system and others have difficulty correctly attaching subjects to verbs above objects.  For a supervised CFG parser, this ordering is implicit in the given structure of VP and S constituents, however, it seems likely that to learn attachment order reliably, an unsupervised system will have to model it explicitly.  Our goal in this work is the induction of highquality, linguistically sensible grammars, not parsing accuracy.  We present two systems, one which does not do disambiguation well and one which does not do it at all.  Both take tagged but unparsed Penn treebank sentences as input.  1 To whatever degree our systems parse well, it can be taken as evidence that their grammars are sensible, but no effort was taken to improve parsing accuracy directly.  There is no claim that human language acquisition is in any way modeled by the systems described here.  However, any success of these methods is evidence of substantial cues present in the data, which could potentially be exploited by humans as well.  Furthermore, mistakes made by these systems could indicate points where human acquisition is likely not being driven by these kinds of statistics.  2 Approach At the heart of any iterative grammar induction system is a method, implicit or explicit, for deciding how to update the grammar.  Two linguistic criteria for constituency in natural language grammars form the basis of this work (Radford, 1988): 1.  External distribution: A constituent is a sequence of words which appears in various structural positions within larger constituents.  1 The Penn tag and category sets used in examples in this paper are documented in Manning and Schutze (1999, 413).  2.  Substitutability: A constituent is a sequence of words with (simple) variants which can be substituted for that sequence.  To make use of these intuitions, we use a distributional notion of context.  Let # be a part-of-speech tag sequence.  Every occurence of # will be in some context x # y, where x and y are the adjacent tags or sentence boundaries.  The distribution over contexts in which # occurs is called its signature, which we denote by #(#).  Criterion 1 regards constituency itself.  Consider the tag sequences IN DT NN and IN DT.  The former is a canonical example of a constituent (of category PP), while the later, though strictly more common, is, in general, not a constituent.  Frequency alone does not distinguish these two sequences, but Criterion 1 points to a distributional fact which does.  In particular, IN DT NN occurs in many environments.  It can follow a verb, begin a sentence, end a sentence, and so on.  On the other hand, IN DT is generally followed by some kind of a noun or adjective.  This example suggests that a sequence's constituency might be roughly indicated by the entropy of its signature, H(#(#)).  This turns out to be somewhat true, given a few qualifications.  Figure 1 shows the actual most frequent constituents along with their rankings by several other measures.  Tag entropy by itself gives a list that is not particularly impressive.  There are two primary causes for this.  One is that uncommon but possible contexts have little impact on the tag entropy value.  Given the skewed distribution of short sentences in the treebank, this is somewhat of a problem.  To correct for this, let # u (#) be the uniform distribution over the observed contexts for #.  Using H(# u (#)) would have the obvious effect of boosting rare contexts, and the more subtle effect of biasing the rankings slightly towards more common sequences.  However, while H(#(#)) presumably converges to some sensible limit given infinite data, H(# u (#)) will not, as noise eventually makes all or most counts non-zero.  Let u be the uniform distribution over all contexts.  The scaled entropy H s (#(#)) = H(#(#))[H(# u (#))=H(u)] turned out to be a useful quantity in practice.  Multiplying entropies is not theoretically meaningful, but this quantity does converge to H(#(#)) given infinite (noisy) data.  The list for scaled entropy still has notable flaws, mainly relatively low ranks for common NPs, which does not hurt system perforSequence Actual Freq Entropy Scaled Boundary GREEDY-RE DT NN
Natural Language Grammar Induction Using a Constituent-Context Model| Abstract This paper presents a novel approach to the unsupervised learning of syntactic analyses of natural language text.  Most previous work has focused on maximizing likelihood according to generative PCFG models.  In contrast, we employ a simpler probabilistic model over trees based directly on constituent identity and linear context, and use an EM-like iterative procedure to induce structure.  This method produces much higher quality analyses, giving the best published results on the ATIS dataset.  1 Overview To enable a wide range of subsequent tasks, human language sentences are standardly given tree-structure analyses, wherein the nodes in a tree dominate contiguous spans of words called constituents, as in figure 1(a).  Constituents are the linguistically coherent units in the sentence, and are usually labeled with a constituent category, such as noun phrase (NP) or verb phrase (VP).  An aim of grammar induction systems is to figure out, given just the sentences in a corpus S, what tree structures correspond to them.  In this sense, the grammar induction problem is an incomplete data problem, where the complete data is the corpus of trees T , but we only observe their yields S.  This paper presents a new approach to this problem, which gains leverage by directly making use of constituent contexts.  It is an open problem whether entirely unsupervised methods can produce linguistically accurate parses of sentences.  Due to the difficulty of this task, the vast majority of statistical parsing work has focused on supervised learning approaches to parsing, where one uses a treebank of fully parsed sentences to induce a model which parses unseen sentences [7, 3].  But there are compelling motivations for unsupervised grammar induction.  Building supervised training data requires considerable resources, including time and linguistic expertise.  Investigating unsupervised methods can shed light on linguistic phenomena which are implicit within a supervised parser's supervisory information (e. g. , unsupervised systems often have difficulty correctly attaching subjects to verbs above objects, whereas for a supervised parser, this ordering is implicit in the supervisory information).  Finally, while the presented system makes no claims to modeling human language acquisition, results on whether there is enough information in sentences to recover their structure are important data for linguistic theory, where it has standardly been assumed that the information in the data is deficient, and strong innate knowledge is required for language acquisition [4].  S NP NN 1 Factory NNS payrolls VP VBD fell PP IN in NN 2 September Node Constituent Context S NN NNS VBD IN NN # -- # NP NN NNS # -- VBD VP VBD IN NN NNS -- # PP IN NN VBD -- # NN 1 NN # -- NNS NNS NNS NN -- VBD VBD VBD NNS -- IN IN IN VBD -- NN NN 2 NNS IN -- # Empty Context # 0 # -- NN # 1 NN -- NNS # 2 NNS -- VBD # 3 VBD -- IN # 4 IN -- NN # 5 NN -- # Figure 1: Example parse tree with the constituents and contexts for each tree node.  2 Previous Approaches One aspect of grammar induction where there has already been substantial success is the induction of parts-of-speech.  Several different distributional clustering approaches have resulted in relatively high-quality clusterings, though the clusters' resemblance to classical parts-of-speech varies substantially [9, 15].  For the present work, we take the part-ofspeech induction problem as solved and work with sequences of parts-of-speech rather than words.  In some ways this makes the problem easier, such as by reducing sparsity, but in other ways it complicates the task (even supervised parsers perform relatively poorly with the actual words replaced by parts-of-speech).  Work attempting to induce tree structures has met with much less success.  Most grammar induction work assumes that trees are generated by a symbolic or probabilistic context-free grammar (CFG or PCFG).  These systems generally boil down to one of two types.  Some fix the structure of the grammar in advance [12], often with an aim to incorporate linguistic constraints [2] or prior knowledge [13].  These systems typically then attempt to find the grammar production parameters 2 which maximize the likelihood P(S|2) using the inside-outside algorithm [1], which is an efficient (dynamic programming) instance of the EM algorithm [8] for PCFGs.  Other systems (which have generally been more successful) incorporate a structural search as well, typically using a heuristic to propose candidate grammar modifications which minimize the joint encoding of data and grammar using an MDL criterion, which asserts that a good analysis is a short one, in that the joint encoding of the grammar and the data is compact [6, 16, 18, 17].  These approaches can also be seen as likelihood maximization where the objective function is the a posteriori likelihood of the grammar given the data, and the description length provides a structural prior.  The "compact grammar" aspect of MDL is close to some traditional linguistic argumentation which at times has argued for minimal grammars on grounds of analytical [10] or cognitive [5] economy.  However, the primary weakness of MDL-based systems does not have to do with the objective function, but the search procedures they employ.  Such systems end up growing structures greedily, in a bottom-up fashion.  Therefore, their induction quality is determined by how well they are able to heuristically predict what local intermediate structures will fit into good final global solutions.  A potential advantage of systems which fix the grammar and only perform parameter search is that they do compare complete grammars against each other, and are therefore able to detect which give rise to systematically compatible parses.  However, although early work showed that small, artificial CFGs could be induced with the EM algorithm [12], studies with large natural language grammars have generally suggested that completely unsupervised EM over PCFGs is ineffective for grammar acquisition.  For instance, Carroll and Charniak [2] describe experiments running the EM algorithm from random starting points, which produced widely varying learned grammars, almost all of extremely poor quality.  1 1 We duplicated one of their experiments, which used grammars restricted to rules of the form x ! x y | y x , where there is one category x for each part-of-speech (such a restricted CFG is isomorphic to a dependency grammar).  We began reestimation from a grammar with uniform rewrite
Candidate Model Problems in Software Architecture| we would like to open a discussion about suitable problems: what characteristics they should have, what specific problems would serve us well.  To start that discussion, we present ten candidate problems and sketches of several distinct architectural approaches to two of them.  We invite refinements and discussion of the problem list, the solution sets, and the criteria for choosing problems.  1 Introduction It is common for a discipline, especially one that is just getting its wits about itself, to adopt some shared, welldefined problems for teaching and study.  Often known as model systems or type problems, they provide a way to compare methods and results, work out new techniques on standard examples, and set a minimum standard of capability for new participants.  In time, a reasonable approach to some of these problems becomes the price of admission to get serious consideration of a new technique.  Model problems also provide a pre-debugged source of educational exercises.  Biology, for example, has n Drosophila melanogaster (the fruit fly) n Rattus rattus Norwegicus (the lab rat) n Escherichia coli (the digestive bacterium) Each of these is part of the common language of discourse in the field.  Each provides a familiar concrete instance that illustrates an important set of issues.  This allows discussions to start from shared knowledge of the basic example and proceed expeditiously to the result, theory, or technique of current interest. 
A* Parsing: Fast Exact Viterbi Parse Selection| Abstract We present an extension of the classic A* search procedure to tabular PCFG parsing.  The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions.  We discuss various estimates and give efficient algorithms for computing them.  On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. 
Evaluating strategies for similarity search on the web| ABSTRACT Finding pages on the Web that are similar to a query page (Related Pages) is an important component of modern search engines.  A variety of strategies have been proposed for answering Related Pages queries, but comparative evaluation by user studies is expensive, especially when large strategy spaces must be searched (e. g. , when tuning parameters).  We present a technique for automatically evaluating strategies using Web hierarchies, such as Open Directory, in place of user feedback.  We apply this evaluation methodology to a mix of document representation strategies, including the use of text, anchor-text, and links.  We discuss the relative advantages and disadvantages of the various approaches examined.  Finally, we describe how to eciently construct a similarity index out of our chosen strategies, and provide sample results from our index. 
Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency| Abstract We present a generative model for the unsupervised learning of dependency structures.  We also describe the multiplicative combination of this dependency model with a model of linear constituency.  The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing.  We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 
Computing PageRank using Power Extrapolation| Abstract.  We present a novel technique for speeding up the computation of PageRank, a hyperlink-based estimate of the "importance" of Web pages, based on the ideas presented in [7].  The original PageRank algorithm uses the Power Method to compute successive iterates that converge to the principal eigenvector of the Markov matrix representing the Web link graph.  The algorithm presented here, called Power Extrapolation, accelerates the convergence of the Power Method by subtracting off the error along several nonprincipal eigenvectors from the current iterate of the Power Method, making use of known nonprincipal eigenvalues of the Web hyperlink matrix.  Empirically, we show that using Power Extrapolation speeds up PageRank computation by 30% on a Web graph of 80 million nodes in realistic scenarios over the standard power method, in a way that is simple to understand and implement. 
Conditional Structure versus Conditional Estimation in NLP Models| Abstract This paper separates conditional parameter
Max-Margin Parsing| Abstract We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying
A Generative Constituent-Context Model for Improved Grammar Induction| Abstract We present a generative distributional model for the
Temporal Response Field Characterization by Dynamic Ripple Spectra| Abstract To understand the neural representation of broadband, dynamic sounds in Primary Auditory Cortex (AI), we characterize responses using the Spectro-Temporal Response Field (STRF).  The STRF describes, predicts and fully characterizes the linear dynamics of neurons in response to sounds with rich spectro-temporal envelopes.  It is computed from the responses to elementary `ripples,' a family of sounds with drifting, sinusoidal, spectral envelopes.  The collection of responses to all elementary ripples is the spectro-temporal transfer function.  The complex spectro-temporal envelope of any broadband, dynamic sound can expressed as the linear sum of individual ripples.  Previous experiments using ripples with downward drifting spectra suggested that the transfer function is separable, i. e. , it is reducible into a product of purely temporal and purely spectral functions.  Here we measure the responses to upward and downward drifting ripples, assuming separability within each direction, to determine if the total bi-directional transfer function is fully separable.  In general, the combined transfer function for two directions is not symmetric, and hence units in AI are not, in general, fully separable.  Consequently, many AI units have complex response properties such as sensitivity to direction of motion, though most inseparable units are not strongly directionally selective.  We show that for most neurons the lack of full separability stems from differences between the upward and downward spectral cross-sections, but not from the temporal cross-sections, which places strong constraints on the neural inputs of these AI units. 
Agenda-Based Chart Parser for Arbitrary Probabilistic Context-Free Grammars| Abstract While O(n 3 ) methods for parsing probabilistic context-free grammars (PCFGs) are well known, a tabular parsing framework for arbitrary PCFGs which allows for botton-up, topdown, and other parsing strategies, has not yet been provided.  This paper presents such an algorithm, and shows its correctness and advantages over prior work.  The paper finishes by bringing out the connections between the algorithm and work on hypergraphs, which permits us to extend the presented Viterbi (best parse) algorithm to an inside (total probability) algorithm. 
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network| Abstract We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.  Using these ideas together, the resulting tagger gives a 97. 24% accuracy on the Penn Treebank WSJ, an error reduction of 4. 4% on the best previous single automatically learned tagging result. 
Unsupervised Learning of Field Segmentation Models for Information Extraction| Abstract The applicability of many current information
Ada: a playful interactive space| Abstract: This paper and the accompanying video describe Ada, an immersive 160m 2 space for human-machine interaction.  Ada is conceived as an artificial organism that plays with its visitors, with multi-modal sensors consisting of pressure-sensitive floor tiles, microphones for localising and recognising sounds, and pan-tilt video cameras called gazers.  Her output modalities include a 360video projection screen capable of showing realtime computer graphics combined with multiple live video streams, RGB neon lights in the floor tiles, pan-tilt lights and an automatic neural music composition system.  Visitors provided input to the space primarily by moving around, clapping their hands and speaking.  Ada was presented at the Swiss national exhibition Expo. 02 in Neuchtel from May 15 to October 20, 2002, and attracted 553,700 visitors.  In this paper we give a brief overview of the Ada exhibit, and describe the accompanying video. 
Combining Heterogeneous Classifiers for Word-Sense Disambiguation| Abstract This paper discusses ensembles of simple but heterogeneous classifiers for word-sense disambiguation, examining the Stanford-CS224N system entered in the SENSEVAL-2 English lexical sample task.  First-order classifiers are combined by a
An 2 2 A?# Agenda-Based Chart Parser for Arbitrary Probabilistic Context-Free Grammars| Abstract While ######### methods for parsing probabilistic context-free grammars (PCFGs) are well known, a tabular parsing framework for arbitrary PCFGs which allows for botton-up, topdown, and other parsing strategies, has not yet been provided.  This paper presents such an algorithm, and shows its correctness and advantages over prior work.  The paper finishes by bringing out the connections between the algorithm and work on hypergraphs, which permits us to extend the presented Viterbi (best parse) algorithm to an inside (total probability) algorithm. 
From Instance-level Constraints to Space-Level Constraints: Making the Most of Prior Knowledge in Data Clustering| Abstract We present an improved method for clustering in the presence of very limited supervisory information, given as pairwise instance constraints.  By allowing instance-level constraints to have spacelevel inductive implications, we are able to successfully incorporate constraints for a wide range of data set types.  Our method greatly improves on the previously studied constrained -means algorithm, generally requiring less than half as many constraints to achieve a given accuracy on a range of real-world data, while also being more robust when over-constrained.  We additionally discuss an active learning algorithm which increases the value of constraints even further. 
Interpreting and Extending Classical Agglomerative Clustering Algorithms using a Model-Based approach| Abstract We present two results which arise from a model-based approach to hierarchical agglomerative clustering.  First, we show formally that the common heuristic agglomerative clustering algorithms -- Ward's method, single-link, complete-link, and a variant of group-average -are each equivalent to a hierarchical model-based method.  This interpretation gives a theoretical explanation of the empirical behavior of these algorithms, as well as a principled approach to resolving practical issues, such as number of clusters or the choice of method.  Second, we show how a model-based viewpoint can suggest variations on these basic agglomerative algorithms.  We introduce adjusted complete-link, Mahalanobis-link, and line-link as variants, and demonstrate their utility. 
Accurate Unlexicalized Parsing| Abstract We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.  Indeed, its performance of 86. 36% (LP/LR F 1 ) is better than that of early
Spectral Learning| Abstract We present a simple, easily implemented spectral learning algorithm which applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples.  In the unsupervised case, it performs consistently with other spectral clustering algorithms.  In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the 20 Newsgroups data set.  Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data.  By using normalized affinity matrices which are both symmetric and stochastic, we also obtain both a probabilistic interpretation of our method and certain guarantees of performance. 
Fast Exact Inference with a Factored Model for Natural Language Parsing| Abstract We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models.  This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models.  Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efficient, exact inference. 
Accuracte unlexicalized parsing|
Parsing and Hypergraphs|
Varieties of off-line simulation|
Society, energy, and injury: Inevitable triad? In Research directions toward the reduction of injury|
The Use of Pseudo-Satellites for Improving GPS Performance", Global Positioning System,|
Carbon/nutrient balance of boreal plants in relation to vertebrate herbivory|
The Unsupervised Learning of Natural Language Structure|
"Candidate model problems in software architecture," Computer Science Department,|
An O(n 3 ) agenda-based chart parser for arbitrary probabilistic context-free grammars|
Candidate Model Problems in Software Architecture", Discussion draft 1|3 in circulation for development of community consensus,. 
Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex|
Left putanimal activation when speaking a second language: Evidence from PET| Neuroreport: An international journal for the rapid communication of research. 
The neural substrates underlying word generation: a bilingual functional-imaging study|
Optimization, Maxent Models, and Conditional Estimation without Magic|
Quality and safety assurance: How voluntary social processes remedy their own shortcomings|
Discrete right hand side parametrization for linear integer programs,|
Succumbing to the dark side of the force: The Internet as seen from an adult web site|
Absence of first-order phase transitions for antiferromagnetic Systems,|
Representation of dynamic broadband spectra in auditory cortex|
Representation of dynamic complex spectra in primary auditory cortex|
Three methods for postoptimal analysis in integer linear programming|
Discrete right hand side parameterization for linear integer programs|
Integer programming postoptimal analysis with cutting planes|
Firm characteristics and the presence of event risk covenants in bond indentures|
Accurate Unlexicalized Parsing Proceedings of the 41st Annual Meeting of the|
Similarity search on the Web: Evaluation and scalability considerations|
2003a| A* parsing: Fast exact viterbi parse selection. 
Combining heterogeneous classifiers for word-sense disambiguation|
A discriminative matching approach to word alignment|
Accurate unlexicalised parsing|
Convergence of hamiltonian systems to billiards|
A generative constituentcontext model for grammar induction|

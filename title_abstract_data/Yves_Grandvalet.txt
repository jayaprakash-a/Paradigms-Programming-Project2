Semi-supervised Learning by Entropy Minimization| Abstract We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data.  In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning.  Our approach includes other approaches to the semi-supervised problem as particular or limiting cases.  A series of experiments illustrates that the proposed solutions benefit from unlabeled data.  The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model.  The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the "cluster assumption".  Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces. 
P'enalisation multiple adaptative| R' esum' e.  Nous proposons un nouvel algorithme de r'egression, la p'enalisation multiple
Bagging Down-Weights Leverage Points| Abstract Bagging is a procedure averaging estimators trained on bootstrap samples.  Numerous experiments have shown that bagged estimates often yield better results than the original predictor, and several explanations have been given to account for this gain.  However, six years from its introduction, bagging is still not fully understood.  Most explanations given until now are based on global properties of the estimates.  Here, we focus on the local effects on leverage points, i. e.  on observations whose fitted values are largely determined by the corresponding response values.  These points are shown experimentally to be down-weighted by bagging.  The performance of the bagged estimate depends on the goodness of these points for the original estimator.  Illustrative examples findings are supported by the study of smoothing matrix, and their consequences are discussed. 
Bagging Can Stabilize without Reducing Variance| Abstract.  Bagging is a procedure averaging estimators trained on bootstrap samples.  Numerous experiments have shown that bagged estimates almost consistently yield better results than the original predictor.  It is thus important to understand the reasons for this success, and also for the occasional failures.  Several arguments have been given to explain the effectiveness of bagging, among which the original \bagging reduces variance by averaging" is widely accepted.  This paper provides experimental evidence supporting another explanation, based on the stabilization provided by spreading the influence of examples.  With this viewpoint, bagging is interpreted as a case-weight perturbation technique, and its behavior can be explained when other arguments fail. 
DIAGNOSIS OF GLASS QUALITY IN A MANUFACTURING PROCESS: TWO CONNECTIONIST SOLUTIONS| Abstract.  We describe and compare two kinds of connexionist solutions for predicting the quality rating of glass according to chemical inlet of the manufacturing process.  We show that according to the approach used to select a predictor in the set of Multilayered perceptrons, the results may vary dramatically.  We also show some practical limitations of prediction error estimates based on resampling techniques. 
Boosting Mixture Models for Semi-supervised Learning| Abstract.  This paper introduces MixtBoost, a variant of AdaBoost dedicated to solve problems in which both labeled and unlabeled data are available.  We propose several definitions of loss for unlabeled data, from which margins are defined.  The resulting boosting schemes implement mixture models as base classifiers.  Preliminary experiments are analyzed and the relevance of loss choices is discussed.  MixtBoost improves on both mixture models and AdaBoost provided classes are structured, and is otherwise similar to AdaBoost. 
Adaptive Scaling for Feature Selection in SVMs| Abstract This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines.  Relevance is measured by scale factors defining the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables.  The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters defining the classifier.  Feature selection is achieved by constraints encouraging the sparsity of scale factors.  The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem. 
Noise Injection: Theoretical Prospects| Abstract Noise Injection consists in adding noise to the inputs during neural network training.  Experimental results suggest
No Unbiased Estimator of the Variance of K-Fold Cross-Validation| In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates.  This paper studies the very commonly used K-fold cross-validation estimator of generalization performance.  The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation.  The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance.  This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance.  This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied. 
Logistic regression for partial labels| Abstract This paper discusses learning from partially labeled data in the framework of probabilistic supervised classi#cation.  Minimum commitment logistic regression is a conservative solution to the problem of imprecise labels, which should be appropriate if the faithful estimation of posterior probabilities is an issue.  Semi-supervised learning is among the problems considered, and a series of experiments shows that our second proposal, self-consistent logistic regression is a serious contender to more classical solutions involving generative models. 
Bagging belief structures in Dempster-Shafer K-NN rule| Abstract This paper introduces bagging in the evidence-theoretic K-nearest neighbor rule (K-NN).  It is known that bagging decreases the variability of classifiers, so the main idea here is to build stable belief structures associated to query samples, before decisions are made.  In order to compare bagged K-NN with the classical algorithm in a precisely controlled environment, data sets were generated according to known distributions.  Results show that bagging improves classi#cation, especially for ambiguous cases and outliers.  Moreover, bagged belief structures give pignistic probabilities closer to the a posteriori class probabilities than the original method. 
Adaptive Noise Injection for Input Variables Relevance Determination| Abstract.  In this paper we consider the application of training with noise in multi-layer perceptron to input variables relevance determination.  Noise injection is modified in order to penalize irrelevant features.  The proposed algorithm is attractive as it requires the tuning of a single parameter.  This parameter controls the penalization of the inputs together with the complexity of the model.  After the presentation of the method, experimental evidences are given on simulated data sets. 
Least Absolute Shrinkage is Equivalent to Quadratic Penalization| Abstract Adaptive ridge is a special form of ridge regression, balancing the quadratic
Resample and Combine: An Approach to Improving Uncertainty Representation in Evidential Pattern Classification| Abstract Uncertainty representation is a major issue in pattern recognition.  In many applications, the outputs of a classifier do not lead directly to a final decision, but are used in combination with other systems, or as input to an interactive decision process.  In such contexts, it may be advantageous to resort to rich and flexible formalisms for representing and manipulating uncertain information.  This paper addresses the issue of uncertainty representation in pattern classification, in the framework of the DempsterShafer theory of evidence.  It is shown that the quality and reliability of the outputs of a classifier may be improved using a variant of bagging, a resample-and-combine approach introduced by Breiman in a conventional statistical context.  This technique is explained and studied experimentally on simulated data and on a character recognition application.  In particular, results show that bagging improves classification accuracy and limits the influence of outliers and ambiguous training patterns. 
Svm and kernel methods matlab toolbox|
Prediction of ozone peaks by mixture model|
Semi-supervised learning by entropy minimization|
Semi supervised margin boost|
Software sensor design based on empirical data,|
Black-box software sensor design for environmental monitoring|
Effets de l'injection de bruit sur les perceptrons multicouches|
Least absolute shrinkage is equivalent to quadratic penalisation|
A comment on noise injection into inputs in back-propagation learning|
Bagging equalizes influence|
Outcomes of the Equivalence of Adaptive Ridge with Least Absolute Shrinkage|

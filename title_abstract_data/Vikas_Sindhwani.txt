Feature Selection in MLPs and SVMs based on Maximum Output Information| Abstract--- We present feature selection algorithms for multilayer Perceptrons (MLPs) and multi-class Support Vector Machines (SVMs), using mutual information between class labels and classifier outputs, as an objective function.  This objective function involves inexpensive computation of information measures only on discrete variables; provides immunity to prior class probabilities; and brackets the probability of error of the classifier.  The Maximum Output Information (MOI) algorithms employ this function for feature subset selection by greedy elimination and directed search.  The output of the MOI algorithms is a feature subset of user-defined size and an associated trained classifier(MLP/SVM).  These algorithms compare favorably with a number of other methods in terms of performance on various artificial and real-world data sets. 
Linear Manifold Regularization for Large Scale Semi-supervised Learning| Abstract The enormous wealth of unlabeled data in many applications of machine learning is beginning to pose challenges to the designers of semi-supervised learning methods.  We are interested in developing linear classification algorithms to efficiently learn from massive partially labeled datasets.  In this paper,
Information Theoretic Feature Crediting in Multiclass Support Vector Machines| Empirical results on a number of datasets suggest e$cient applicability to data with a very large number of features. 
Manifold Regularization: A Geometric Framework for Learning from Examples| Abstract We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution.  We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. 
Beyond the point cloud: from Transductive to Semi-supervised Learning Under submission,|

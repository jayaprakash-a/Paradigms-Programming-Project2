Limits of Spectral Clustering| Abstract An important aspect of clustering algorithms is whether the partitions constructed on finite samples converge to a useful clustering of the whole data space as the sample size increases.  This paper investigates this question for normalized and unnormalized versions of the popular spectral clustering algorithm.  Surprisingly, the convergence of unnormalized spectral clustering is more difficult to handle than the normalized case.  Even though recently some first results on the convergence of normalized spectral clustering have been obtained, for the unnormalized case we have to develop a completely new approach combining tools from numerical integration, spectral and perturbation theory, and probability.  It turns out that while in the normalized case, spectral clustering usually converges to a nice partition of the data space, in the unnormalized case the same only holds under strong additional assumptions which are not always satisfied.  We conclude that our analysis gives strong evidence for the superiority of normalized spectral clustering.  It also provides a basis for future exploration of other Laplacian-based methods. 
Tracking a Small Set of Experts by Mixing Past Posteriors| Abstract In this paper, we examine on-line learning problems in which the target concept is allowed to change over time.  In each trial a master algorithm receives predictions from a large set of n experts.  Its goal is to predict almost as well as the best sequence of such experts chosen o-line by partitioning the training sequence into k + 1 sections and then choosing the best expert for each section.  We build on methods developed by Herbster and Warmuth and consider an open problem posed by Freund where the experts in the best partition are from a small pool of size m.  Since k m, the best expert shifts back and forth between the experts of the small pool.  We propose algorithms that solve this open problem by mixing the past posteriors maintained by the master algorithm.  We relate the number of bits needed for encoding the best partition to the loss bounds of the algorithms.  Instead of paying log n for choosing the best expert in each section we first pay log n m bits in the bounds for identifying the pool of m experts and then log m bits per new section.  In the bounds we also pay twice for encoding the boundaries of the sections. 
New Approaches to Statistical Learning Theory| Abstract.  We present new tools from probability theory that can be applied to the analysis of learning algorithms.  These tools allow to derive new bounds on the generalization performance of learning algorithms and to propose alternative measures of the complexity of the learning task, which in turn can be used to derive new learning algorithms. 
PAC-Bayesian Generic Chaining| Abstract There exist many different generalization error bounds for classification.  Each of these bounds contains an improvement over the others for certain situations.  Our goal is to combine these different improvements into a single bound.  In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classifiers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2].  This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classifiers, and such priors also arise in the PAC-bayesian setting. 
Moment inequalities for functions of independent random variables| Abstract A general method for obtaining moment inequalities for functions of independent random variables is presented.  It is a generalization of the entropy method which has been used to derive concentration inequalities for such functions [7], and is based on a generalized tensorization inequality due to Latala and Oleszkiewicz [25].  The new inequalities prove to be a versatile tool in a wide range of applications.  We illustrate the power of the method by showing how it can be used to effortlessly re-derive classical inequalities including Rosenthal and Kahane-Khinchine-type inequalities for sums of independent random variables, moment inequalities for suprema of empirical processes, and moment inequalities for Rademacher chaos and U-statistics.  Some of these corollaries are apparently new.  In particular, we generalize Talagrand's exponential inequality for Rademacher chaos of order two to any order.  We also discuss applications for other complex functions
A Protection Scheme for a CORBA Environment| A prototype has been implemented on top of the Orbix system.  Preliminary experiments with simple distributed applications have shown the feasibility and the advantage of this method. 
Spatial Learning and Localization in Rodents: A Computational Model of the Hippocampus and its Implications for Mobile Robots| Abstract The ability to acquire a representation of the spatial environment and the ability to localize within it are essential for successful navigation in a-priori unknown environments.  The hippocampal formation is believed to play a key role in spatial learning and localization in animals in general and rodents in particular.  This paper briefly reviews the relevant neurobiological and cognitive data, and their relation to computational models of spatial learning and localization used in contemporary mobile robots.  It proposes a hippocampal model of spatial learning and localization, and characterizes it using a Kalman filter based tool for information fusion from multiple uncertain sources.  The resulting model not only explains neurobiological and behavioral data from rodent experiments, but also allows a robot to learn a place-based metric representation of space and to localize itself in a stochastically optimal manner.  The paper presents an algorithmic implementation of the model and results of several experiments that demonstrate its capabilities.  These include the ability to disambiguate perceptually similar places, scale well with increasing errors, and the automatic acquisition of spatial information at multiple resolutions. 
ESAIM: Probability and Statistics Will be set by the publisher URL:| Abstract.  The last few years have witnessed important new developments in the theory and practice of pattern classification.  We intend to survey some of the main new ideas that have lead to these important recent developments.  Resume.  Durant ces dernires annees, la theorie et la pratique de la reconnaissance des formes ont ete marquees par des developpements originaux.  Ce survol presente certaines des principales idees novatrices qui ont conduit `a ces developpements importants. 
Ranking on Data Manifolds| Abstract The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks.  Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data.  The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data.  Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method. 
Measure Based Regularization| Abstract We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm.  We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms.  We also propose practical implementations. 
On the Complexity of Learning the Kernel Matrix| Abstract We investigate data based procedures for selecting the kernel when learning with Support Vector Machines.  We provide generalization error bounds by estimating the Rademacher complexities of the corresponding function classes.  In particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors, i. e. , we allow to vary the spectrum and keep the eigenvectors fix.  This bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel.  However, optimizing the margin over such classes leads to overfitting.  We thus propose a suitable way of constraining the class.  We use an efficient algorithm to solve the resulting optimization problem, present preliminary experimental results, and compare them to an alignment-based approach. 
Some Local Measures of Complexity of Convex Hulls and Generalization Bounds| Abstract.  We investigate measures of complexity of function classes based on continuity moduli of Gaussian and Rademacher processes.  For Gaussian processes, we obtain bounds on the continuity modulus on the convex hull of a function class in terms of the same quantity for the class itself.  We also obtain new bounds on generalization error in terms of localized Rademacher complexities.  This allows us to prove new results about generalization performance for convex hulls in terms of characteristics of the base class.  As a byproduct, we obtain a simple proof of some of the known bounds on the entropy of convex hulls. 
Choosing Multiple Parameters for Support Vector Machines| Abstract The problem of automatically tuning multiple parameters for pattern recognition Support Vector Machines (
Local Rademacher Complexities| Abstract We propose new bounds on the error of learning algorithms in terms of a data-dependent notion of complexity.  The estimates we establish give optimal rates and are based on a local and empirical version of Rademacher averages, in the sense that the Rademacher averages are computed from the data, on a subset of functions with small empirical error.  We present some applications to classification and prediction with convex function classes, and with kernel classes in particular. 
Stability and Generalization|
Algorithmic Stability and Generalization Performance|
Feature selection and transduction for prediction of molecular bioactivity for drug design|
Introduction to Statistical Learning Theory|
Maximal Margin Classification for Metric Spaces|
Concentration Inequalities|
On the Convergence of Spectral Clustering on Random Samples: The Normalized Case|
Localized Rademacher complexities| To appear,. 
Distance-Based Classification with Lipschitz Functions|

Semi-supervised Protein Classification Using Cluster Kernels| Abstract A key issue in supervised protein classification is the representation of input sequences of amino acids.  Recent work using string kernels for protein data has achieved state-of-the-art classification performance.  However, such representations are based only on labeled data --- examples with known 3D structures, organized into structural classes --- while in practice, unlabeled data is far more plentiful.  In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences.  We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data.  We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efficiency. 
Prediction on Spike Data Using Kernel Algorithms| Abstract We report and compare the performance of different learning algorithms based on data from cortical recordings.  The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons.  We compare several ways of improving the coding of the input (i. e. , the spike data) as well as of the output (i. e. , the orientation), and report the results obtained using different kernel algorithms. 
Feature Selection for SVMs| Abstract We introduce a method of feature selection for Support Vector Machines.  The method is based upon finding those features which minimize bounds on the leave-one-out error.  This search can be efficiently performed via gradient descent.  The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA microarray data. 
Mismatch string kernels for discriminative protein classification| Abstract We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem.  These kernels measure sequence similarity based on shared occurrences of k-length subsequences, counted with up to m mismatches, and do not rely on any generative model for the positive training sequences.  We compute the kernels efficiently using a mismatch tree data structure and report experiments on a benchmark SCOP dataset, where we show that the mismatch kernel used with an SVM classifier performs as well as the Fisher kernel, the most successful method for remote homology detection, while achieving considerable computational savings. 
Support Vector Regression with ANOVA Decomposition Kernels| Abstract Support Vector Machines using ANOVA Decomposition Kernels (SVAD) [Vapng] are a way of imposing a structure on multi-dimensional kernels which are generated as the tensor product of one-dimensional kernels.  This gives more accurate control over the capacity of the learning machine (VCdimension).  SVAD uses ideas from ANOVA decomposition methods and extends them to generate kernels which directly implement these ideas.  SVAD is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel.  The Boston housing data set from UCI has been tested on Bagging [Bre94] and Support Vector methods before [DBK + 97] and these results are compared to the SVAD method. 
Adaptive Margin Support Vector Machines for Classification| Abstract In this paper we propose a new learning algorithm for classification learning based on the Support Vector Machine (SVM) approach.  Existing approaches for constructing SVMs [12] are based on minimization of a regularized margin loss where the margin is treated equivalently for each training pattern.  We propose a reformulation of the minimization problem such that adaptive margins for each training pattern are utilized, which we call the Adaptive Margin (AM--) SVM.  We give bounds on the generalization error of AM--SVMs which justify their robustness against outliers, and show experimentally that the generalization error of AM--SVMs is comparable to classical SVMs on benchmark datasets from the UCI repository. 
Ranking on Data Manifolds| Abstract The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks.  Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data.  The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data.  Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method. 
A kernel method for multi-labelled classification| Abstract This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems.  Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7].  We explore a new direct approach.  It is based on a large margin ranking system that shares a lot of common properties with SVMs.  We tested it on a Yeast gene functional classification problem with positive results. 
Dealing with Large Diagonals in Kernel Matrices| Abstract In kernel methods, all the information about the training data is contained in the Gram matrix.  If this matrix has large diagonal values, which arises for many types of kernels, then kernel methods do not perform well.  We propose and test several methods for dealing with this problem by reducing the dynamic range of the matrix while preserving the positive definiteness of the Hessian of the quadratic programming problem that one has to solve when training a Support Vector Machine, which is a common kernel approach for pattern recognition. 
Learning to Find Pre-Images| Abstract We consider the problem of reconstructing patterns from a feature map.  Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS.  We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images.  The introduced technique avoids difficult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 
Breaking SVM Complexity with Cross-Training| Abstract We propose to selectively remove examples from the training set using probabilistic estimates related to editing algorithms (Devijver and Kittler, 1982).  This heuristic procedure aims at creating a separable distribution of training examples with minimal impact on the position of the decision boundary.  It breaks the linear dependency between the number of SVs and the number of training examples, and sharply reduces the complexity of SVMs during both the training and prediction stages. 
Learning Gene Functional Classifications from Multiple Data Types| Abstract In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data.  We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons.  We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task.  Our results suggest the importance of exploiting prior information about the heterogeneity of the data.  In particular, we propose an SVM kernel function that is explicitly heterogeneous.  In addition, we describe feature scaling methods for further exploiting prior knowledge of heterogeneity by giving each data type different weights. 
Cluster Kernels for Semi-Supervised Learning| Abstract We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label.  This is achieved by modifying the eigenspectrum of the kernel matrix.  Experimental results assess the validity of this approach. 
Kernel Dependency Estimation| Abstract We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects.  The objects can be for example: vectors, images, strings, trees or graphs.  Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces.  We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images. 
Gene functional classification from heterogeneous data| Abstract In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data.  We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons.  We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task.  Our results suggest the importance of exploiting prior information about the heterogeneity of the data.  In particular, we propose an SVM kernel function that is explicitly heterogeneous.  We also show how to use knowledge about heterogeneity to aid in feature selection. 
Multi-class Support Vector Machines|
Fisher discriminant analysis with kernels",|
Support vector channel selection in BCI|
Support vector machine reference manual|
Density estimation using Support Vector Machines|
Support vector machines for multiclass pattern recognition|
Gene Selection for Cancer Classification using Support Vector Machines|
Adaptive margin support vector machines,|
Support vector density estimation,|
Leave-on-out support vector machines|
Invariant Feature Extraction and Classification in Kernel Spaces|
Determination of fluoxetine (Prozac) and norfluoxetine in aquatic environments|
Density estimation using sv machines|
Theory of Support Vector Machine",|
A kernel approach for learning from almost orthogonal patterns|
Statistical Learning and Kernel Methods in Bioinformatics,|
Kernel methods for multi-labelled classification and categorical regression problems|
Advances in Neural Information Processing Systems 12, chapter Transductive inference for estimating values of functions|
Gene functional classification from heterogenous data|
Feature selection and transduction for prediction of molecular bioactivity for drug design|
Feature selection fof svms|
Spider: object-orientated machine learning library|
Feature selection for SVMs| NIPS 13. 
Dealing with Large Diagonals in Kernel Matrices,|
\Multi-class support vector machines," tech| rep. ,. 
Cluster kernels for semi-supervised protein classification|
Online on an even tighter budget|
Artificial neural networks and support vector machines|
Semi-supervised learning by maximizing smoothness|
Extension of the -svm range for classification|
Transductive Inference for Estimating Values of Functions|
Extensions to the support vector method|
The use of zero-norm with linear models and kernel methods|
Online (and offline) on an even tighter budget|
Advanced in Kernel methods: Support Vector Learning| Chap.  Support vector regression with ANOVA decomposition kernels. 
Leave-One-Out Support Vector Machines|
Use of the ` 0 -norm with linear models and kernel methods|
Constructing Descriptive and Discriminative Nonlinear Features: Rayleigh Coefficients in Kernel Feature Spaces|

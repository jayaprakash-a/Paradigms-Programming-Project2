Metric Learning by Collapsing Classes| Abstract We present an algorithm for learning a quadratic Gaussian metric (Mahalanobis distance) for use in classification tasks.  Our method relies on the simple geometric intuition that a good metric is one under which points in the same class are simultaneously near each other and far from points in different classes.  We construct a convex optimization problem which generates such metrics by trying to collapse all examples in the same class to a single point and push examples in other classes infinitely far away.  We show that when the metric we learn is used in a simple classifier, it yields substantial improvements over standard alternatives on a variety of problems; we also discuss how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space allowing more efficient classification with very little reduction in performance. 
Sucient Dimensionality Reduction - A novel analysis principle| Abstract Data dimensionality reduction of empirical co-occurrence data is one of the most fundamental problems in unsupervised learning and complex data analysis.  One principled approach to this problem is to represent the data in low dimension with minimal loss of information contained in the original data.  In this paper we present a novel information theoretic principle and algorithm for extracting low dimensional representations, or feature-vectors, that approximately preserve the information in one variable about another.  Unlike previous work in this direction, here we do not cluster or quantize the variables, but rather extract continuous feature functions directly from the co-occurrence matrix, using a converging iterative projection algorithm.  Our approach is both simpler and more general than clustering techniques and proves effective in generating a very small feature set for document categorization. 
Sufficient Dimensionality Reduction| Abstract Dimensionality reduction of empirical co-occurrence data is a fundamental problem in unsupervised learning.  It is also a well studied problem in statistics known as the analysis of cross-classified data.  One principled approach to this problem is to represent the data in low dimension with minimal loss of (mutual) information contained in the original data.  In this paper we introduce an information theoretic nonlinear method for finding such a most informative dimension reduction.  In contrast with previously introduced clustering based approaches, here we extract continuous feature functions directly from the co-occurrence matrix.  In a sense, we automatically extract functions of the variables that serve as approximate sufficient statistics for a sample of one variable about the other one.  Our method is different from dimensionality reduction methods which are based on a specific, sometimes arbitrary, metric or embedding.  Another interpretation of our method is as generalized - multi-dimensional - non-linear regression, where rather than fitting one regression function through two dimensional data, we extract d-regression functions whose expectation values capture the information among the variables.  It thus presents a new learning paradigm that unifies aspects from both supervised and unsupervised learning.  The resulting dimension reduction can be described by two conjugate d-dimensional differential manifolds that are coupled through Maximum Entropy I-projections.  The Riemannian
Information Bottleneck for Gaussian Variables| Abstract The problem of extracting the relevant aspects of data was addressed through the information bottleneck (IB) method, by (soft) clustering one variable while preserving information about another - relevance - variable.  An interesting question addressed in the current work is the extension of these ideas to obtain continuous representations (embeddings) that preserve relevant information, rather than discrete clusters.  We give a formal definition of the general continuous IB problem and obtain an analytic solution for the optimal representation for the important case of multivariate Gaussian variables.  The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized correlation matrix # xjy # 1 x , which is also the basis obtained in Canonical Correlation Analysis.  However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector.  This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level.  Our analysis also provides analytic expression for the optimal tradeoff - the information curve - in terms of the eigenvalue spectrum. 
Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway| Abstract The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach.  Identifying the case of stimulus-conditioned independent neurons, we develop redundancy measures that allow enhanced information estimation for groups of neurons.  These measures are then applied to study the collaborative coding eciency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (A1).  Under two different coding paradigms we show differences in both information content and group redundancies between IC and cortical auditory neurons.  These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized by Barlow (1959).  The redundancy effects under the single-spikes coding paradigm are significant only for groups larger than ten cells, and cannot be revealed with the standard redundancy measures that use only pairs of cells.  Our results suggest that redundancy reduction transformations are not limited to low level sensory processing (aimed to reduce redundancy in input statistics) but are applied even at cortical sensory stations. 
Suffificient dimensionality redcution with irrelevence statistics|
Distributional clustering of movements based on neural responses|
Information bottleneck and linear projections of gaussian processes|
Cholinergic stimulation modulates apoptosis and differentiation of murine thymocytes via a nicotinic effect on thymic epithelium|

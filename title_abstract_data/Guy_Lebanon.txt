Hyperplane margin classifiers on the multinomial manifold| Abstract The assumptions behind linear classifiers for categorical data are examined and reformulated in the context of the multinomial manifold, the simplex of multinomial models furnished with the Riemannian structure induced by the Fisher information.  This leads to a new view of hyperplane classifiers which, together with a generalized margin concept, shows how to adapt existing margin-based hyperplane models to multinomial geometry.  Experiments show the new classification framework to be effective for text classification, where the categorical structure of the data is modeled naturally within the multinomial family. 
Boosting and Maximum Likelihood for Exponential Models| Abstract We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels.  In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood.  Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression. 
Learning Riemannian Metrics| Abstract We consider the problem of learning a Riemannian metric associated with a given differentiable manifold and a set of points.  Our approach to the problem involves choosing a metric from a parametric family that is based on maximizing the inverse volume of a given dataset of points.  From a statistical perspective, it is related to maximum likelihood under a model that assigns probabilities inversely proportional to the Riemannian volume element.  We discuss in detail learning a metric on the multinomial simplex where the metric candidates are pull-back metrics of the Fisher information under a continuous group of transformations.  When applied to documents, the resulting geodesics resemble, but outperform, the TFIDF cosine similarity measure in classification. 
Cranking: Combining Rankings Using Conditional Probability Models on Permutations| Abstract A new approach to ensemble learning is introduced that takes ranking rather than classification as fundamental, leading to models on the symmetric group and its cosets.  The approach uses a generalization of the Mallows model on permutations to combine multiple input rankings.  Applications include the task of combining the output of multiple search engines and multiclass or multilabel classification, where a set of input classifiers is viewed as generating a ranking of class labels.  Experiments for both types of applications are presented. 
Diusion Kernels on Statistical Manifolds| Abstract A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models.  The kernels are based on the heat equation on the Riemannian manifold defined by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space.  As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data.  Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds.  Experimental results are presented for document classification, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classification. 
Information Diffusion Kernels| Abstract A new family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models.  Based on the heat equation on the Riemannian manifold defined by the Fisher information metric, information diffusion kernels generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning.  As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data.  Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for text classification. 
A Variational Approach to Moir'e Pattern Synthesis| Abstract Moir'e phenomena occur when two or more images are nonlinearly combined to create a new "superposition image".  Moir'e patterns are patterns that don't exist in any of the original images but appear in the superposition image as the result of a multiplicative superposition rule.  The topic of moir'e pattern synthesis deals with creating images that, when superimposed, will reveal certain desired moir'e patterns.  Conditions ensuring that a desired moir'e pattern will be present in the superposition of two images are known, however they do not specify these images uniquely.  The freedom in choosing the superimposed images can be exploited to produce various degrees of visibility and ensure desired properties.  Performance criteria for the images that measure when one superposition is better than another are introduced.  These criteria are based on the visibility of the moir'e patterns to the human visual system and on the digitization which takes place when presenting the images on discrete displays.  We here propose to resolve the freedom in moir'e synthesis by choosing the images that optimize the chosen criteria. 
Combining rankings using conditional probability models on permutations|
Boosting and ML for exponential models|
Information diusion kernels| Advances in Neural Information Processing Systems 15. 

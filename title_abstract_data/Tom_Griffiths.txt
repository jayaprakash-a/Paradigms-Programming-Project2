Infinite Latent Feature Models and the Indian Buffet Process| Abstract We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns.  This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features.  We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process.  We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset. 
Discovering Latent Classes in Relational Data| We present a framework for learning abstract relational knowledge with the aim of explaining how people acquire intuitive theories of physical, biological, or social systems.  Our approach is based on a generative relational model with latent classes, and simultaneously determines the kinds of entities that exist in a domain, the number of these latent classes, and the relations between classes that are possible or likely.  This model goes beyond previous psychological models of category learning, which consider attributes associated with individual categories but not relationships between categories.  We apply this domain-general framework to two specific problems: learning the structure of kinship systems and learning causal theories. 
A probabilistic approach to semantic representation| Abstract Semantic networks produced from human data have statistical properties that cannot be easily captured by spatial representations.  We explore a
Teacakes, Trains, Taxicabs and Toxins: A Bayesian Account of Predicting the Future| Abstract This paper explores how people make predictions about the future.  Statistical approaches to predicting the future are discussed, focussing on the method for predicting the future suggested by J.  R.  Gott (1993).  A generalized Bayesian form of Gott's method is presented, and a specific psychological model suggested.  Three experiments show that the predictions people make about the future are consistent with a Bayesian approach.  Despite the difficulty of predicting the future, people happily do it every day.  We are confident about being able to predict the durations of events, how much time we will need to get home after work, and how long it will take to finish the shopping.  In many cases we have a great deal of information guiding our judgments.  However, sometimes we have to make predictions based upon much less evidence.  When faced with new situations our decisions about how much longer we can expect events to last are based on whatever evidence is available.  When the only information we possess concerns how long a particular event has lasted until now, predicting the future becomes a task of induction.  In this paper we explore the question of how people predict the future when told only about the past.  We examine a simple statistical method of predicting the future, and consider how such a method could be made sufficiently flexible to be useful in everyday situations.  The resulting Bayesian model makes strong predictions about the effects of providing further information, the symmetry of this form of reasoning, and how it should be affected by prior knowledge.  We test these predictions empirically.  The Copernican Anthropic Principle A simple solution to the problem of predicting the future was recently proposed by the cosmologist J.  Richard Gott III (1993).  Gott's method is founded upon what he calls the "Copernican anthropic principle", which holds that .  .  .  the location of your birth in space and time in the Universe is priveleged (or special) only to the extent implied by the fact that you are an intelligent observer, that your location among intelligent observers is not special but rather picked at random (1993, p.  316) Gott extends this principle to reasoning about our position in time -- given no evidence to the contrary, we should not assume that we are in a "special" place in time.  This means that the time at which an observer encounters a phenomenon should be randomly located in the total duration of that phenomenon.  Denoting the time since the start of a phenomenon t past , and its total duration t total , Gott forms what he terms the "delta t argument".  Define the ratio r = t past t total (1) and assume that this is a random number between 0 and 1.  It is possible to form probabilistic predictions about the value of r.  For example, r will be between 0. 025 and 0. 975 with a probability P = 0:95, meaning that 1 39 t past ! t future ! 39t past (2) with 95% confidence, where t future = t total\Gamma t past .  Similarly, r will be less than 0. 5 with probability P = 0:5, so t past ! t future with 50% confidence.  This method of reasoning has been used to predict a wide range of phenomena.  Gott (1993) tells of his visit to the Berlin Wall in 1969 (t past = 8 years).  Assuming that his visit was randomly located in the period of the wall's existence, the 95% confidence interval for t future would be 2. 46 months to 312 years.  The wall fell 20 years later, consistent with these predictions.  Gott made similar calculations of t future for Stonehenge, the journal Nature, the U. S. S. R. , and even the human race.  Subsequent targets of the principle have included Broadway musicals and the Conservative government in Britain (Landsberg, Dewynne,
Prediction and Semantic Association| Abstract We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context.  We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language. 
Generalization, Similarity, and Bayesian Inference Behavioral and Brain Sciences| a commentary unless you have received the hard copy, invitation, instructions and deadline information.  For information on becoming a commentator on this or other BBS target articles, write to: bbs@soton. ac. uk For information about subscribing or purchasing offprints of the published version, with commentaries and author's response, write to: journals_subscriptions@cup. org (North America) or journals_marketing@cup. cam. ac. uk (All other countries). 
Using Physical Theories to Infer Hidden Causal Structure| Everyday reasoning draws on notions that go far beyond the observable world, just as modern science draws upon theoretical constructs beyond the limits of measurement.  The richness of our naive theories is a direct result of our ability to postulate hidden causal structure.  This capacity to reason about unobserved causes forms an essential part of cognition from early in life, whether we are reasoning about the forces involved in physical systems (e. g. , Shultz, 1982), the mental states of others (e. g. , Perner, 1991), or the essential properties of natural kinds (e. g. , Gelman & Wellman, 1991).  The central role of hidden causes in naive theories makes the question of how people infer hidden causal structure fundamental to understanding human reasoning.  Psychological research has shown that people can infer the existence of hidden causes from otherwise unexplained events (Ahn & Luhmann, 2003), and determine hidden causal structure from very little data (Kushnir, Gopnik, Schulz, & Danks, 2003).  This work has parallels in computer science, where the development of a formalism for reasoning about causality -- causal graphical models -- has led to algorithms that use patterns of dependency to identify causal relationships (Pearl, 2000; Spirtes, Glymour, & Scheines, 1993).  It has recently been proposed, chiefly by Gopnik, Glymour, and their colleagues (Glymour, 2001; Gopnik, Glymour, Sobel, Schulz, Kushnir, & Danks, 2004), that these algorithms may also explain how people infer causal structure.  A fundamental issue in explaining how people infer causal relationships is accounting for the interaction between abstract causal knowledge and statistical inference.  The classic debate between approaches that emphasize cause-effect covariation and those that emphasize mechanism knowledge (e. g. , Newsome, 2003) turns on this issue.  Causal graphical models provide a language in which the problem of causal induction can be formally expressed.  However, conventional algorithms for inducing causal structure (e. g. , Pearl, 2000; Spirtes et al. , 1993) do not provide a satisfying account of either the roles of causal knowledge or statistical inference, or their interaction.  These algorithms use tests of statistical independence to establish constraints that must be satisfied by causal structures consistent with the observed data.  No knowledge of how causal mechanisms operate, or the functional form of relationships between cause and effect, enters into the inference process.  As we argue below, such knowledge is necessary to explain how people are able to infer causal structure from very small samples, and to infer hidden causes from purely observational data.  Constraint-based methods are also unable to explain people's graded sensitivity to the strength of evidence for a causal structure, because they reason deductively from constraints to consistent structures.  We will present a rational account of human inference, Theory-Based Causal Induction, which emphasizes the interaction between causal knowledge and statistical learning.  Causal knowledge appears in the form of causal theories, specifying the principles by which causal relationships operate in a given domain.  These theories are used to generate hypothesis spaces of causal models -- some with hidden causes, some without -- that can be evaluated by domain-general statistical inference.  We will use this framework to develop models of people's inferences about hidden causes in two physical systems: a mechanical system called the stick-ball machine (Kushnir et al. , 2003), and a dynamical system involving an explosive compound called Nitro X.  Theory-based causal induction Our account of causal induction builds on causal graphical models, extending the formalism to incorporate the abstract knowledge about causal mechanism that plays an essential role in human inferences.  We will briefly introduce causal graphical models, consider how prior knowledge influences causal induction, and describe how we formalize the contribution of causal theories.  Causal graphical models Graphical models represent the dependency structure of a joint probability distribution using a graph in which nodes are variables and edges indicate dependence.  The graphical structure supports efficient computation of the probabilities of events involving these variables.  In a causal graphical model the edges indicate causal dependencies, with the direction of the arrow indicating the direction of causation, and they support inferences about the effects of interventions (Pearl, 2000).  An intervention is an event in which a variable is forced to hold a value, independent of any other variables on which it might depend.  Intervention on a variable A is denoted do(A).  Probabilistic inference on a modifified graph, in which incoming edges to A are removed, can be used to assess the consequences of intervening on A.  The structure of a causal graphical model implies a pattern of dependency among variables under observation and intervention.  Conventional algorithms for inferring causal structure use standard statistical tests, such as Pearson's # 2 test, to find the pattern of dependencies among variables, and then deductively identify the structure(s) consistent with that pattern (e. g. , Spirtes et al. , 1993).  These "constraint-based" algorithms can also exploit the results of interventions, and often require both observations and interventions in order to identify the hidden causal structure.  Gopnik, Glymour, and colleagues have suggested that this kind of constraint-based reasoning may underlie human causal induction (Glymour, 2001; Gopnik et al. , 2004; Kushnir et al. , 2003).  The role of causal theories Constraint-based algorithms for causal induction make relatively little use of prior knowledge.  While particular causal relationships can be ruled out a priori, there is no way to represent the belief that one structure may be more likely than another.  Furthermore, the use of statistical tests like # 2 makes only weak assumptions about the form of causal relationships: these tests simply assess dependency, regardless of whether a relationship is positive or negative, deterministic or probabilistic, strong or weak.  Several researchers (e. g. , Shultz, 1982) have argued that knowledge of causal mechanism plays a central role in human causal induction.  Mechanism knowledge is usually cited in arguments against statistical causal induction, but we view it as critical to explaining how statistical inferences about causal structure are possible from sparse data.  Knowledge about causal mechanisms provides two kinds of restrictions on possible causal models: restrictions on which relationships are plausible, and restrictions on the functional form of those relationships.  Restrictions on plausibility might indicate that one causal structure is more likely than another, while restrictions on functional form might indicate that a particular relationship should be positive and strong.  These restrictions have important implications for causal induction algorithms.  If all structures are possible, both observations and interventions are typically required to identify hidden causes, and without strong assumptions about the functional form of causal relationships, samples must be relatively large.  With limitations on the set of possible causal structures and expectations about functional form, however, it is possible to make causal inferences from just observations and from small samples -- important properties of human causal induction.  Using causal theories in causal induction The causal mechanism knowledge that is relevant for statistical causal inference may be quite abstract, and may also vary across domains.  Much of this knowledge may be represented in intuitive domain theories.  In contrast to Gopnik et al.  (2004), who suggest that causal graphical models are the primary substrate for intuitive theories, we emphasize the role of intuitive theories at a more abstract level, providing restrictions on the set of causal models under consideration.  Such restrictions cannot be represented as part of a causal graphical model: causal graphical models express the relations that hold among a finite set of propositions, while causal theories involve statements about all relations that could hold among entities in a given domain.  Formally, we view causal theories as hypothesis space generators: a theory is a set of principles that can be used to generate a hypothesis space of causal models, which are compared via Bayesian inference.  The principles that comprise a theory specify which relations are plausible and the functional form of those relations.  These principles articulate how causal relationships operate in a given domain, but need not identify the mechanisms underlying such relationships: all that is necessary for causal induction is the possibility that some mechanism exists, and expectations about the functional form associated with that mechanism.  This vague and abstract mechanism knowledge is consistent with the finding that people's understanding of causal mechanism is surprisingly shallow (Rozenblit & Keil, 2002).  In the remainder of the paper, we will demonstrate how Theory-Based Causal Induction can be used to explain human inferences about hidden causes in physical systems.  Different systems require different causal theories.  We will examine inferences in a mechanical system, the stick-ball machine (Kushnir et al. , 2003), and in a dynamical system, Nitro X, which we explore in a new experiment.  When reasoning about these systems, people infer hidden causal structure from very few observations, and are sensitive to graded degrees of evidence.  The stick-ball machine Kushnir et al.  (2003) conducted two experiments in which participants had to infer the causal structure
Semi-Supervised Learning with Trees| Abstract We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain.  The tree (or a distribution over trees) may be inferred using the unlabeled data.  A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification function from the labeled examples.  We test our approach on eight real-world datasets. 
Dynamical Causal Learning| Abstract Current psychological theories of human causal learning and judgment focus primarily on long-run predictions: two by estimating parameters of a causal Bayes nets (though for different parameterizations), and a third through structural learning.  This paper focuses on people's short-run behavior by examining dynamical versions of these three theories, and comparing their predictions to a real-world dataset. 
Theory-Based Causal Inference| Abstract People routinely make sophisticated causal inferences unconsciously, effortlessly, and from very little data -- often from just one or a few observations.  We argue that these inferences can be explained as Bayesian computations over a hypothesis space of causal graphical models, shaped by strong top-down prior knowledge in the form of intuitive theories.  We present two case studies of our approach, including quantitative models of human causal judgments and brief comparisons with traditional bottom-up models of inference. 
Structure Learning in Human Causal Induction| Abstract We use graphical models to explore the question of how people learn simple causal relationships from data.  The two leading psychological theories can both be seen as estimating the parameters of a fixed graph.  We argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure, and we propose to model this inductive process as a Bayesian inference.  Our argument is supported through the discussion of three data sets. 
The Author-Topic Model for Authors and Documents| Abstract We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information.  Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words.  A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors.  We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts.  Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions.  We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics.  We show topics recovered by the authortopic model, and demonstrate applications to computing similarity between authors and entropy of author output. 
Probabilistic author-topic models for information discovery| ABSTRACT We propose a new unsupervised learning technique for extracting information from large text collections.  We model documents as if they were generated by a two-stage stochastic process.  Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic.  The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture.  The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm.  We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics.  We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors.  An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer. 
Unsupervised learning of overlapping concepts| Abstract Natural concepts can overlap arbitrarily, with objects being members of a number of categories.  This kind of structure cannot be captured by existing hierarchical or probabilistic clustering methods.  Motivated by the problem of word learning by young children, we present two simple approaches to identifying overlapping clusters in data, based on fitting multiple independent mixture models.  We apply these approaches to two benchmark synthetic data sets. 
Parametric Embedding for Class Visualization| Abstract In this paper, we propose a new method, Parametric Embedding (PE), for visualizing the posteriors estimated over a mixture model.  PE
Interpolating Between Types and Tokens by Estimating Power-Law Generators| Abstract Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens.  We present a framework for developing statistical models that generically produce power-laws, augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies.  We show that taking a particular stochastic process -- the Pitman-Yor process -- as an adaptor justifies the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology. 
Running head: ELEMENTAL CAUSAL INDUCTION Elemental causal induction| Abstract We present a framework for the rational analysis of elemental causal induction -- learning about the existence of a relationship between a single cause and effect -- based upon causal graphical models.  This framework makes precise the distinction between causal structure and causal strength: the difference between asking whether a causal relationship exists and asking how strong that causal relationship might be.  We show that two leading rational models of elemental causal induction, #P and causal power, both estimate causal strength, and introduce a new rational model, causal support, that assesses causal structure.  Causal support predicts several key phenomena of causal induction that cannot be accounted for by other rational models, which we explore through a series of experiments.  These phenomena include the complex interaction between #P and the base-rate probability of the effect in the absence of the cause, sample size effects, inferences from incomplete contingency tables, and causal learning from rates.  Causal support also provides a better account of a number of existing datasets than either #P or causal power. 
Dynamical Causal Learning| Abstract Current psychological theories of human causal learning and judgment focus primarily on long-run predictions: two by estimating parameters of a causal Bayes nets (though for different parameterizations), and a third through structural learning.  This paper focuses on people's short-run behavior by examining dynamical versions of these three theories, and comparing their predictions to a real-world dataset. 
Hierarchical Topic Models and the Nested Chinese Restaurant Process| Abstract We address the problem of learning topic hierarchies from data.  The model selection problem in this domain is daunting---which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process.  This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections.  We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation.  We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 
Integrating Topics and Syntax| Abstract Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words.  We present a generative model that uses both kinds of dependencies, and is capable of simultaneously finding syntactic classes and semantic topics despite having no knowledge of syntax or semantics beyond statistical dependency.  This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 
The Rational Basis of Representativeness| Why do people think that Linda, the politically active, single, outspoken, and very bright 31-year-old, is more likely to be a feminist bankteller than to be a bankteller, even though this is logically impossible? Why do we think that the sequence HHTHT is more likely than the sequence HHHHH to be produced by flipping a fair coin, even though both are equally likely? The standard answer in cognitive psychology (Kahneman & Tversky, 1972) is that our brains are designed to judge "representativeness", not probability: Linda is more representative of feminist banktellers than of banktellers, and HHTHT is more representative of flipping a fair coin than is HHHHH, despite anything that probability theory tells us.  Not only errors in probabilistic reasoning, but numerous other phenomena of categorization, comparison, and inference have been attributed to the influence of representativeness (or prototypicality or "goodness of example"; Mervis & Rosch, 1981; Osherson, Smith, Wilkie, Lopez, & Shafir, 1990; Rips, 1975).  However, a principled account of representativeness has not been easy to come by.  Its leading proponents (Kahneman & Tversky, 1996; Mervis & Rosch, 1981) have asserted that representativeness should be defined only operationally in terms of people's judgments; an a priori, analytic definition need not be given.  Critics have countered that this concept is too vague to serve as an explanation of intuitive probability judgment (Gigerenzer, 1996).  This paper presents a framework for constructing rational models of representativeness, based on a Bayesian analysis of what makes an observation a good example of a category or process.  The goal is to identify precisely one sense of representativeness and show that it has a rational basis in normative principles of inductive reasoning.  We will first point out some shortcomings of previous accounts based on likelihood or similarity, and show how a Bayesian approach can overcome those problems.  We will then compare the quantitative predictions of Bayesian, likelihood, and similarity models on two sets of representativeness judgments.  Previous approaches Likelihood.  In trying to relate intuitions about representativeness to rational statistical inferences, a natural starting point is the concept of likelihood.  Let d denote some observed data, such as a sequence of coin tosses, and h denote some hypothesis about the source of d, such as flipping a fair coin.  The probability of observing d given that h is true, P(d|h), is called a likelihood.  Let R(d,h) denote representativeness -- how representative the observation d is of the generative process in h.  Gigerenzer & Hoffrage (1995) have proposed that representativeness, to the extent that it can be defined rigorously, is equivalent to likelihood: R(d,h)=P(d|h).  This proposal is appealing in that, other factors aside, the more frequently h leads to observing d, the more representative d should be of h.  It is also consistent with some classic errors in probability judgment, such as the conjunction fallacy: a person is almost certainly more likely to match Linda's description given that she is a bankteller and a feminist than given only that she is a bankteller.  While likelihood and representativeness seem related, however, they are not equivalent.  Two observations with equal likelihood may differ in representativeness.  Knowing that HHHHH and HHTHT are equally likely to be produced by a fair coin does not change our judgment that the latter is the more representative outcome.  Tversky & Kahneman (1983) provide several examples of cases in which a more representative outcome is actually less likely.  Any sequence of fair coin flips, such as THHHTHT, is less likely than one of its subseqences, such as H or HHH, but may easily be more representative.  More colorfully, "being divorced four times" is more representative of Hollywood actresses than is "voting democratic", but the former is certainly less likely.  Figure 1 illustrates a simple version of the dissociation between representativeness and likelihood.  Each panel shows a sample of three points from a Gaussian distribution.  With independent sampling, the total likelihood of a sample equals the product of the likelihoods for X p(X|h) X Figure 1: Given a normal distribution, the left sample has greater likelihood but the right is more representative.  each item in the sample.  Thus the left sample has much greater likelihood, because each point is much closer to the peak of the distribution than in the right sample.  Yet the more spread-out sample on the right seems more representative.  We tested this intuition in a survey of 138 Stanford undergraduates.  They were first shown a normally distributed set of thirty "widgets" produced by a factory.  The widgets were simple drawings resembling nuts or bolts, varying only in their sizes.  They were then shown three different samples, each with three widgets, and asked to rate on a scale of 1-10 how representative each sample was of the widgets produced by this factory.  Each sample contained a point at the mean of the original distribution, and points at z = 2. 85 ("broad sample"), z = 1 ("intermediate sample"), or z = 0. 05 ("narrow sample").  The intermediate sample, with a standard deviation similar to the population, received a significantly higher rating than did the much more likely narrow sample (7. 1 vs.  5. 2, p <. 05).  The broad sample, with lowest likelihood of all, also received a lower rating (6. 9) than the intermediate sample, but not by a significant margin.  We also tested whether intermediate-range samples are more representative for natural categories, using as stimuli black-and-white pictures of birds.  In a design parallel to the widget study, 135 different Stanford undergraduates saw three samples of birds, each containing three members, and rated how representative they were of birds in general.  The samples consisted of either three robins ("narrow"); a robin, an eagle, and a seagull ("intermediate"); or a robin, an ostrich, and a penguin ("broad").  Although the robins were individually rated as more representative than the other birds (by a separate group of 100 subjects), the set of three robins was considered the least representative of the three samples.  As with the widgets, the intermediate sample was rated more representative (6. 3) than either the narrow (5. 1) or broad (5. 3) samples (p <. 05 for both differences).  For natural categories as well as for the artificial widgets, a set of representative examples turns out not to be the most representative set of examples.  Sample likelihood, because it is merely the product of each example's individual likelihood, cannot capture this phenomenon.  At best, then, likelihood may be only one factor contributing to the computation of representativeness.  Similarity.  Most attempts to explicate the mechanisms of representativeness, including that of Kahneman & Tversky (1972), rely not on likelihood but on some sense of similarity.  That is, an observation d is representative of a category or process h to the extent that it is similar to the set of observations h typically generates.  Similarity seems to avoid some of the problems that likelihood encounters.  HHTHT may be more representative of a fair coin than HHHHH because it is more similar on average to other coin flip sequences, based on such features as the number of heads or the number of alternations.  Likewise, someone who has been divorced four times may be more similar to the prototypical Hollywood actress than someone who votes democratic, if marital status is weighted more heavily than political affiliation in computing similarity to Hollywood actresses.  However, the explanatory power of a similarity-based account hinges on being able to specify what makes two stimuli more or less similar, what the relevant features are and how are they weighted.  Similarity unconstrained is liable to lead to circular explanations: having had multiple divorces is more representative of Hollywood actresses because marital status is more highly weighted in computing similarity to Hollywood actresses, but why is marital status so highly weighted, if not because having multiple divorces is so typical of Hollywood actresses? Equating representativeness with similarity also runs into a problem when evaluating the representativeness of a set of objects, as in Figure 1.  Similarity is usually defined as a relation between pairs of stimuli, but here we require a judgment of similarity between two sets of stimuli, the sample and the population.  It is not immediately obvious how best to extend similarity from a pairwise to a setwise measure.  The individual elements of the left sample are certainly more similar to the average member of the population than are the elements of the right sample.  The left sample also comes closer to minimizing the average distance between elements of the population and elements of the sample.  If similarity between sets is defined according to one of these measures, it will fail to match up with representativeness.  Finally, and most problematic for our purposes here, a definition in terms of similarity fails to elucidate the rational basis of representativeness, and thus brings us no closer to explaining when and why representativeness leads to reasonable statistical inferences.  Hence we seem to be left with two less-than-perfect options for defining representativeness: the simple, rational, but clearly insufficient concept of likelihood, or the more flexible but notoriously slippery concept of similarity.  A Bayesian analysis In this section we present a Bayesian analysis of representativeness that addresses some of the shortcomings of the likelihood and similarity proposals.  As with likelihood, Bayesian representativeness takes the form of a simple probabilistic quantity, which in fact includes likelihood as one component.  But like the similarity approach, it can account for dissociations of representativeness and likelihood, when a less probable feature of the stimuli is also more diagnostic of the process or category in question.  Moreover, it applies just as well to
Using Vocabulary Knowledge in Bayesian Multinomial Estimation| Abstract Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks.  Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity.  We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates.  We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 
Finding scientific topics|
Randomness and coincidences: Reconciling intuition and probability theory|
in press)| Generalization, similarity, and Bayesian inference. 
in press)| Teacakes, trains, taxicabs, and toxins: A Bayesian account of predicting the future. 
in press)| Theory-based causal inference. 
Structure and strength in causal judgments,|
Intuitive theories and rational causal inference|
The author-topic model for authors and documents|
Finding scientific topis|
in press)| Elemental causal induction. 

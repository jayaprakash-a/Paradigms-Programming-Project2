On the Eigenspectrum of the Gram Matrix and Its Relationship to the Operator Eigenspectrum| Abstract.  In this paper we analyze the relationships between the eigenvalues of the m # m Gram matrix K for a kernel k(#; #) corresponding to a sample x1 ; : : : ; xm drawn from a density p(x) and the eigenvalues of the corresponding continuous eigenproblem.  We bound the differences between the two spectra and provide a performance bound on kernel PCA. 
Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning| Abstract We present an algorithm based on convex optimization for constructing kernels for semi-supervised learning.  The kernel matrices are derived from the spectral decomposition of graph Laplacians, and combine labeled and unlabeled data in a systematic fashion.  Unlike previous work using diffusion kernels and Gaussian random field kernels, a nonparametric kernel approach is presented that incorporates order constraints during optimization.  This results in flexible kernels and avoids the need to choose among different parametric forms.  Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally feasible for large datasets.  We evaluate the kernels on real datasets using support vector machines, with encouraging results. 
Spectral Kernel Methods for Clustering| Abstract In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix.  All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices.  We use two different but related cost functions, the Alignment and the `cut cost'.  The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts.  Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels.  We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions.  We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information.  The resulting simple algorithms are tested on real world data with positive results. 
On the Concentration of Spectral Properties| Abstract We consider the problem of measuring the eigenvalues of a randomly drawn sample of points.  We show that these values can be reliably estimated as can the sum of the tail of eigenvalues.  Furthermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample.  Experiments are presented that confirm the theoretical results. 
On the Applications of Diffusion Kernels to Text Data| Abstract Kernel methods, such as Support Vector Machines, have successfully been used for text categorization.  A standard choice of kernel function has been the inner product between the vector-space representation of two documents, in analogy with classical information retrieval (IR) approaches.  In this paper we consider diffusion kernels (Kondor, 2001) and their suitability for text data.  We motivate their use from a graph theoretic framework.  We propose an approach based on alignment for selecting the optimal decay parameter # in these kernels.  We provide experimental results demonstrating that diffusion kernels are attractive choices for modelling text data. 
Learning Semantic Similarity| Abstract The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms.  Attempts to incorporate some notion of term similarity include latent semantic indexing [8], the use of semantic networks [9], and probabilistic methods [5].  In this paper we propose two methods for inferring such similarity from a corpus.  The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure.  The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information.  Both approaches produce valid kernel functions parametrised by a real number.  The paper shows how the alignment measure can be used to successfully perform model selection over this parameter.  Combined with the use of support vector machines we obtain positive results. 
On the Extensions of Kernel Alignment| Abstract In this paper we address the problem of measuring the degree of agreement between a kernel and a learning task.  The quantity that we use to capture this notion is alignment (Cristianini et al. , 2001a).  We motivate its theoretical properties, and derive a series of algorithms for adapting a kernel in two important machine learning problems: regression and classification with uneven datasets.  We also propose a novel inductive algorithm within the framework of kernel alignment that can be used for kernel combination and kernel selection.  The algorithms presented have been tested on both artificial and realworld datasets. 
The Perceptron Algorithm with Uneven Margins| Abstract The perceptron algorithm with margins is a simple, fast and effective learning algorithm for linear classifiers; it produces decision hyperplanes within some constant ratio of the maximal margin.  In this paper we study this algorithm and a new variant: the perceptron algorithm with uneven margins, tailored for document categorisation problems (i. e.  problems where classes are highly unbalanced and performance depends on the ranking of patterns).  We discuss the interest of these algorithms from a theoretical point of view, provide a generalisation of Noviko#'s theorem for uneven margins, give a geometrically description of these algorithms and show experimentally that both algorithms yield equal or better performances than support vector machines, while reducing training time and sparsity, in classification (USPS) and document categorisation (Reuters) problems. 
Online Classification on a Budget| Abstract Online algorithms for classification often require vast amounts of memory and computation time when employed in conjunction with kernel functions.  In this paper we describe and analyze a simple approach for an on-the-fly reduction of the number of past examples used for prediction.  Experiments performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine (SVM) although the latter, being a batch algorithm, accesses each training example multiple times. 
On the Concentration of Spectral Properties|
Structural Modelling with Sparse Kernels|
On the eigenspectrum of the gram matrix and the generalisation error of kernel pca|
Structural modeling with sparse kernels|

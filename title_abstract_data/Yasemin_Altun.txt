Support vector machine learning for interdependent and structured output spaces| Abstract Learning general functional dependencies is one of the main goals in machine learning.  Recent progress in kernel-based methods has focused on designing flexible and powerful input representations.  This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces.  We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs.  The resulting optimization problem is solved eciently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem.  We demonstrate the versatility and eectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment. 
Large Margin Methods for Label Sequence Learning| Abstract Label sequence learning is the problem of inferring a state sequence from an observation sequence, where the state sequence may encode a labeling, annotation or segmentation of the sequence.  In this paper we give an overview of discriminative methods developed for this problem.  Special emphasis is put on large margin methods by generalizing multiclass Support Vector Machines and AdaBoost to the case of label sequences.  An experimental evaluation demonstrates the advantages over classical approaches like Hidden Markov Models and the competitiveness with methods like Conditional Random Fields. 
Reading Comprehension Programs in a Statistical-Language-Processing Class| Abstract We present some new results for the reading comprehension task described in [3] that improve on the best published results { from 36% in [3] to 41% (the best of the systems described herein).  We discuss a variety of techniques that tend to give small improvements, ranging from the fairly simple (give verbs more weight in answer selection) to the fairly complex (use specific techniques for answering specific kinds of questions). 
Investigating Loss Functions and Optimization Methods for Discriminative Learning of Label Sequences| Abstract Discriminative models have been of interest in the NLP community in recent years.  Previous research has shown that they are advantageous over generative models.  In this paper, we investigate how different objective functions and optimization methods affect the performance of the classifiers in the discriminative learning framework.  We focus on the sequence labelling problem, particularly POS tagging and NER tasks.  Our experiments show that changing the objective function is not as effective as changing the features included in the model. 
Using Conditional Random Fields to Predict Pitch Accents in Conversational Speech| Abstract The detection of prosodic characteristics is an important aspect of both speech synthesis and speech recognition.  Correct placement of pitch accents aids in more natural sounding speech, while automatic detection of accents can contribute to better wordlevel recognition and better textual understanding.  In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.  We introduce Conditional Random Fields (CRFs) to pitch accent prediction task in order to incorporate these factors efficiently in a sequence model.  We demonstrate the usefulness and the incremental effect of these factors in a sequence model by performing experiments on hand labeled data from the Switchboard Corpus.  Our model outperforms the baseline and previous models of pitch accent prediction on the Switchboard Corpus. 
Exponential Families for Conditional Random Fields| Abstract In this paper we define conditional random fields in reproducing kernel Hilbert spaces and show connections to Gaussian Process classification.  More specifically, we prove decomposition results for undirected graphical models and we give constructions for kernels.  Finally we present efficient means of solving the optimization problem using reduced rank decompositions and we show how stationarity can be exploited efficiently in the optimization process. 
Hidden Markov Support Vector Machines| Abstract This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most
Discriminative Learning for Label Sequences via Boosting| Abstract This paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function.  The proposed method combines many of the advantages of boosting schemes with the eciency of dynamic programming methods and is attractive both, conceptually and computationally.  In addition, we also discuss alternative approaches based on the Hamming loss for label sequences.  The sequence boosting algorithm offers an interesting alternative to methods based on HMMs and the more recently proposed Conditional Random Fields.  Applications areas for the presented technique range from natural language processing and information extraction to computational biology.  We include experiments on named entity recognition and part-of-speech tagging which demonstrate the validity and competitiveness of our approach. 
Gaussian process classification for segmenting and annotating sequences| Abstract Multiclass classification refers to the problem of assigning labels to instances where labels belong to some finite set of elements.  Often, however, the instances to be labeled do not occur in isolation, but rather in observation sequences.  One is then interested in predicting the joint label configuration, i. e.  the sequence of labels, using models that take possible interdependencies between label variables into account.  This scenario subsumes problems of sequence segmentation and annotation.  In this paper, we investigate the use of Gaussian Process (GP) classification for label sequences. 

How Much Of A Hypertree Can Be Captured By Windmills?| Abstract Current approximation algorithms for maximum weight hypertrees find heavy windmill farms, and are based on the fact that a constant ratio (for constant width k) of the weight of a k-hypertree can be captured by a k-windmill farm.  However, the exact worst case ratio is not known and is only bounded to be between 1/(k + 1)! and 1/(k + 1).  We investigate this worst case ratio by searching for weighted hypertrees that minimize the ratio of their weight that can be captured with a windmill farm.  To do so, we use a novel approach in which a linear program is used to find "bad" inputs to a dynamic program. 
Loss Functions for Preference Levels: Regression with Discrete Ordered Labels| Abstract We consider different types of loss functions for discrete ordinal regression, i. e.  fitting labels that may take one of several discrete, but ordered, values.  These types of labels arise when preferences are specified by selecting, for each item, one of several rating "levels", e. g.  one through five stars.  We present two general threshold-based constructions which can be used to generalize loss functions for binary labels, such as the logistic and hinge loss, and another generalization of the logistic loss based on a probabilistic model for discrete ordered labels.  Experiments on the 1 Million MovieLens data set indicate that one of our construction is a significant improvement over previous classification- and regression-based approaches. 
Large-Margin Matrix Factorization| Abstract We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations.  The approach is inspired by, and has strong connections to, large-margin linear discrimination.  We show how to learn low-norm factorizations by solving a semi-definite program, and present generalization error bounds based on analyzing the Rademacher complexity of low-norm factorizations. 
Linear Dependent Dimensionality Reduction| Abstract We formulate linear dimensionality reduction as a semi-parametric estimation problem,
Generalized Low-Rank Approximations| Abstract We study the frequent problem of approximating a target matrix with a matrix of lower rank.  We provide a simple and efficient (EM) algorithm for solving weighted low rank approximation problems, which, unlike simple matrix factorization problems, do not admit a closed form solution in general.  We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low rank representation, and extend the formulation to non-Gaussian noise models such as classification (collaborative filtering). 
for Collaborative Prediction| Abstract Maximum Margin Matrix Factorization (MMMF) was recently suggested (Srebro et al. , 2005) as a convex, infinite dimensional alternative to low-rank approximations and standard factor models.  MMMF can be formulated as a semi-definite programming (SDP) and learned using standard SDP solvers.  However, current SDP solvers can only handle MMMF problems on matrices of dimensionality up to a few hundred.  Here, we investigate a direct gradient-based optimization method for MMMF and demonstrate it on large collaborative prediction problems.  We compare against results obtained by Marlin (2004) and find that MMMF substantially outperforms all nine methods he tested. 
A Dynamic Data Structure for Checking Hyperacyclicity| Abstract We present a dynamic data structure that keeps track of an acyclic hypergraph (equivalently, a triangulated graph) and enables verifying that adding a candidate hyperedge (clique) will not break the acyclicity of the augmented hypergraph.  This is a generalization of the use of Tarjan's Union-Find data structure for maintaining acyclicity when augmenting forests, and the amortized time per operation has a similar almost-constant dependence on the size of the hypergraph.  Such a data structure is useful when augmenting acyclic hypergraphs, e. g.  ~ in order to greedily construct a high-weight acyclic hypergraph.  In designing this data structure, we introduce a hierarchical decomposition of acyclic hypergraphs that aid in understanding hyper-connectivity, and introduce a novel concept of a hypercycle which is excluded from acyclic hypergraphs. 
Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices| Abstract We prove generalization error bounds for predicting entries in a partially observed matrix by fitting the observed entries with a low-rank matrix.  In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of finite pseudodimension such that the sums of functions from this class have unbounded pseudodimension. 
Learning with Matrix Factorization|
A dynamic data structure for checking hyperacyclicity| Available on theory. 

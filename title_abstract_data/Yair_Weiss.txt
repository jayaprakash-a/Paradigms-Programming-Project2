Deriving Intrinsic Images from Image Sequences| Abstract Intrinsic images are a useful midlevel description of scenes proposed by Barrow and Tenenbaum [1].  An image is decomposed into two images: a reflectance image and an illumination image.  Finding such a decomposition remains a difficult problem in computer vision.  Here we focus on a slightly easier problem: given a sequence of T images where the reflectance is constant and the illumination changes, can we recover T illumination images and a single reflectance image? We show that this problem is still illposed and suggest approaching it as a maximum-likelihood estimation problem.  Following recent work on the statistics of natural images, we use a prior that assumes that illumination images will give rise to sparse filter outputs.  We show that this leads to a simple, novel algorithm for recovering reflectance images.  We illustrate the algorithm's performance on real and synthetic image sequences. 
Phase Transitions and the Perceptual Organization of Video Sequences| Abstract Estimating motion in scenes containing multiple moving objects remains a difficult problem in computer vision.  A promising approachtothis problem involves using mixture models, where the motion of each object is a component in the mixture.  However, existing methods typically require specifying in advance the number of components in the mixture, i. e.  the number of objects in the scene.  Here we show that the number of objects can be estimated automatically in a maximum likelihood framework, given an assumption about the level of noise in the video sequence.  We derive analytical results showing the number of models which maximize the likelihood for a given noise level in a given sequence.  We illustrate these results on a real video sequence, showing how the phase transitions correspond to different perceptual organizations of the scene. 
On Spectral Clustering: Analysis and an algorithm| Abstract Despite many empirical successes of spectral clustering methods| algorithms that cluster points using eigenvectors of matrices derived from the data|there are several unresolved issues.  First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways.  Second, many of these algorithms have no proof that they will actually compute a reasonable clustering.  In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab.  Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well.  We also show surprisingly good experimental results on a number of challenging clustering problems. 
Learning Object Detection from a Small Number of Examples: The Importance of Good Features| Abstract Face detection systems have recently achieved high detection rates[11, 8, 5] and real-time performance[11].  However, these methods usually rely on a huge training database (around 5; 000 positive examples for good performance).  While such huge databases may be feasible for building a system that detects a single object, it is obviously problematic for scenarios where multiple objects (or multiple views of a single object) need to be detected.  Indeed, even for multiview face detection the performance of existing systems is far from satisfactory.  In this work we focus on the problem of learning to detect objects from a small training database.  We show that performance depends crucially on the features that are used to represent the objects.  Specifically, we show that using local edge orientation histograms (EOH) as features can significantly improve performance compared to the standard linear features used in existing systems.  For frontal faces, local orientation histograms enable state of the art performance using only a few hundred training examples.  For profile view faces, local orientation histograms enable learning a system that seems to outperform the state of the art in real-time systems even with a small number of training examples. 
User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior| a Gaussian prior and it enables good separations from a small number of labeled gradients. 
Beyond junctions: nonlocal form constraints on motion interpretation|
Perceptually organized EM: A framework for motion segmentation that combines information about form and motion| Abstract Recent progress in motion analysis has been achieved with systems that estimate global parameterized motion byintegrating multiple constraints.  The success of these approaches depends critically on the ability to segment constraints derived from different motions.  Hence the problems of motion estimation and segmentation are tightly coupled.  We believe it is impossible to solve these problems solely in the motion domain, and that mechanisms of spatial form analysis must be incorporated into the motion estimation procedure.  We present a new framework which allows the incorporation of form information in a graceful manner.  It combines concepts from perceptual organization with the powerful optimization technique of EM.  We show that the algorithm is guaranteed to decrease a cost function at every iteration, and that in the absence of form information the cost function reduces to the one minimized by EM.  We demonstrate that the approach can achieve good motion estimation and segmentation with challenging motion sequences.  Recent progress in motion analysis has been achieved with systems that estimate global parameterized motion [Black and Jepson, 1994, Wang and Adelson, 1994, Hsu et al. , 1994, Bergen et al. , 1992] These methods have advantages over local optic flow in that they overcome the local ill-posedness of the motion estimation problem by integrating multiple constraints.  The sucess of these approaches, however, depends critically on the ability to segment constraints derived from different motions.  Hence the problems of motion estimation and segmentation have become tightly coupled.  The joint solution of these problems remains difficult, even for scenes that are very simple.  Consider, for example, the scene shown in fig 1(a) (see also [Bergen et al. , 1990]).  Two bars of different grey shades are moving, one to the left and one to the right.  We will consider how several kinds of motion analyses treat this input.  First, the output of a standard least-squares optic flow routine is shown in fig.  1(b), as an arrow plot; the x and y components of velocity are shown in fig.  1(c) and (d) (velocities below some threshold confidence are set to zero, a b c d Figure 1: a A simple image sequence which causes problems for traditional motion estimation algorithms.  b Least squares optical flow shown as an arrow plot c Least squares optical flow horizontal component.  d Least squares optical #owvertical component.  the algorithm is an implementation of Lucas and Kanade (1981) modified according to [Simoncelli et al. , 1991]).  Although this sequence is a synthetic one, it illustrates problems that occur frequently in analyzing real sequences.  1.  The flow is underconstrained in regions containing extended contours.  2.  The T-junctions that occur where one contour crosses the other form spurious features that move with spurious upward velocities; moreover these features are assigned high confidence by standard techniques because they have \good" 2-D structure (the local estimation is overconstrained).  3.  The interiors of the bars, being textureless, haveno motion information, although one would like them to be filled with the motion assigned their contour.  But simply propagating the motion away from the contour will spread it into the exterior as well as the interior.  Even propagation along contours is problematic since the spurious T-junction motions will be propagated along with the correct corner motions.  4.  The flow field cannot explicitly convey the fact that v x v y v x v y abc e df Figure 2: (a) Normal flow (b) Constraint lines in velocity space (c)-(e) Unambiguous feature motion the two halves of the occluded bar are moving together in \common fate. " Indeed there is no information about grouping and segmentation in the flow field representation.  The shortcomings of local analysis can be ameliorated by accumulating constraints over larger regions as in several recent approaches (e. g.  [Black and Anandan, 1993]) but figure 2 shows that difficulties persist.  The normal flows along the ambiguous contours are shown in fig.  2(a).  The constraint lines may be accumulated into velocity space as shown in fig.  2(b).  There are four constraint lines, and their thickness corresponds to the number of votes.  Clearly there are four major motion candidates, two of which are correct (leftward and rightward), and two of which are incorrect (upward and downward).  The spurious upward motion has more votes than any other motion.  As an alternative, one might ignore the ambiguous contour motions and consider only the unambiguous motions of the features.  These are regions where the local regression matrix is non-singular.  As shown in fig.  2(c), (d), and (e), four such points move to the left, four move to the right, and four moveupward.  The upward motions are spurious but there is no waytoknow this by looking at the local regression matrix.  In velocity space the feature motions support three of the four motions that were supported by the normal motions including the spurious upward motion.  Another global approach is the iterativenulling technique used by Bergen et al.  (1990).  In this approach the entire image is warped by a parameterized flow field in an attempt to null one of the motions; the procedure finds the dominant motion, removes it, and proceeds to the next.  Success can be verified by aligning two frames and subtracting; the region undergoing the motion should be zeroed out.  Fig.  3 shows the results of nulling with the four candidate motions wehave described above.  The leftward and rightward motions, in fig.  3(a) and (b), successfully null much of the image.  However, the upward motion in fig.  3(c) is even more successful.  (The downward motion in fig.  3(d) is less successful).  The upward motion finds a large spurious object { the X { and tries to null it as a whole, as if it were abcd Figure 3: Results of nulling with the four candidate motions described above a.  leftward b.  rightward . cupward d.  downward.  The upward motion nulls the most pixels.  rigidly translating.  The model fails to fully explain the motion of either bar, but the model doesn't know about bars.  It only knows about pixels, and it nulls more pixels than any other single motion.  The problems with motion analysis may be summarized as follows.  Standard optic flow techniques move from one local representation (e. g.  local gradients) to another local representation (a flow field), and thus are limited in their abilitytointegrate information across an image.  Recent techniques use more sophisticated representations and allow more powerful integration of information.  However they still fall short of what is needed.  A successful approach will need to deal with such issues as occlusions, segmentation, contour ownership, and grouping.  That is to say,itis impossible to analyze motion without simultaneously analyzing static form.  Issues in perceptual organization are likely to be critical for further progress in motion processing.  Our goal is to introduce a new framework that will make use of recent advances in motion analysis and optimization, and will also allow us to incorporate form information in a graceful manner.  The optimization is based on Expectation Maximization (EM) [Dempster et al. , 1977], and we combine it with concepts from perceptual organization (PO).  We call the new approach POEM.  Since current understanding of PO is rapdily evolving, wehave designed the POEM to be flexible enough to take advantage of various new PO algorithms as they become available. 
Pairwise Clustering and Graphical Models| Abstract Significant progress in clustering has been achieved by algorithms that are based on pairwise affinities between the datapoints.  In particular, spectral clustering methods have the advantage of being able to divide arbitrarily shaped clusters and are based on efficient eigenvector calculations.  However, spectral methods lack a straightforward probabilistic interpretation which makes it difficult to automatically set parameters using training data.  In this paper we use the previously proposed typical cut framework for pairwise clustering.  We show an equivalence between calculating the typical cut and inference in an undirected graphical model.  We show that for clustering problems with hundreds of datapoints exact inference may still be possible.  For more complicated datasets, we show that loopy belief propagation (BP) and generalized belief propagation (GBP) can give excellent results on challenging clustering problems.  We also use graphical models to derive a learning algorithm for affinity matrices based on labeled data. 
Learning From a Small Number of Training Examples by Exploiting Object Categories| Abstract In the last few years, object detection techniques have progressed immensely.  Impressive detection results have been achieved for many objects such as faces [11, 14, 9] and cars [11].  The robustness of these systems emerges from a training stage utilizing thousands of positive examples.  One approach to enable learning from a small set of training examples is to find an efficient set of features that accurately represent the target object.  Unfortunately, automatically selecting such a feature set is a difficult task in itself.  In this paper we present a novel feature selection method that is based on the notion of object categories.  We assume that when learning to recognize a new object (like an apple) we also know a category it belongs to (fruit).  We further assume that features that are useful for learning other objects in the same category (e. g.  pear or orange) will also be useful for learning the novel object.  This leads to a simple criterion for selecting features and building classifiers.  We show that our method gives significant improvement in detection performance in challenging domains. 
Seamless Image Stitching in the Gradient Domain| Abstract.  The quality of image stitching is measured by the similarity of the stitched image to each of the input images, and by the visibility of the seam between the stitched images.  In order to define and get the best possible stitching, we introduce several formal cost functions for the evaluation of the quality of stitching.  In these cost functions, the similarity to the input images and the visibility of the seam are defined in the gradient domain, minimizing the disturbing edges along the seam.  A good image stitching will optimize these cost functions, overcoming both photometric inconsistencies and geometric misalignments between the stitched images.  This approach is demonstrated in various applications, including generation of panoramic images, object blending, and removal of compression artifacts.  Comparisons with existing methods show the benefits of optimizing the measures in the gradient domain. 
Approximate Inference and Protein-Folding| Abstract Side-chain prediction is an important subtask in the protein-folding problem.  We show that finding a minimal energy side-chain configuration is equivalent to performing inference in an undirected graphical model.  The graphical model is relatively sparse yet has many cycles.  We used this equivalence to assess the performance of approximate inference algorithms in a real-world setting.  Specifically we compared belief propagation (BP), generalized BP (GBP) and naive mean field (MF).  In cases where exact inference was possible, max-product BP always found the global minimum of the energy (except in few cases where it failed to converge), while other approximation algorithms of similar complexity did not.  In the full protein data set, maxproduct BP always found a lower energy configuration than the other algorithms, including a widely used protein-folding software (SCWRL). 
Smoothness in Layers: Motion segmentation using nonparametric mixture estimation| Abstract Grouping based on common motion, or "common fate" provides a powerful cue for segmenting image sequences.  Recently a number of algorithms have been developed that successfully perform motion segmentation by assuming that the motion of each group can be described by a low dimensional parametric model (e. g.  affine).  Typically the assumption is that motion segments correspond to planar patches in 3D undergoing rigid motion.  Here we develop an alternative approach, where the motion of each group is described by a smooth dense flow field and the stability of the estimation is ensured by means of a prior distribution on the class of flow fields.  We present a variant of the EM algorithm that can segment image sequences by fitting multiple smooth flow fields to the spatiotemporal data.  Using the method of Green's functions, we show how the estimation of a single smooth flow field can be performed in closed form, thus making the multiple model estimation computationally feasible.  Furthermore, the number of models is estimated automaticallyusing similar methods to those used in the parametric approach.  We illustrate the algorithm's performance on synthetic and real image sequences. 
Loopy Belief Propagation for Approximate Inference: An Empirical Study| Abstract Recently, researchers have demonstrated that ``loopy belief propagation" --- the use of Pearl's polytree algorithm in a Bayesian network with loops --- can perform well in the context of error-correcting codes.  The most dramatic instance of this is the near Shannon-limit performance of "Turbo Codes" --- codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network.  In this paper we ask: is there something special about the error-correcting code context, or does loopy propagation work as an approximate inference scheme in a more general setting? We compare the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR.  We find that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals.  However, on the QMR network, the loopy beliefs oscillated and had no obvious relationship to the correct posteriors.  We present some initial investigations into the cause of these oscillations, and show that some simple methods of preventing them lead to the wrong results. 
Adventures with gelatinous ellipsesconstraints on models of human motion analysis|
Bethe free energy, Kikuchi approximations and belief propagation algorithms| Abstract Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes.  However, there
Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology| Abstract Graphical models, suchasBayesian networks and Markov Random Fields represent statistical dependencies of variables by a graph.  Local \belief propagation" rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities in singly connected graphical models.  Recently, a number of researchers have empirically demonstrated good performance of \loopy belief propagation"{using these same rules on graphs with loops.  Perhaps the most dramatic instance is the near Shannon-limit performance of \Turbo codes", whose decoding algorithm is equivalenttoloopy belief propagation.  Except for the case of graphs with a single loop, there has been little theoretical understanding of the performance of loopy propagation.  Here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly Gaussian random variables.  We give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation.  We give sufficient conditions for convergence and show that when belief propagation converges it gives the correct posterior means for all graph topologies, not just networks with a single loop.  The related \max-product" belief propagation algorithm finds the maximum posterior probability estimate for singly connected networks.  We show that, even for non-Gaussian probability distributions, the convergence points of the maxproduct algorithm in loopy networks are at least local maxima of the posterior probability.  These results motivate using the powerful belief propagation algorithm in a broader class of networks, and help clarify the empirical performance results. 
Separating Reflections from a Single Image Using Local Features| Abstract When we take a picture through a window the image we obtain is often a linear superposition of two images: the image of the scene beyond the window plus the image of the scene reflected by the window.  Decomposing the single input image into two images is a massively ill-posed problem: in the absence of additional knowledge about the scene being viewed there is an infinite number of valid decompositions.  In this paper we describe an algorithm that uses an extremely simple form of prior knowledge to perform the decomposition.  Given a single image as input, the algorithm searches for a decomposition into two images that minimize the total amount of edges and corners.  The search is performed using belief propagation on a patch representation of the image.  We show that this simple prior is surprisingly powerful: our algorithm obtains "correct" separations on challenging reflection scenes using only a single image. 
Segmentation using Eigenvectors: A Unifying View| In this paper we give a uni#edtreatment of these algorithms, and show the close connections between them while highlighting their distinguishing features.  We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings.  Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms.  We illustrate our analysis with results on real and synthetic images.  Human perceiving a scene can often easily segment it into coherent segments or groups.  There has been a tremendous amount of effort devoted to achieving the same level of performance in computer vision.  In many cases, this is done by associating with each pixel a feature vector (e. g.  color, motion, texture, position) and using a clustering or grouping algorithm on these feature vectors.  Perhaps the cleanest approach to segmenting points in feature space is based on mixture models in which one assumes the data were generated bymultiple processes and estimates the parameters of the processes and the number of components in the mixture.  The assignment of points to clusters can then be easily performed by calculating the posterior probabilityofa point belonging to a cluster.  Despite the elegance of this approach, the estimation process leads to a notoriously difficult optimization.  The frequently used EM algorithm [3] often converges to a local maximum that depends on the initial conditions.  Recently,anumber of authors [11, 10, 8, 9, 2] have suggested alternative segmentation methods that are based on eigenvectors of the (possibly normalized) \affinity matrix".  Figure 1a shows two clusters of points and figure 1b shows the affinity matrix defined by: W (i; j)=e, d(x i ;x j )=2# 2 (1) with # a free parameter.  In this case wehave used d(x i ;x j )=kx i , x j k 2 but different definition of a#nities are possible.  The affinities do not even haveto obey the metric axioms (e. g.  [7]), we will only assume that d(x i ;x j )=d(x j ;x i ).  Note that wehave ordered the points so that all points belonging to the first cluster appear first and the points in the second cluster.  This helps the visualization of the matrices but does not change the algorithms --- eigenvectors of permuted matrices are the permutations of the eigenvectors of the original matrix.  From visual inspection, the affinity matrix contains information about the correct segmentation.  In the next section we review four algorithms that look at eigenvectors of affinity matrices.  We show that while seemingly quite different, these algorithms are closely related and all use dominant eigenvectors of matrices to perform segmentation.  However, these approaches use different matrices, focus on different eigenvectors and use a different method of going from the continuous eigenvectors to the discrete segmentation.  In section 2 we prove results on eigendecompositions of block matrices and use these results to analyze the behavior of these algorithms and motivate a new hybrid algorithm.  Finally, in section 3 we discuss the application of these algorithms to affinity matrices derived from images. 
Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms| Abstract Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs.  The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles.  We show that BP fixed points correspond to the stationary points of the Bethe approximation to the free energy for a factor graph.  We explain how to obtain regionbased free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms.  We emphasize the conditions a free energy approximation must satisfy in order to be a "valid" approximation.  We describe the relationship between four different methods that can be used to generate valid approximations: the "Bethe method," the "junction graph method," the "cluster variation method," and the "region graph method. " The region graph method is the most general of these methods, and it subsumes all the other methods.  Region graphs also provide the natural graphical setting for GBP algorithms.  We explain how to obtain three different versions of GBP algorithms and show that their fixed points will always correspond to stationary points of the region graph approximation to the free energy.  We also show that the region graph approximation is exact when the region graph has no cycles. 
A Conversation about the Bethe Free Energy and Sum-Product| Abstract This discussion document records an email conversation in preparation for the Trieste meeting.  Background The result that `belief propagation fixed-points are zero gradient points of the Bethe free energy' (Yedidia, 2000; Yedidia et al. , 2000c)
Information Bottleneck for Gaussian Variables| Abstract The problem of extracting the relevant aspects of data was addressed through the information bottleneck (IB) method, by (soft) clustering one variable while preserving information about another - relevance - variable.  An interesting question addressed in the current work is the extension of these ideas to obtain continuous representations (embeddings) that preserve relevant information, rather than discrete clusters.  We give a formal definition of the general continuous IB problem and obtain an analytic solution for the optimal representation for the important case of multivariate Gaussian variables.  The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized correlation matrix # xjy # 1 x , which is also the basis obtained in Canonical Correlation Analysis.  However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector.  This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level.  Our analysis also provides analytic expression for the optimal tradeoff - the information curve - in terms of the eigenvalue spectrum. 
Multibody Factorization with Uncertainty and Missing Data Using the EM Algorithm| Abstract Multibody factorization algorithms [2, 1, 16] give an elegant and simple solution to the problem of structure from motion even for scenes containing multiple independent motions.  Despite this elegance, it is still quite difcult to apply these algorithms to arbitrary scenes.  First, their performance deteriorates rapidly with increasing noise.  Second, they cannot be applied unless all the points can be tracked in all the frames (as will rarely happen in real scenes).  Third, they cannot incorporate prior knowledge on the structure or the motion of the objects.  In this paper we present a multibody factorization algorithm that can handle arbitrary noise covariance for each feature as well as missing data.  We show how to formulate the problem as one of factor analysis and derive an expectation-maximization based maximum-likelihood algorithm.  One of the advantages of our formulation is that we can easily incorporate prior knowledge, including the assumption of temporal coherence.  We show that this assumption greatly enhances the robustness of our algorithm and present results on challenging sequences. 
Colorization using optimization| Figure 1: Given a grayscale image marked with some color scribbles by the user (left), our algorithm produces a colorized image (middle). 
IEEE Workshop on Neural Networks for Signal Processing Motion Estimation and Segmentation Using a Recurrent Mixture of Experts Architecture| Abstract Estimating motion in scenes containing multiple motions remains a difficult problem for computer vision.  Here we describe a novel recurrent network architecture which solves this problem by simultaneously estimating motion and segmenting the scene.  The network is comprised of locally connected units which carry out simple calculations in parallel.  We present simulation results illustrating the successful motion estimation and rapid convergence of the network on real image sequences. 
Finding the M Most Probable Configurations Using Loopy Belief Propagation| Abstract Loopy belief propagation (BP) has been successfully used in a number of dicult graphical models to find the most probable configuration of the hidden variables.  In applications ranging from protein folding to image analysis one would like to find not just the best configuration but rather the top M .  While this problem has been solved using the junction tree formalism, in many real world problems the clique size in the junction tree is prohibitively large.  In this work we address the problem of finding the M best configurations when exact inference is impossible.  We start by developing a new exact inference algorithm for calculating the best configurations that uses only max-marginals.  For approximate inference, we replace the max-marginals with the beliefs calculated using max-product BP and generalized BP.  We show empirically that the algorithm can accurately and rapidly approximate the M best configurations in graphs with hundreds of variables. 
Correctness of Local Probability Propagation in Graphical Models with Loops| Graphical models, such as Bayesian networks and Markov networks, represent joint distributions over a set of variables by means of a graph.  When the graph is singly connected, local propagation rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities.  Recently a number of researchers have empirically demonstrated good performance of these same local propagation schemes on graphs with loops, but a theoretical understanding of this performance has yet to be achieved.  For graphical models with a single loop, we derive an analytical relationship between the probabilities computed using local propagation and the correct marginals.  Using this relationship we show a category of graphical models with loops for which local propagation gives rise to provably optimal maximum a posteriori assignments (although the computed marginals will be incorrect).  We also show how nodes can use local information in the messages they receive in order to correct their computed marginals.  We discuss how these results can be extended to graphical models with multiple loops and show simulation results suggesting that some properties of propagation on single-loop graphs may hold for a larger class of graphs.  Specifically we discuss the implication of our results for understanding a class of recently proposed error-correcting codes known as turbo codes. 
Interpreting Images by Propagating Bayesian Beliefs| Abstract A central theme of computational vision research has been the realization that reliable estimation of local scene properties requires propagating measurements across the image.  Many authors have therefore suggested solving vision problems using architectures of locally connected units updating their activity in parallel.  Unfortunately, the convergence of traditional relaxation methods on such architectures has proven to be excruciatingly slow and in general they do not guarantee that the stable point will be a global minimum.  In this paper we show that an architecture in which Bayesian Beliefs about image properties are propagated between neighboring units yields convergence times which are several orders of magnitude faster than traditional methods and avoids local minima.  In particular our architecture is non-iterative in the sense of Marr [5]: at every time step, the local estimates at a given location are optimal given the information which has already been propagated to that location.  We illustrate the algorithm's performance on real images and compare it to several existing methods. 
Bayesian Belief Propagation for Image Understanding| Abstract A central theme of computational vision research has been the realization that reliable estimation of local scene properties requires propagating measurements across the image.  Many authors have suggested solving vision problems using architectures of locally connected units updating their activity in parallel.  Unfortunately,the convergence of traditional relaxation methods on such architectures has proven to be excruciatingly slow and in general they do not guarantee that the stable point will be a global minimum.  In this paper we show that a scheme in which Bayesian Beliefs about image properties are propagated between neighboring units may yield convergence times which are several orders of magnitude faster than traditional methods and avoids local minima.  In particular, for some vision problems the scheme provably does not take \too many iterations" in the sense of Marr [11]: at every time step, the local estimates at a given location are optimal given the information which has already been propagated to that location.  We illustrate the algorithm's performance on real images and synthetic images and compare it to several existing methods.  1 1 A preliminary version of this paper appeared in [23] a Y* Y Y* ,# # ,# # # ,# # b c Figure 1: a.  a prototypical ill-posed problem b.  Traditional relaxation approach: dense array of units represent the value of the interpolated function.  Units update their activity based on local information and the activity of neighboring units.  c.  The Bayesian Belief Propagation (BBP) approach.  Units transmit probabilities and combine them according to probability calculus in two non-interacting streams.  Many problems in vision are analogous to the problem shown in figure a { the interpolation of a function from sparse local noisy data.  Many solutions in vision are analogous to that shown in figure b |thefunction is represented byasetof discrete nodes each representing the value of the function at a given location.  The interpolating function is found by minimizing a cost function consisting of two terms | a data cost and a coherence cost, e. g. : J(Y )= X k w k (y k y # k ) 2 + # X i (y i y i+1 ) 2 (1) where y k is the value of the function at location k, y # k is the observed data and we have defined w k = 0 for grid points with no data, and w k = 1 for points with data.  Since J is quadratic, any local update in the direction of the gradientwillconverge to the optimal estimate.  This yields updates of the sort: y k y k + # k (#( y k 1 + y k+1 2 y k )+w k (y # k y k )) (2) Relaxation algorithms differ in their choice of #: # = 1=(# + w k ) corresponds to Gauss-Seidel relaxation and # =1:9=(# +w k ) corresponds to successiveover relaxation (SOR) [20].  There is something attractive about update rules of the type in equation 2 |they consist of simple, local operations that can be carried out in parallel.  This is particularly appealing from the point of view of modeling human perception | one can imagine similar operations being carried out by simple neuronal circuits. 
Maximum Likelihood and the Information Bottleneck| Abstract The information bottleneck (IB) method is an information-theoretic formulation for clustering problems.  Given a joint distribution ########### , this method constructs a new variable # that defines partitions over the values of # that are informative about # .  Maximum likelihood (ML) of mixture models is a standard statistical approach to clustering problems.  In this paper, we ask: how are the two methods related ? We define a simple mapping between the IB problem and the ML problem for the multinomial mixture model.  We show that under this mapping the problems are strongly related.  In fact, for uniform input distribution over # or for large sample size, the problems are mathematically equivalent.  Specifically, in these cases, every fixed point of the IB-functional defines a fixed point of the (log) likelihood and vice versa.  Moreover, the values of the functionals at the fixed points are equal under simple transformations.  As a result, in these cases, every algorithm that solves one of the problems, induces a solution for the other. 
Noise and the the two-thirds power law| Abstract The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement.  It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction.  This ubiquity has fostered various attempts to uncover the origins of this empirical relationship.  In these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion.  We show here that white Gaussian noise also obeys the power-law.  Analysis of signal and noise combinations shows that trajectories which were synthetically created not to comply with the power-law are transformed to power-law compliant ones after their combination with low levels of noise.  Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing.  These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise.  Our results could also suggest that the power-law might be derived not from smoothness or smoothnessinducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system. 
Belief Propagation and Revision in Networks with Loops| Abstract Local belief propagation rules of the sort proposed byPearl (1988) are guaranteed to converge to the optimal beliefs for singly connected networks.  Recently,anumber of researchers have empirically demonstrated good performance of these same algorithms on networks with loops, but a theoretical understanding of this performance has yet to be achieved.  Here welay a foundation for an understanding of belief propagation in networks with loops.  For networks with a single loop, we derive an analytical relationship between the steady state beliefs in the loopy network and the true posterior probability.  Using this relationship we show a category of networks for which the MAP estimate obtained by belief update and by belief revision can be proven to be optimal (although the beliefs will be incorrect).  We showhow nodes can use local information in the messages they receive in order to correct the steady state beliefs.  Furthermore we prove that for all networks with a single loop, the MAP estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states.  The result is independent of the length of the cycle and the size of the state space.  For networks with multiple loops, weintroduce the concept of a \balanced network" and show simulation results comparing belief revision and update in such networks.  We show that the Turbo code structure is balanced and present simulations on a toyTurbo code problem indicating the decoding obtained by belief revision at convergence is significantly more likely to be correct. 
Representation of similarity as a goal of early visual processing| Abstract We consider the representational capabilities of systems of receptive fields found in early mammalian vision, under the assumption that the successive stages of processing remap the retinal representation space in a manner that makes objectively similar stimuli (such as different views of the same 3D object) closer to each other, and dissimilar stimuli farther apart.  We present theoretical analysis and computational experiments that compare the similarity between stimuli as they are represented at the successive levels of the processing hierarchy, from the retina to the nonlinear cortical units.  Our results indicate that the representations at the higher levels of the hierarchy are indeed more useful for the classification of natural objects such as human faces.  1 Motivation Systems of receptive fields (RFs) are probably the most prominent and ubiquitous computational mechanism employed in biological information processing, and, in particular, in vision.  A natural question suggested by the hierarchy of RF types found in the visual pathway is, what is it good for? It may seem that the answer is to be found, jointly, in the many models of visual function based on population coding of various stimulus qualities, especially as some of these models draw explicit parallels between the representations they employ and the RFs found in biological vision.  However, mere invocation of the idea of population coding, if not accompanied by a computational (in the sense of Marr, 1982) statement of what it is that the visual system does with its representations, simply begs the question: At all levels of the visual system, complex objects appear to be coded by the activity of populations, or networks, of cells, and the representation of a particular object may be widely distributed throughout one or more visual areas.  That said, the goal of the anatomical pathway for object recognition becomes less obvious.  The photoreceptors are a population of cells, for example, and they are necessarily capable of coding, by their population response, any conceivable stimulus.  Why are subsequent populations needed? (Desimone and Ungerleider, 1989, p. 268).  A number of recent works that do address the computational problem of representation tend to employ information-theoretic terms such as redundancy reduction and efficient coding (Field, 1994; Daugman, 1988; Atick, 1992).  In this paper we suggest an alternative approach, based on the observation that object classification (which may be considered an ultimate goal of vision) requires faithful representation of true similarity between shapes (Edelman, 1993).  This observation leads to the hypothesis that successive stages of early visual processing remap the retinal space in a manner that makes objectively similar shapes closer to each other.  The paper is organized as follows.  In section 2, we consider a general formulation of the issue of similarity under different representations.  Section 3 contains an experimental evaluation of several similarity measures on a database of face images, and, in particular, a comparison of the similarity induced by realistic RFs with that of two control cases.  Section 4 contains a discussion of the results and lists directions for future research.  2 The effect of the choice of representation on similarity between images Recent theories of object recognition based on view interpolation (Poggio and Edelman, 1990) or on linear combination of views (Ullman and Basri, 1991) have underscored the possibility of representing 3D shapes by collections of their views, or images.  However, in models of biological vision, the notion of an image is ill-defined at any stage past the projection of the world onto the photoreceptor sheet in retina.  At all the subsequent levels, the visual system has at its disposal only the activities evoked by this input image in the units of the preceding level, and it can compare two images only by comparing these population activity vectors.  Thus, a representation scheme together with a metric in the representation space naturally induce a measure of similarity among (input) images.  For a recognition scheme such as view interpolation to succeed, this induced or proximal similarity between image representations must correspond in a principled manner to the objective or distal similarity between objects that give rise to the images.  Consequently, we propose to compare various representation schemes according to the similarity measures they induce. 
On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs| Problems involving probabilistic belief propagation arise in a wide variety of applications, including error correcting codes, speech recognition and image understanding.  Typically, a probability distribution is assumed over a set of variables and the task is to infer the values of the unobserved variables given the observed ones.  The assumed probability distribution is described using a graphical model [13] --- the qualitative aspects of the distribution are specified bya graph structure.  The graph may either be directed as in a Bayesian network [17], [11] or undirected as in a Markov Random Field [17], [9].  Here we focus on the problem of finding an assignment for the unobserved variables that is most probable given the observed ones.  In general, this problem is NP hard [18] but if the graph is singly connected (i. e.  there is only one path between anytwo given nodes) then there exist efficient local message{passing schemes to perform this task.  Pearl [17] derived suchascheme for singly connected Bayesian networks.  The algorithm, which he called \belief revision", is identical to his algorithm for finding posterior marginals over nodes except that the summation operator is replaced with a maximization.  Aji et al.  [2] have shown that both of Pearl's algorithms can be seen as special cases of generalized distributive laws over particular semirings.  In particular, Pearl's algorithm for finding maximum a posteriori (MAP) assignments can be seen as a generalized distributivelawover the max-product semiring.  We will henceforth refer to it as the \max-product" algorithm.  Pearl showed that for singly connected networks, the max-product algorithm is guaranteed to converge and that the assignment based on the messages at convergence is guaranteed to give the optimal assignment{values corresponding to the MAP solution.  Several groups have recently reported excellent experimental results by running the max-product algorithm on graphs with loops [22], [6], [3], [19], [6], [10].  Benedetto et al.  used the max-product algorithm to decode \turbo" codes and obtained excellent results that were slightly inferior to the original turbo decoding algorithm (which is equivalent to the sum-product algorithm).  Weiss [19] compared the performance of sum-product and max-product on a \toy" turbo code problem while distinguishing between converged and unconverged cases.  He found that if one considers only the convergent cases, the performance of max-product decoding is significantly better than sum-product decoding.  However, the max-product algorithm converges less often so its overall performance (including both convergent and nonconvergent cases) is inferior.  Progress in the analysis of the max-product algorithm has been made for two special topologies: single loop graphs, and \cycle codes".  For graphs with a single loop [22], [19], [20], [5], [2], it can be shown that the algorithm converges to a stable fixed point or a periodic oscillation.  If it converges to a stable fixed-point, then the assignment based on the fixed-point messages is the optimal assignment.  For graphs that correspond to cycle codes (low density paritycheck codes in which each bit is checked by exactly twocheck nodes), Wiberg [22] gave sufficient conditions for max-product to converge to the transmitted codeword and Horn [10] gave sufficient conditions for convergence to the MAP assignment.  In this paper we analyze the max-product algorithm in graphs of arbitrary topology.  We show that at a #xed-pointof the algorithm, the assignment is a \neighborhood maximum" of the posterior probability: the posterior probabilityof the max-product assignment is guaranteed to be greater than all other assignments in a particular large region around that assignment.  These results motivate using this powerful algorithm in a broader class of networks. 
Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence| Abstract The problem of "Structure From Motion" is a central problem in vision: given the 2D locations of certain points we wish to recover the camera motion and the 3D coordinates of the points.  Under simplified camera models, the problem reduces to factorizing a measurement matrix into the product of two low rank matrices.  Each element of the measurement matrix contains the position of a point in a particular image.  When all elements are observed, the problem can be solved trivially using SVD, but in any realistic situation many elements of the matrix are missing and the ones that are observed have a dierent directional uncertainty.  Under these conditions, most existing factorization algorithms fail while human perception is relatively unchanged.  In this paper we use the well known EM algorithm for factor analysis to perform factorization.  This allows us to easily handle missing data and measurement uncertainty and more importantly allows us to place a prior on the temporal trajectory of the latent variables (the camera position).  We show that incorporating this prior gives a significant improvement in performance in challenging image sequences. 
Generalized Belief Propagation| Abstract Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes.  However, there
Understanding belief propagation and its generalizations|
Understanding belief propagation and its generalization|
Bethe free energies, Kikuchi approximations, and belief propagation algorithms|
The Factored Frontier Algorithm for Approximate Inference in DBNs|
Generalised belief propagation|
"Bethe free energy, Kikuchi approximations, and belief propagation algorithms,"|
Perceptually organized EM: A framework for motion segmentaiton that combines information about form and motion|
Characterizing belief propagation and its generalizations|
"The computation of occlusion for motion perception"|
The role of occlusion in motion interpretation: Insights from occluded crosses" (submitted)|
Motin estimation and segmentation with a recurrent mixture-ofexperts architecture|

You can load a die but you can't bias a coin| Abstract Dice can be loaded---that is, one can easily alter a die so that the probabilities of landing on the six sides are dramatically
An analysis of the NYPD's stop-and-frisk policy in the context of claims of racial bias| Abstract Recent studies by police departments and researchers confirm that police stop racial and ethnic minority citizens more often than whites, relative to their proportions in the population.  However, it has been argued stop rates more accurately reflect rates of crimes committed by each ethnic group, or that stop rates reflect elevated rates in specific social areas such as neighborhoods or precincts.  Most of the research on stop rates and policecitizen interactions has focused on traffic stops, and analyses of pedestrian stops are rare.  In this paper, we analyze data from 175,000 pedestrian stops by the New York Police Department over a fifteen-month period.  We disaggregate stops by police precinct, and compare stop rates by racial and ethnic group controlling for previous race-specific arrest rates.  We use hierarchical multilevel models to adjust for precinct-level variability, thus directly addressing the question of geographic heterogeneity that arises in the analysis of pedestrian stops.  We find that persons of African and Hispanic descent were stopped more frequently than whites, even after controlling for precinct variability and race-specific estimates of crime participation. 
Some Issues in Monitoring Convergence of Iterative Simulations| In this paper, we discuss some recent results and open questions concerning monitoring convergence of iterative simulations.  We begin by discussing the various approaches to convergence assessment proposed in the literature, grouping the methods according to their underlying principles.  We then discuss how MCMC simulations can be constructed so that convergence monitoring is simplified.  Finally, we discuss some new convergence assessment ideas that are the focus of current work. 
Regression Modeling and Meta-Analysis for Decision Making: A Cost-Benefit Analysis of Incentives in Telephone Surveys| Regression models are often used, explicitly or implicitly, for decision making.  However, the choices made in setting up the models (e. g. , inclusion of predictors based on statistical significance) do not map directly into decision procedures.  Bayesian inference works more naturally with decision analysis but presents problems in practice when noninformative prior distributions are used with sparse data.  We do not attempt to provide a general solution to this problem, but rather present an application of a decision problem in which inferences from a regression model are used to estimate costs and benefits.  Our example is a reanalysis of a recent meta-analysis of incentives for reducing survey nonresponse.  We then apply the results of our fitted model to the New York City Social Indicators Survey, a biennial telephone survey with a high nonresponse rate.  We consider the balance of estimated costs, cost savings, and response rate for different choices of incentives.  The explicit analysis of the decision problem reveals the importance of interactions in the fitted regression model. 
Bridges between deterministic and probabilistic models for binary data \Lambda| Abstract For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models.  We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random.  In the context of binary data, we consider two error models: in the first model, all predictions are equally likely to be in error; in the second model, the probability of error depends on the model prediction.  We show how to fit these models using a stochastic modification of deterministic optimization schemes.  The advantages of fitting the stochastic models explicitly (rather than implicitly, by simply fitting a deterministic model and accepting the occurrence of errors) include quantification of uncertainty in the deterministic model's parameter estimates, better estimation of the true model error rate, and the ability to check the fit of the model nontrivially.  We illustrate with a simple theoretical example of item response data and with empirical examples from archaeology and the psychology of choice. 
Fully Bayesian Computing| Abstract A fully Bayesian computing environment calls for the
Bayesian Analysis of Serial Dilution Assays| Summary.  In a serial dilution assay, the concentration of a compound is estimated by combining measurements of several different dilutions of an unknown sample. The relation between concentration and measurement is nonlinear and heteroscedastic, and so it is not appropriate to weight these measurements equally. In the standard existing approach for analysis of these data, a large proportion of the measurements are discarded as being above or below detection limits. We present a Bayesian method for jointly estimating the calibration curve and the unknown concentrations using all the data. Compared to the existing method, our estimates have much lower standard errors and give estimates even when all the measurements are outside the "detection limits. " We evaluate our method empirically using laboratory data on cockroach allergens measured in house dust samples. Our estimates are much more accurate than those obtained using the usual approach. In addition, we develop a method for determining the "effective weight" attached to each measurement, based on a local linearization of the estimated model. The effective weight can give insight into the information conveyed by each data point and suggests potential improvements in design of serial dilution experiments. 
Estimating the Probability of Events That have Never Occurred: When is Your Vote Decisive? \Lambda| Abstract Researchers sometimes argue that statisticians have little to contribute when few realizations of the process being estimated are observed.  We show that this argument is incorrect even in the extreme situation of estimating the probabilities of events so rare that they have never occurred.  We show how statistical forecasting models allow us to use empirical data to improve inferences about the probabilities of these events.  Our application is estimating the probability that your vote will be decisive in a U. S.  presidential election, a problem that has been studied by political scientists for more than two decades.  The exact value of this probability is of only minor interest, but the number has important implications for understanding the optimal allocation of campaign resources, whether states and voter groups receive their fair share of attention from prospective presidents, and how formal "rational choice" models of voter behavior might be able to explain why people vote at all. 
Adaptively scaling the Metropolis algorithm using expected squared jumped distance| Abstract A good choice of the proposal distribution is crucial for the rapid convergence of the Metropolis
Bayesian Multilevel Estimation with Poststratification: State-Level Estimates from National Polls| Abstract We fit a multilevel logistic regression model for the mean of a binary response variable conditional on poststratification cells.  This approach combines the modeling approach often used in small-area estimation with the population information used in poststratification (see Gelman and Little (1997)).  To validate the method, we apply it to U. S pre-election polls for 1988 and 1992, poststratified by state, region, and the usual demographic variables.  We evaluate the model by comparing it to state-level election outcomes.  The multilevel model outperforms more commonly used models in political science.  We envision the most important usage of this method, not in forecasting elections, but in estimating public opinion on a variety of issues at the state level. 
A method for estimating sampling variances for surveys with weighting, poststrati#cation, and raking| Abstract It is common practice to use weighting, poststrati#cation, and raking to correct for sampling and nonsampling biases and to improve eciency of estimation in sample surveys.  In general, the sampling variances of the resulting estimates depend on the weighting procedures, not just on the numerical values of the weights.  In this paper we develop a method for estimating the sampling variance of survey estimates with these adjustments, using three ideas: (1) a general notation that unifies the different forms of weighting adjustment, (2) a variance decomposition to estimate sampling variances conditional and unconditional on sample sizes within poststrati#cation categories, and (3) the delta method applied to uncertainties in sample sizes within poststrata.  The resulting variance estimates are design-based and comparable to those obtained by the jackknife.  We focus on estimation of population and subgroup means but also discuss more complicated summaries such as ratios and regression coecients.  We apply our approach to the problem that motivated this research, the New York City Social Indicators Survey, a telephone survey that uses inverse-probability weighting, poststrati#cation, and raking to correct for sampling design and nonresponse.  Our variance estimates systematically differ from those obtained using methods that do not account for the design of the weighting scheme.  Assuming simple random sampling leads to underestimating the sampling variance, and treating all weights as inverse-probability causes variances to be overestimated. 
Parameter Expansion for Data Augmentation \Lambda| Abstract Viewing the observed data of a statistical model as incomplete and augmenting its missing parts are useful for clarifying concepts and central to the invention of two well-known statistical algorithms: expectation-maximization (EM) and data augmentation.  Recently, Liu, Rubin, and Wu (1998) demonstrate that expanding the parameter space along with augmenting the missing data is useful for accelerating iterative computation in an EM algorithm.  The main purpose of this article is to rigorously define a parameter expanded data augmentation (PX-DA) algorithm and to study its theoretical properties.  The PX-DA is a special way of using auxiliary variables to accelerate Gibbs sampling algorithms and is closely related to reparameterization techniques.  Theoretical results concerning the convergence rate of the PX-DA algorithm and the choice of prior for the expansion parameter are obtained.  In order to understand the role of the expansion parameter, we establish a new theory for iterative conditional sampling under the transformation group formulation, which generalizes the standard Gibbs sampler.  Using the new theory, we show that the PX-DA algorithm with a Haar measure prior (often improper) for the expansion parameter is always proper and is optimal among a class of such algorithms including reparameterization. 
Some Class-participation Demonstrations for Decision Theory and Bayesian Statistics \Lambda| Abstract We present several classroom demonstrations that have sparked student involvement in our undergraduate course in decision theory and Bayesian statistics.  Some of the demonstrations involve student participation, while others are essentially lectures with extra class discussion. 
Bayesian probabilistic extensions of a deterministic classification model \Lambda| Summary This paper extends deterministic models for Boolean regression within a Bayesian framework.  For a given binary criterion variable Y and a set of k binary predictor variables X 1 ; : : : ; X k , a Boolean regression model is a conjunctive (or disjunctive) logical combination consisting of a subset S of the X variables, which predicts Y .  Formally, Boolean regression models include a specification of a k-dimensional binary indicator vector (` 1 ; : : : ; ` k ) with ` j = 1 iff X j 2 S.  In a probabilistic extension, a parameter is added which represents the probability of the predicted value ^ y i and the observed value y i to differ (for any observation i).  Within Bayesian estimation, a posterior distribution of the parameters (` 1 ; : : : ; ` k ; ) is looked for.  The advantages of such a Bayesian approach include a proper account for the uncertainty in the model estimates and various possibilities for model checking (using posterior predictive checks).  We illustrate in an example using real data. 
An Experimental Study of Storable Votes| Abstract The storable votes mechanism is a method of voting for committees that meet periodically to consider a series of binary decisions.  Each member is allocated a fixed budget of votes to be cast as desired over the multiple decisions.  Voters are induced to spend more votes on those decisions that matter to them most, shifting the ex ante probability of winning away from decisions they value less and towards decisions they value more, typically generating welfare gains over standard majority voting with non-storable votes.  The equilibrium strategies have a very intuitive feature--the number of votes cast must be monotonic in the voter's intensity of preferences--but are otherwise di6 cult to calculate, raising questions of practical implementation.  In our experiments, realized e ciency levels were remarkably close to theoretical equilibrium predictions, while subjects adopted monotonic but o -equilibrium strategies.  We are lead to conclude that concerns about the complexity of the game may have limited practical relevance. 
DIVISION OF THE HUMANITIES AND SOCIAL SCIENCES| Abstract In an election, the probability that a single voter is decisive is aected by the electoral system---that is, the rule for aggregating votes into a single outcome.  Under the assumption that all votes are equally likely (i. e. , random voting), we prove that the average probability of a vote being decisive is maximized under a popular-vote (or simple majority) rule and is lower under any coalition system, such as the U. S.  Electoral College system, no matter how complicated.  Forming a coalition increases the decisive vote probability for the voters within a coalition, but the aggregate eect of coalitions is to decrease the average decisiveness of the population of voters.  We then review results on voting power in an electoral college system.  Under the random voting assumption, it is well known that the voters with the highest probability of decisiveness are those in large states.  However, we show using empirical estimates of the closeness of historical U. S.  Presidential elections that voters in small states have been advantaged because the random voting model overestimates the frequencies of close elections in the larger states.  Finally, we estimate the average probability of decisiveness for all U. S.  Presidential elections from 1960 to 2000 under three possible electoral systems: popular vote, electoral vote, and winner-take-all within Congressional districts.  We find that the average probability of decisiveness is about the same under all three systems. 
A method for quantifying artifacts in mapping methods, illustrated by application to headbanging \Lambda| Abstract Maps of disease rates (and other quantities) often must contend with variance associated with variable population sizes and low incidence within spatial units.  These characteristics can lead to substantial statistical noise that can mask underlying spatial variation.  As Gelman and Price 1 illustrated, most conventional mapping methods fail to address this problem, and in fact can introduce statistical artifacts: mapped quantities can show spatial patterns even when there are no spatial patterns in the underlying parameter of interest.  Kafadar 2 evaluated the performance of the headbanging algorithm for spatial smoothing (Tukey and Tukey 3 , Hansen 4 ) for eliminating small scale variation and preserving edge structure.  Here we perform a simulation study to investigate the artifacts of maps smoothed by unweighted and weighted headbanging.  We find substantial artifacts that depend on the spatial structure of the statistical variation (e. g. , the spatial pattern of sample sizes) and on the details of the spatial distribution of geographic units.  The methods used here could readily be adapted to study other spatial smoothers; we choose headbanging because (1) it is an important method used in practice, and (2) its heavily computational nature is naturally studied using simulation (in contrast to the analytical methods used by Gelman and Price 1 ). 
Estimating incumbency advantage and its variation, as an example of a before/after study| Abstract Incumbency advantage is one of the most studied features in American legislative elections.  In this paper, we construct and implement an estimate that allows incumbency advantage to vary between individual incumbents.  This model predicts that open-seat elections will be less variable than those with incumbents running, an observed empirical pattern that is not explained by previous models.  We apply our method to the U. S.  House of Representatives in the twentieth century: our estimate of the overall pattern of incumbency advantage over time is similar to previous estimates (although slightly lower), and we also find a pattern of increasing variation.  More generally, our multilevel model represents a new method for estimating effects in before/after studies. 
Models, assumptions, and model checking in ecological regressions \Lambda| Abstract Ecological regression is based on assumptions that are untestable from aggregate data.  However, these assumptions seem more questionable in some applications than others.  There has been some research on implicit models of individual data underlying aggregate ecological regression modeling.  We discuss ways in which these implicit models can be checked from aggregate data.  We also explore the differences in applications of ecological regressions in two examples: estimating the effect of radon on lung cancer in the United States, and estimating voting patterns for different ethnic groups in New York City. 
Type S error rates for classical and Bayesian single and multiple comparison procedures| Summary In classical statistics, the significance of comparisons (e. g. , ` 1\Gamma ` 2 ) is calibrated using the Type 1 error rate, relying on the assumption that the true difference is zero, which makes no sense in many applications.  We set up a more relevant framework in which a true comparison can be positive or negative, and, based on the data, you can state "` 1 ? ` 2 with confidence," ``` 2 ? ` 1 with confidence," or "no claim with confidence. " We focus on the Type S (for sign) error, which occurs when you claim "` 1 ? ` 2 with confidence" when ` 2 ? ` 1 (or vice-versa).  We compute the Type S error rates for classical and Bayesian confidence statements and find that classical Type S error rates can be extremely high (up to 50%).  Bayesian confidence statements are conservative, in the sense that claims based on 95% posterior intervals have Type S error rates between 0 and 2. 5%.  For multiple comparison situations, the conclusions are similar. 
Diagnostic checks for discrete-data regression models using posterior predictive simulations \Lambda| Abstract Model checking with discrete data regressions can be difficult because usual methods such as residual plots have complicated reference distributions that depend on the parameters in
Death by Survey: Estimating Adult Mortality without Selection Bias| Abstract The widely used methods for estimating adult mortality rates from sample survey responses about the survival of siblings, parents, spouses, and others depend crucially on an assumption that we demonstrate does not hold in real data.  We show that when this assumption is violated --- so that the mortality rate varies with sibship size --- mortality estimates can be massively biased.  By using insights from work on the statistical analysis of selection bias, survey weighting, and extrapolation problems, we propose a new and relatively simple method of recovering the mortality rate with both greatly reduced potential for bias and increased clarity about the source of necessary assumptions. 
Bayesian Model Selection in Social Research (with Discussion by| Abstract It is argued that P -values and the tests based upon them give unsatisfactory results, especially in large samples.  It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results.  Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest.  The Bayesian approach to hypothesis testing, model selection and accounting for model uncertainty is presented.  Implementing this is straightforward using the simple and accurate BIC approximation, and can be done using the output from standard software.  Specific results are presented for most of the types of model commonly used in sociology.  It is shown that this approach overcomes the difficulties with P values and standard model selection procedures based on them.  It also allows easy comparison of non-nested models, and permits the quantification of the evidence for a null hypothesis of interest, such as a convergence theory or a hypothesis about societal norms. 
Physiological pharmacokinetic analysis using population modeling and informative prior distributions \Lambda| Abstract We describe a general approach using Bayesian analysis for the estimation of parameters in physiological pharmacokinetic models.  The chief statistical difficulty in estimation with these models is that any physiological model that is even approximately realistic will have a large number of parameters, often comparable to the number of observations in a typical pharmacokinetic experiment (for example, 28 measurements and 15 parameters for each subject).  In addition, the parameters are generally poorly identified, akin to the well-known ill-conditioned problem of estimating a mixture of declining exponentials.  Our modeling includes (1) hierarchical population modeling as in Wakefield (1994), which allows partial pooling of information among different experimental subjects; (2) a pharmacokinetic model including compartments for well-perfused tissues, poorly-perfused tissues, fat, and the liver; and (3) informative prior distributions for population parameters, which is possible be\Lambda Scheduled to appear (with figures) in the Applications and Case Studies section of the Journal of the American Statistical Association, December, 1996 cause the parameters represent real physiological variables.  We discuss how to estimate the models using Bayesian posterior simulation, a method that automatically includes the uncertainty inherent in estimating such a large number of parameters (see, e. g. , Wakefield et al. , 1994).  We also discuss how to check model fit and sensitivity to the prior distribution using posterior predictive simulation.  We illustrate the application to the toxicokinetics of tetrachloroethylene (perchloroethylene, PERC), the problem that motivated this work. 
Rational voting and voter turnout| Abstract For voters with \social" preferences, the expected utility of voting is approximately
Convergence Assessment of Monte Carlo Simulations via the Score Statistic| Abstract In this paper, we present several applications of the score statistic in the context of convergence assessment for Monte Carlo simulations.  We begin by observing that the expected value of the score statistic is zero so that we may monitor the sample mean of this statistic throughout a simulation as a means to determine whether or not the simulation has been run for a suciently long time.  We also demonstrate a second assessment method based upon the idea of path sampling.  We demonstrate how the score statistic can be used to accurately estimate the stationary density using only a small number of simulated values and highlight the value of this technique for parametric density estimation.  In particular, these methods provide a powerful suite of tools which can be generically applied when alternatives such as the Rao-Blackwell density estimator is not available.  In this paper we focus upon the use of these density estimates for convergence assessment.  By running several replications of the chain, the corresponding estimated densities may be compared to assess how \close" the chains are to one another and to the true stationary distribution.  We explain how this may be done using both L 1 and L 2 distance measures.  We illustrate these new methods via the analysis of MCMC output arising from the study of first an autoregressive time series and then a collection censored survival data. 
Bayesian Computation for Parametric Models of Heteroscedasticity in the Linear Model| ABSTRACT In the linear model with unknown variances, one can often model the heteroscedasticity as var(y i ) = oe 2 f(w i ; `); where f is a fixed function, w i are the "weights" for the problem and ` is an unknown parameter (f(w i ; `) = w\Gamma ` i is a traditional choice).  We show how to do a fully Bayesian computation in this simple linear setting and also for a hierarchical model.  The full Bayesian computation has the advantage that we are able to average over our uncertainty in ` instead of using a point estimate.  We carry out the computations for a problem involving forecasting U. S.  Presidential elections, looking at different choices for f and the effects on both estimation and prediction. 
Direct data manipulation for local decision analysis, as applied to the problem of arsenic in drinking water from tube wells in Bangladesh| Abstract A wide variety of tools are available, both parametric and nonparametric, for analyzing spatial data.  However, it is not always clear how to translate statistical inferences into decision recommendations.  This paper explores the possibilities of estimating the effects of decision options using very direct manipulation of data, bypassing formal statistical analysis.  We illustrate with the application that motivated this research, a study of arsenic in drinking water in nearly 5000 wells in a small area in rural Bangladesh.  We estimate the potential benefits of two possible remedial actions: (1) recommendations that people switch to nearby wells with lower arsenic levels; and (2) drilling new community wells.  We use simple nonparametric clustering methods and estimate uncertainties using cross-validation. 
Analysis of Large-Scale Social Surveys| Abstract Large-scale social surveys are an important source of information for a
Improving upon probability weighting for household size \Lambda| Abstract By comparing data from national telephone polls to Census figures on household size (number of adults in household), we find large differences between population and sample, even after weighting respondents proportional to household size.  This presumably occurs because larger households are easier to reach and more likely to respond to the survey.  If a user wishes to weight on household size, we recommend poststratification on known population proportions of household size instead. 
Markov Chain Monte Carlo in Practice: A Roundtable Discussion| Markov chain Monte Carlo (MCMC) methods make possible the use of flexible Bayesian models that would otherwise be computationally infeasible.  In recent years, a great variety of such applications have been described in the literature.  Applied statisticians who are new to these methods may have several questions and concerns, however: How much effort and expertise are needed to design and use a Markov chain sampler? How much confidence can one have in the answers that MCMC produces? How does the use of MCMC affect the rest of the model-building process? At the Joint Statistical Meetings in August, 1996, a panel of experienced MCMC users discussed these and other issues, as well as various "tricks of the trade. " This article is an edited recreation of that discussion.  Its purpose is to offer advice and guidance to novice users of MCMC---and to notso-novice users as well.  Topics include building confidence in simulation results, methods for speeding and assessing convergence, estimating standard errors, identification of models for which good MCMC algorithms exist, and the current state of software development. 
DIVISION OF THE HUMANITIES AND SOCIAL SCIENCES| Abstract We investigate the construction of more precise estimates of a collection of population means using information about a related variable in the context of repeated sample surveys.  The method is illustrated using poll results concerning presidential approval rating (our related variable is political party identification).  We use post-stratification to construct these improved estimates, but since we don't have population level information on the post-stratifying variable, we construct a model for the manner in which the post-stratifier develops over time.  In this manner, we obtain more precise estimates without making possibly untenable assumptions about the dynamics of our variable of interest, the presidential approval rating. 
Comovement of International Financial Markets Loran Chollete, Victor de la Pe~ na, and Ching-| Abstract Do developed and emerging markets move together more in downturns than in upturns? In order to address this question we estimate the structure of dependence in international stock markets, using various parametric copulas.  Moreover, we nest symmetric and asymmetric dependence using a novel mixture model approach.  Our two most significant findings are as follows.  First, for G5 countries, asymmetric dependence dominates the model.  Second, and more surprisingly, for Latin American and Asian countries, there is mixed evidence on asymmetry of dependence.  In sum, our results suggest the interesting possibility that developed and developing economies differ in their exposure to downside risk, with developing economies having potentially less downside risk. 
General Methods for Monitoring Convergence of Iterative Simulations| We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence.  We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used.  We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences.  Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously. 
A probability model for golf putting| Abstract We derive a model, using trigonometry and the normal distribution, for the probability that a golf putt is successful.  We describe a class activity in which we lead the students through the steps of examining the data, considering possible models, constructing a probability model, and checking the fit.  The model is, of necessity, oversimplified, a point which the class discusses at the end of the demonstration.  1 Looking at data on golf putts Golf is a harder game than it looks.  A study of professional golf players found that they made less than 60% of their #ve-foot putts.  Figure 1 shows the success rate of golf putts as a function of distance from the hole.  (We found these data in the textbook by Berry, 1995, and this example is discussed further in Gelman and Nolan, 2001.  Further quantitative information on golf putting appears in Pelz, 1989. ) What do these data tell us about the accuracy of
Analysis of local decisions using hierarchical modeling, applied to home radon measurement and remediation \Lambda| Abstract This paper examines the decision problems associated with measurement and remediation of environmental hazards, using the example of indoor radon (a carcinogen) as a case study.  Innovative methods developed here include (1) the use of results from a previous hierarchical statistical analysis to obtain probability distributions with local variation in both predictions and uncertainties, (2) graphical methods to display the aggregate consequences of decisions by individuals, and (3) alternative parameterizations for individual variation in the dollar value of a given reduction in risk.  We perform cost-benefit analyses for a variety of decision strategies, as a function of home types and geography, so that measurement and remediation can be recommended where it is most effective.  We also briefly discuss the sensitivity of policy recommendations and outcomes to uncertainty in inputs.  For the home radon example, we estimate that if the recommended decision rule were applied to all houses in the United States, it would be possible to save the same number of lives as with the current official recommendations for about 40% less cost. 
in progress, "Storable Votes and Committee Size",|
Party Competition and Media Messages in U|S.  Presidential Election Campaigns. 
Estimating Incumbency Advantage without Bias|
[1990], \?,|
Systematic Consequences of Incumbency|
*** Rubin, and ***|
Validation of software for Bayesian models|
raham, attrition: the importance of nonresponse mechanisms and use of fo|
Racial Fairness in Legislative Redistricting|

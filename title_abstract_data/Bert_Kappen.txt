Monte Carlo Methods for Tempo Tracking and Rhythm Quantization| Abstract We present a probabilistic generative model for timing deviations in expressive music performance.  The structure of the proposed model is equivalent to a switching state space model.  The switch variables correspond to discrete note locations as in a musical score.  The continuous hidden variables denote the tempo.  We formulate two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as filtering and maximum a posteriori (MAP) state estimation tasks.  Exact computation of posterior features such as the MAP state is intractable in this model class, so we introduce Monte Carlo methods for integration and optimization.  We compare Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated annealing and iterative improvement) and sequential Monte Carlo methods (particle filters).  Our simulation results suggest better results with sequential methods.  The methods can be applied in both online and batch scenarios such as tempo tracking and transcription and are thus potentially useful in a number of music applications such as adaptive automatic accompaniment, score typesetting and music information retrieval. 
Bayesian Real-time Adaptation for Interactive Performance Systems| Abstract We introduce Bayesian online learning for real time parameter adaptation on a tempo tracking task.  We employ a variational extension of the Expectation-Maximization algorithm for online parameter estimation.  Simulation results on a real dataset indicate that online adaptation has the potential of capturing performer specific features in real time. 
On-line learning processes in artificial neural networks| Abstract We study on-line learning processes in artificial neural networks from a general point of view.  On-line learning means that a learning step takes place at each presentation of a randomly drawn training pattern.  It can be viewed as a stochastic process governed by a continuous-time master equation.  On-line learning is necessary if not all training patterns are available all the time.  This occurs in many applications when the training patterns are drawn from a time-dependent environmental distribution.  Studying learning in a changing environment, we encounter a conflict between the adaptability and the confidence of the network's representation.  Minimization of a criterion incorporating both effects yields an algorithm for on-line adaptation of the learning parameter.  The inherent noise of on-line learning makes it possible to escape from undesired local minima of the error potential on which the learning rule performs (stochastic) gradient descent.  We try to quantify these often made claims by considering the transition times between various minima.  We apply our results on the transitions from "twists" in twodimensional self-organizing maps to perfectly ordered configurations.  Finally, we discuss the capabilities of on-line learning for global optimization. 
Approximations of Bayesian Networks through KL Minimisation| Abstract Exact inference in large, complex Bayesian networks is computationally intractable.  Approximate schemes are therefore of great importance for real world computation.  In this paper we consider an approximation scheme in which the original Bayesian network is approximated by another Bayesian network.  The approximating network is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two networks.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  An important question in this scheme is how to choose the structure of the approximating network.  In this paper we show how redundant structures of the approximating model can be pruned in advance.  Simulation results of model selection and model optimisation are provided to illustrate the methods. 
A bridge between mean field theory and exact inference in probabilistic graphical models| Abstract Exact inference in large and complex probabilistic graphical models (e. g.  Bayesian networks, Boltzmann machines) is computationally intractable.  Approximate inference methods are therefore of great importance.  In this paper we provide a general scheme in which the original intractable graphical model is approximated by a model with a tractable structure.  The approximating model is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two models.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  The scheme provides a bridge between mean-field theory and exact computation.  Simulation results are provided to illustrate the method. 
Dynamic feature linking in stochastic networks with short range interactions| Abstract.  It is well established that cortical neurons display synchronous firing for some stimuli and not for others.  The resulting synchronous subpopulation of neurons is thought to form the basis of object perception.  In this paper this 'dynamic linking' phenomenon is demonstrated in networks of binary neurons with stochastic dynamics.  Feed-forward connections implement feature detectors and lateral connections implement memory traces or cell assemblies. 
On tempo tracking: Tempogram Representation and Kalman filtering| Abstract We formulate tempo tracking in a Bayesian framework where a tempo tracker is modeled as a stochastic dynamical system.  The tempo is modeled as a hidden state variable of the system and is estimated from a MIDI performance by Kalman filtering and smoothing.  We also introduce the Tempogram representation, a wavelet-like multiscale expansion of a real performance, on which the Kalman filter operates. 
Neural network analysis to predict treatment outcome in patients with ovarian cancer| Abstract The traditional technique to model survival probabilities is the Cox regression analysis [Cox and Oakes, 1984].  Recently, also neural networks have been applied for survival analysis and the prediction of prognosis in cancer treatment [Liestl K, 1994].  The main advantages of the neural network approach are the relative ease with which time dependencies in prognostic factors can be obtained, the improved prediction performance on independent test data for large numbers of input parameters [Kappen and Neijt, 1993], and the potential to model non-linear relations using hidden units.  Although this last point has only been established on artificial data [De Laurentiis and Ravdin, 1994], it probably constitutes the most important difference for future applications.  Neural networks are different from Cox's survival analysis in both the cost criterion that is optimized, the data model that is being used, and the treatment of censored patients.  In this paper we analyze the differences between the two approaches theoretically.  In addition, we illustrate the time dependent influence of the prognostic factors on a data base of 917 patients on patients with ovarian cancer. 
Integrating Tempo Tracking and Quantization using Particle Filtering| Abstract We present a probabilistic switching state space model for timing deviations in expressive music performance.  We formulate tempo tracking and automatic transcription (rhythm quantization) as filtering and maximum a posteriori (MAP) state estimation tasks.  The resulting model is suitable for real-time tempo tracking and transcription and hence useful in a number of music applications such as adaptive automatic accompaniment and score typesetting. 
A Dynamical Bayesian Network for Tempo and Polyphonic Pitch Tracking| The model is readily extensible to more complex sound generation processes. 
Learning processes in neural networks| Abstract We study the learning dynamics of neural networks from a general point of view.  The environment from which the network learns is defined as a set of input stimuli.  At discrete points in time, one of these stimuli is presented and an incremental learning step takes place.  If the time between learning steps is drawn from a Poisson distribution, the dynamics of an ensemble of learning processes is described by a continuous-time master equation.  A learning algorithm that enables a neural network to adapt to a changing environment, must have a nonzero learning parameter.  This constant adaptability, however, goes at the cost of fluctuations in the plasticities, such as synapses and thresholds.  The ensemble description allows us to study the asymptotic behavior of the plasticities for a large class of neural networks.  For small learning parameters we derive an expression for the size of the fluctuations in an unchanging environment.  In a changing environment, there is a trade-off between adaptability and accuracy (i. e. , size of the fluctuations).  We use the networks of Grossberg [J.  Stat.  Phys.  48, 105 (1969)] and Oja [J.  Math.  Biol.  15, 267 (1982)] as simple examples to analyze and simulate the performance of neural networks in a changing environment.  In some cases an optimal learning parameter can be calculated. 
Minimizing the System Error in Feedforward Neural Networks with Evolution Strategy| Abstract In this paper evolution strategy is applied to minimize the system error in feedforward neural networks.  The evolution strategy does not use externally tunedlearning parameters.  Moreover, it is not necessary to evaluate a gradient information as required by the backpropagation algorithm.  Experimental results arepresented and compared with the standardbackpropagation technique. 
Variational methods for approximate reasoning in graphical models| Abstract Exact inference in large and complex graphical models (e. g.  Bayesian networks) is computationally intractable.  Approximate schemes are therefore of great importance for real world computation.  In this paper we consider a general scheme in which the original intractable graphical model is approximated by a model with a tractable structure.  The approximating model is optimised by an iterative procedure, which minimises the Kullback-Leibler divergence between the two models.  The procedure is guaranteed to converge to a local minimum of the Kullback-Leibler divergence.  The scheme provides a bridge between naive mean-field theory and exact computation.  Simulation results are provided to illustrate the method. 
Survival Analysis: A Neural-Bayesian Approach| Abstract In this article we show that traditional Cox survival analysis can be improved upon when written in terms of a multi-layered perceptron and analyzed in the context of the Bayesian evidence framework.  The obtained posterior distribution of network parameters is approximated both by Hybrid Markov Chain Monte Carlo sampling and by variational methods.  We discuss the merits of both approaches.  We argue that the neuralBayesian approach circumvents the shortcomings of the original Cox analysis, and therefore yields better predictive results.  As a bonus, we apply the Bayesian posterior (the probability distribution of the network parameters given the data) to estimate p-values on the inputs. 
BACKGROUND INVARIANT FACE RECOGNITION| Abstract As a contribution to handling the symbol grounding problem in AI an object recognition system is presented that is exemplified with human faces.  It differs from earlier systems byapyramidal representation and the ability to cope with structured background. 
Improving Cox survival analysis with a neural-Bayesian approach| SUMMARY In this article we show that traditional Cox survival analysis can be improved upon when supplemented with sensible priors and analysed within a neural Bayesian framework.  We demonstrate that the Bayesian method gives more reliable predictions, in particular for relatively small data sets.  The obtained posterior (the probability distribution of network parameters given the data) which in itself is intractable, can be made accessible by several approximations.  We review approximations by Hybrid Markov Chain Monte Carlo sampling, a variational method and the Laplace approximation.  We argue that although each Bayesian approach circumvents the shortcomings of the original Cox analysis, and therefore yields better predictive results, in practice the use of variational methods or Laplace is preferable.  Since Cox survival analysis is infamous for its poor results with (too) many inputs, we use the Bayesian posterior to estimate p-values on the inputs and to formulate an algorithm for backward elimination.  We show that after removal of irrelevant inputs Bayesian methods still achieve significantly better results than classical Cox. 
Making Decisions with Probability Models| Abstract This paper considers the problem of classification based on a minimum number of measurements.  We compare two different approaches.  The first approach is to determine the optimal measurements as it is done in decision tree algorithms.  The second is to estimate a probability distribution from a database and to determine the optimal measurements from this distribution.  Test results indicate that the method using probability models results in better classifying trees. 
Boltzmann Machines and the EM algorithm| Abstract In this paper we formulate the Expectation Maximization (EM) algorithm for Boltzmann Machines and we prove that the Kullback distance is a Lyaponov function for the EM algorithm.  As a result the EM algorithm yields the same solutions as the original learning rule of Ackley, Hinton and Sejnowski.  We give an example of the EM algorithm applied to a special class of Boltzmann Machines (BM).  This class of BM's includes feedforward networks, radial basis networks and unsupervised clustering and probability density estimation networks.  For this Boltzmann Machine the EM algorithm gives a significant speed up compared to standard methods such as (conjugate) gradient descent. 
The actual modeling effort is done by specialists in internal medicine| Their knowledge is crucial to obtain the correct graphical structure and probabilities. 
Rhythm Quantization for Transcription| Abstract Automatic Music Transcription is the extraction of an acceptable notation from performed music.  One important task in this problem is rhythm quantization which refers to
Stochastic Resonance and Multi-modal Firing Patterns in Single-neuron Models| Abstract In analyzing data from experiments on periodically stimulated neurons, the interspike interval histogram shows a multi-modal distribution and under certain conditions, stochastic resonance.  We discuss a simple integrate and fire model of a single neuron which is capable of describing these two features.  As input for the neuron we use a periodic signal combined with poisson noise, this gives results that resemble data from experiments. 
Tempo tracking and rhythm quantization by sequential Monte Carlo| Abstract We present a probabilistic generative model for timing deviations in expressive music performance.  The structure of the proposed model is equivalent to a switching state space model.  We formulate two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as #ltering and maximum a posteriori (MAP) state estimation tasks.  The inferences are carried out using sequential Monte Carlo integration (particle filtering) techniques.  For this purpose, we have derived a novel Viterbi algorithm for Rao-Blackwellized particle #lters, where a subset of the hidden variables is integrated out.  The resulting model is suitable for realtime tempo tracking and transcription and hence useful in a number of music applications such as adaptive automatic accompaniment and score typesetting. 
Self-organization and nonparametric regression| Abstract We describe a framework for self-organization which relates the formation of topologic maps to minimization of a free energy function.  In the zero noise limit the resulting on-line learning rule is similar to the Kohonen learning rule.  We derive a fast EM-algorithm for finite training sets.  Choosing different noise parameters for input and output variables we obtain an algorithm for knot placement in nonparametric regression.  This algorithm naturally fits into projection pursuit regression when we treat the noise parameter as a projection vector. 
Application of Cluster Variation Method to Genetic Linkage Analysis| Abstract In this paper we discuss the application of the Cluster Variation Method to the problem of genetic linkage analysis.  The objective of genetic linkage analysis is to link an observed affection status (phenotype) of all individuals in a pedigree to the inheritance pattern observed certain locations on the chromosome.  The Bayesian approach introduced by [7] is computationally intractable for large pedigrees and we propose to apply the Cluster Variation Method to approximately compute quantities such as the likelihood of the data.  We obtain good results on an important subproblem, the computation of the intractable probability distribution of the inheritance pattern.  These results encourage the incorporation of the disease model into the inheritance model. 
An Application of Linear Response Learning| Abstract Linear response is an approximation method for Boltzmann machines based on mean field theory.  It is known that in the absence of hidden units this method can learn the network quite accurately with the costs of only one matrix inversion.  We show that adding a flat distribution to the target can decrease the classification error.  We apply linear response learning to a real world data set of digit recognition.  We show that the this method can compete with other known methods.  An advantage of linear response is the fast learning. 
Approximate Inference and Constrained Optimization| Abstract Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks.  Fixed points of these algorithms correspond to extrema of the Bethe and Kikuchi free energy (Yedidia et al. , 2001).  However, belief propagation does not always converge, which motivates approaches that explicitly minimize the Kikuchi/Bethe free energy, such as CCCP (Yuille, 2002) and UPS (Teh and Welling, 2002).  Here we describe a class of algorithms that solves this typically non-convex constrained minimization problem through a sequence of convex constrained minimizations of upper bounds on the Kikuchi free energy.  Intuitively one would expect tighter bounds to lead to faster algorithms, which is indeed convincingly demonstrated in our simulations.  Several ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over CCCP. 
A Generative Model for Music Transcription| Abstract In this paper we present a graphical model for polyphonic music transcription.  Our model, formulated as a Dynamical Bayesian Network, embodies a transparent and computationally tractable approach to this acoustic analysis problem.  An advantage of our approach is that it places emphasis on explicitly modelling the sound generation procedure.  It provides a clear framework in which both high level (cognitive) prior information on music structure can be coupled with low level (acoustic physical) information in a principled manner to perform the analysis.  The model is a special case of the, generally intractable, switching Kalman filter model.  Where possible, we derive, exact polynomial time inference procedures, and otherwise efficient approximations.  We argue that our generative model based approach is computationally feasible for many music applications and is readily extensible to more general auditory scene analysis scenarios. 
Learning process in neural networks,|
Error potentials for self-organization|
Cooling schedules for learning in neural networks|
Learning in neural networks with local minima|
Rhythm quantization for transcription" Computer Music|
Using Boltzmann Machines for probability estimation|
Means, Correlations and Bounds|
Robust time-quantization for music|
Learning-parameter adjustment in neural networks|
Error potential for self-organization|
A generative model for music transcription|
Deterministic learning rules for Boltzmann Machines|
Monte Carlo Methods for Tempo Tracking and Rhythm Quantization|
Radial basis Boltzmann machines and learning with missing values|

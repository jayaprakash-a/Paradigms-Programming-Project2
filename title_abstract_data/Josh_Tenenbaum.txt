Discovering Latent Classes in Relational Data| We present a framework for learning abstract relational knowledge with the aim of explaining how people acquire intuitive theories of physical, biological, or social systems.  Our approach is based on a generative relational model with latent classes, and simultaneously determines the kinds of entities that exist in a domain, the number of these latent classes, and the relations between classes that are possible or likely.  This model goes beyond previous psychological models of category learning, which consider attributes associated with individual categories but not relationships between categories.  We apply this domain-general framework to two specific problems: learning the structure of kinship systems and learning causal theories. 
Teacakes, Trains, Taxicabs and Toxins: A Bayesian Account of Predicting the Future| Abstract This paper explores how people make predictions about the future.  Statistical approaches to predicting the future are discussed, focussing on the method for predicting the future suggested by J.  R.  Gott (1993).  A generalized Bayesian form of Gott's method is presented, and a specific psychological model suggested.  Three experiments show that the predictions people make about the future are consistent with a Bayesian approach.  Despite the difficulty of predicting the future, people happily do it every day.  We are confident about being able to predict the durations of events, how much time we will need to get home after work, and how long it will take to finish the shopping.  In many cases we have a great deal of information guiding our judgments.  However, sometimes we have to make predictions based upon much less evidence.  When faced with new situations our decisions about how much longer we can expect events to last are based on whatever evidence is available.  When the only information we possess concerns how long a particular event has lasted until now, predicting the future becomes a task of induction.  In this paper we explore the question of how people predict the future when told only about the past.  We examine a simple statistical method of predicting the future, and consider how such a method could be made sufficiently flexible to be useful in everyday situations.  The resulting Bayesian model makes strong predictions about the effects of providing further information, the symmetry of this form of reasoning, and how it should be affected by prior knowledge.  We test these predictions empirically.  The Copernican Anthropic Principle A simple solution to the problem of predicting the future was recently proposed by the cosmologist J.  Richard Gott III (1993).  Gott's method is founded upon what he calls the "Copernican anthropic principle", which holds that .  .  .  the location of your birth in space and time in the Universe is priveleged (or special) only to the extent implied by the fact that you are an intelligent observer, that your location among intelligent observers is not special but rather picked at random (1993, p.  316) Gott extends this principle to reasoning about our position in time -- given no evidence to the contrary, we should not assume that we are in a "special" place in time.  This means that the time at which an observer encounters a phenomenon should be randomly located in the total duration of that phenomenon.  Denoting the time since the start of a phenomenon t past , and its total duration t total , Gott forms what he terms the "delta t argument".  Define the ratio r = t past t total (1) and assume that this is a random number between 0 and 1.  It is possible to form probabilistic predictions about the value of r.  For example, r will be between 0. 025 and 0. 975 with a probability P = 0:95, meaning that 1 39 t past ! t future ! 39t past (2) with 95% confidence, where t future = t total\Gamma t past .  Similarly, r will be less than 0. 5 with probability P = 0:5, so t past ! t future with 50% confidence.  This method of reasoning has been used to predict a wide range of phenomena.  Gott (1993) tells of his visit to the Berlin Wall in 1969 (t past = 8 years).  Assuming that his visit was randomly located in the period of the wall's existence, the 95% confidence interval for t future would be 2. 46 months to 312 years.  The wall fell 20 years later, consistent with these predictions.  Gott made similar calculations of t future for Stonehenge, the journal Nature, the U. S. S. R. , and even the human race.  Subsequent targets of the principle have included Broadway musicals and the Conservative government in Britain (Landsberg, Dewynne,
The Role of Causal Models in Reasoning Under Uncertainty| Abstract Numerous studies of how people reason with statistical data suggest that human judgment often fails to approximate rational probabilistic (Bayesian) inference.  We argue that a major source of error in these experiments may be misunderstanding causal structure.  Most laboratory studies demonstrating probabilistic reasoning deficits fail to explain the causal relationships behind the statistics presented, or they suggest causal mechanisms that are not compatible with people's prior theories.  We propose that human reasoning under uncertainty naturally operates over causal mental models, rather than pure statistical representations, and that statistical data typically support correct Bayesian inference only when they can be incorporated into a causal model consistent with people's theory of the relevant domain.  We show that presenting people with questions that clearly explain an intuitively natural causal structure responsible for a set of statistical data significantly improves their performance.  In particular, we describe two modifications to the standard medical diagnosis scenario that each eliminates the phenomenon of base-rate neglect, merely by clarifying the causal structure behind false-positive test results. 
Generalization, Similarity, and Bayesian Inference Behavioral and Brain Sciences| a commentary unless you have received the hard copy, invitation, instructions and deadline information.  For information on becoming a commentator on this or other BBS target articles, write to: bbs@soton. ac. uk For information about subscribing or purchasing offprints of the published version, with commentaries and author's response, write to: journals_subscriptions@cup. org (North America) or journals_marketing@cup. cam. ac. uk (All other countries). 
Using Physical Theories to Infer Hidden Causal Structure| Everyday reasoning draws on notions that go far beyond the observable world, just as modern science draws upon theoretical constructs beyond the limits of measurement.  The richness of our naive theories is a direct result of our ability to postulate hidden causal structure.  This capacity to reason about unobserved causes forms an essential part of cognition from early in life, whether we are reasoning about the forces involved in physical systems (e. g. , Shultz, 1982), the mental states of others (e. g. , Perner, 1991), or the essential properties of natural kinds (e. g. , Gelman & Wellman, 1991).  The central role of hidden causes in naive theories makes the question of how people infer hidden causal structure fundamental to understanding human reasoning.  Psychological research has shown that people can infer the existence of hidden causes from otherwise unexplained events (Ahn & Luhmann, 2003), and determine hidden causal structure from very little data (Kushnir, Gopnik, Schulz, & Danks, 2003).  This work has parallels in computer science, where the development of a formalism for reasoning about causality -- causal graphical models -- has led to algorithms that use patterns of dependency to identify causal relationships (Pearl, 2000; Spirtes, Glymour, & Scheines, 1993).  It has recently been proposed, chiefly by Gopnik, Glymour, and their colleagues (Glymour, 2001; Gopnik, Glymour, Sobel, Schulz, Kushnir, & Danks, 2004), that these algorithms may also explain how people infer causal structure.  A fundamental issue in explaining how people infer causal relationships is accounting for the interaction between abstract causal knowledge and statistical inference.  The classic debate between approaches that emphasize cause-effect covariation and those that emphasize mechanism knowledge (e. g. , Newsome, 2003) turns on this issue.  Causal graphical models provide a language in which the problem of causal induction can be formally expressed.  However, conventional algorithms for inducing causal structure (e. g. , Pearl, 2000; Spirtes et al. , 1993) do not provide a satisfying account of either the roles of causal knowledge or statistical inference, or their interaction.  These algorithms use tests of statistical independence to establish constraints that must be satisfied by causal structures consistent with the observed data.  No knowledge of how causal mechanisms operate, or the functional form of relationships between cause and effect, enters into the inference process.  As we argue below, such knowledge is necessary to explain how people are able to infer causal structure from very small samples, and to infer hidden causes from purely observational data.  Constraint-based methods are also unable to explain people's graded sensitivity to the strength of evidence for a causal structure, because they reason deductively from constraints to consistent structures.  We will present a rational account of human inference, Theory-Based Causal Induction, which emphasizes the interaction between causal knowledge and statistical learning.  Causal knowledge appears in the form of causal theories, specifying the principles by which causal relationships operate in a given domain.  These theories are used to generate hypothesis spaces of causal models -- some with hidden causes, some without -- that can be evaluated by domain-general statistical inference.  We will use this framework to develop models of people's inferences about hidden causes in two physical systems: a mechanical system called the stick-ball machine (Kushnir et al. , 2003), and a dynamical system involving an explosive compound called Nitro X.  Theory-based causal induction Our account of causal induction builds on causal graphical models, extending the formalism to incorporate the abstract knowledge about causal mechanism that plays an essential role in human inferences.  We will briefly introduce causal graphical models, consider how prior knowledge influences causal induction, and describe how we formalize the contribution of causal theories.  Causal graphical models Graphical models represent the dependency structure of a joint probability distribution using a graph in which nodes are variables and edges indicate dependence.  The graphical structure supports efficient computation of the probabilities of events involving these variables.  In a causal graphical model the edges indicate causal dependencies, with the direction of the arrow indicating the direction of causation, and they support inferences about the effects of interventions (Pearl, 2000).  An intervention is an event in which a variable is forced to hold a value, independent of any other variables on which it might depend.  Intervention on a variable A is denoted do(A).  Probabilistic inference on a modifified graph, in which incoming edges to A are removed, can be used to assess the consequences of intervening on A.  The structure of a causal graphical model implies a pattern of dependency among variables under observation and intervention.  Conventional algorithms for inferring causal structure use standard statistical tests, such as Pearson's # 2 test, to find the pattern of dependencies among variables, and then deductively identify the structure(s) consistent with that pattern (e. g. , Spirtes et al. , 1993).  These "constraint-based" algorithms can also exploit the results of interventions, and often require both observations and interventions in order to identify the hidden causal structure.  Gopnik, Glymour, and colleagues have suggested that this kind of constraint-based reasoning may underlie human causal induction (Glymour, 2001; Gopnik et al. , 2004; Kushnir et al. , 2003).  The role of causal theories Constraint-based algorithms for causal induction make relatively little use of prior knowledge.  While particular causal relationships can be ruled out a priori, there is no way to represent the belief that one structure may be more likely than another.  Furthermore, the use of statistical tests like # 2 makes only weak assumptions about the form of causal relationships: these tests simply assess dependency, regardless of whether a relationship is positive or negative, deterministic or probabilistic, strong or weak.  Several researchers (e. g. , Shultz, 1982) have argued that knowledge of causal mechanism plays a central role in human causal induction.  Mechanism knowledge is usually cited in arguments against statistical causal induction, but we view it as critical to explaining how statistical inferences about causal structure are possible from sparse data.  Knowledge about causal mechanisms provides two kinds of restrictions on possible causal models: restrictions on which relationships are plausible, and restrictions on the functional form of those relationships.  Restrictions on plausibility might indicate that one causal structure is more likely than another, while restrictions on functional form might indicate that a particular relationship should be positive and strong.  These restrictions have important implications for causal induction algorithms.  If all structures are possible, both observations and interventions are typically required to identify hidden causes, and without strong assumptions about the functional form of causal relationships, samples must be relatively large.  With limitations on the set of possible causal structures and expectations about functional form, however, it is possible to make causal inferences from just observations and from small samples -- important properties of human causal induction.  Using causal theories in causal induction The causal mechanism knowledge that is relevant for statistical causal inference may be quite abstract, and may also vary across domains.  Much of this knowledge may be represented in intuitive domain theories.  In contrast to Gopnik et al.  (2004), who suggest that causal graphical models are the primary substrate for intuitive theories, we emphasize the role of intuitive theories at a more abstract level, providing restrictions on the set of causal models under consideration.  Such restrictions cannot be represented as part of a causal graphical model: causal graphical models express the relations that hold among a finite set of propositions, while causal theories involve statements about all relations that could hold among entities in a given domain.  Formally, we view causal theories as hypothesis space generators: a theory is a set of principles that can be used to generate a hypothesis space of causal models, which are compared via Bayesian inference.  The principles that comprise a theory specify which relations are plausible and the functional form of those relations.  These principles articulate how causal relationships operate in a given domain, but need not identify the mechanisms underlying such relationships: all that is necessary for causal induction is the possibility that some mechanism exists, and expectations about the functional form associated with that mechanism.  This vague and abstract mechanism knowledge is consistent with the finding that people's understanding of causal mechanism is surprisingly shallow (Rozenblit & Keil, 2002).  In the remainder of the paper, we will demonstrate how Theory-Based Causal Induction can be used to explain human inferences about hidden causes in physical systems.  Different systems require different causal theories.  We will examine inferences in a mechanical system, the stick-ball machine (Kushnir et al. , 2003), and in a dynamical system, Nitro X, which we explore in a new experiment.  When reasoning about these systems, people infer hidden causal structure from very few observations, and are sensitive to graded degrees of evidence.  The stick-ball machine Kushnir et al.  (2003) conducted two experiments in which participants had to infer the causal structure
Semi-Supervised Learning with Trees| Abstract We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain.  The tree (or a distribution over trees) may be inferred using the unlabeled data.  A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification function from the labeled examples.  We test our approach on eight real-world datasets. 
Dynamical Causal Learning| Abstract Current psychological theories of human causal learning and judgment focus primarily on long-run predictions: two by estimating parameters of a causal Bayes nets (though for different parameterizations), and a third through structural learning.  This paper focuses on people's short-run behavior by examining dynamical versions of these three theories, and comparing their predictions to a real-world dataset. 
Active inference in concept learning| Abstract People are active experimenters, constantly seeking new information relevant to their goals.  A reasonable approach to active information gathering is to ask questions and conduct experiments that minimize the expected state of uncertainty, or maximize the expected information gain, given current beliefs (Fedorov, 1972; MacKay, 1992; Oaksford & Chater, 1994).  In this paper we present results on an exploratory experiment designed to study people's active information gathering behavior on a concept learning task.  The results of the experiment suggest subjects' behavior may be explained well from the point of view of Bayesian information maximization. 
Learning bilinear models for two-factor problems in vision| Abstract In many vision problems, wewant to infer two (or more) hidden factors which interact to produce our observations.  We may want to disentangle illuminant and object colors in color constancy; rendering conditions from surface shape in shape-from-shading; face identity and head pose in face recognition; or font and letter class in character recognition.  We refer to these two factors generically as \style" and \content".  Bilinear models offer a powerful framework for extracting the two-factor structure of a set of observations, and are familiar in computational vision from several well-known lines of research.  This paper shows how bilinear models can be used to learn the style-content structure of a pattern analysis or synthesis problem, which can then be generalized to solve related tasks using di#erentstyles and/or content.  We focus on three kinds of tasks: extrapolating the style of data to unseen content classes, classifying data with known content under a novel style, and translating two sets of data, generated in di#erentstyles and with distinct content, into each other's styles.  We show examples from color constancy, face pose estimation, shape-from-shading, typography and speech. 
Theory-Based Causal Inference| Abstract People routinely make sophisticated causal inferences unconsciously, effortlessly, and from very little data -- often from just one or a few observations.  We argue that these inferences can be explained as Bayesian computations over a hypothesis space of causal graphical models, shaped by strong top-down prior knowledge in the form of intuitive theories.  We present two case studies of our approach, including quantitative models of human causal judgments and brief comparisons with traditional bottom-up models of inference. 
Theory-Based Induction| Abstract We show how an abstract domain theory can be
Sparse multidimensional scaling using landmark points| Abstract: In this paper, we discuss a computationally efficient approximation to the classical multidimensional scaling (MDS) algorithm, called Landmark MDS (LMDS), for use when the number of data points is very large.  The first step of the algorithm is to run classical MDS to embed a chosen subset of the data, referred to as the `landmark points', in a low-dimensional space.  Each remaining data point can be located within this space given knowledge of its distances to the landmark points.  We give an elementary and explicit theoretical analysis of this procedure, and demonstrate with examples that LMDS is effective in practical use. 
Structure Learning in Human Causal Induction| Abstract We use graphical models to explore the question of how people learn simple causal relationships from data.  The two leading psychological theories can both be seen as estimating the parameters of a fixed graph.  We argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure, and we propose to model this inductive process as a Bayesian inference.  Our argument is supported through the discussion of three data sets. 
Mapping a Manifold of Perceptual Observations| Abstract Nonlinear dimensionality reduction is formulated here as the problem of trying to find a Euclidean feature-
The Large-Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth| Abstract We present statistical analyses of the large-scale structure of three types of semantic networks: word associations, WordNet, and Roget's thesaurus.  We show that they have a small-world structure, characterized by sparse connectivity, short average path-lengths between words, and strong local clustering.  In addition, the distributions of the number of connections follow power laws that indicate a scale-free pattern of connectivity, with most nodes having relatively few connections joined together through a small number of hubs with many connections.  These regularities have also been found in certain other complex natural networks, such as the world wide web, but they are not consistent with many conventional models of semantic organization, based on inheritance hierarchies, arbitrarily structured networks, or high-dimensional vector spaces.  We propose that these structures reflect the mechanisms by which semantic networks grow.  We describe a simple model for semantic growth, in which each new word or concept is connected to an existing network by differentiating the connectivity pattern of an existing node.  This model generates appropriate small-world statistics and power-law connectivity distributions, and also suggests one possible mechanistic basis for the effects of learning history variables (age-ofacquisition, usage frequency) on behavioral performance in semantic processing tasks. 
Rules and Similarity in Concept Learning| Abstract A popular view holds that learning and generalizing concepts depends on two fundamentally distinct modes of representation: rules and similarityto-exemplars.  Through a combination of experiments and formal analysis, I show how a Bayesian framework offers a unifying account of both rule-based and similarity-based generalization.  Bayes explains the specific workings of these two modes -- which rules are abstracted, how similarity is measured -- as well as why generalization appears rule-based or similarity-based in different situations.  I conclude that the distinction between rules and similarity in concept learning may be useful at the level of heuristic algorithms, but is not computationally fundamental. 
Word learning as Bayesian inference| Abstract We apply a computational theory of concept learning based on Bayesian inference (Tenenbaum, 1999) to the problem of learning words from examples.  The theory provides a framework for understanding how people can generalize meaningfully from just one or a few positive examples of a novel word, without assuming that words are mutually exclusive or map only onto basic-level categories.  We also describe experiments with adults and children designed to evaluate the model. 
Bayesian models of human action understanding| Abstract We present a Bayesian framework for explaining how people reason about and predict the actions of an intentional agent, based on observing its behavior.  Action-understanding is cast as a problem of inverting a probabilistic generative model, which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their environment.  Working in a simple sprite-world domain, we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change.  The model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform, and also fits quantitative predictions that adult observers make in a new experiment. 
Unsupervised learning of overlapping concepts| Abstract Natural concepts can overlap arbitrarily, with objects being members of a number of categories.  This kind of structure cannot be captured by existing hierarchical or probabilistic clustering methods.  Motivated by the problem of word learning by young children, we present two simple approaches to identifying overlapping clusters in data, based on fitting multiple independent mixture models.  We apply these approaches to two benchmark synthetic data sets. 
Inferring causal networks from observations and interventions| Abstract Information about the structure of a causal system can come in the form of observational data--random samples of the system's autonomous behavior---or interventional data---samples conditioned on the particular values of one or more variables that have been experimentally manipulated.  Here we study people's ability to infer causal structure from both observation and intervention, and to choose informative interventions on the basis of observational data.  In three causal inference tasks, participants were to some degree capable of distinguishing between competing causal hypotheses on the basis of purely observational data.  Performance improved substantially when participants were allowed to observe the effects of interventions that they performed on the systems.  We develop computational models of how people infer causal structure from data and how they plan intervention experiments, based on the representational framework of causal graphical models and the inferential principles of optimal Bayesian decision-making and maximizing expected information gain.  These analyses suggest that people can make rational causal inferences, subject to psychologically reasonable representational assumptions and computationally reasonable processing constraints. 
Learning Causal Laws| Abstract Attempts to characterize people's causal knowledge of a domain in terms of causal network structures miss a key level of abstraction: the laws that allow people to formulate meaningful causal network hypotheses, and thereby learn and reason about novel causal systems so effectively.  We outline a preliminary framework for modeling causal laws in terms of generative grammars for causal networks.  We then present an experiment showing that causal grammars can be learned rapidly in a novel domain and used to support one-shot inferences about the unobserved causal properties of new objects.  Finally, we give a Bayesian analysis explaining how causal grammars may be induced from the limited data available in our experiments.  Causal Grammars Recently there has been substantial progress in understanding how people learn causal relations, or causal networks connecting multiple causes and effects.  Here we construe causal network broadly to include any collection of (domain-specific) causal beliefs that can be represented as a set of nodes and a set of (directed) links between nodes.  Nodes may represent objects, properties of or relations between objects, or events.  Links may have different causal semantics depending on the semantics of the nodes.  For instance, the network N 0 (Figure 1) might represent some aspects of a person's knowledge about several common diseases, their effects (symptoms), and causes (risky behaviors).  Our thesis here is that attempts to characterize people's causal knowledge of a domain primarily in terms of such network structures (e. g. , Gopnik & Glymour, 2002; Rehder, in press), while revealing in some important ways, miss a key level of abstraction: the laws that allow people to formulate meaningful causal network hypotheses, and thereby learn and reason about novel causal systems so effectively.  For instance, in Figure 1, there appears to be a common domain theory underlying networks N 0 , N 1 , and N 2 , which distinguishes them from N 3 , but is not explicitly represented in any of them.  We present a framework for representing such abstract causal knowledge, which we call causal grammar.  The framework is surely incomplete and oversimplified; we view it as merely a first pass at a deep and hard problem.  We also describe an experimental study of how people learn and use causal grammars, and briefly sketch a theory of learning based on Bayesian inference.  The networks N 1 and N 2 differ from N 0 in the precise causal links or disease nodes they posit, but they express the same essential regularities: three classes N 0
Parametric Embedding for Class Visualization| Abstract In this paper, we propose a new method, Parametric Embedding (PE), for visualizing the posteriors estimated over a mixture model.  PE
An example-based approach to style translation for line drawings| Abstract We present an example-based system for translating line drawings into different styles.  The system is given a training set of many different lines, each drawn by an artist in various styles, which is used to translate new lines made by a user into a particular desired style with a K-nearest neighbor algorithm.  This algorithm fits each input line as a linear combination of the several training lines in the same style which are most similar to it.  The fit line can then be rendered in different styles because the training set contains versions of each training line in eachstyle.  By describing input lines as linear combinations of training set lines, this procedure is expressive enough to fit a broad range of input drawings.  By restricting these linear combinations to contain only the most similar training set lines, this procedure is constrained enough to preserve the distinctive stylistic features of translated lines.  We represent input lines by splines with nonuniformly spaced control points, which emphasizes these stylistic features.  Our example-based approachhasanumber of advantages over conventional parameteric approaches to translating style.  It can handle styles which are difficult to describe parametrically, and its repertoire can be easily extended by the user at any time.  Moreover, given appropriate representations, it can be generalized to modify the style of other kinds of graphics objects, such as the font of a letter or the movementstyle of an animated character. 
Children's causal inferences from indirect evidence: Backwards blocking and Bayesian reasoning in preschoolers| Abstract Previous research suggests that children can infer causal relations from patterns of events.  However, what appear to be cases of causal inference may simply reduce to children recognizing relevant associations among events, and responding based on those associations.  To examine this claim, in Experiments 1 and 2, children were introduced to a "blicket detector," a machine that lit up and played music when certain objects were placed upon it.  Children observed patterns of contingency between objects and the machine's activation that required them to use indirect evidence to make causal inferences.  Critically, associative models either made no predictions, or made incorrect predictions about these inferences.  In general, children were able to make these inferences, but some developmental differences between 3and 4-year-olds were found.  We suggest that children's causal inferences are not based on recognizing associations, but rather that children develop a mechanism for Bayesian structure learning.  Experiment 3 explicitly tests a prediction of this account.  Children were asked to make an inference about ambiguous data based on the base rate of certain events occurring. 
Running head: ELEMENTAL CAUSAL INDUCTION Elemental causal induction| Abstract We present a framework for the rational analysis of elemental causal induction -- learning about the existence of a relationship between a single cause and effect -- based upon causal graphical models.  This framework makes precise the distinction between causal structure and causal strength: the difference between asking whether a causal relationship exists and asking how strong that causal relationship might be.  We show that two leading rational models of elemental causal induction, #P and causal power, both estimate causal strength, and introduce a new rational model, causal support, that assesses causal structure.  Causal support predicts several key phenomena of causal induction that cannot be accounted for by other rational models, which we explore through a series of experiments.  These phenomena include the complex interaction between #P and the base-rate probability of the effect in the absence of the cause, sample size effects, inferences from incomplete contingency tables, and causal learning from rates.  Causal support also provides a better account of a number of existing datasets than either #P or causal power. 
Learning Domain Structures| Psychologists have argued that cognition in different domains draws on qualitatively different mental representations.  Tree structures appear well-suited to representing relationships between animal species [1, 2, 10], while a one-dimensional structure (the liberalconservative spectrum) seems better for representing people's political views.  The possibility of different structures raises a fundamental question: how do people learn what kind of structure is appropriate in each domain? The standard approach to this question is to reject one of its assumptions.  Nativists deny that core structures are learned, at least for evolutionarily important domains like folkbiology.  Instead, infants come equipped with innate knowledge about which structures are appropriate for which domains.  Atran [1], for example, argues that folkbiology is a core domain of human knowledge, and that the tendency to group living kinds into hierarchies reflects an "innately determined cognitive structure. " More generally, Keil [8] has argued that ontological knowledge obeys an innate "M-constraint", requiring the extensions of predicates to conform to rigidly treestructured hierarchies of objects.  Alternatively, empiricists generally deny that structured representations are present at all.  Domain-specific representations are merely emergent properties of unstructured, domain-general associative learning architectures.  McClelland and Rogers [12], for example, have recently suggested that the acquisition of semantic knowledge in domains such as intuitive biology can be explained as learning in a generic connectionist network.  Their architecture never explicitly represents any tree structure, although with repeated training, its hidden unit representations may implicitly come to approximate the taxonomic relations between biological species.  This paper proposes an alternative approach -- structure learning -- that combines important insights from both of these traditions.  Our key contribution is to show how structured domain representations can be acquired within a domain-general framework for Bayesian inference.  Like nativists, we suggest that different domains are represented with qualitatively different structures, and we show how these structured representations serve as critical constraints on inductive generalization.  Like empiricists, though, we emphasize the importance of learning, and attempt to show how domain structures can be acquired through domain-general statistical inference.  This is not only more parsimonious than the nativist position, but allows us to explain the origin of structured representations in novel domains, where the prior existence of domain-specific innate structure is highly implausible.  After describing our structure learning framework, we present two empirical tests of its performance.  First, we show that it chooses the appropriate domain structure for both synthetic and real-world data sets.  It correctly chooses a tree structure for a biological domain (animal feature judgments), and a linear structure for a political domain (US Supreme Court decisions).  Second, we model two classic data sets of inductive judgments in biology [13] and show that our framework performs better than an unstructured connectionist approach.  Bayesian structure learning Our proposal takes the form of a rational analysis.  We aim to demonstrate the computational plausibility and explanatory value of Bayesian structure learning, but leave for future work the question of how these computations might be implemented or approximated by cognitive processes.  Assume the learner's data consist of a binary-valued object-feature matrix D specifying the features of each object in a given domain.  In biology, for instance, the rows of D might correspond to species, and the columns to anatomical and behavioral attributes.  The entry in row i and column j would then specify the value of feature j for species i.  Structurelearning includes computational problems at two levels.  First, which structure class is most appropriate for the domain? Second, given a structure class, which structure in that class provides the best account of the data? For instance, suppose that a learner exposed to biological data ends up organizing animal species into a taxonomic tree.  The first problem asks how she knew to use a tree rather than some other kind of structure.  The second problem asks why she settled on one specific tree instead of the many other trees she might have chosen.  Our focus here is on the first problem -- the problem of inferring the right structure class for a domain.  A solution to the second problem, however, falls out of our probabilistic approach.  We assume that learners come to a domain equipped with a hypothesis space of structure classes, either constructed from innate primitives or based on analogies with previously learned domains.  For simplicity, this paper considers a hypothesis space of just three canonical classes: taxonomic trees, one-dimensional (linear) spaces, and independent feature models.  People surely have access to other classes, including higherdimensional spaces, flat (non-hierarchical) clusterings, and causal networks.  We leave it to future work to characterize the full range of structure classes accessible to human cognition.  In particular, it is an open question whether this space is small enough to be explicitly enumerated as we do here, or is so large (perhaps infinite or uncountable) that it can be specified only implicitly through some generating mechanism.  Future work should also consider the possibility that multiple structures may apply within a single domain.  Given a set of probabilistic models, Bayesian techniques can be used to evaluate which of the models is most likely to have generated some data [7].  Before these techniques can be applied to inferring domain structures, we need to associate each structure class in our hypothesis space with a probabilistic generative model for the features of objects.  The next section defines these models, but here we show how Bayesian inference can be used to choose between them.  Let D be an object-feature matrix generated from one of several structure classes.  The posterior probability of each class C i is proportional to the product of the likelihood p(D|C i ) and the prior probability p(C i ).  If we assign equal prior probabilities to each class (as we do throughout this paper), the best class is the class that makes the data most likely.  Computing the likelihood p(D|C i ) requires integrating over all structures S belonging to structure class C i : p(D|C i ) = Z p(D|S, C i )p(S|C i )dS, (1) Intuitively, this means that a structure class C i provides a good account of object-feature data D if the data are highly probable under a range of structures S in class C i , and if these structures themselves have high prior probability within C i .  The following section explains how the fit of each structure to the data, p(D|S, C i ), is computed for several structure classes.  We estimate the integral in Equation 1 using stochastic approximations.  First we run a Markov chain Monte Carlo simulation to draw a sample of m structures, {S j }, from the distribution p(S|D, C i ).  We then approximate p(D|C i ) by the harmonic mean estimator [7]: p(D|C i ) = 0 @ 1 m m X j=1 1 p(D|S j , C i ) 1 A1 1 .  (2) This estimator does not satisfy a central limit theorem, and can be thrown off by a sample with very low likelihood.  Despite its limitations, it is often sufficient to identify a model that is very much better than its competitors.  In future work we plan to estimate these integrals more accurately using path sampling [4].  From structures to probabilistic models We will work with three probabilistic models, each appropriate for a different structure class, and show how to compute the likelihoods p(D|S, C i ) for structures in each class.  For simplicity we assume here that all features are binary, but our framework extends naturally to multi-valued or continuous features.  C T : Taxonomic trees Class C T is the set of taxonomic trees --- rooted trees with the objects in D as their leaves.  This is a natural representation when the objects are the outcome of an evolutionary process.  We restrict ourselves to ultrametric trees --- trees where each leaf node is at the same distance from the root.  Assume that each feature is generated by a mutation process over the tree.  We formalize the mutation process using a simple biological model [11].  Suppose that a feature F is defined at every point along every branch, not just at the leaf nodes where the data points lie.  Imagine F spreading out over the tree from root to leaves --- it starts out at the root with some value and could switch values at any point along any branch.  Whenever a branch splits, both lower branches inherit the value of F at the point immediately before the split.  Figure 1(a) shows one mutation history for a binary feature on a tree with four objects.  A B C D (a) A B C D (b) Figure 1: (a) A tree with four objects (A, B, C and D) and three internal nodes.  A mutation history for a single feature is shown.  The feature is off at the root, but switches on at two places in the tree.  Shaded nodes have value 1, clear nodes have value 0, and crosses indicate mutations.  (b) A line with four objects.  We formalize this model of mutation using a Poisson arrival process.  Under this process, the probability that
Learning the Structure of Similarity| Abstract The additive clustering (ADCLUS) model (Shepard & Arabie, 1979) treats the similarity of two stimuli as a weighted additive measure of their common features.  Inspired by recent work in unsupervised learning with multiple cause models, we propose a new, statistically well-motivated algorithm for discovering the structure of natural stimulus classes using the ADCLUS model, which promises substantial gains in conceptual simplicity, practical efficiency, and solution quality over earlier efforts.  We also present preliminary results with artificial data and two classic similarity data sets. 
Dynamical Causal Learning| Abstract Current psychological theories of human causal learning and judgment focus primarily on long-run predictions: two by estimating parameters of a causal Bayes nets (though for different parameterizations), and a third through structural learning.  This paper focuses on people's short-run behavior by examining dynamical versions of these three theories, and comparing their predictions to a real-world dataset. 
Learning style translation for the lines of a drawing| MIT We present an example-based method for translating line drawings into different styles.  We fit each line as a linear combination of similar lines in a training set, and interpolate between the corresponding training examples in the output style.  The synthesized lines preserve the desired stylistic features of the output style. 
Hierarchical Topic Models and the Nested Chinese Restaurant Process| Abstract We address the problem of learning topic hierarchies from data.  The model selection problem in this domain is daunting---which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process.  This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections.  We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation.  We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 
Integrating Topics and Syntax| Abstract Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words.  We present a generative model that uses both kinds of dependencies, and is capable of simultaneously finding syntactic classes and semantic topics despite having no knowledge of syntax or semantics beyond statistical dependency.  This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 
Global versus local methods in nonlinear dimensionality reduction| Abstract Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]).  We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps. 
Separating Style and Content with Bilinear Models| Abstract PERCEPTUAL systems routinely separate \content" from \style", classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions.  Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive.  Existing factor models are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms.  Here we show how perceptual systems may learn to solve these crucial tasks using surprisingly simple bilinear models.  We report promising results in three realistic perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants. 
The Rational Basis of Representativeness| Why do people think that Linda, the politically active, single, outspoken, and very bright 31-year-old, is more likely to be a feminist bankteller than to be a bankteller, even though this is logically impossible? Why do we think that the sequence HHTHT is more likely than the sequence HHHHH to be produced by flipping a fair coin, even though both are equally likely? The standard answer in cognitive psychology (Kahneman & Tversky, 1972) is that our brains are designed to judge "representativeness", not probability: Linda is more representative of feminist banktellers than of banktellers, and HHTHT is more representative of flipping a fair coin than is HHHHH, despite anything that probability theory tells us.  Not only errors in probabilistic reasoning, but numerous other phenomena of categorization, comparison, and inference have been attributed to the influence of representativeness (or prototypicality or "goodness of example"; Mervis & Rosch, 1981; Osherson, Smith, Wilkie, Lopez, & Shafir, 1990; Rips, 1975).  However, a principled account of representativeness has not been easy to come by.  Its leading proponents (Kahneman & Tversky, 1996; Mervis & Rosch, 1981) have asserted that representativeness should be defined only operationally in terms of people's judgments; an a priori, analytic definition need not be given.  Critics have countered that this concept is too vague to serve as an explanation of intuitive probability judgment (Gigerenzer, 1996).  This paper presents a framework for constructing rational models of representativeness, based on a Bayesian analysis of what makes an observation a good example of a category or process.  The goal is to identify precisely one sense of representativeness and show that it has a rational basis in normative principles of inductive reasoning.  We will first point out some shortcomings of previous accounts based on likelihood or similarity, and show how a Bayesian approach can overcome those problems.  We will then compare the quantitative predictions of Bayesian, likelihood, and similarity models on two sets of representativeness judgments.  Previous approaches Likelihood.  In trying to relate intuitions about representativeness to rational statistical inferences, a natural starting point is the concept of likelihood.  Let d denote some observed data, such as a sequence of coin tosses, and h denote some hypothesis about the source of d, such as flipping a fair coin.  The probability of observing d given that h is true, P(d|h), is called a likelihood.  Let R(d,h) denote representativeness -- how representative the observation d is of the generative process in h.  Gigerenzer & Hoffrage (1995) have proposed that representativeness, to the extent that it can be defined rigorously, is equivalent to likelihood: R(d,h)=P(d|h).  This proposal is appealing in that, other factors aside, the more frequently h leads to observing d, the more representative d should be of h.  It is also consistent with some classic errors in probability judgment, such as the conjunction fallacy: a person is almost certainly more likely to match Linda's description given that she is a bankteller and a feminist than given only that she is a bankteller.  While likelihood and representativeness seem related, however, they are not equivalent.  Two observations with equal likelihood may differ in representativeness.  Knowing that HHHHH and HHTHT are equally likely to be produced by a fair coin does not change our judgment that the latter is the more representative outcome.  Tversky & Kahneman (1983) provide several examples of cases in which a more representative outcome is actually less likely.  Any sequence of fair coin flips, such as THHHTHT, is less likely than one of its subseqences, such as H or HHH, but may easily be more representative.  More colorfully, "being divorced four times" is more representative of Hollywood actresses than is "voting democratic", but the former is certainly less likely.  Figure 1 illustrates a simple version of the dissociation between representativeness and likelihood.  Each panel shows a sample of three points from a Gaussian distribution.  With independent sampling, the total likelihood of a sample equals the product of the likelihoods for X p(X|h) X Figure 1: Given a normal distribution, the left sample has greater likelihood but the right is more representative.  each item in the sample.  Thus the left sample has much greater likelihood, because each point is much closer to the peak of the distribution than in the right sample.  Yet the more spread-out sample on the right seems more representative.  We tested this intuition in a survey of 138 Stanford undergraduates.  They were first shown a normally distributed set of thirty "widgets" produced by a factory.  The widgets were simple drawings resembling nuts or bolts, varying only in their sizes.  They were then shown three different samples, each with three widgets, and asked to rate on a scale of 1-10 how representative each sample was of the widgets produced by this factory.  Each sample contained a point at the mean of the original distribution, and points at z = 2. 85 ("broad sample"), z = 1 ("intermediate sample"), or z = 0. 05 ("narrow sample").  The intermediate sample, with a standard deviation similar to the population, received a significantly higher rating than did the much more likely narrow sample (7. 1 vs.  5. 2, p <. 05).  The broad sample, with lowest likelihood of all, also received a lower rating (6. 9) than the intermediate sample, but not by a significant margin.  We also tested whether intermediate-range samples are more representative for natural categories, using as stimuli black-and-white pictures of birds.  In a design parallel to the widget study, 135 different Stanford undergraduates saw three samples of birds, each containing three members, and rated how representative they were of birds in general.  The samples consisted of either three robins ("narrow"); a robin, an eagle, and a seagull ("intermediate"); or a robin, an ostrich, and a penguin ("broad").  Although the robins were individually rated as more representative than the other birds (by a separate group of 100 subjects), the set of three robins was considered the least representative of the three samples.  As with the widgets, the intermediate sample was rated more representative (6. 3) than either the narrow (5. 1) or broad (5. 3) samples (p <. 05 for both differences).  For natural categories as well as for the artificial widgets, a set of representative examples turns out not to be the most representative set of examples.  Sample likelihood, because it is merely the product of each example's individual likelihood, cannot capture this phenomenon.  At best, then, likelihood may be only one factor contributing to the computation of representativeness.  Similarity.  Most attempts to explicate the mechanisms of representativeness, including that of Kahneman & Tversky (1972), rely not on likelihood but on some sense of similarity.  That is, an observation d is representative of a category or process h to the extent that it is similar to the set of observations h typically generates.  Similarity seems to avoid some of the problems that likelihood encounters.  HHTHT may be more representative of a fair coin than HHHHH because it is more similar on average to other coin flip sequences, based on such features as the number of heads or the number of alternations.  Likewise, someone who has been divorced four times may be more similar to the prototypical Hollywood actress than someone who votes democratic, if marital status is weighted more heavily than political affiliation in computing similarity to Hollywood actresses.  However, the explanatory power of a similarity-based account hinges on being able to specify what makes two stimuli more or less similar, what the relevant features are and how are they weighted.  Similarity unconstrained is liable to lead to circular explanations: having had multiple divorces is more representative of Hollywood actresses because marital status is more highly weighted in computing similarity to Hollywood actresses, but why is marital status so highly weighted, if not because having multiple divorces is so typical of Hollywood actresses? Equating representativeness with similarity also runs into a problem when evaluating the representativeness of a set of objects, as in Figure 1.  Similarity is usually defined as a relation between pairs of stimuli, but here we require a judgment of similarity between two sets of stimuli, the sample and the population.  It is not immediately obvious how best to extend similarity from a pairwise to a setwise measure.  The individual elements of the left sample are certainly more similar to the average member of the population than are the elements of the right sample.  The left sample also comes closer to minimizing the average distance between elements of the population and elements of the sample.  If similarity between sets is defined according to one of these measures, it will fail to match up with representativeness.  Finally, and most problematic for our purposes here, a definition in terms of similarity fails to elucidate the rational basis of representativeness, and thus brings us no closer to explaining when and why representativeness leads to reasonable statistical inferences.  Hence we seem to be left with two less-than-perfect options for defining representativeness: the simple, rational, but clearly insufficient concept of likelihood, or the more flexible but notoriously slippery concept of similarity.  A Bayesian analysis In this section we present a Bayesian analysis of representativeness that addresses some of the shortcomings of the likelihood and similarity proposals.  As with likelihood, Bayesian representativeness takes the form of a simple probabilistic quantity, which in fact includes likelihood as one component.  But like the similarity approach, it can account for dissociations of representativeness and likelihood, when a less probable feature of the stimuli is also more diagnostic of the process or category in question.  Moreover, it applies just as well to
Using Vocabulary Knowledge in Bayesian Multinomial Estimation| Abstract Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks.  Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity.  We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates.  We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 
A global geometric framework for nonlinear dimensionality reduction,|
Global Versus Local Methods in Nonlinear Dimensionality Reduction|
Theory-based induction|
Separating style and content with biliear models|
Randomness and coincidences: Reconciling intuition and probability theory|
Bayesian Modeling of Human Concept Learning|
Separating Style and Content|
Small worlds in semantic networks|
Separating style and context with bilinear models|
in press)| Generalization, similarity, and Bayesian inference. 
in press)| Word learning as bayesian inference. 
in press)| Teacakes, trains, taxicabs, and toxins: A Bayesian account of predicting the future. 
in press)| Theory-based causal inference. 
Structure and strength in causal judgments,|
Unsupervised learning of curved manifolds|
V1 neurons signal acquisition of an internal representation of stimulus location|
Factorial Learning by Clustering Features|
in press)| Elemental causal induction. 
in preparation) Sparse multidimensional scaling using landmark points|
Learning causal structure: Adults and children use Bayesian reasoning to make inferences about ambiguous causal events|
Rational statistical inference: a critical component for word learning|

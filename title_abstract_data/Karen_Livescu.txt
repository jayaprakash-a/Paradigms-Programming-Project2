PRONUNCIATION MODELING USING A FINITE-STATE TRANSDUCER REPRESENTATION| ABSTRACT The MIT SUMMIT speech recognition system models pronunciation using a phonemic baseform dictionary along with rewrite rules for modeling phonological variation and multi-word reductions.  Each pronunciation component is encoded within a finitestate transducer (FST) representation whose transition weights can be probabilistically trained using a modified EM algorithm for finite-state networks.  This paper explains the modeling approach we use and the details of its realization.  We demonstrate the benefits and weaknesses of the approach both conceptually and empirically using the recognizer for our JUPITER weather information system.  Our experiments demonstrate that the use of phonological rewrite rules within our system reduces word error rates by between 4% and 8% over different test sets when compared against a system using no phonological rewrite rules. 
for Automatic Speech Recognition| Abstract The performance of automatic speech recognizers has been observed to be dramatically worse for speakers with non-native accents than for native speakers.  This poses a problem for many speech recognition systems, which need to handle both native and non-native speech.  The problem is further complicated by the large number of non-native accents, which makes modeling separate accents difficult, as well as the small amount of non-native speech that is often available for training.  Previous work has attempted to address this issue by building accent-specific acoustic and pronunciation models or by adapting acoustic models to a particular non-native speaker.  In this thesis, we examine the problem of non-native speech in a speaker-independent, large-vocabulary, spontaneous speech recognition system for American English, in which a large amount of native training data and a relatively small amount of non-native data are available.  We investigate some of the major differences between native and non-native speech and attempt to modify the recognizer to better model the characteristics of nonnative data.  This work is performed using the summit speech recognition system in the jupiter weather information domain.  We first examine the modification of acoustic models for recognition of non-native speech.  We show that interpolating native and non-native models reduces the word error rate on a non-native test set by 8. 1% relative to a baseline recognizer using models trained on pooled native and non-native data (a reduction from 20. 9% to 19. 2%).  In the area of lexical modeling, we describe a small study of native and non-native pronunciation using manual transcriptions and outline some of the main differences between them.  We then attempt to model non-native word pronunciation patterns by applying phonetic substitutions, deletions, and insertions to the pronunciations in the lexicon.  The probabilities of these phonetic confusions are estimated from non-native training data by aligning automatically-generated phonetic transcriptions with the baseline lexicon.  Using this approach, we obtain a relative reduction of 10. 0% in word error rate over the baseline recognizer on the non-native test set.  Using both phonetic confusions and interpolated acoustic models, we further reduce the word error rate to 12. 4% below baseline.  Finally, we describe a study of language model differences between native and non-native speakers in the jupiter domain.  We find that, within the resolution of our analysis, language model differences do not account for a significant part of the degradation in recognition performance between native and non-native test speakers. 
Feature-based Pronunciation Modeling with Trainable Asynchrony Probabilities| Abstract We report on ongoing work on a pronunciation model based on explicit representation of the evolution of multiple linguistic feature streams.  In this type of model, most pronunciation variation is viewed as the result of asynchrony between features and changes in feature values.  We have implemented such a model using dynamic Bayesian networks.  In this paper, we extend our previous work with a mechanism for learning feature asynchrony probabilities from data.  We present experimental results on a word classification task using phonetic transcriptions of utterances from the Switchboard corpus. 
STRUCTURALLY DISCRIMINATIVE GRAPHICAL MODELS FOR AUTOMATIC SPEECH RECOGNITION -- RESULTS FROM THE 2001 JOHNS HOPKINS SUMMER WORKSHOP| ABSTRACT In recent years there has been growing interest in discriminative parameter training techniques, resulting from notable improvements in speech recognition performance on tasks ranging in size from digit recognition to Switchboard.  Typified by Maximum Mutual Information training, these methods assume a fixed statistical modeling structure, and then optimize only the associated numerical parameters (such as means, variances, and transition matrices).  In this paper, we explore the significantly different methodology of discriminative structure learning.  Here, the fundamental dependency relationships between random variables in a probabilistic model are learned in a discriminative fashion, and are learned separately from the numerical parameters.  In order to apply the principles of structural discriminability, we adopt the framework of graphical models, which allows an arbitrary set of variables with arbitrary conditional independence relationships to be modeled at each time frame.  We present results using a new graphical modeling toolkit (described in a companion paper) from the recent 2001 Johns Hopkins Summer Workshop.  These results indicate that significant gains result from discriminative structural analysis of both conventional MFCC and novel AM-FM features on the Aurora continuous digits task. 
LEXICAL MODELING OF NON-NATIVE SPEECH FOR AUTOMATIC SPEECH RECOGNITION| ABSTRACT This paper examines the recognition of non-native speech in jupiter, a speaker-independent, spontaneous-speech conversational system.  Because the non-native speech in this domain is limited and varied, speaker- and accent-specific methods are impractical.  We therefore chose to model all of the non-native data with a single model.  In particular, this paper describes an attempt to better model non-native lexical patterns.  These patterns are incorporated by applying context-independent phonetic confusion rules, whose probabilities are estimated from training data.  Using this approach, the word error rate on a non-native test set is reduced from 20. 9% to 18. 8%. 
Hidden Feature Models for Speech Recognition Using Dynamic Bayesian Networks| Abstract In this paper, we investigate the use of dynamic Bayesian networks (DBNs) to explicitly represent models of hidden features, such as articulatory or other phonological features, for automatic speech recognition.  In previous work using the idea of hidden features, the representation has typically been implicit, relying on a single hidden state to represent a combination of features.  We present a class of DBN-based hidden feature models, and show that such a representation can be not only more expressive but also more parsimonious.  We also describe a way of representing the acoustic observation model with fewer distributions using a product of models, each corresponding to a subset of the features.  Finally, we describe our recent experiments using hidden feature models on the Aurora 2. 0 corpus. 
Landmark-based speech recognition: report of the 2004 Johns Hopkins summer workshop|
Segment-based recognition on the PhoneBook task: initial results and observations on duration modeling,|
Analysis and modeling of non-native speech for automatic speech recognition|

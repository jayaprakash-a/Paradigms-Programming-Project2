Statistical Suciency for Classes in Empirical| Abstract We explore the notion of " sucient linear statistics for a class of real valued functions.  We show that for function classes with a polynomial rate of the Parametric Pollard dimension one can find a set of linear empirical functionals of polynomial size in the dimension that are sucient for " approximation of any function in the class.  We also present a probabilistic scheme for producing those functionals. 
Universality and Individuality in a Neural Code| Abstract The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions.  One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals.  We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system.  We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate.  On the other hand, all the flies in our ensemble exhibit a high coding eciency, so that every spike carries the same amount of information in all the individuals.  Thus the neural code has a quantifiable mixture of individuality and universality. 
Beyond Word N-Grams| Abstract We describe, analyze, and experimentally evaluate a new probabilistic model for wordsequence prediction in natural languages, based on prediction suffix trees (PSTs).  By using efficient data structures, we extend the notion of PST to unbounded vocabularies.  We also show how to use a Bayesian approach based on recursive priors over all possible PSTs to efficiently maintain tree mixtures.  These mixtures have provably and practically better performance than almost any single model.  Finally, we evaluate the model on several corpora.  The low perplexity achieved by relatively small PST mixture models suggests that they may be an advantageous alternative, both theoretically and practically, to the widely used n-gram models. 
On the Learnability and Usage of Acyclic Probabilistic Finite Automata| Abstract We propose and analyze a distribution learning algorithm for a subclass of Acyclic Probabilistic Finite Automata (APFA).  This subclass is characterized by a certain distinguishability property of the automata's states.  Though hardness results are known for learning distributions generated by general APFAs, we prove that our algorithm can efficiently learn distributions generated by the subclass of APFAs we consider.  In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made arbitrarily small with high confidence in polynomial time.  We present two applications of our algorithm.  In the first, we show how to model cursively written letters.  The resulting models are part of a complete cursive handwriting recognition system.  In the second application we demonstrate how APFAs can be used to build multiplepronunciation models for spoken words.  We evaluate the APFA based pronunciation models on labeled speech data.  The good performance (in terms of the log-likelihood obtained on test data) achieved by the APFAs and the little time needed for learning suggests that the learning algorithm of APFAs might be a powerful alternative to commonly used probabilistic models. 
Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity| Abstract We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity.  Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm.  It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization.  We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey.  By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data. 
Temporally Dependent Plasticity: An Information Theoretic Account| Abstract The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes.  This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity.  We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level.  Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs.  These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters
The Hierarchical Hidden Markov Model: Analysis and Applications| Abstract.  We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM).  Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech.  We seek a systematic unsupervised approach to the modeling of such structures.  By extendingthe standard forward-backward(BaumWelch) algorithm, we derive an efficient procedure for estimating the model parameters from unlabeled data.  We then use the trained model for automatic hierarchical parsing of observation sequences.  We describe two applications of our model and its parameter estimation procedure.  In the first application we show how to construct hierarchical models of natural English text.  In these models different levels of the hierarchy correspond to structures on different length scales in the text.  In the second application we demonstrate how HHMMs can be used to automatically identify repeated strokes that represent combination of letters in cursive handwriting. 
Document clustering using word clusters via the information bottleneck method| Abstract We present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering.  Given a joint empirical distribution of words and documents, p(x; y), we first cluster the words, Y , so that the obtained word clusters, ~ Y , maximally preserve the information on the documents.  The resulting joint distribution, p(X; ~ Y ), contains most of the original information about the documents, I(X; ~ Y ) # I(X;Y ), but it is much less sparse and noisy.  Using the same procedure we then cluster the documents, X , so that the information about the word-clusters is preserved.  Thus, we first find word-clusters that capture most of the mutual information about the set of documents, and then find document clusters, that preserve the information about the word clusters.  We tested this procedure over several document collections based on subsets taken from the standard 20Newsgroups corpus.  The results were assessed by calculating the correlation between the document clusters and the correct labels for these documents.  Finding from our experiments show that this double clustering procedure, which uses the information bottleneck method, yields significantly superior performance compared to other common document distributional clustering algorithms.  Moreover, the double clustering procedure improves all the distributional clustering methods examined here. 
Information Capacity and Robustness of Stochastic Neuron Models| Abstract The reliability and accuracy of spike trains have been shown to depend on the nature of the stimulus that the neuron encodes.  Adding ion channel stochasticity to neuronal models results with a macroscopic behavior that replicates the input-dependent reliability and precision of real neurons.  We calculate the amount of information that an ion channel based stochastic Hodgkin-Huxley (HH) neuron model can encode about a wide set of stimuli.  We show that both the information rate and the information per spike of the stochastic model is similar to the values reported experimentally.  Moreover, the amount of information that the neuron encodes is correlated with the amplitude of fluctuations in the input, and less so with the average firing rate of the neuron.  We also show that for the HH ion channel density, the information capacity is robust to changes in the density of ion channels in the membrane, whereas changing the ratio between the Na + and K + ion channels has a considerable effect on the information that the neuron can encode.  This suggests that neurons may maximize their information capacity by appropriately balancing the density of the different ion channels that underlies neuronal excitability. 
Distributional Word Clusters vs| Words for Text Categorization.  Abstract We study an approach to text categorization that combines distributional clustering of words and a Support Vector Machine (SVM) classifier.  This word-cluster representation is computed using the recently introduced Information Bottleneck method, which generates a compact and efficient representation of documents.  When combined with the classification power of the SVM, this method yields high performance in text categorization.  This novel combination of SVM with word-cluster representation is compared with SVM-based categorization using the simpler bag-of-words (BOW) representation.  The comparison is performed over three known datasets.  On one of these datasets (the 20 Newsgroups) the method based on word clusters significantly outperforms the word-based representation in terms of categorization accuracy or representation efficiency.  On the two other sets (Reuters-21578 and WebKB) the word-based representation slightly outperforms the word-cluster representation.  We investigate the potential reasons for this behavior and relate it to structural differences between the datasets. 
The Information Of Observations And Application For Active Learning With Uncertainty| Abstract A fundamental problem in learning theory is bounding the information gained by an example about the unknown target concept.  This problem is most critical in the context of active learning, when the learner has to select the most informative examples to be labled in order to minimize the number of lables required for good generalization.  The Mutual Information allows one to measure the average knowledge one gains by knowing the value of one random variable A about a different random variable, B.  However, in concrete learning cases one is interested in a more precise measure, namely, how much does a speci#c value a tells one about B.  Different observations, or examples, give different amount of information about the unknown status of the world, or about the underlying labeling rule.  Here we present an information measure which quantifies the amount of information an observation gives about the state of the world.  We show that with high probability this specific mutual information is bounded by the logarithm of the \covering number of the world" or the \concept class" in the learning context, and establish a version of the Information Processing Inequality suitable for this quantity.  In the second part of the paper we apply our bound to active learning schemes in the presence of uncertainty.  Assume furthermore that the labels one sees are corrupted, either due to noise or by some uncertainty of the model.  We present a generalization of the Query By Committee algorithm and use the information measure presented in the first part to prove its eciency. 
cmp-lg| Abstract We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts.  Deterministic annealing is used to find lowest distortion sets of clusters.  As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical "soft" clustering of the data.  Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. 
Generalization in Clustering with Unobserved Features| Abstract We argue that when objects are characterized by many attributes, clustering them on the basis of a relatively small random subset of these attributes can capture information on the unobserved attributes as well.  Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set.  We prove a finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting.  The scheme is demonstrated for collaborative filtering of users with movies rating as attributes. 
Locating Transcription Factors Binding Sites Using a Variable Memory Markov Model| Abstract We propose a novel method for locating transcription factors binding sites in upstream regions, by employing a variable memory Markov model (VMM) based feature selection method.  Using this method we find a set of features which discriminate well between genes upstream sequences and random sequences, and give each feature a score based on its discriminatory power.  We predict that regions in which high-scoring features are abundant, will be well correlated with transcription factors' binding sites.  We propose a rather strict benchmark for comparing such predictions with known Data.  Rather than look for consensus sequences, we go over the upstream sequences (of Saccharomyces Cerevisiae), where some binding sites are known (according to the SCPD), and try to find these sites.  Our results are consistently and significantly better than random, and are comparable with those one gets when using MatInspector to make such prediction.  MatInspector is a software that looks for specified consensus pattern in a DNA sequence.  In contrast with it, our method is completely unsupervised and does not require any prior knowledge about the data. 
Acoustic Spectral Estimation using Higher Order Statistics| Abstract Assuming an auto regressive (AR) filter model driven by a non-Gaussian white noise we formulate a general parameter estimation problem.  A maximum likelihood solution gives an AR estimate of the filter and the probability distribution function parameters for non-Gaussian input.  The proposed method is optimal in the information theoretic sense, giving the most probable model for the source and filter under the higher order statistics constrains of the observed signal.  Analysis of human singing voices and musical instruments is presented and its acoustic interpretation is discussed. 
On Feature Distributional Clustering for Text Categorization| ABSTRACT We describe a new powerful text categorization method that is based on a combination of distributional features with a support vector machine (SVM) classifier.  Our feature selection approach uses distributional clustering of words via the recently introduced information bottleneck method, which generates a more ecient representation of the documents.  When combined with the classification power of Support Vector Machines we produce the best known multilabel categorization results on the 20 Newsgroups dataset. 
Noise Tolerant Learnability via the Dual Learning Problem| Abstract.  Much of the research in machine learning and neural computation assumes that the teacher gives the correct answers to the learning algorithm.  However, in many cases this assumption is doubtful, since different sorts of noise may disable the learner from getting the correct answers all the time.  The noise could be caused by noisy communication, human errors, measuring equipment and many other causes.  In some cases, problems which are eciently learnable without noise, become hard to learn when noise is introduced [3].  Partial answers to the noisy learning problem are known for specific classes, ([11]), but in general, no simple parameters are known which distinguish between classes that are learnable in the presence of noise and those which become hard to learn.  The goal of this work is to introduce such parameters.  We use the Membership Query model to show that if the VC-dimension of the dual learning problem is moderate and if the dual learning problem is dense in some sense, then the class is learnable in the presence of persistent random classification noise.  We show that our assumptions hold for a variety of learning problems, such as smooth functions, general geometrical concepts, and monotone monomials.  We are particularly interested in the analysis of smooth function classes.  We show uniform upper bound on the fat-shattering dimension of both the primal and dual learning problem which is derived by a geometric property of the class called type. 
Sucient Dimensionality Reduction - A novel analysis principle| Abstract Data dimensionality reduction of empirical co-occurrence data is one of the most fundamental problems in unsupervised learning and complex data analysis.  One principled approach to this problem is to represent the data in low dimension with minimal loss of information contained in the original data.  In this paper we present a novel information theoretic principle and algorithm for extracting low dimensional representations, or feature-vectors, that approximately preserve the information in one variable about another.  Unlike previous work in this direction, here we do not cluster or quantize the variables, but rather extract continuous feature functions directly from the co-occurrence matrix, using a converging iterative projection algorithm.  Our approach is both simpler and more general than clustering techniques and proves effective in generating a very small feature set for document categorization. 
Objective Classification of Galaxy Spectra using the Information Bottleneck Method| ABSTRACT A new method for classification of galaxy spectra is presented, based on a recently introduced information theoretical principle, the information bottleneck.  For any desired number of classes, galaxies are classified such that the information content about the spectra is maximally preserved.  The result is classes of galaxies with similar spectra, where the similarity is determined via a measure of information.  We apply our method to # 6000 galaxy spectra from the ongoing 2dF redshift survey, and a mock-2dF catalogue produced by a Cold Dark Matter-based semi-analytic model of galaxy formation.  We find a good match between the mean spectra of the classes found in the data and in the models.  For the mock catalogue, we find that the classes produced by our algorithm form an intuitively sensible sequence in terms of physical properties such as colour, star formation activity, morphology, and internal velocity dispersion.  We also show the correlation of the classes with the projections resulting from a Principal Component Analysis. 
The Power of Word Clusters for Text Classification| Abstract The recently introduced Information Bottleneck method [21] provides an information theoretic framework, for extracting features of one variable, that are relevant for the values of another variable.  Several previous works already suggested applying this method for document clustering, gene expression data analysis, spectral analysis and more.  In this work we present a novel implementation of this method for supervised text classification.  Specifically, we apply the information bottleneck method to find word-clusters that preserve the information about document categories and use these clusters as features for classification.  Previous work [1] used a similar clustering procedure to show that word-clusters can significantly reduce the feature space dimensionality, with only a minor change in classification accuracy.  In this work we reproduce these results and go further to show that when the training sample is small word clusters can yield significant improvement in classification accuracy (up to 18%) over the performance using the words directly. 
Distributional Clustering of English Words| Abstract We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts.  Deterministic annealing is used to find lowest distortion sets of clusters.  As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical "soft" clustering of the data.  Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. 
A Universal Noise Cleaning Procedure via the Dual Learning Problem| Abstract We present a new scheme for cleaning a sample corrupted by labeling noise.  Our scheme is universal in the sense that we only make general assumptions on the dual learning problem and therefore it is completely detached from the specifics of the primal problem itself.  In a nutshell, we turn to the dual learning problem to exploit valuable information about the underlying structure of the primal one, which in turn provides the means to device a simple "noise cleaning" mechanism, using Membership Queries.  We demonstrate the strength and applicability of the suggested method with a few learning problems of different nature.  Of particular interest is the problem of learning in the restricted class of parity functions, where only k out of n bits are active.  We show that in the MQ model we can outperforme the recent result by Blum et al.  [3] and handle k = O (n \Gamma c log (n) log log (n)).  This also provides a sharp separation between our method and the SQ model.  The suggested procedure works not only for classification problems but for regression problems as well.  To this end, we present a uniform upper bound on the fat-shattering dimension of both the primal and dual problems, which is derived from a geometric property of classes of real-valued functions - called type. 
Analyzing neural codes using the information bottleneck method| Abstract A basic aspect of understanding the neural code of a neuron (or a neural system), is the ability to form a dictionary from the stimuli presented to the neuron (or the system) and the patterns of spikes that the neuron responds with.  As neurons may respond unreliably to their stimuli, such a dictionary will be stochastic by nature.  If the neuron responds to many different stimuli in a similar way (i. e.  the number of stimulus features that the neuron 'cares about' is small), then the dictionary can be compressed, without a significant loss of its properties.  Here we apply the agglomerative information bottleneck algorithm to study the properties of the dictionary (and neural code) of the identified H1 neuron in the fly visual system.  We find that the neural code dictionaries of different flies are highly compressible, suggesting that a small number of features are the key components of the H1 neural code.  We also compare the encoded features of the different flies and find similar general structure, but differences in the details. 
Discriminative Variable Memory Markov Model for Feature Selection| Abstract We propose a novel feature selection method, based on a variable memory Markov model (VMM).  The VMM was originally proposed as a generative model for language and handwriting[14], trying to preserve the original source statistics from training data.  Here we consider the VMM models for extracting discriminative statistics for hypotheses testing.  In this context one is provided training data for different sources and the goal is to build a model that preserve the most discriminative features between the sources.  We show that a new criterion should be used for pruning the non-discriminative features and propose a new algorithm for achieving that.  We demonstrate the new method on text and protein classification and show the validity and power of the method. 
Predictability, complexity and learning| We de ne predictive information I pred (T) as the mutual information between the past and the future of a time series. Three qualitatively different behaviors are found in the limit of large observation times T: I pred (T) can remain nite, grow logarithmically, or grow as a fractional power law.  If the time series allows us to learn a model with a nite number of parameters, then I pred (T) grows logarithmically with a coef cient that counts the dimensionality of the model space.  In contrast, power-law growth is associated, for example, with the learning of in nite parameter (or nonparametric) models such as continuous functions with smoothness constraints.  There are connections between the predictive information and measures of complexity that have been de ned both in learning theory and the analysis of physical systems through statistical mechanics and dynamical systems theory.  Furthermore, in the same way that entropy provides the unique measure of available information consistent with some simple and plausible conditions, we argue that the divergent part of I pred (T) provides the unique measure for the complexity of dynamics underlying a time series.  Finally, we discuss how these ideas may be useful in problems in physics, statistics, and biology. 
WebSuite: A Tool Suite for Harnessing Web Data| Abstract We present a system for searching, collecting, integrating and managing Web-resident data.  The system consists of tools, each providing a specific functionality aimed at solving one aspect of the complex task of using and managing Web data.  Each tool can be used in a stand-alone mode, in combination with the other tools, or even in conjunction with other systems.  Together, the tools offer a range of capabilities that overcome many of the limitations in existing systems for harnessing Web data.  The paper describes each tool, and possible ways of combining the tools. 
Clustering By Friends : A New Nonparametric Pairwise Distance Based Clustering Algorithm| Abstract.  We present a novel pairwise clustering method.  Given a proximity matrix of pairwise
Unsupervised Segmentation and Classification of Mixtures of Markovian Sources| Abstract We describe a novel algorithm for unsupervised segmentation of sequences into alternating Variable Memory Markov sources, first presented in [SBT01].  The algorithm is based on competitive learning between Markov models, when implemented as Prediction Sux Trees [RST96] using the MDL principle.  By applying a model clustering procedure, based on rate distortion theory combined with deterministic annealing, we obtain a hierarchical segmentation of sequences between alternating Markov sources.  The method is applied successfully to unsupervised segmentation of multilingual texts into languages where it is able to infer correctly both the number of languages and the language switching points.  When applied to protein sequence families (results of the [BSMT01] work), we demonstrate the method's ability to identify biologically meaningful sub-sequences within the proteins, which correspond to signatures of important functional sub-units called domains.  Our approach to proteins classification (through the obtained signatures) is shown to have both conceptual and practical advantages over the currently used methods. 
The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length| Abstract.  We propose and analyze a distribution learning algorithm for variable memory length Markov processes.  These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata.  The learning algorithm is motivated by real applications in man-machine interaction such as handwriting and speech recognition.  Conventionally used fixed memory Markov and hidden Markov models have either severe practical or theoretical drawbacks.  Though general hardness results are known for learning distributions generated by sources with similar structure, we prove that our algorithm can indeed efficiently learn distributions generated by our more restricted sources.  In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time and sample complexity.  We present two applications of our algorithm.  In the first one we apply our algorithm in order to construct a model of the English language, and use this model to correct corrupted text.  In the second application we construct a simple stochastic model for E. coli DNA. 
Data Clustering by Markovian Relaxation and the Information Bottleneck Method| Abstract We introduce a new, non-parametric and principled, distance based clustering method.  This method combines a pairwise based approach with a vector-quantization method which provide a meaningful interpretation to the resulting clusters.  The idea is based on turning the distance matrix into a Markov process and then examine the decay of mutual-information during the relaxation of this process.  The clusters emerge as quasi-stable structures during this relaxation, and then are extracted using the information bottleneck method.  These clusters capture the information about the initial point of the relaxation in the most effective way.  The method can cluster data with no geometric or other bias and makes no assumption about the underlying distribution. 
Hearing beyond the spectrum| Abstract In this work we focus on the problem of acoustic signals modeling and analysis, with particular interest in models that can capture the timbre of musical sounds.  Traditional methods usually relate to several "dimensions" which represent the spectral properties of the signal and their change in time.  Here we confine ourselves to the stationary portion of the sound signal, who's analysis is generalized by incorporating polyspectral techniques.  We suggest that by means of looking at the higher order statistics of the signal we obtain additional information not present in the standard autocorellation or it's Fourier related power-spectra.  It is shown that over the bispectral plane several acoustically meaningful measures could be devised, which are sensitive to properties such as harmonicity and phase coherence among the harmonics.  Effects such as reverberation and chorusing are demonstrated to be clearly detected by the above measures.  In the second part of the paper we perform an information theoretic analysis of the spectral and bispectral planes.  We introduce the concept of statistical divergence which is used for measuring the "similarity" between signals.  A comparative matrix is presented which shows the similarity measure between several instruments based on spectral and bispectral information.  The instruments group into similarity classes with a good correspondance to the human acoustic perception.  The last part of the paper is devoted to acoustical modelling of the above phenomena.  We suggest a simple model which accounts for some of the polyspectral aspects of musical sound discussed above.  One of the main results of our work is generalization of acoustic distortion measure based on our model and which takes into account higher order statistical properties of the signal. 
Sufficient Dimensionality Reduction| Abstract Dimensionality reduction of empirical co-occurrence data is a fundamental problem in unsupervised learning.  It is also a well studied problem in statistics known as the analysis of cross-classified data.  One principled approach to this problem is to represent the data in low dimension with minimal loss of (mutual) information contained in the original data.  In this paper we introduce an information theoretic nonlinear method for finding such a most informative dimension reduction.  In contrast with previously introduced clustering based approaches, here we extract continuous feature functions directly from the co-occurrence matrix.  In a sense, we automatically extract functions of the variables that serve as approximate sufficient statistics for a sample of one variable about the other one.  Our method is different from dimensionality reduction methods which are based on a specific, sometimes arbitrary, metric or embedding.  Another interpretation of our method is as generalized - multi-dimensional - non-linear regression, where rather than fitting one regression function through two dimensional data, we extract d-regression functions whose expectation values capture the information among the variables.  It thus presents a new learning paradigm that unifies aspects from both supervised and unsupervised learning.  The resulting dimension reduction can be described by two conjugate d-dimensional differential manifolds that are coupled through Maximum Entropy I-projections.  The Riemannian
The Statistical Mechanics of k-Satisfaction| Abstract The satisfiability of random CNF formulae with precisely k variables per clause ("k-SAT") is a popular testbed for the performance of search algorithms.  Formulae have M clauses from N variables, randomly negated, keeping the ratio ff = M=N fixed.  For k = 2, this model has been proven to have a sharp threshold at ff = 1 between formulae which are almost aways satisfiable and formulae which are almost never satisfiable as N ! 1.  Computer experiments for k = 2, 3, 4, 5 and 6, (carried out in collaboration with B.  Selman of ATT Bell Labs) show similar threshold behavior for each value of k.  Finite-size scaling, a theory of the critical point phenomena used in statistical physics, is shown to characterize the size dependence near the threshold.  Annealed and replica-based mean field theories give a good account of the results. 
Inferring Probabilistic Acyclic Automata Using the Minimum Description Length Principle| Abstract --- The use of Rissanen's minimum description length principle for the construction of probabilistic acyclic automata (PAA) is explored.  We propose a learning algorithm for a PAA that is adaptive both in the structure and the dimension of the model.  The proposed algorithm was tested on synthetic data as well as on real pattern recognition problems. 
Spotting Neural Spike Patterns via Decomposition Into Types| Abstract Detecting a specific stochastic pattern embedded in an unknown background noise is a difficult pattern recognition problem.  A similar problem appears when trying to detect a multi-neural spike pattern in a single electrical recording, embedded in the complex cortical activity of a behaving animal.  The technical difficulty of this detection is due to the lack of a good statistical model for the background activity, which is rapidly changing with the recording conditions and activity of the animal.  By employing a "type-based" information theoretic approach we introduce a new method for detecting patterns of non-homogeneous Poisson processes.  The main idea is a decomposition of the log-likelihood distribution according to "types" of the empirical counts.  This amounts to the assumption that the background activity has equal probability to contain all permutations of the required pattern.  We demonstrate the application of this method for detection the "reward" patterns in the Basal-Ganglia of behaving monkeys, yielding some unexpected biological results. 
Kernel Query By Committee (KQBC)| Abstract The Query By Committee (QBC) algorithm is the only algorithm in the active learning framework that has a full theoretical justification.  It was proved that it can reduce exponentially the number of labels needed for learning.  Unfortunately, a naive implementation of this algorithm is impossible due to impractical time complexity.  In this paper we make another step toward a practical implementation.  The running time of our novel method does not depend on the input dimension but only on the number of labels obtained already.  Moreover, it can be expressed in term of inner products only, and therefore we get a kernel version of the QBC algorithm. 
Query By Committee Made Real| Abstract Training a learning algorithm is a costly task.  A major goal of active learning is to reduce this cost.  In this paper we introduce a new algorithm, KQBC, which is capable of actively learning large scale problems by using selective sampling.  The algorithm overcomes the costly sampling step of the well known Query By Committee (QBC) algorithm by projecting onto a low dimensional space.  KQBC also enables the use of kernels, providing a simple way of extending QBC to the non-linear scenario.  Sampling the low dimension space is done using the hit and run random walk.  We demonstrate the success of this novel algorithm by applying it to both artificial and a real world problems
Margin based feature selection - theory and algorithms| Abstract Feature selection is the task of choosing a small set out of a given set of features that capture the relevant properties of the data.  In the context of supervised classification problems the relevance is determined by the given labels on the training data.  A good choice of features is a key for building compact and accurate classifiers.  In this paper we introduce a margin based feature selection criterion and apply it to measure the quality of sets of features.  Using margins we devise novel selection algorithms for multi-class classification problems and provide theoretical generalization bound.  We also study the well known Relief algorithm and show that it resembles a gradient ascent over our margin criterion.  We apply our new algorithm to various datasets and show that our new Simba algorithm, which directly optimizes the margin, outperforms Relief. 
Permanent Uncertainty: On the quantum evaluation of the determinant and the permanent of a matrix| We investigate the possibility of evaluating permanents and determinants of matrices by quantum computation.  All current algorithms for the evaluation of the permanent of a real matrix have exponential time complexity and are known to be in the class P #P .  Any method to evaluate or approximate the permanent is thus of fundamental interest to complexity theory.  Permanents and determinants of matrices play a basic role in quantum statistical mechanics of identical bosons and fermions, as the only possible many particle states made of single particle wave functions.  We study a novel many-particle quantum-computational model in which an observable operator can be constructed, in polynomial-time complexity, to yield the determinant or the permanent of an arbitrary matrix as its expectation value.  Both quantities are estimated, in this model, by quantum mechanics systems in a polynomial time, using fermions and bosons respectively.  It turns out that the variance of the measurements in a "noise-free case" is zero for the determinant and exponential (in the rank of the matrix) for the permanent.  The intrinsic measurement variance is therefore the quantum mechanical correspondence to the computational complexity gap. 
The information bottleneck method| Abstract We define the relevant information in a signal x 2 X as being the information that this signal provides about another signal y 2 Y .  Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken.  Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction.  We formalize the problem as that of finding a short code for X that preserves the maximum information about Y .  That is, we squeeze the information that X provides about Y through a `bottleneck' formed by a limited set of codewords ~ X.  This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x; ~ x) emerges from the joint statistics of X and Y .  The approach yields an exact set of self-consistent equations for the coding rules X ! ~ X and ~ X ! Y .  Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut{Arimoto algorithm.  Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere. 
Selective Sampling Using the Query by Committee Algorithm| Abstract.  We analyze the "query by committee" algorithm, a method for filtering informative queries from a random stream of inputs.  We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries.  We show that, in particular, this exponential decrease holds for query learning of perceptrons. 
Discriminative Feature Selection via Multiclass Variable Memory Markov Model| Abstract We propose a novel feature selection method based on a Variable Memory Markov model (VMM).  The VMM was originally proposed as a generative model trying to preserve the original source statistics from training data.  We extend this technique to simultaneously handle several sources, and further apply a new criterion to prune out non-discriminative features out of the model.  This results in a multiclass Discriminative VMM (DVMM), which is highly ecient, scaling linearly with data size.  Moreover, we suggest a natural scheme to sort the remaining features based on their discriminative power with respect to the sources at hand.  We demonstrate the utility of our method for text and protein classification tasks. 
Spike sorting in the frequency domain with overlap detection| Abstract This paper deals with the problem of extracting the activity of individual neurons from multi-electrode recordings.  Important aspects of this work are: 1) the sorting is done in two stages - a statistical model of the spikes from different cells is built and only then are occurrences of these spikes in the data detected by scanning through the original data, 2) the spike sorting is done in the frequency domain, 3) strict statistical tests are applied to determine if and how a spike should be classified, 4) the statistical model for detecting overlaping spike events is proposed, 5) slow dynamics of spike shapes are tracked during long experiments.  Results from the application of these techniques to data collected from the escape response system of the American cockroach, Periplaneta americana, are presented. 
Global self organization of all known protein sequences reveals inherent biological signatures| The self organization method presented is very general and applies to any data with a consistent and computable measure of similarity between data items. 
The Power of Amnesia| Abstract We propose a learning algorithm for a variable memory length Markov process.  Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales.  On short scales it is characterized mostly by the dynamics that generate the process, whereas on large scales, more syntactic and semantic information is carried.  For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures.  On the other hand using long memory models uniformly is not practical even for as short memory as four.  The algorithm we propose is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small.  We demonstrate the algorithm by learning the structure of natural English text and applying the learned model to the correction of corrupted text.  Using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states.  We also show how the algorithm can be applied to intergenic E.  coli DNA base prediction with results comparable to HMM based methods. 
Complexity through nonextensivity| Abstract The problem of de,ningand studyingcomplexity of a time series has interested people for years.  In the context of dynamical systems, Grassberger has suggested that a slow approach of the entropy to its extensive asymptotic limit is a sign of complexity.  We investigate this idea further by information theoretic and statistical mechanics techniques and show that these arguments can be made precise, and that they generalize many previous approaches to complexity, in particular, unifyingideas from the physics literature with ideas from learningand codingtheory; there are even connections of this statistical approach to algorithmic or Kolmogorov complexity.  Moreover, a set of simple axioms similar to those used by Shannon in his development of information theory allows us to prove that the divergent part of the subextensive component of the entropy is a unique complexity measure.  We classify time series by their complexities and demonstrate that beyond the "logarithmic" complexity classes widely anticipated in the literature there are qualitatively more complex, "power-law" classes which deserve more attention. 
Agglomerative Information Bottleneck| Abstract We introduce a novel distributional clustering algorithm that explicitly maximizes the mutual information per cluster between the data and given categories.  This algorithm can be considered as a bottom up hard version of the recently introduced "Information Bottleneck Method".  We relate the mutual information between clusters and categories to the Bayesian classification error, which provides another motivation for using the obtained clusters as features.  The algorithm is compared with the top-down soft version of the information bottleneck method and a relationship between the hard and soft results is established.  We demonstrate the algorithm on the 20 Newsgroups data set.  For a subset of two news-groups we achieve compression by 3 orders of magnitudes loosing only 10% of the original mutual information. 
Agglomerative Multivariate Information Bottleneck| Abstract The Information bottleneck method is an unsupervised non-parametric data organization technique.  Given a joint distribution P (A; B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B.  The information bottleneck has already been applied to document classification, gene expression, neural code, and spectral analysis.  In this paper, we introduce a general principled framework for multivariate extensions of the information bottleneck method.  This allows us to consider multiple systems of data partitions that are inter-related.  Our approach utilizes Bayesian networks for specifying the systems of clusters and what information each captures.  We show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations.  We also present a general framework for iterative algorithms for constructing solutions, and apply it to several examples. 
Information Bottleneck for Gaussian Variables| Abstract The problem of extracting the relevant aspects of data was addressed through the information bottleneck (IB) method, by (soft) clustering one variable while preserving information about another - relevance - variable.  An interesting question addressed in the current work is the extension of these ideas to obtain continuous representations (embeddings) that preserve relevant information, rather than discrete clusters.  We give a formal definition of the general continuous IB problem and obtain an analytic solution for the optimal representation for the important case of multivariate Gaussian variables.  The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized correlation matrix # xjy # 1 x , which is also the basis obtained in Canonical Correlation Analysis.  However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector.  This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level.  Our analysis also provides analytic expression for the optimal tradeoff - the information curve - in terms of the eigenvalue spectrum. 
Markovian domain fingerprinting: statistical segmentation of protein sequences| ABSTRACT Motivation: Characterization of a protein family by its distinct sequence domains is crucial for functional annotation and correct classification of newly discovered proteins.  Conventional Multiple Sequence Alignment (MSA) based methods find difficulties when faced with heterogeneous groups of proteins.  However, even many families of proteins that do share a common domain contain instances of several other domains, without any common underlying linear ordering.  Ignoring this modularity may lead to poor or even false classification results.  An automated method that can analyze a group of proteins into the sequence domains it contains is therefore highly desirable.  Results: We apply a novel method to the problem of protein domain detection.  The method takes as input an unaligned group of protein sequences.  It segments them and clusters the segments into groups sharing the same underlying statistics.  A Variable Memory Markov (VMM) model is built using a Prediction Suffix Tree (PST) data structure for each group of segments.  Refinement is achieved by letting the PSTs compete over the segments, and a deterministic annealing framework infers the number of underlying PST models while avoiding many inferior solutions.  We show that regions of similar statistics correlate well with protein sequence domains, by matching a unique signature to each domain.  This is done in a fully automated manner, and does not require or attempt an MSA.  Several representative cases are analyzed.  We identify a protein fusion event, refine an HMM superfamily classification into the underlying families the HMM cannot separate, and detect all 12 instances of a short domain in a group of 396 sequences.  Contact: jill@cs. huji. ac. il; tishby@cs. huji. ac. il # To whom correspondence should be addressed. 
Unsupervised document classification using sequential information maximization| ABSTRACT We present a novel sequential clustering algorithm which is motivated by the Information Bottleneck (IB) method.  In contrast to the agglomerative IB algorithm, the new sequential (sIB) approach is guaranteed to converge to a local maximum of the information, as required by the original IB principle.  Moreover, the time and space complexity are significantly improved.  We apply this algorithm to unsupervised document classification.  In our evaluation, on small and medium size corpora, the sIB is found to be consistently superior to all the other clustering methods we examine, typically by a significant margin.  Moreover, the sIB results are comparable to those obtained by a supervised Naive Bayes classifier.  Finally, we propose a simple procedure for trading cluster's recall to gain higher precision, and show how this approach can extract clusters which match the existing topics of the corpus almost perfectly. 
Margin Analysis of the LVQ Algorithm| Abstract Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classifiers.  In this paper we discuss theoretical and algorithmical aspects of such algorithms.  On the theory side, we present margin based generalization bounds that suggest that these kinds of classifiers can be more accurate then the 1-NN rule.  Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles.  We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 
Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway| Abstract The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach.  Identifying the case of stimulus-conditioned independent neurons, we develop redundancy measures that allow enhanced information estimation for groups of neurons.  These measures are then applied to study the collaborative coding eciency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (A1).  Under two different coding paradigms we show differences in both information content and group redundancies between IC and cortical auditory neurons.  These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized by Barlow (1959).  The redundancy effects under the single-spikes coding paradigm are significant only for groups larger than ten cells, and cannot be revealed with the standard redundancy measures that use only pairs of cells.  Our results suggest that redundancy reduction transformations are not limited to low level sensory processing (aimed to reduce redundancy in input statistics) but are applied even at cortical sensory stations. 
Extraction of Protein Domains and Signatures through Unsupervised Statistical Sequence Segmentation| Abstract We present a novel information theoretic method for protein domain and statistical signature extraction.  We apply a new algorithm [20] for unsupervised segmentation of sequences into alternating Variable Memory Markov sources, to families of protein sequences.  The algorithm is based on competitive learning between Markov models, implemented as Prediction Suffix Trees [18], which we have shown in earlier works to model well protein families [6, 7]. 
Rigorous Learning Curve Bounds from Statistical Mechanics| Abstract In this paper weintroduce and investigate a mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics.  The advantage of our theory over the well-established Vapnik-Chervonenkis theory is that our bounds can be considerably tighter in many cases, and are also more reflective of the true behavior (functional form) of learning curves.  This behavior can often exhibit dramatic properties such as phase transitions, as well as power law asymptotics not explained by the VC theory.  The disadvantages of our theory are that its application requires knowledge of the input distribution, and it is limited so far to finite cardinality function classes.  We illustrate our results with many concrete examples of learning curve bounds derived from our theory. 
Bayes and Tukey Meet at the Center Point| Abstract.  The Bayes classifier achieves the minimal error rate by constructing a weighted majority over all concepts in the concept class.  The Bayes Point [1] uses the single concept in the class which has the minimal error.  This way, the Bayes Point avoids some of the deficiencies of the Bayes classifier.  We prove a bound on the generalization error for Bayes Point Machines when learning linear classifiers, and show that it is at most # 1. 71 times the generalization error of the Bayes classifier, independent of the input dimension and length of training.  We show that when learning linear classifiers, the Bayes Point is almost identical to the Tukey Median [2] and Center Point [3].  We extend these de#nitions beyond linear classifiers and define the Bayes Depth of a classifier.  We prove generalization bound in terms of this new definition.  Finally we provide a new concentration of measure inequality for multivariate random variables to the Tukey Median. 
Unsupervised Sequence Segmentation by a Mixture of Switching Variable Memory Markov Sources| Abstract We present a novel information theoretic algorithm for unsupervised segmentation of sequences into alternating Variable Memory Markov sources.  The algorithm is based on competitive learning between Markov models, when implemented as Prediction Sux Trees (Ron et al. , 1996) using the MDL principle.  By applying a model clustering procedure, based on rate distortion theory combined with deterministic annealing, we obtain a hierarchical segmentation of sequences between alternating Markov sources.  The algorithm seems to be self regulated and automatically avoids over segmentation.  The method is applied successfully to unsupervised segmentation of multilingual texts into languages where it is able to infer correctly both the number of languages and the language switching points.  When applied to protein sequence families, we demonstrate the method's ability to identify biologically meaningful sub-sequences within the proteins, which correspond to important functional sub-units called domains. 
Decoding Cursive Scripts| Abstract Online cursive handwriting recognition is currently one of the most intriguing challenges in pattern recognition.  This study presents a novel approach to this problem which is composed of two complementary phases.  The first is dynamic encoding of the writing trajectory into a compact sequence of discrete motor control symbols.  In this compact representation we largely remove the redundancy of the script, while preserving most of its intelligible components.  In the second phase these control sequences are used to train adaptive probabilistic acyclic automata (PAA) for the important ingredients of the writing trajectories, e. g.  letters.  We present a new and efficient learning algorithm for such stochastic automata, and demonstrate its utility for spotting and segmentation of cursive scripts.  Our experiments show that over 90% of the letters are correctly spotted and identified, prior to any higher level language model.  Moreover, both the training and recognition algorithms are very efficient compared to other modeling methods, and the models are `on-line' adaptable to other writers and styles. 
Agnostic Classification of Markovian Sequences| Abstract Classification of finite sequences without explicit knowledge of their statistical nature is a fundamental problem with many important applications.  We propose a new information theoretic approach to this problem which is based on the following ingredients: (i) sequences are similar when they are likely to be generated by the same source; (ii) cross entropies can be estimated via "universal compression"; (iii) Markovian sequences can be asymptotically-optimally merged.  With these ingredients we design a method for the classification of discrete sequences whenever they can be compressed.  We introduce the method and illustrate its application for hierarchical clustering of languages and for estimating similarities of protein sequences. 
Agnostic classification of markovian sequences|
A Statistical Approach to Learning and Generalization in Layered Neural Networks|
The Information Bottleneck Method In|
Information, Prediction, and Query by Committee|
Statistical Modeling of Cell Assemblies Activities in Associative Cortex of Behaving Monkeys|
Distributional clustering of English words, in:|
Beyond Word N grams| In Natural Language Processing Using Very Large Corpora. 
Predicting Transcription Factor Binding Sites in Gene-Upstream Regions using a Discriminative VMM model|
Agglomerative information bottlenecks|
Learning Curves in Large Neural Networks|
Hearing beyond the spectrum|
Learning from examples in large neural networks|
A New Nonparametric Pairwise Clustering Algorithm Based on Iterative Estimation of Distance Profiles|
Stability and Likelihood of Views of Three Dimensional Objects|
Hidden Markov modeling of simultaneously recorded cells in the associative cortex of behaving monkeys|
A Map of the Protein Space: An Automatic Hierarchical Classification of all Protein Sequences|
On the learnability and usage of acyclic probabilistic automata|
Learning Probabilistic Automata with Variable Memory Length|
Multi-Electrode Spike Sorting by Clustering Transfer Functions|
Global organization of protein segments using new algorithms for metric embedding and clustering'|
Unsupervised Document Classification using Sequential Informaiton Maximization|
Distributional clustering of movements based on neural responses|
The information bottleneck method: Extracting relevant information from concurrent data|
Noise tolerant learning using early predictors|
"Predictive Information|" E-print, arxiv. 
The Hierarchical Hidden Markov Model: Analysis and Applicaitons|
The power of word clustering for text classification|
Distributional clustering of similar words|
Query by committee made real - supplementary material|
The Hard Clustering Limit of the Information Bottleneck Method|
The Information Bottlencek Method In Proc|
Multivariate Agglomerative Information Bottleneck|
Objective spectral classification of galaxies using the information bottleneck method|
Objective Spectral Classification of Galaxies using the Information Bottleneck Method| In "Monthly Notices of the Royal Astronomical Society",. 

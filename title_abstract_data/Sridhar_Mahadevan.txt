Spatial and Temporal Abstractions in POMDPs Applied to Robot Navigation| Abstract Partially observable Markov decision processes (POMDPs) are a well studied paradigm for programming autonomous robots, where the robot sequentially chooses actions to achieve long term goals efficiently.  Unfortunately, for real world robots and other similar domains, the uncertain outcomes of the actions and the fact that the true world state may not be completely observable make learning of models of the world extremely difficult, and using them algorithmically infeasible.  In this paper we show that learning POMDP models and planning with them can become significantly easier when we incorporate into our algorithms the notions of spatial and temporal abstraction.  We demonstrate the superiority of our algorithms by comparing them with previous flat approaches for large scale robot navigation. 
Learning to Communicate and Act in Cooperative Multiagent Systems using Hierarchical Reinforcement Learning| Abstract In this paper, we address the issue of rational communication behavior among autonomous agents.  We extend our previously reported cooperative hierarchical reinforcement learning (HRL) algorithm to include communication decision and propose a new multiagent HRL algorithm, called COM-Cooperative HRL.  In this algorithm, at specific levels of the hierarchy, called cooperation levels, a group of subtasks, in which coordination among agents has significant effect on the performance of the overall task, are defined as cooperative subtasks.  Coordination skills among agents are learned faster by sharing information at cooperation levels, rather than the level of primitive actions.  We add a communication level to the hierarchical decomposition of the problem, below each cooperation level.  A communication action has a certain cost and is used by each agent to obtain the actions selected by the cooperative subtasks of the other agents.  Before making a decision at a cooperative subtask, agents decide if it is worthwhile to perform a communication action in order to acquire the actions chosen by the cooperative subtasks of the other agents.  Using this algorithm, agents learn a policy to balance the amount of communication needed for proper coordination, and communication cost.  We demonstrate the efficacy of the COM-Cooperative HRL algorithm as well as the relation between communication cost and the learned communication policy, using a multiagent taxi domain.  1
Dynamic Abstraction Networks| Abstract We propose dynamic abstraction networks, a new type of hierarchical graphical model that combines state and temporal abstraction.  Fundamental to our approach is the belief that temporal abstraction and state abstraction are intertwined, and should be modeled jointly in the same network.  For the state abstraction component of the model we use the hierarchical hidden Markov model, while for the policy abstraction component we use a structure based on the abstract hidden Markov model.  We present results on both real and synthetic activity data comparing the policy abstractions learned with and without the use of state abstraction. 
Learning Hierarchical Models of Activity| Abstract--- This paper investigates learning hierarchical statistical activity models in indoor environments.  The Abstract Hidden Markov Model (AHMM) is used to represent behaviors in stochastic environments.  We train the model using both labeled and unlabeled data and estimate the parameters using Expectation Maximization (EM).  Results are shown on three datasets: data collected in lab, entryway, and home environments.  The results show that hierarchical models outperform flat models. 
Coarticulation in Markov Decision Processes| Abstract We investigate an approach for simultaneously committing to multiple activities, each modeled as a temporally extended action in a semi-Markov decision process (SMDP).  For each activity we define a set of admissible solutions consisting of the redundant set of optimal policies, and those policies that ascend the optimal state-value function associated with them.  A plan is then generated by merging them in such a way that the solutions to the subordinate activities are realized in the set of admissible solutions satisfying the superior activities.  We present our theoretical results and empirically evaluate our approach in a simulated domain. 
STUDENT PAPER: A Multiagent Reinforcement Learning Algorithm by Dynamically Merging Markov Decision Processes| ABSTRACT One general strategy for accelerating the learning of cooperative multiagent tasks is to reuse (good or optimal) solutions to the task when each agent is acting alone.  In this paper, we formalize this approach as dynamically merging solutions to multiple Markov decision processes (MDPs), each representing an individual agent's solution when acting alone, to obtain (good or optimal) solutions to the overall multiagent MDP when all the agents act together.  We present a new temporal-di#erence learning algorithm called MAPLE (MultiAgent Policy LEarning) that uses Q-learning and dynamic merging to efficiently construct global solutions to the overall multiagent problem from solutions to the individual MDPs.  We illustrate the efficiency of MAPLE by comparing its performance with standard Q-learning applied to the overall multiagent MDP.  We also describe a corresponding planning algorithm that, given complete knowledge of the underlying single agent MDPs, uses dynamic merging to ef#ciently solve the multiagent MDP.  We also illustrate how the dynamic merging framework can be extended to the case when agents use temporally extended actions, by using semiMarkov decision processes (SMDPs) to represent variablelength decision epochs. 
Hierarchical Reinforcement Learning Using Graphical Models| Abstract The graphical models paradigm provides a general framework for automatically learning hierarchical models using ExpectationMaximization, enabling both abstract states and abstract policies to be learned.  In this paper we describe a two-phased method for incorporating policies learned with a graphical model to bias the behaviour of an SMDP Q-learning agent.  In the first reward-free phase, a graphical model is trained from sample trajectories; in the second phase, policies are extracted from the graphical model and improved by incorporating reward information.  We present results from a simulated grid world Taxi task showing that the SMDP Q-learning agent using the learned policies quickly does as well as an SMDP Q-learning agent using hand-coded policies. 
Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions| Abstract We investigate the problem of automatically constructing efficient representations or basis functions for approximating value functions based on analyzing the structure and topology of the state space.  In particular, two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds: one approach uses the eigenfunctions of the Laplacian, in effect performing a global Fourier analysis on the graph; the second approach is based on diffusion wavelets, which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph.  Together, these approaches form the foundation of a new generation of methods for solving large Markov decision processes, in which the underlying representation and policies are simultaneously learned. 
Spatiotemporal Abstraction of Stochastic Sequential Processes| Abstract.  Probabilistic finite state machines have become a popular modeling tool for representing sequential processes, ranging from images and speech signals to text documents and spatial and genomic maps.  In this paper, I describe two hierarchical abstraction mechanisms for simplifying the (estimation) learning and (control) optimization of complex Markov processes: spatial decomposition and temporal aggregation.  I present several approaches to combining spatial and temporal abstraction, drawing upon recent work of my group as well as that of others.  I show how spatiotemporal abstraction enables improved solutions to three dicult sequential estimation and decision problems: hidden state modeling and control, learning parallel plans, and coordinating with multiple agents. 
Switching Kalman Filters for Prediction and Tracking in an Adaptive Meteorological Sensing Network| Abstract--- We consider the problem of configuring sensors in an adaptive sensor network being used to monitor meteorological features.  One way to decide future sensor configurations is to base them on information currently being collected.  For instance, if a meteorological sensor network is being used to monitor storms in Oklahoma, then the sensors could be dynamically configured based on the predicted storm locations.  While Kalman filters and their extensions are commonly used for prediction and tracking, they have been primarily applied to objects with known or fixed dynamics such as missiles or people.  We explore the advantages and limitations of using Kalman filters to track objects with nonstationary dynamics (e. g. , a storm can grow in size).  In particular, we focus on tracking meteorological features over time with the objective of using this information to determine where radars should focus their sensing.  We present results for tracking storm cells comparing least-squares regression with Kalman filter and switching Kalman filter methods.  Our results show that on average the Kalman filter methods better predict the future location of a storm centroid than does a least-squares regression algorithm currently in use for meteorological storm tracking. 
Evaluating the Feasibility of Learning Student Models from Data| Abstract Recent work on intelligent tutoring systems has used Bayesian networks to model students' acquisition of skills.  In many cases, researchers have hand-coded the parameters of the networks, arguing that the conditional probabilities of models containing hidden variables are too difficult to learn from data.  We present a machine learning approach that uses Expectation-Maximization to learn the parameters of a dynamic Bayesian network with hidden variables.  We test our methodology on data that was simulated using a state-based model of skill acquisition.  Results indicate that it is possible to learn the parameters of hidden variables given enough sequential data of training sessions on similar problems. 
Incremental Learning of Factorial Markov Decision Processes| Abstract We investigate a general approach to approximately learning a compact and structured representation of the transition model for Factorial Markov Decision Processes (FMDPs).  FMDPs are based on mixed memory Markov models, in which the transition probabilities are factored into a mixture of terms depending on each state variable.  We develop an incremental ExpectationMaximization (EM) procedure for learning the transition probabilities, more suited to an online reinforcement learning approach for learning real-time control than the traditional batch-mode EM.  A key advantage of FMDPs is that the agent is able to rapidly generalize observed transition experience to unseen regions of the environment.  We describe detailed experiments in which a modelbased reinforcement learning agent incrementally learns the transition model of an underlying FMDP while simultaneously learning the optimal policy in a certainty-equivalent control regime.  Our results show that exploiting the structure of an FMDP results in much faster model and control learning compared to the default tabular maximumlikelihood approach. 
to Continuous-Time, Average-Reward, and Multi-Agent Models| Abstract Hierarchical reinforcement learning (HRL) is a general framework that studies how to exploit the structure of actions and tasks to accelerate policy learning in large domains.  Prior work on HRL has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model.  In this paper we generalize the setting of HRL to averagereward, continuous-time and multi-agent SMDP models.  We also describe experimental results from a large-scale real-world domain, attesting to the benefits of HRL generally, and to our extensions more specifically.  Although in principle any HRL framework could suce, we focus in this paper on the MAXQ framework.  We describe three new hierarchical reinforcement learning algorithms: continuous-time discounted reward MAXQ, discrete-time average reward MAXQ, and continuous-time average reward MAXQ.  We also investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multiagent tasks.  We extend the MAXQ framework to the multiagent case which we term cooperative MAXQ, where each agent uses the same task hierarchy.  Learning is decentralized, with each agent learning three interrelated skills: how to perform subtasks, which order to do them in, and how to coordinate with other agents.  Coordination skills among agents are learned by using joint actions at the highest level(s) of the hierarchy.  We use two experimental testbeds to study the empirical performance of our proposed extensions.  One domain is a simulated robot trash collection task.  The other domain is a much larger real-world multi-agent autonomous guided vehicle (MAGV) problem.  We compare the performance of our proposed algorithms with each other, as well as with the original MAXQ method and to standard Q-learning.  In the MAGV domain, we show that our proposed extensions outperform widely used industrial heuristics, such as \first come first serve", "highest queue first" and "nearest station first". 
Representation Policy Iteration| Abstract This paper addresses a fundamental issue central to approximation methods for solving large Markov decision processes (MDPs): how to automatically learn the underlying representation for value function approximation? A novel theoretically rigorous framework is proposed that automatically generates geometrically customized orthonormal sets of basis functions, which can be used with any approximate MDP solver like leastsquares policy iteration (LSPI).  The key innovation is a coordinate-free representation of value functions, using the theory of smooth functions on a Riemannian manifold.  Hodge theory yields a constructive method for generating basis functions for approximating value functions based on the eigenfunctions of the self-adjoint (Laplace-Beltrami) operator on manifolds.  In effect, this approach performs a global Fourier analysis on the state space graph to approximate value functions, where the basis functions reflect the largescale topology of the underlying state space.  A new class of algorithms called Representation Policy Iteration (RPI) are presented that automatically learn both basis functions and approximately optimal policies.  Illustrative experiments compare the performance of RPI with that of LSPI using two handcoded basis functions (RBF and polynomial state encodings). 
Extending Hierarchical Reinforcement Learning to Continuous-Time, Average-Reward, and Multi-Agent Models| Abstract Hierarchical reinforcement learning (HRL) is a general framework that studies how to exploit the structure of actions and tasks to accelerate policy learning in large domains.  Prior work on HRL has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model.  In this paper we generalize the setting of HRL to averagereward, continuous-time and multi-agent SMDP models.  We also describe experimental results from a large-scale real-world domain, attesting to the benefits of HRL generally, and to our extensions more specifically.  Although in principle any HRL framework could suce, we focus in this paper on the MAXQ framework.  We describe three new hierarchical reinforcement learning algorithms: continuous-time discounted reward MAXQ, discrete-time average reward MAXQ, and continuous-time average reward MAXQ.  We also investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multiagent tasks.  We extend the MAXQ framework to the multiagent case which we term cooperative MAXQ, where each agent uses the same task hierarchy.  Learning is decentralized, with each agent learning three interrelated skills: how to perform subtasks, which order to do them in, and how to coordinate with other agents.  Coordination skills among agents are learned by using joint actions at the highest level(s) of the hierarchy.  We use two experimental testbeds to study the empirical performance of our proposed extensions.  One domain is a simulated robot trash collection task.  The other domain is a much larger real-world multi-agent autonomous guided vehicle (MAGV) problem.  We compare the performance of our proposed algorithms with each other, as well as with the original MAXQ method and to standard Q-learning.  In the MAGV domain, we show that our proposed extensions outperform widely used industrial heuristics, such as \first come first serve", "highest queue first" and "nearest station first". 
Partially Observable Semi-Markov Decision Processes: Theory and Applications in Engineering and Cognitive Science| Abstract In this paper we argue that many application areas, both in engineering as well as in biological modeling and cognitive science, require broadening the class of sequential decision-making models from synchronous discrete-time models to asynchronous discrete-event models.  In the latter, actions can be modeled as persisting for extended time periods, and can involve a complex set of lower-level routines or behaviors.  In addition, decision-making epochs are longer strictly in correspondence with state changes.  Rather, decisions are made only in states satisfying one of a set of discrete conditions (or events).  Finally, the environment can change state independently of the agent, due to a set of asynchronous state-changing processes running in parallel.  Although there has been past research in reinforcement learning on discrete event-based models, these studies assume states are complete observable.  We focus in this paper on modeling asynchronous discreteevent systems using partially observable semi-Markov decision processes (POSMDPs).  We describe the basic state estimation and planning algorithms for POSMDPs.  We examine the applicability of the POSMDP model to several engineering problems, in particular robot navigation and flexible manufacturing.  We also discuss specific applications of POSMDPs in modeling the behavior of biological agents, specifically the human foveated visual control system and honeybee navigation.  Motivation Decision-making in many domains can be (rather abstractly) viewed as follows.  At each step, the agent perceives (perhaps imperfectly) the underlying environment as being in one of a (possibly very large, but finite) set of states.  The agent chooses one of a set of finite actions in a given state, and carries it out.  The action modifies the environment in some way (or transports the agent around), thereby modifying the perceived state into a new state.  One time unit later, the agent repeats the process.  Much recent work in control of autonomous agents, including reinforcement learning (Sutton & Barto 1998), decision-theoretic planning (Boutilier, Dearden, & Goldszmidt 1995), and robot navigation (Koenig & Simmons 1997), has adopted this framework.  Although this discrete-time framework has led to some successful applications, it is a significant simplification of the actual process of decision making in both engineering and biological systems.  For example, consider building a robotic agent (or modeling a human agent (Liu 1998)) that has to drive to a designated location.  There is a great deal of complexity that underlies this task which is not being explicitly modeled in the discrete-time synchronous model.  For example, actions (be they information gathering steps, such as looking in the driving mirror, or state altering steps, such as lane changing) usually invoke a complex set of lower level procedures (moving the head and eyes to a designated target location, turning the steering wheel at a given rate and then a reverse action to recenter).  Actions persist for extended time periods, during which the state is continually changing (the location of the car is changing while the driver is saccading to the driving mirror, and other cars in neighboring lanes are also moving independently).  The agent has to explicitly reason about time: expected arrival time is not solely a function of distance, but depends on traffic conditions, number of traffic lights, stop signs, and turns along the way.  In this paper we extend this discrete-time framework to a discrete-event framework, thereby generalizing it in several ways.  Time is explicitly modeled as a continuous variable, but the agent observes the environment and makes decisions only at certain discrete points (or decision epochs).  In between these epochs, the state of the system can be changing in some complex way, but these changes may (or may not) provide the agent with any additional information.  Furthermore, actions take non-constant time periods, modeled by some arbitrary time distribution.  We use the framework of generalized semi-Markov decision processes (Puterman 1994) as a formal model of event-based state dynamics, but also combine it with the well-known partially observable Markov decision process (POMDP) model (Littman, Cassandra, & Kaelbling 1992).  The POSMDP model makes a critical distinction between the natural process, the actual (possibly unobservable) state trajectory resulting from the combination of (an arbitrarily complex set of) asynchronous process, versus the (partially observable) state of the system at a decision making epoch.  The state transitions at decision-making epochs is termed a semi-Markov model, because the transition depends not only on the current state and action, but also on how long the system has been in the current state.  This is critical in both engineering and biological applications.  For example, whether a machine will transition to a failed state depends not just on the current state, but on the entire history of past states since the last time it was maintained or repaired.  Similarly, the transition from one foveated point in a scene to another depends not only on the past state, but on gaze duration as well.  We discuss a number of applications that motivate the need for considering POSMDP models.  These range from engineering domains, including robot navigation and flexible manufacturing, to biological and cognitive modeling problems, specifically human visual information gathering and honeybee navigation.  The POSMDP model Formally, we model the evolution of the environment at decision epochs as a generalized semi-Markov decision process (SMDP) (Puterman 1994).  An SMDP is defined as a five tuple (S; A; P; R; F ), where S is a finite set of states, A is a set of actions, P is a set of state and action dependent transition probabilities, R is a reward function, and F specifies the probability distribution of transition times for each state-action pair.  P (y j x; a) denotes the probability that action a will cause the system to transition from state x 2 S to y 2 S.  This transition probability function describes the transitions at decision epochs only.  F is a function where F (t j s; a) is the probability that the next decision epoch occurs within t time units, after the agent chooses action a in state s at a decision epoch.  From F , and P , we can compute Q by Q(t; y j x; a) = P (y j x; a)F (t j x; a) where Q denotes the probability that the system will be in state y for the next decision epoch, at or before t time units after choosing action a in state s, at the last decision epoch.  Q can be used to calculate the expected transition time between decision epochs. 
Kalman Filters for Prediction and Tracking in an Adaptive Sensor Network| Abstract We consider the problem of configuring sensors in an adaptive sensor network being used to monitor meteorological features.  One way to decide future sensor configurations is to base them on information currently being collected.  For instance, if the network is being used to monitor storms in Oklahoma, then the sensors could be dynamically configured based on the predicted storm locations.  While Kalman filters and their extensions are commonly used for prediction and tracking, they have been primarily applied to objects with known or fixed dynamics such as missiles or people.  We explore the advantages and limitations of using Kalman filters to track objects with nonstationary dynamics (e. g. , a storm can grow in size).  In particular, we focus on tracking meteorological features over time with the objective of using this information to intelligently configure radars.  We present results for tracking storm cells comparing least-squares regression with Kalman filter and switching Kalman filter methods. 
Fast Direct Policy Evaluation using Multiscale Analysis of Markov Diffusion Processes| Abstract Policy evaluation is a critical step in the approximate solution of large Markov decision processes (MDPs), typically requiring O(|S| 3 ) to directly solve the Bellman system of |S| linear equations (where |S| is the state space size).  In this paper we apply a recently introduced multiscale framework for analysis on graphs to design a faster algorithm for policy evaluation.  For a fixed policy #, this framework efficiently constructs a multiscale decomposition of the random walk P # associated with the policy #.  This enables efficiently computing medium and long term state distributions, approximation of value functions, and the direct computation of the potential operator (I - #P # )- 1 needed to solve Bellman's equation.  We show that even a preliminary non-optimized version of the solver competes with highly optimized iterative techniques, and can be computed in time O(|S| log 2 |S|). 
Take-Home Intelligent Agents| 3.  Explain why extending SMDP to multiagent problems is not straightforward.  How important is the choice of the termination strategies? does the choice depend on the specific application domain chosen? 4.  Describe briefly the multi-agent taxi domain used in the paper.  What are its major features? What makes it a good application domain (or what not)? 5.  Comment on the experimental methodology used in the paper.  Are the experimental results convincing? if not, why not? what additional experiments would you have done? Be specific. 
Samuel meets Amarel: Automating Value Function Approximation using Global State Space Analysis| Abstract Most work on value function approximation adheres to Samuel's original design: agents learn a task-specific value function using parameter estimation, where the
Coarticulation: An Approach for Generating Concurrent Plans in Markov Decision Processes| Abstract We study an approach for performing concurrent activities in Markov decision processes (MDPs) based on the coarticulation framework.  We assume that the agent has multiple degrees of freedom (DOF) in the action space which enables it to perform activities simultaneously.  We demonstrate that one natural way for generating concurrency in the system is by coarticulating among the set of learned activities available to the agent.  In general due to the multiple DOF in the system, often there exists a redundant set of admissible sub-optimal policies associated with each learned activity.  Such flexibility enables the agent to concurrently commit to several subgoals according to their priority levels, given a new task defined in terms of a set of prioritized subgoals .  We present ecient approximate algorithms for computing such policies and for generating concurrent plans.  We also evaluate our approach in a simulated domain. 
Hierarchical Learning and Planning Using Multi-Scale Models| Abstract We study multi-scale learning and planning in partially observable environments using the framework of Hierarchical Hidden Markov Models (HHMMs).  HHMMs are tree-based structures which enable variable resolution modeling of sequential stochastic data.  We undertake a detailed study of the HHMM framework using a real indoor robot navigation problem.  Our results show several advantages of HHMM models compared to standard HMM models, including a better fit to the training data, better localization of the robot, and more rapid model learning through \reuse" of previously learned sub-models.  We also describe a case study of planning with HHMM models and extend these models to include abstract and primitive actions.  We describe a hierarchical planning algorithm for HHMM models with actions that works better than a standard POMDP planning heuristic (MLS) on a simulated oce navigation task. 
A Variational Learning Algorithm for the Abstract Hidden Markov Model| Abstract We present a fast algorithm for learning the parameters of the abstract hidden Markov model,
LEAP:|
Hierarchical memory-based reinforcement learning|
Kalman Filters for Prediction and Tracking in an Adaptive Sensor Network,"|
Learning to act and communicate in cooperative multiagent systems using hierarchical reinforcement learning|
An expert system for assigning patients into clinical trials based on Bayesian networks|
Using determinations in EBL: A solution to the incomplete theory problem|
A framework for learning as improving problem-solving performance|

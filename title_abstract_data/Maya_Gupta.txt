DISPLAY OF HYPERSPECTRAL IMAGERY BY SPECTRAL WEIGHTING ENVELOPES| ABSTRACT Design goals and solutions are proposed for the display of hyperspectral imagery on tristimulus displays.  The requirements of a hyperspectral visualization depend on the task.  We focus on creating consistent representations of hyperspectral data that could lead to quick understanding and analysis of hyperspectral scenes.  Fixed linear spectral weighting envelopes are given which create natural looking imagery where hue, brightness, saturation and whitepoint have meanings consistent with the human visual system interpretation of natural scenes.  For AVIRIS images, hue interpretation of water and vegetation is also preserved.  The proposed designs avoid the pre-attentive distractions of PCA imagery, and appear to provide comparable or enhanced feature and edge discriminability. 
Vector Color Filter Array Demosaicing| ABSTRACT Single-sensor digital cameras spatially sample the incoming image using a color filter array (CFA).  Consequently, each pixel only contains a single color value.  In order to reconstruct the original full-color image, a demosaicing step must be performed which interpolates the missing colors at each pixel.  Goals in CFA demosaicing include color fidelity, spatial resolution, no false colors, no jagged edges, and computational practicality.  Most demosaicing algorithms do well for color fidelity, but there is often a trade-off between a sharp image and the so-called "zipper effect" or jagged edge look.  We propose a novel demosaicing algorithm called Vector Demosaicing that interpolates missing colors jointly by selecting the color vector that minimizes the sum of distances to the surrounding pixels.  The selected color vector is a vector median of the surrounding pixels.  The vector median forms an "average", but preserves sharp edges.  We will discuss the theory behind our approach and show experimentally how the theoretical advantages manifest themselves to improve edge resolution while retaining smoothness.  Computational complexity is shown to be possibly quite low, and we discuss how different approximations may affect the output. 
the Requirements for the Degree of Bachelor of Technology by| Preface The developers of modern embedded systems feel the need of automatic development tools such as Functional Simulator, Timing Simulator, Cross compiler, Disassembler etc.  for faster development.  These tools help them in verifying their design early in the design cycle and for studying various design tradeoffs.  Such tools put together can form an integrated environment, where the designers can specify their designs in a high level language and use these tools for verification and exploring various design parameters.  These tools are also heavily used by researchers for specifying next generation Instruction Sets and microarchitectures. 
CUSTOM COLOR ENHANCEMENTS BY STATISTICAL LEARNING| ABSTRACT We consider the problem of automatically learning color enhancements from a small set of sample color pairs, and describing the enhancement by a three-dimensional look-uptable that can be stored and implemented as an ICC profile.  We propose a new method for adapting the neighborhood size for local statistical learning methods such as local linear regression, and show that this leads to relatively accurate descriptions of the desired color transformation and results in images that appear smooth and have natural depth of detail.  In a previous work we showed that learning arbitrary color enhancements can result in colored specular highlights, causing images to look unnatural.  We show that this can be solved by adding a null sample that maps white to white. 
ROBUST SPEECH RECOGNITION USING WAVELET COEFFICIENT FEATURES| ABSTRACT We propose a new vein of feature vectors for robust speech recognition that use denoised wavelet coefficients.  Greater robustness to unexpected additive noise or spectrum distortions begins with more robust acoustic features.  The use of wavelet coefficients is motivated by human acoustic process modelling and by the ability of wavelet coefficients to capture important time and frequency features.  Wavelet denoising accentuates the most salient information about the speech signal and adds robustness.  We show encouraging results using denoised cosine packet features on small-scale experiments with the TIMIT database, its NTIMIT counterpart, and low-pass filter distortions. 
Simulating the Eect of Illumination Using Color Transformations| ABSTRACT We investigate design and estimation issues for using the standard color management profile architecture for general custom image enhancement.  Color management profiles are a flexible architecture for describing a mapping from an original colorspace to a new colorspace.  We investigate use of this same architecture for describing color enhancements that could be defined by a non-technical user using samples of the mapping, just as color management is based on samples of a mapping between an original colorspace and a new colorspace.  As an example enhancement, we work with photos of the 24 color patch Macbeth chart under dierent illuminations, with the goal of defining transformations that would take, for example, a studio D65 image and reproduce it as though it had been taken during a particular sunset.  The color management profile architecture includes a lookup-table and interpolation.  We concentrate on the estimation of the look-up-table points from minimal number of color enhancement samples (comparing interpolative and extrapolative statistical learning techniques), and evaluate the feasibility of using the color management architecture for custom enhancement definitions. 
Applied Physics B Lasers and Optics| Abstract.  We review recent progress in the field of terahertz "T-ray" imaging.  This relatively new imaging technique, based on terahertz time-domain spectroscopy, has the potential to be the first portable far-infrared imaging spectrometer.  We give several examples which illustrate the possible applications of this technology, using both the amplitude and phase information contained in the THz waveforms.  We describe the latest results in tomographic imaging, in which waveforms reflected from an object can be used to form a three-dimensional representation.  Advanced signal processing tools are exploited for the purposes of extracting tomographic results, including spectroscopic information about each reflecting layer of a sample.  We also describe the application of optical near-field techniques to the THz imaging system.  Substantial improvements in the spatial resolution are demonstrated. 
Inverting color transforms| ABSTRACT We consider experimental methods for creating regular grids for applications such as color management where the grids must be estimated from non-grid samples.  To estimate the regular grid, we propose applying a generalization of linear interpolation, called linear interpolation with maximum entropy (LIME).  Evaluating dierent estimation methods for this problem is dicult and does not correspond to the standard statistical learning paradigm of using iid training and test sets in order to compare algorithms.  In this paper we consider the experimental issues and propose considering the end goal of the regular grid in evaluating an estimated grid's value.  Preliminary experimental results compare LIME, traditional linear interpolation, linear regression and ridge regression. 
Block Color Quantization: A New Method for Color Halftoning| ABSTRACT An important halftoning problem faced in the design of many color printers and copiers is to represent a 24-bit color image by a small number of preset output colors, generally at a higher spatial resolution.  We propose dividing the input and output image into corresponding small blocks, calculating each input block's color average, and then determining a set of the printing colors for the corresponding output block to best render the input block's average color. 
A Principle of Minimum Expected Risk| Abstract --- The problem of estimating a pmf q over a discrete and finite set of mutually exclusive events given prior (but incomplete) information does not generally have a unique solution, and a unique estimate is often determined by exercising a principle, such as the maximum likelihood principle, or the principle of maximum entropy.  In this paper, we propose a nonparametric principle of minimum expected risk and explain why it might be an appropriate tool of inference for some applications. 
REDUCING BIAS IN SUPERVISED LEARNING| ABSTRACT Nonparametric statistical supervised learning methods often suffer from bias caused by non-uniformity of the probability distribution of training samples.  We discuss this problem and propose a new nonparametric neighborhood method for classification and estimation that significantly reduces the bias.  Simulations exemplify the advantages, and theoretical results are noted.  1.  THE SUPERVISED LEARNING PROBLEM The supervised learning problem, or pattern recognition, arises in many applications of engineering and science, from failure prediction to color management.  Nonparametric pattern classification and estimation can be advantageous when no trusted model exists for the distribution of the observations or for the decision boundaries.  Error rates for simple neighborhood nonparametric methods like k-NN are often competitively low for real data [1].  Even 1-NN for classification and regression achieves at most twice the Bayes risk of the Bayes rule as sample size grows without bound [2].  Neighborhood methods entail estimation of class conditional probability densities based on relative frequencies of samples `near' the test point in question.  Often relative frequencies are weighted, perhaps as distance from the point varies or by a kernel, preassigned or adapted to local data.  The local nature of these algorithms makes nonparametric neighborhood methods sensitive to behavior of the probability distribution of the training sample in neighborhoods where inferences are sought.  There can be bias in classification or regression that owes to the local gradient of the distribution.  We propose and explore the behavior of a method that uses linear interpolation to reduce the bias, while diversifying the weighting of the training samples by maximizing the Shannon entropy of neighborhood weights.  The term `feature vector' denotes a real valued random vector X 2 R d , and the term `observation' denotes a real
Design goals and solutions for display of hyperspectral images| Abstract--- Design goals and solutions are proposed for the display of hyperspectral imagery on tristimulus displays.  The requirements of a hyperspectral visualization depend on the task.  We focus on creating consistent representations of hyperspectral data that can facilitate understanding and analysis of hyperspectral scenes, and may be used in conjunction with taskspecific visualizations.  Fixed linear spectral weighting envelopes are given which create natural looking imagery where hue, brightness, saturation and whitepoint have meanings consistent with the human visual system interpretation of natural scenes.  For AVIRIS images, hue interpretation of water and vegetation is also preserved.  The proposed designs avoid the pre-attentive distractions of PCA imagery, and appear to provide comparable or enhanced spectral and edge discriminability. 
COLOR CONVERSIONS USING MAXIMUM ENTROPY ESTIMATION| ABSTRACT We propose a new estimation method using the maximum entropy principle and show that it is successfully used for the three-dimensional interpolation step involved in many color conversions.  Color conversions are a key part of color management systems, and many device characterizations, especially for printers, rely on multidimensional interpolation to perform the conversion.  Our method is a linear interpolation that is not limited in the number of sample points used to estimate new color values.  We find a unique solution to the underdetermined inverse matrix problem by invoking the maximum entropy principle.  We compare our approach to the standard technique of tetrahedral interpolation and demonstrate that more accurate and more robust approximations may result. 
ANALYSIS AND CLASSIFICATION OF INTERNAL PIPELINE IMAGES| ABSTRACT Recently developed optical inspection tools provide images from the inside of natural gas pipelines to monitor pipeline integrity.  The vast amounts of data generated prohibits human inspection of the resulting images.  We designed an image processing and classification method to identify abnormal events.  Non-overlapping image blocks are classified into twelve categories:
Two-stage color palettization for error diffusion| ABSTRACT Image-adaptive color palettization chooses a decreased number of colors to represent an image.  Palettization is one way to decrease storage and memory requirements for low-end displays.  Palettization is generally approached as a clustering problem, where one attempts to find the k palette colors that minimize the average distortion for all the colors in an image.  This would be the optimal approach if the image was to be displayed with each pixel quantized to the closest palette color.  However, to improve the image quality the palettization may be followed by error diffusion.  In this work, we propose a two-stage palettization where the first stage finds some m << k clusters, and the second stage chooses palette points that cover the spread of each of the M clusters.  After error diffusion, this method leads to better image quality at less computational cost and with faster display speed than full k-means palettization. 
Nonlinear vector multiresolution analysis| Abstract We explore the use of multiresolution analysis for vector signals, such as multispectral images or stock market portfolio time series.  These signals often contain local correlations among components that are overlooked in a component-by-component analysis.  We show that a coarse signal defined by taking local arithmetic averages is equivalent to analyzing the signal component by component, but that using the average that minimizes the L 2 distance to the local points results in a non-separable vector multiresolution analysis.  We propose using the vector multiresolution representation for signal processing tasks such as compression and denoising.  We prove some results in denoising and present color image examples. 
QUALITY ASSESSMENT OF LOW FREE-ENERGY PROTEIN STRUCTURE PREDICTIONS| ABSTRACT Analyzing and engineering cellular signaling processes requires accurate estimation of cellular subprocesses such as protein-folding.  We apply parametric and nonparametric classification to the problem of assessing three-dimensional protein domain structure predictions generated by the Rosetta ab initio structure prediction method.  The assessment is based on whether the predicted structure is similar enough to a known protein structure to be classified as being in the same protein superfamily.  We develop appropriate features and apply Gaussian mixture models, K-nearest-neighbors, and the recently developed linear interpolation with maximum entropy method (LIME).  The proposed learning methods outperform a previous quality assessment method based on generalized linear models.  Results show that the proposed methods reject the vast majority of poor structural predictions while identifying a useful number of good predictions. 
Multi-rate Modeling, Model Inference, and Estimation for Statistical Classifiers| Date: In presenting this dissertation in partial fulfillment of the requirements for the doctoral degree at the University of Washington, I agree that the Library shall make its copies freely available for inspection.  I further agree that extensive copying of this dissertation is allowable only for scholarly purposes, consistent with "fair use" as prescribed in the
Imaging with THz Radiation| ABSTRACT The terahertz or far-infrared range of the electromagnetic spectrum offers a vast array of non-destructive evaluation possibilities.  However, only over the last decade has progress in nonlinear optics and optoelectronics made it possible to efficiently generate and detect terahertz radiation for use in such applications.  As a result, these possibilities have not been exploited.  In this paper we first describe a novel technique for imaging with THz radiation.  Examples of "T-ray" images which illustrate the capabilities of this new technology are presented.  Then, we discuss recent results in 3D tomographic THz imaging.  In particular, we describe a new signal processing scheme for image reconstruction which uses cepstral analysis and wavelets.  We discuss other imaging work with THz radiation and applications for commercial use. 
Automatic Classification of Images from Internal Optical Insepection of Gas Pipelines,|
Recent advances in terahertz imaging,"|
An information theory approach to supervised learning,|
"Recent Advances in Terahertz Imaging," Applied Physics B|
Simulating the effect of illumination using color transformation,|
Maximum entropy classification applied to speech,|
Inverting color transformations,|

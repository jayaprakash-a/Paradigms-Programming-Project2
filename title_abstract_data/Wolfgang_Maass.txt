Bounds for the Computational Power and Learning Complexity of Analog Neural Nets| Abstract It is shown that feedforward neural nets of constant depth with piecewise polynomial activation functions and arbitrary real weights can be simulated for boolean inputs and outputs by neural nets of a somewhat larger size and depth with heaviside gates and weights from f0; 1g.  This provides the first known upper bound for the computational power and VC-dimension of such neural nets.  It is also shown that in the case of piecewise linear activation functions one can replace arbitrary real weights by rational numbers with polynomially many bits, without changing the boolean function that is computed by the neural net.  In addition we improve the best known lower bound for the VC-dimension of a neural net with w weights and gates that use the heaviside function (or other common activation functions such as oe) from \Omega(w) to \Omega(w log w).  This implies the somewhat surprising fact that the Baum-Haussler upper bound for the VC-dimension of a neural net with heaviside gates is asymptotically optimal.  Finally it is shown that neural nets with piecewise polynomial activation functions and a constant number of analog inputs are probably approximately learnable (in Valiant's model for PAC-learning). 
Analog Neural Nets with Gaussian or other Common Noise Distributions cannot Recognize Arbitrary Regular Languages| Abstract We consider recurrent analog neural nets where the output of each gate is subject to Gaussian noise, or any other common noise distribution that is nonzero on a large set.  We show that many regular languages cannot be recognized by networks of this type, and we give a precise characterization of those languages which can be recognized.  This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise.  On the other hand we present a method for constructing feedforward analog neural nets that are robust with regard to analog noise of this type. 
Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons| Abstract We employ an efficient method using Bayesian and linear classifiers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models.  It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. 
Neural Circuits for Pattern Recognition with Small Total Wire Length| Abstract One of the most basic pattern recognition problems is whether a certain local feature occurs in some linear array to the left of some other local feature.  We construct in this article circuits that solve this problem with an asymptotically optimal number of threshold gates.  Furthermore it is shown that much fewer threshold gates are needed if one employs in addition a small number of winner-take-all gates.  In either case the circuits that are constructed have linear or almost linear total wire length, and are therefore not unrealistic from the point of view of physical implementations. 
Methods for Estimating the Computational Power and Generalization Capability of Neural Microcircuits| Abstract What makes a neural microcircuit computationally powerful? Or more precisely, which measurable quantities could explain why one microcircuit C is better suited for a particular family of computational tasks than another microcircuit C 0 ? We propose in this article quantitative measures for evaluating the computational power and generalization capability of a neural microcircuit, and apply them to generic neural microcircuit models drawn from different distributions.  We validate the proposed measures by comparing their prediction with direct evaluations of the computational performance of these microcircuit models.  This procedure is applied first to microcircuit models that differ with regard to the spatial range of synaptic connections and with regard to the scale of synaptic efficacies in the circuit, and then to microcircuit models that differ with regard to the level of background input currents and the level of noise on the membrane potential of neurons.  In this case the proposed method allows us to quantify differences in the computational power and generalization capability of circuits in different dynamic regimes (UP- and DOWN-states) that have been demonstrated through intracellular recordings in vivo. 
On the Complexity of Learning for a Spiking Neuron| Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity.  They provide a way of analyzing neural computation that is not captured by the traditional neuron models such as sigmoidal and threshold gates (or "Perceptrons").  We introduce a simple model of a spiking neuron that, in addition to the weights that model the plasticity of synaptic strength, also has variable transmission delays between neurons as programmable parameters.  For coding of input and output values two modes are taken into account: binary coding for the Boolean and analog coding for the real-valued domain.  We investigate the complexity of learning for a single spiking neuron within the framework of PAC-learnability.  With regard to sample complexity, we prove that the VC-dimension is \Theta(n log n) and, hence, strictly larger than that of a threshold gate.  In particular, the lower bound holds for binary coding and even if all weights are kept fixed.  The upper bound is valid for the case of analog coding if weights and delays are programmable.  With regard to computational complexity, we show that there is no polynomial-time PAClearning algorithm, unless RP = NP, for a quite
Learning of Depth Two Neural Networks with Constant Fan-in at the Hidden Nodes| Abstract We present algorithms for learning depth two neural networks where the hidden nodes are threshold gates with constant fan-in.  The transfer function of the output node might be more general: we have results for the cases when the threshold function, the logistic function or the identity function is used as the transfer function at the output node.  We give batch and on-line learning algorithms for these classes of neural networks and prove bounds on the performance of our algorithms.  The batch algorithms work for real valued inputs whereas the on-line algorithms assume that the inputs are discretized.  The hypotheses of our algorithms are essentially also neural networks of depth two.  However, their number of hidden nodes might be much larger than the number of hidden nodes of the neural network that has to be learned.  Our algorithms can handle such a large number of hidden nodes since they rely on multiplicative weight updates at the output node, and the performance of these algorithms scales
Agnostic PAC-Learning of Functions on Analog Neural Nets| Abstract We consider learning on multi-layer neural nets with piecewise polynomial activation functions and a fixed number k of analog inputs.  We exhibit arbitrarily powerful network architectures for which efficient and provably successful learning algorithms exist in the rather realistic refinement of Valiant's model for probably approximately correct learning ("PAC-learning") where no a-priori assumptions are required about the "target function" (agnostic learning), arbitrary noise is permitted in the training sample, and the target outputs as well as the network outputs may be arbitrary reals.  The number of computation steps of the learning algorithm LEARN that we construct is bounded by a polynomial in the bit-length n of the fixed number of input variables, in the bound s for the allowed bit-length of weights, and in 1 '' , where '' is some arbitrary given bound for the true error of the neural net after training and for the probability that the learning algorithm fails for a randomly drawn training sample. 
A Precise Characterization of the Class of Languages Recognized by Neural Nets under Gaussian and Other Common Noise Distributions| Abstract We consider recurrent analog neural nets where each gate is subject to Gaussian noise, or any other common noise distribution whose probability density function is nonzero on a large set.  We show that many regular languages cannot be recognized by networks of this type, for example the language fw 2 f0; 1g \Lambda j w begins with 0g, and we give a precise characterization of those languages which can be recognized.  This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise.  On the other hand we present a method for constructing feedforward analog neural nets that are robust with regard to analog noise of this type. 
On the Complexity of Learning for Spiking Neurons with Temporal Coding \Lambda| Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal pattern of their activity.  In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection).  It is known that these delays are tuned in biological neural systems through a variety of mechanisms.  We investigate the VC-dimension of networks of spiking neurons where the delays are viewed as programmable parameters and we prove tight bounds for this VC-dimension.  Thus we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays.  In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights.  The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons.  Results about the computational complexity of such algorithms are also given. 
The Computational Power of Spiking Neurons Depends on the Shape of the Postsynaptic Potentials| Abstract Recently one has started to investigate the computational power of spiking neurons (also called "integrate and fire neurons").  These are neuron models that are substantially more realistic from the biological point of view than the ones which are traditionally employed in artificial neural nets.  It has turned out that the computational power of networks of spiking neurons is quite large.  In particular they have the ability to communicate and manipulate analog variables in spatio-temporal coding, i. e.  encoded in the time points when specific neurons "fire" (and thus send a "spike" to other neurons).  These preceding results have motivated the question which details of the firing mechanism of spiking neurons are essential for their computational power, and which details are "accidental" aspects of their realization in biological "wetware".  Obviously this question becomes important if one wants to capture some of the advantages of computing and learning with spatio-temporal coding in a new generation of artificial neural nets, such as for example pulse stream VLSI.  The firing mechanism of spiking neurons is defined in terms of their postsynaptic potentials or "response functions", which describe the change in their electric membrane potential as a result of the firing of another neuron.  We consider in this article the case where the response functions of spiking neurons are assumed to be of the mathematically most elementary type: they are assumed to be step-functions (i. e.  piecewise constant functions).  This happens to be the functional form which has so far been adapted most frequently in pulse stream VLSI as the form of potential changes ("pulses") that mimic the role of postsynaptic potentials in biological neural systems.  We prove the rather surprising result that in models without noise the computational power of networks of spiking neurons with arbitrary piecewise constant response functions is strictly weaker than that of networks where the response functions of neurons also contain short segments where they increase respectively decrease in a linear fashion (which is in fact biologically more realistic).  More precisely we show for example that an addition of analog numbers is impossible for a network of spiking neurons with piecewise constant response functions (with any bounded number of computation steps, i. e.  spikes), whereas addition of analog numbers is easy if the response functions have linearly increasing segments. 
Total Wire Length as a Salient Circuit Complexity Measure for Sensory Processing| We introduce total wire length as salient complexity measure for analyzing the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering.  The new complexity measure is applied in this paper to two basic computational problems that arise in translation- and scale-invariant pattern recognition, and hence appear to be useful as benchmark problems for sensory processing.  We exhibit new circuit design strategies for these benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.  In addition we derive general bounds for the total wire length of circuits in terms of traditional complexity measures. 
On the Role of Time and Space in Neural Computation| Abstract We discuss structural differences between models for computation in biological neural systems and computational models in theoretical computer science. 
Computing the Maximum Bichromatic Discrepancy, with applications to Computer Graphics and Machine Learning| Abstract Computing the maximum bichromatic discrepancy is an interesting theoretical problem with important applications in computational learning theory, computational geometry and computer graphics.  In this paper we give algorithms to compute the maximum bichromatic discrepancy for simple geometric ranges, including rectangles and halfspaces.  In addition, we give extensions to other discrepancy problems. 
Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations| Address for Correspondence: Wolfgang Maass A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real-time.  We propose a new framework for neural computation that provides an alternative to previous approaches based on attractor neural networks.  It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a neural circuit may serve as a universal source of information about past stimuli, from which readout neurons can extract particular aspects needed for diverse tasks in real-time.  Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system.  Our approach is based on a rigorous computational model, the liquid state machine, that unlike Turing machines, does not require sequential transitions between discrete internal states.  Like the Turing machine paradigm it allows for universal computational power under idealized conditions, but for real-time processing of time-varying input.  The resulting new framework for neural computation has novel implications for the interpretation of neural coding, for the design of experiments and data-analysis in neurophysiology, and for neuromorphic engineering. 
A Simple Model for Neural Computation with Firing Rates and Firing Correlations| Introduction 1 Abstract A simple extension of standard neural network models is introduced that provides a model for neural computations that involve both firing rates and firing correlations.  Such extension appears to be useful since it has been shown that firing correlations play a significant computational role in many biological neural systems.  Standard neural network models are only suitable for describing neural computations in terms of firing rates.  The resulting extended neural network models are still relatively simple, so that their computational power can be analyzed theoretically.  We prove rigorous separation results, which show that the use of firing correlations in addition to firing rates can drastically increase the computational power of a neural network.  On the side one of our separation results also throws new light on a question that involves just standard neural network models: We prove that the gap between the computational power of high-order and first-order neural nets is substantially larger than shown previously. 
Neural Computation with Winner-Take-All as the Only Nonlinear Operation| Abstract Everybody "knows" that neural networks need more than a single layer of nonlinear units to compute interesting functions.  We show that this is false if one employs winner-take-all as nonlinear unit: ffl Any boolean function can be computed by a single k-winner-take-all unit applied to weighted sums of the input variables.  ffl Any continuous function can be approximated arbitrarily well by a single soft winner-take-all unit applied to weighted sums of the input variables.  ffl Only positive weights are needed in these (linear) weighted sums.  This may be of interest from the point of view of neurophysiology, since only 15% of the synapses in the cortex are inhibitory.  In addition it is widely believed that there are special microcircuits in the cortex that compute winner-take-all.  ffl Our results support the view that winner-take-all is a very useful basic computational unit in Neural VLSI: 2 it is wellknown that winner-take-all of n input variables can be computed very efficiently with 2n transistors (and a total wire length and area that is linear in n) in analog VLSI [Lazzaro et al. , 1989] 2 we show that winner-take-all is not just useful for special purpose computations, but may serve as the only nonlinear unit for neural circuits with universal computational power 2 we show that any multi-layer perceptron needs quadratically in n many gates to compute winner-take-all for n input variables, hence winner-take-all provides a substantially more powerful computational unit than a perceptron (at about the same cost of implementation in analog VLSI).  Complete proofs and further details to these results can be found in [Maass, 2000]. 
A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity| Abstract We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong "teacher inputs" to the same neuron.  It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simplified neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP.  But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron.  This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs.  In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 
An Efficient Implementation of Sigmoidal Neural Nets in Temporal Coding with Noisy Spiking Neurons| Abstract We show that networks of rather realistic models for biological neurons can in principle simulate arbitrary feedforward sigmoidal neural nets in a way which has previously not been considered.  This new approach is based on temporal coding by single spikes (respectively by the timing of synchronous firing in pools of neurons), rather than on the traditional interpretation of analog variables in terms of firing rates.  The resulting new simulation is substantially faster and apparently more consistent with experimental results about fast information processing in cortical neural systems.  As a consequence we can show that networks of noisy spiking neurons are "universal approximators" in the sense that they can approximate with regard to temporal coding any given continuous function of several variables.  Our new proposal for the possible organization of computations in biological neural systems has some interesting consequences for the type of learning rules that would be needed to explain the self-organization of such neural circuits.  Finally, our fast and noise-robust implementation of sigmoidal neural nets via temporal coding points to possible new ways of implementing sigmoidal neural nets with pulse stream VLSI. 
Finding the Key to a Synapse| Abstract Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train.  Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood.  We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses.  To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature. 
Towards an Ontology-Based Distributed Architecture for Paid Content| Abstract.  Business models on the basis of digital content require sophisticated descriptions of that content, as well as service-oriented carrier architectures that allow to negotiate and enforce contract and license schemes in heterogeneous digital application environments.  We describe Knowledge Content Objects (KCO), that provide expressive semantic descriptions of digital content, based on an ontology of Information Objects, built under the DOLCE, DnS and Plan Ontologies (DDPO).  In particular, we discuss how this structure supports business requirements within the context of paid content.  Interactions between agents are embedded into digital infrastructures that are implemented on an advanced knowledge content carrier architecture (KCCA) that communicates via a dedicated protocol (KCTP).  We show how this architecture allows to integrate existing digital repositories so that the content can be made available to a semantically rich digital environment. 
Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics| Abstract Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models.  Biological synapses are dynamic, i. e. , their "weight" changes on a short time scale by several hundred percent in dependence of the past input to the synapse.  In this article we explore the consequences that these synaptic dynamics entail for the computational power of feedforward neural networks.  We show that gradient descent suffices to approximate a given (quadratic) filter by a rather small neural system with dynamic synapses.  We also compare our network model to artificial neural networks designed for time series processing.  Our numerical results are complemented by theoretical analysis which show that even with just a single hidden layer such networks can approximate a surprisingly large large class of nonlinear filters: all filters that can be characterized by Volterra series.  This result is robust with regard to various changes in the model for synaptic dynamics. 
Networks of Spiking Neurons: The Third Generation of Neural Network Models| Abstract The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i. e.  threshold gates) respectively sigmoidal gates.  In particular it is shown that networks of spiking neurons are computationally more powerful than these other neural network models.  A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net.  This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.  1 Definitions and Motivations If one classifies neural network models according to their computational units, one can distinguish three different generations.  The first generation is based on McCulloch-Pitts neurons as computational units.  These are also referred to as perceptrons or threshold-gates.  They give rise to a variety of neural network models such as multilayer perceptrons (also called threshold circuits), Hopfield nets, and Boltzmann machines.  A characteristic feature of these models is that they can only give digital output.  In fact they are universal for computations with digital input and output, and every boolean function can be computed by some multi-layer perceptron with a single hidden layer.  The second generation is based on computational units that apply to a weighted sum (or polynomial) of the inputs an "activation function" with a continuous set of possible output values, such as the sigmoid function oe(y) = 1=(1+ e\Gamma y ) or the linear saturated function with (y) = y for 0 y 1; (y) = 0 for y ! 0; (y) = 1 for y ? 1 .  Besides piecewise polynomial activation functions we consider in this paper also "piecewise exponential" activation functions, whose pieces can be defined by expressions involving exponentiation (such as the definition of oe).  Typical examples for networks from this second generation are feedforward and recurrent sigmoidal neural nets, as well as networks of radial basis function units.  These nets are also able to compute (with the help of rounding at the network output) arbitrary boolean functions.  Actually it has been shown that neural nets from the second generation can compute certain boolean functions with fewer gates than neural nets from the first generation ([41], [11]).  In addition, neural nets from the second generation are able to compute functions with analog input and output.  In fact they are universal for analog computations in the sense that any continuous function with a compact domain and range can be approximated arbitrarily well (with regard to uniform convergence, i. e.  the L1 -norm) by a network of this type with a single hidden layer.  Another characteristic feature of this second generation of neural network models is that they support learning algorithms that are based on gradient
Neural Systems as Nonlinear Filters| Abstract Experimental data show that biological synapses behave quite differently from the symbolic synapses in all common artificial neural network models.  Biological synapses are
Dynamic Stochastic Synapses as Computational Units| Abstract In most neural network models, synapses are treated as static weights that change only on the slow time scales of learning.  It is well known, however, that synapses are highly dynamic, and show use-dependent plasticity over a wide range of time scales.  Moreover, synaptic transmission is an inherently stochastic process: a spike arriving at a presynaptic terminal triggers release of a vesicle of neurotransmitter from a release site with a probability that can be much less than one.  We consider a simple model for dynamic stochastic synapses that can easily be integrated into common models for networks of integrate-and-fire neurons ("spiking neurons").  The parameters of this model have direct interpretations in terms of synaptic physiology.  We investigate the consequences of the model for computing with individual spikes, and demonstrate through rigorous theoretical results that the computational power of the network is increased through the use of dynamic synapses. 
Networks of Spiking Neurons Can Emulate Arbitrary Hopfield Nets in Temporal Coding| Abstract A theoretical model for analog computation in networks of spiking neurons with temporal coding is introduced and tested through simulations in GENESIS.  It turns out that the use of multiple synapses yields very noise robust mechanisms for analog computations via the timing of single spikes in networks of detailed compartmental neuron models.  One arrives in this way at a method for emulating arbitrary Hopfield nets with spiking neurons in temporal coding, yielding new models for associative recall of spatio-temporal firing patterns.  We also show that it suffices to store these patterns in the efficacies of excitatory synapses.  A corresponding layered architecture yields a refinement of the synfirechain model that can assume a fairly large set of different stable firing patterns for different inputs. 
Neural Systems as Nonlinear| Abstract Experimental data show that biological synapses behave quite differently from the symbolic synapses in all common artificial neural network models.  Biological synapses are dynamic, i. e. , their "weight" changes on a short time scale by several hundred percent in dependence of the past input to the synapse.  In this article we address the question how this inherent synaptic dynamics -- which should not be confused with long term "learning" -- affects the computational power of a neural network.  In particular we analyze computations on temporal and spatio-temporal patterns, and we give a complete mathematical characterization of all filters that can be approximated by feedforward neural networks with dynamic synapses.  It turns out that even with just a single hidden layer such networks can approximate a very rich class of nonlinear filters: all filters that can be characterized by Volterra series.  This result is robust with regard to various changes in the model for synaptic dynamics.  Our characterization result provides for all nonlinear filters that are approximable by Volterra series a new complexity hierarchy which is related to the cost of implementing such filters in neural systems. 
On the Relevance of the Shape of Postsynaptic Potentials for the Computational Power of Spiking Neurons| Abstract The firing of a neuron in a biological neural system causes in certain other neurons excitatory postsynaptic potential changes (EPSP's) that are not "rectangular", but have the form of a smooth hill.  We prove in this article for a formal model of a network of spiking neurons, that the rising respectively declining segments of these EPSP's are in fact essential for the computational power of the model. 
A Robust Implementation of Autonomous Fast Learning in a Mobile Robot| Abstract| We discuss a task for mobile robots that requires fast learning in a completely autonomous fashion.  Furthermore we describe a rather inexpensive robot \Oskar" that solves this task.  Tasks of a similar nature arise in many application areas where one needs to proceed by trial und error, for example in the area of construction, repair, or rescue.  The strategy as well as details of the algorithm QUICK-LEARN that Oskar employs { in addition to longterm learning via a neural network { are presented in this article.  Further details and videos of the robot are
Optimizing the Layout of a Complete Tree| Abstract.  Tree-like organizations are a fundamental parallel processing structure.  The layout of trees has so far been studied mostly for trees of degree four or less.  In this article, we give tight upper and lower bounds on the total wire length of complete m-ary trees (m # 2) on a twodimensional grid if the leaves are constraint to lie on a grid line.  For the case of m = 2 we reproof an older result of [5] in a more direct way.  The construction results in a significant reduction of the total wire length compared to the obvious \symmetric" layout. 
On the Computational Complexity of Networks of Spiking Neurons (Extended Abstract)| Abstract We investigate the computational power of a formal model for networks of spiking neurons.  It is shown that simple operations on phasedifferences between spike-trains provide a very powerful computational tool that can in principle be used to carry out highly complex computations on a small network of spiking neurons.  We construct networks of spiking neurons that simulate arbitrary threshold circuits, Turing machines, and a certain type of random access machines with real valued inputs.  We also show that relatively weak basic assumptions about the response- and threshold-functions of the spiking neurons are sufficient in order to employ them for such computations.  Furthermore we prove upper bounds for the computational power of networks of spiking neurons with arbitrary piecewise linear response- and threshold-functions, and show that they are with regard to real-time simulations computationally equivalent to a certain type of random access machine, and to recurrent analog neural nets with piecewise linear activation functions.  In addition we give corresponding results for networks of spiking neurons with a limited timing precision, and we prove upper and lower bounds for the VC-dimension and pseudo-dimension of networks of spiking neurons. 
On the Complexity of Function Learning| Abstract The majority of results in computational learning theory are concerned with concept learning, i. e.  with the special case of function learning for classes of functions with range f0; 1g.  Much less is known about the theory of learning functions with a larger range such as IN or IR.  In particular relatively few results exist about the general structure of common models for function learning, and there are only very few nontrivial function classes for which positive learning results have been exhibited in any of these models.  We introduce in this paper the notion of a binary branching adversary tree for function learning, which allows us to give a somewhat surprising equivalent characterization of the optimal learning cost for learning a class of real-valued functions (in terms of a max-min definition which does not involve any "learning" model).  Another general structural result of this paper relates the cost for learning a union of function classes to the learning costs for the individual function classes.  Furthermore, we exhibit an efficient learning algorithm for learning convex piecewise linear functions from IR d into IR.  Previously, the class of linear functions from IR d into IR was the only class of functions with multidimensional domain that was known to be learnable within the rigorous framework of a formal model for on-line learning.  Finally we give a sufficient condition for an arbitrary class F of functions from IR into IR that allows us to learn the class of all functions that can be written as the pointwise maximum of k functions from F .  This allows us to exhibit a number of further nontrivial classes of functions from IR into IR for which there exist efficient learning algorithms. 
On the Effect of Analog Noise in Discrete-Time Analog Computations| Abstract We introduce a model for analog computation with discrete time in the presence of analog noise that is flexible enough to cover the most important concrete cases, such as noisy analog neural nets and networks of spiking neurons.  This model subsumes the classical model for digital computation in the presence of noise.  We show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and we also prove a new type of upper bound for the VC-dimension of computational models with analog noise. 
Theory and Applications of Agnostic PAC-Learning with Small Decision Trees| Abstract We exhibit a theoretically founded algorithm T2 for agnostic PAC-learning of decision trees of at most 2 levels, whose computation time is almost linear in the size of the training set.  We evaluate the performance of this learning algorithm T2 on 15 common "real-world" datasets, and show that for most of these datasets T2 provides simple decision trees with little or no loss in predictive power (compared with C4. 5).  In fact, for datasets with continuous attributes its error rate tends to be lower than that of C4. 5.  To the best of our knowledge this is the first time that a PAC-learning algorithm is shown to be applicable to "real-world" classification problems.  Since one can prove that T2 is an agnostic PAClearning algorithm, T2 is guaranteed to produce close to optimal 2-level decision trees from sufficiently large training sets for any (!) distribution of data.  In this regard T2 differs strongly from all other learning algorithms that are considered in applied machine learning, for which no guarantee can be given about their performance on new datasets.  We also demonstrate that this algorithm T2 can be used as a diagnostic tool for the investigation of the expressive limits of 2-level decision trees.  Finally, T2, in combination with new bounds on the VC-dimension of decision trees of bounded depth that we derive, provides us now for the first time with the tools necessary for comparing learning curves of decision trees for "real-world" datasets with the theoretical estimates of PAClearning theory. 
Optimizing the Layout of a Balanced Tree| This paper is related to computational and structural complexity, graph drawing.  We exhibit in this article layouts of balanced m-ary trees on a grid that are optimal with regard to their total wire length (up to lower order terms).  It is shown that the total wire length of layouts of a balanced binary tree on a 2-dimensional grid can be reduced by 33 % if one does not choose the obvious \symmetric" layout strategy.  Neither the construction of the optimal layout, nor the proof of its optimality, are obvious.  Hence the results of this paper appear to be of interest not only from the point of view of practical circuit design, but also from the theoretical point of view. 
Neural Nets with Superlinear VC-Dimension| Abstract It has been known for quite a while that the Vapnik-Chervonenkis dimension (VC-dimension) of a feedforward neural net with linear threshold gates is at most O(w \Delta log w), where w is the total number of weights in the neural net.  We show in this paper that this bound is in fact asymptotically optimal.  More precisely, we exhibit for any depth d 3 a large class of feedforward neural nets of depth d with w weights that have VC-dimension \Omega(w \Delta log w).  This lower bound holds even if the inputs are restricted to boolean values.  The proof of this result relies on a new method that allows us to encode more "program-bits" in the weights of a neural net than previously thought possible.  The Vapnik-Chervonenkis dimension VC-dimension(N ) of a neural net N with n input nodes is defined as the size of the largest set S ` R n which is "shattered" by N in the sense that every function F : S ! f0; 1g can be computed by N with some assignment of real numbers to its weights.  The VC-dimension of a neural net N is an important measure for the expressiveness for N , i. e.  for the variety of functions that can be computed by N with different choices for its weights.  In particular it has been shown in [BEHW] and [EHKV] that the VC-dimension of N essentially determines the number of training examples that are needed to train N in Valiant's model [V] for probably approximately correct learning ("PAC-learning").  It has been known for quite a while that the VC-dimension of a neural net with linear threshold gates and w edges (respectively w weights) is at most O(w \Delta log w).  This result, which holds for arbitrary real valued input patterns, was first shown by Cover in [C 64] (see also [C 68]), and later by Baum and Haussler [BH].  It has frequently been conjectured that the "true" upper bound is O(w).  This conjecture is quite plausible, since a single linear threshold gate with w edges has VC-dimension w + 1.  Furthermore it is hard to imagine that the VC-dimension of a network of linear threshold gates can be larger than the sum of the VC-dimensions of the individual linear threshold gates in the network.  We disprove this popular conjecture by showing that for any depth d 3 quite a number of neural nets N of depth d have a VC-dimension that is superlinear in the number w of edges in N .  In particular, we exhibit for arbitrarily large w 2 N neural nets N of depth 3 (i. e.  with 2 hidden layers) with w weights that have VC-dimension \Omega(w \Delta log w).  This shows that the quoted upper bound of O(w log w) is in fact asymptotically optimal.  It is of some interest to note that the upper bound O(w \Delta log w) for the VCdimension of a neural net with w weights holds even for the case of real valued inputs, whereas our matching lower bound \Omega(w \Delta log w) for the VC-dimension of certain neural nets Nw with w weights holds already for the restriction of Nw to boolean inputs.  Our lower bound also shows that the well-known upper bound 2w log(eN) for the VC-dimension of a neural net with w weights and N computation nodes (due to Baum and Haussler [BH]) is asymptotically optimal.  The result of this paper may also be viewed as mathematical evidence for a certain type of "connectionism thesis": that a network of neuron-like elements is more than just the sum of its elements.  We show that in a large neural net a single edge may add more than a constant to the VC-dimension of the neural net: its contribution may increase with the logarithm of the total size of the neural net.  Although we consider in this paper only neural nets with linear threshold gates, it is obvious that the same lower bound can also be derived for neural nets with other activation functions such as oe(y) = 1 1+e\Gamma y (see [RM]) or piecewise linear (resp.  polynomial) functions of a similar type (see [S], [MSS], [M 93]).  This paper improves our earlier results from [M 92] (see [M 93] for an extended abstract), where we had exhibited neural nets of depth 4 with superlinear VC-dimension.  Both our preceding results and the proof of our new result employ classical circuit construction methods due to Neciporuk [N] and
On the Computational Power of Winner-Take-All| Abstract This article initiates a rigorous theoretical analysis of the computational power of circuits that employ modules for computing winner-take-all.  Computational models that involve competitive stages have so far been neglected in computational complexity theory, although they are widely used in computational brain models, artificial neural networks, and analog VLSI.  Our theoretical analysis shows that winner-take-all is a surprisingly powerful computational module in comparison with threshold gates (= McCullochPitts neurons) and sigmoidal gates.  We prove an optimal quadratic lower bound for computing winner-take-all in any feedforward circuit consisting of threshold gates.  In addition we show that arbitrary continuous functions can be approximated by circuits employing a single soft winner-take-all gate as their only nonlinear operation.  Our theoretical analysis also provides answers to two basic questions that have been raised by neurophysiologists in view of the well-known asymmetry between excitatory and inhibitory connections in cortical circuits: how much computational power of neural networks is lost if only positive weights are employed in weighted sums, and how much adaptive capability is lost if only the positive weights are subject to plasticity. 
A Model for Real-Time Computation in Generic Neural Microcircuits| Abstract A key challenge for neural modeling is to explain how a continuous stream of multi-modal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real-time.  We propose a new computational model that is based on principles of high dimensional dynamical systems in combination with statistical learning theory.  It can be implemented on generic evolved or found recurrent circuitry. 
On the Complexity of Learning for Spiking Neurons with Temporal Coding| Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity.  In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection).  It is known that these delays are tuned in biological neural systems through a variety of mechanisms.  In this article we consider the arguably most simple model for a spiking neuron, which can also easily be implemented in pulsed VLSI.  We investigate the VC dimension of networks of spiking neurons where the delays are viewed as programmable parameters and we prove tight bounds for this VC dimension.  Thus we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays.  In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights.  The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons.  Results about the computational complexity of such algorithms are also given. 
Motion Planning Among Time Dependent Obstacles|
On the Computational Power of Sigmoid versus Boolean Threshold Circuits|
Foundations for a Circuit Complexity Theory of Sensory Processing|
Threshold circuits of bounded depth|
Constraint-basierte Verarbeitung graphischen Wissens|
Efficient Learning with Virtual Threshold Gates|
Fast Sigmoidal Networks via Spiking Neurons|
On the Complexity of Learning from Counterexamples and Membership Queries (abstract)|
Lower Bound Methods and Separation Results for On-Line Learning Models|
Gyorgy Tur'an| Algorithms and lower bounds for on-line learning of geometrical concepts. 
Gyorgy Tur'an| Lower bound methods and separation results for on-line learning models. 
On-line Learning of Rectangles|
Gyorgy Tur'an| On the complexity of learning from counterexamples and membership queries. 
and Gyorgi Tur'an| Threshold circuits of bounded depth. 
Approximation Schemes for Covering and Packing Problems in Image Processing and VLSI|
Fast Approximation Algorithms for a Nonconvex Covering Problem|
Recursively Enumerable Generic Sets|
Lower Bounds for the Computational Power of Networks of Spiking Neurons|
Reducing Communication for Distributed Learning in Neural Networks|
On the Use of Inaccessible Numbers and Order Indiscernibles in Lower Bound Arguments for Random Access Machines|
On-line prediction of depth two linear threshold circuits|
and Gyorgy Tur'an| On the communication complexity of graph properties. 
Meanders and Their Applications in Lower Bounds Arguments|
Gyorgy Tur'an| How fast can a threshold gate learn? Computational Learning Theory and Natural Learning Systems: Constraints and Prospects,. 
Fast Identification of Geometric Objects with Membership Queries|
On-Line Learning with an Oblivious Environment and the Power of Randomization|
Analog Neural Nets with Gaussian or Other Common Noise Distribution Cannot Recognize Arbitrary Regular Languages|
On the Computational Complexity of Networks of Spiking Neurons|
On Computation with Pulses|
Algorithms and Lower Bounds for On-Line Learning of Geometrical Concepts|
Combinatorial lower bound arguments for deterministic and nondeterministic Turing machines|
On the Communication Complexity of Graph Properties|
Variations on Promptly Simple Sets|
Two Tapes Versus One for Off-Line Turing Machines|
On-line with an oblivious environment and the power of randomization|
Meanders, Ramsey Theory and Lower Bounds for Branching Programs|
On the Orbits of Hyperhypersimple Sets|
Efficient Agnostic PAC-Learning with Simple Hypothesis|
Vapnik-Chervonenkis Dimension of Neural Nets| NeuroCOLT. 
On the Computational Power of Recurrent Circuits of Spiking Neurons|
On-line learning with an oblivious environment and the power of randomization|
A New Approach towards Vision Suggested by Biologically Realistic Neural Microcircuit Models|
The Complexity of Matrix Transposition on One-Tape Off-Line Turing Machines with Output Tape|
An Optimal Lower Bound for Turing Machines with One Work Tape and a Two- way Input Tape|
Szemer'edi, and Gyorgy Tur'an| Two tapes versus one for off-line turing machines. 

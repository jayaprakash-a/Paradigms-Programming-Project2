On the convergence of a clustering algorithm for protein-coding regions in microbial genomes| Abstract Motivation: As the number of fully sequenced prokaryotic genomes continues to grow rapidly, computational methods for reliably detecting protein-coding regions become even more important.  Audic and Claverie (1998) Proc.  Natl Acad.  Sci.  USA, 95, 10026--10031, have proposed a clustering algorithm for protein-coding regions in microbial genomes.  The algorithm is based on three Markov models of order k associated with subsequences extracted from a given genome.  The parameters of the three Markov models are recursively updated by the algorithm which, in simulations, always appear to converge to a unique stable partition of the genome.  The partition corresponds to three kinds of regions: (1) coding on the direct strand, (2) coding on the complementary strand, (3) non-coding.  Results: Here we provide an explanation for the convergence of the algorithm by observing that it is essentially a form of the expectation maximization (EM) algorithm applied to the corresponding mixture model.  We also provide a partial justification for the uniqueness of the partition based on identifiability.  Other possible variations and improvements are briefly discussed. 
Large-Scale Prediction of Disulphide Bond Connectivity| Abstract The formation of disulphide bridges among cysteines is an important feature of protein structures.  Here we develop new methods for the prediction of disulphide bond connectivity.  We first build a large curated data set of proteins containing disulphide
Learning in Linear Neural Networks: a Survey| Abstract--- Networks of linear units are the simplest kind of networks, where the basic questions related to learning, generalization, and self-organisation can sometimes be answered analytically.  We survey most of the known results on linear networks, including: (1) back-propagation learning and the structure of the error function landscape; (2) the temporal evolution of generalization; (3) unsupervised learning algorithms and their properties.  The connections to classical statistical ideas, such as principal component analysis (PCA), are emphasized as well as several simple but challenging open questions.  A few new results are also spread across the paper, including an analysis of the effect of noise on back-propagation networks and a unified view of all unsupervised algorithms. 
Bayesian Surprise Attracts Human Attention| Abstract The concept of surprise is central to sensory processing, adaptation, learning, and attention.  Yet, no widely-accepted mathematical theory currently exists to quantitatively characterize surprise elicited by a stimulus or event, for observers that range from single neurons to complex natural or engineered systems.  We describe a formal Bayesian definition of surprise that is the only consistent formulation under minimal axiomatic assumptions.  Surprise quantifies how data affects a natural or artificial observer, by measuring the difference between posterior and prior beliefs of the observer.  Using this framework we measure the extent to which humans direct their gaze towards surprising items while watching television and video games.  We find that subjects are strongly attracted towards surprising locations, with 72% of all human gaze shifts directed towards locations more surprising than the average, a figure which rises to 84% when considering only gaze targets simultaneously selected by all subjects.  The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.  Life is full of surprises, ranging from a great christmas gift or a new magic trick, to wardrobe malfunctions, reckless drivers, terrorist attacks, and tsunami waves.  Key to survival is our ability to rapidly attend to, identify, and learn from surprising events, to decide on present and future courses of action [1].  Yet, little theoretical and computational understanding exists of the very essence of surprise, as evidenced by the absence from our everyday vocabulary of a quantitative unit of surprise: Qualities such as the "wow factor" have remained vague and elusive to mathematical analysis.  Informal correlates of surprise exist at nearly all stages of neural processing.  In sensory neuroscience, it has been suggested that only the unexpected at one stage is transmitted to the next stage [2].  Hence, sensory cortex may have evolved to adapt to, to predict, and to quiet down the expected statistical regularities of the world [3, 4, 5, 6], focusing instead on events that are unpredictable or surprising.  Electrophysiological evidence for this early sensory emphasis onto surprising stimuli exists from studies of adaptation in visual [7, 8, 4, 9], olfactory [10, 11], and auditory cortices [12], subcortical structures like the LGN [13], and even retinal ganglion cells [14, 15] and cochlear hair cells [16]: neural response greatly attenuates with repeated or prolonged exposure to an initially novel stimulus.  Surprise and novelty are also central to learning and memory formation [1], to the point that surprise is believed to be a necessary trigger for associative learning [17, 18], as supported by mounting evidence for a role of the hippocampus as a novelty detector [19, 20, 21].  Finally, seeking novelty is a well-identified human character trait, with possible association with the dopamine D4 receptor gene [22, 23, 24].  In the Bayesian framework, we develop the only consistent theory of surprise, in terms of the difference between the posterior and prior distributions of beliefs of an observer over the available class of models or hypotheses about the world.  We show that this definition derived from first principles presents key advantages over more ad-hoc formulations, typically relying on detecting outlier stimuli.  Armed with this new framework, we provide direct experimental evidence that surprise best characterizes what attracts human gaze in large amounts of natural video stimuli.  We here extend a recent pilot study [25], adding more comprehensive theory, large-scale human data collection, and additional analysis.  1 Theory Bayesian Definition of Surprise.  We propose that surprise is a general concept, which can be derived from first principles and formalized across spatio-temporal scales, sensory modalities, and, more generally, data types and data sources.  Two elements are essential for a principled definition of surprise.  First, surprise can exist only in the presence of uncertainty, which can arise from intrinsic stochasticity, missing information, or limited computing resources.  A world that is purely deterministic and predictable in real-time for a given observer contains no surprises.  Second, surprise can only be defined in a relative, subjective, manner and is related to the expectations of the observer, be it a single synapse, neuronal circuit, organism, or computer device.  The same data may carry different amount of surprise for different observers, or even for the same observer taken at different times.  In probability and decision theory it can be shown that the only consistent and optimal way for modeling and reasoning about uncertainty is provided by the Bayesian theory of probability [26, 27, 28].  Furthermore, in the Bayesian framework, probabilities correspond to subjective degrees of beliefs in hypotheses or models which are updated, as data is acquired, using Bayes' theorem as the fundamental tool for transforming prior belief distributions into posterior belief distributions.  Therefore, within the same optimal framework, the only consistent definition of surprise must involve: (1) probabilistic concepts to cope with uncertainty; and (2) prior and posterior distributions to capture subjective expectations.  Consistently with this Bayesian approach, the background information of an observer is captured by his/her/its prior probability distribution {P (M)}M2M over the hypotheses or models M in a model space M.  Given this prior distribution of beliefs, the fundamental effect of a new data observation D on the observer is to change the prior distribution {P (M)}M2M into the posterior distribution {P (M |D)}M2M via Bayes theorem, whereby 8M 2 M, P (M |D) = P (D|M) P (D)
New Machine Learning Methods for the Prediction of Protein Topologies| Abstract.  Protein structures are translation and rotation invariant.  In protein structure prediction, it is therefore important to be able to assess and predict intermediary topological representations, such as distance or contact maps, that are translation and rotation invariant.  Here we develop several new machine learning methods for the prediction and assessment of fine-grained and coarse topological representations of proteins.  In particular, we introduce a general class of graphical model architectures together with the corresponding neural network implementations.  These architectures can be viewed as Bayesian network generalizations of input-output hidden Markov models (GIOHMMs), involving an input layer, an output layer, and a hidden layer supported by one or several directed acyclic graphs.  The corresponding generalized recursive neural network (GRNN) architectures are derived by preserving the graphical structures of the GIOHMMs, but replacing the conditional probability tables with learnable deterministic functions.  Two methods are proposed for the prediction of protein topological structures.  The first method uses a GIOHMM organized into six horizontal layers: one input plane, four hidden planes, and one output plane that directly represents the adjacency matrix of the contact map (or the distance matrix).  Each hidden plane is associated with one of the four cardinal corners towards which all the edges of the corresponding lattice are oriented.  The corresponding GRNNs are used to construct a fine-grained contact map predictor.  The second method uses a GIOHMM approach to learn a graph scoring function which, in turn, is used to efficiently search the space of possible configurations.  The corresponding GRNNs are used to construct a coarse-grained contact map predictor.  Computer simulations show that the predictors for both tasks achieve stateof-the-art performance. 
Machine Learning Structural and Functional Proteomics| Abstract While new high-throughput experimental techniques are being developed for proteomics applications (e. g.  mass spectrometry, protein chips), it is clear that given the fundamental importance of proteins to biology, biotechnology, and medicine, computer methods that can rapidly sift through massive amounts of data and help determine the structure and function of a large number of proteins in a given genome remain important.  We provide a brief overview of the application of machine learning methods to proteomic problems.  In particular, we outline a novel strategy for the complete prediction of protein 3D coordinates.  The strategy relies on three main successive stages: prediction of structural features, prediction of topology, and prediction of actual coordinates.  We provide a progress report under this strategy and describe the corresponding suite of web servers available through
A Bayesian framework for the analysis of microarray expression data: regularized t -test and statistical inferences of gene changes| ABSTRACT Motivation: DNA microarrays are now capable of providing genome-wide patterns of gene expression across many different conditions.  The first level of analysis of these patterns requires determining whether observed differences in expression are significant or not.  Current methods are unsatisfactory due to the lack of a systematic framework that can accommodate noise, variability, and low replication often typical of microarray data.  Results: We develop a Bayesian probabilistic framework for microarray data analysis.  At the simplest level, we model log-expression values by independent normal distributions, parameterized by corresponding means and variances with hierarchical prior distributions.  We derive point estimates for both parameters and hyperparameters, and regularized expressions for the variance of each gene by combining the empirical variance with a local background variance associated with neighboring genes.  An additional hyperparameter, inversely related to the number of empirical observations, determines the strength of the background variance.  Simulations show that these point estimates, combined with a t -test, provide a systematic inference approach that compares favorably with simple t -test or fold methods, and partly compensate for the lack of replication.  Availability: The approach is implemented in software called Cyber-T accessible through a Web interface at www. 
REVIEW Assessing the Accuracy of Prediction Algorithms for Classification: An Overview| Abstract We provide a unified overview of methods that currently are widely used to assess the accuracy of prediction algorithms, from raw percentages, quadratic error measures and other distances, correlation coefficients and to information theoretic measures such as relative entropy and mutual information.  We briefly discuss the advantages and disadvantages of each approach.  For classification tasks, we derive new learning algorithms for the design of prediction systems by directly optimising the correlation coefficient.  We observe and prove several results relating sensitivity and specificity of optimal systems.  While the principles are general, we illustrate the applicability on specific problems such as protein secondary structure and signal peptide prediction. 
Structural basis for triplet repeat disorders: a computational analysis| Abstract Motivation: Over a dozen major degenerative disorders, including myototonic distrophy, Huntington's disease, and fragile X syndrome, result from unstable expansions of particular trinucleotides.  Remarkably, only some of all the possible triplets, namely CAG/CTG, CGG/CCG and GAA/TTC, have been associated with the known pathological expansions.  This raises some basic questions at the DNA level.  Why do particular triplets seem to be singled out? What is the mechanism for their expansion and how does it depend on the triplet itself? Could other triplets or longer repeats be involved in other diseases? Results: Using several different computational models of DNA structure, we show that the triplets involved in the pathological repeats generally fall into extreme classes.  Thus, CAG/CTG repeats are particularly flexible, whereas GCC, CGG and GAA repeats appear to display both flexible and rigid (but curved) characteristics depending on the method of analysis.  The fact that (1) trinucleotide repeats often become increasingly unstable when they exceed a length of approximately 50 repeats, and (2) repeated 12-mers display a similar increase in instability above 13 repeats, together suggest that approximately 150 bp is a general threshold length for repeat instability.  Since this is about the length of DNA wrapped up in a single nucleosome core particle, we speculate that chromatin structure may play an important role in the expansion mechanism.  We furthermore suggest that expansion of a dodecamer repeat, which we predict to have very high flexibility, may play a role in
Sequence analysis by additive scales: DNA structure for sequences and repeats of all lengths| Abstract Motivation: DNA structure plays an important role in a variety of biological processes.  Different di- and trinucleotide scales have been proposed to capture various aspects of DNA structure including base stacking energy, propeller twist angle, protein deformability, bendability, and position preference.  Yet, a general framework for the computational analysis and prediction of DNA structure is still lacking.  Such a framework should in particular address the following issues: (1) construction of sequences with extremal properties; (2) quantitative evaluation of sequences with respect to a given genomic background; (3) automatic extraction of extremal sequences and profiles from genomic data bases; (4) distribution and asymptotic behavior as the length N of the sequences increases; and (5) complete analysis of correlations between scales.  Results: We develop a general framework for sequence analysis based on additive scales, structural or other, that addresses all these issues.  We show how to construct extremal sequences and calibrate scores for automatic genomic and data base extraction.  We show that distributions rapidly converge to normality as N increases.  Pairwise correlations between scales depend both on background distribution and sequence length and rapidly converge to an analytically predictable asymptotic value.  For di- and trinucleotide scales, normal behavior and asymptotic correlation values are attained over a characteristic window length of about 10-15 bp.  With a uniform background distribution, pairwise correlations between empirically-derived scales remain relatively small and roughly constant at all lengths, except for propeller twist and protein deformability which are positively correlated.  There is a positive (resp.  negative) correlation between dinucleotide base stacking (resp.  propeller twist and protein deformability) and AT-content that increases in magnitude with length.  The framework is applied to the analysis of various DNA tandem repeats.  We derive exact expressions for counting the number of repeat unit classes at all lengths.  Tandem repeats are likely to
The Biology of Eukaryotic Promoter Prediction - A Review| Abstract Computational prediction of eukaryotic promoters from the nucleotide sequence is one of the most attractive problems in sequence analysis today, but it is also a very difficult one.  Thus, current methods predict in the order of one promoter per kilobase in human DNA, while the average distance between functional promoters has been estimated to be in the range of 30-40 kilobases.  Although it is conceivable that some of these predicted promoters correspond to cryptic initiation sites that are used in vivo, it is likely that most are false positives.  This suggests that it is important to carefully reconsider the biological data that forms the basis of current algorithms, and we here present a review of data that may be useful in this regard.  The review covers the following topics: (1) basal transcription and core promoters, (2) activated transcription and transcription factor binding sites, (3) CpG islands and DNA methylation, (4) chromosomal structure and nucleosome modification, and (5) chromosomal domains and domain boundaries.  We discuss the possible lessons that may be learned, especially with respect to the wealth of information about epigenetic regulation of transcription that has been appearing in recent years. 
An Acoustic Identification Scheme for Location Systems| Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. 
Periodic Sequence Patterns in Human Exons| Abstract We analyse the sequential structure of human exons and their flanking introns by hidden Markov models.  Together, models of donor site regions, acceptor site regions and flanked internal exons, show that exons --besides the reading frame --- hold a specific periodic pattern.  The pattern, which has the consensus: non-T(A/T)G and a minimal periodicity of roughly 10 nucleotides, is not a consequence of the nucleotide statistics in the three codon positions, nor of the well known nucleosome positioning signal.  We discuss the relation between the pattern and other known sequence elements responsible for the intrinsic bending or curvature of DNA. 
DNA Structure in Human RNA polymerase II Promoters| Summary The fact that DNA three-dimensional structure is important for transcriptional regulation begs the question of whether eukaryotic promoters contain general structural features independently of what genes they control.  In this paper we present an analysis of a large set of human RNA polymerase II promoters with very low sequence similarity.  The sequences, which include both TATA-containing and TATA-less promoters, are aligned by hidden Markov models (HMMs).  Using three different models of sequence-derived DNA bendability, the aligned promoters display a common structural profile with bendability being low in a region upstream of the transcriptional start point and significantly higher downstream.  Investigation of the sequence composition in the two regions shows that the bendability profile originates from the sequential structure of the DNA, rather than the general nucleotide composition.  Several trinucleotides known to have high propensity for major groove compression are found much more frequently in the regions downstream of the transcriptional start point, while the upstream regions contain more low-bendability triplets.  Within the region downstream of the start point, we observe a periodic pattern in sequence and bendability, which is in phase with the DNA helical pitch.  The periodic bendability profile shows bending peaks roughly at every 10 bp with stronger bending at 20 bp intervals.  These observations suggest that DNA in the region downstream of the transcriptional start point is able to wrap around protein in a manner reminiscent of DNA in a nucleosome.  This notion is further supported by the finding that the periodic bendability is mainly caused by the complementary triplet pairs CAG/CTG and GGC/GCC, which previously have been found to correlate with nucleosome positioning.  We present models where the high bendability regions position nucleosomes at the downstream end of the transcriptional start point, and consider the possibility of interaction between histone-like TAFs and this area.  We also propose the use of this structural signature in computational promoter finding algorithms. 
Improved prediction of the number of residue contacts in proteins by recurrent neural networks| Abstract Knowing the number of residue contacts in a protein is crucial for deriving constraints useful in modeling protein folding, protein structure, and/or scoring remote homology searches.  Here we use an ensemble of bi-directional recurrentneural network architectures and evolutionary information to improve the state-of-the-art in contact prediction using a large corpus of curated data.  The ensemble is used to discriminate between two different states of residue contacts, characterized by a contact number higher or lower than the average value of the residue distribution.  The ensemble achieves performances ranging from 70. 1% to 73. 1% depending on the radius adopted to discriminate contacts (6 A to 12 A).  These performances represent gains of 15% to 20% over the base line statistical predictors always assigning an aminoacid to the most numerous state, 3% to 7% better than any previous method.  Combination of different radius predictors further improves the performance. 
Distribution patterns of over-represented k-mers in non-coding yeast DNA| Abstract Motivation: Over-represented k-mers in genomic DNA regions are often of particular biological interest.  For example, over-represented k-mers in co-regulated families of genes are associated with the DNA binding sites of transcription factors.  To measure over-representation, we introduce a statistical background model based on single-mismatches, and apply it to the pooled 500bp ORF upstream regions of yeast.  More importantly, we investigate the context and spatial distribution of overrepresented k-mers in yeast upstream regions.  Results: Single and double-stranded spatial distributions of most over-represented k-mers are highly nonrandom, and predominantly cluster into a small number of classes that are robust with respect to overrepresentation measures.  Specifically, we show that the threemostcommon distribution patterns can berelatedto DNA structure, function, and evolution and correspond to: a) homologous ORF clusters associated with sharply localized distributions; b) regulatory elements associated with a symmetric broad hill-shaped distribution in the 50-200 bp upstream region; and c) runs of As, Ts, and ATs associated with a broad hill-shaped distribution also in the 50-200 bp upstream region, with extreme structural properties.  Analysis of over-representation, homology, localization, and DNA structureare essential components of a general data-mining approach to finding biologically important k-mers in raw genomic DNA and understanding the "lexicon"ofregulatory regions. 
Prediction of Protein Topologies Using Generalized IOHMMS and RNNs| Abstract We develop and test new machine learning methods for the prediction of topological representations of protein structures in the form of coarse- or fine-grained contact or distance maps that are translation and rotation invariant.  The methods are based on generalized input-output hidden Markov models (GIOHMMs) and generalized recursive neural networks (GRNNs).  The methods are used to predict topology directly in the fine-grained case and, in the coarsegrained case, indirectly by first learning how to score candidate graphs and then using the scoring function to search the space of possible configurations.  Computer simulations show that the predictors achieve state-of-the-art performance. 
Flexibility of the genetic code with respect to DNA structure| Abstract Motivation: The primary function of DNA is to carry genetic information through the genetic code.  DNA, however, contains a variety of other signals related, for instance, to reading frame, codon bias, pairwise codon bias, splice sites and transcription regulation, nucleosome positioning and DNA structure.  Here we study the relationship between the genetic code and DNA structure and address two questions.  First, to which degree does the degeneracy of the genetic code and the acceptable amino acid substitution patterns allow for the superimposition of DNA structural signals to protein coding sequences? Second, is the origin or evolution of the genetic code likely to have been constrained by DNA structure? Results: We develop an index for code flexibility with respect to DNA structure.  Using five different di- or tri-nucleotide models of sequence-dependent DNA structure, we show that the standard genetic code provides a fair level of flexibility at the level of broad amino acid categories.  Thus the code generally allows for the superimposition of any structural signal on any protein-coding sequence, through amino acid substitution.  The flexibility observed at the level of single amino acids allows only for the superimposition of punctual and loosely positioned signals to conserved amino acid sequences.  The degree of flexibility of the genetic code is low or average with respect to several classes of alternative codes.  This result is consistent with the view that DNA structure is not likely to have played a significant role in the origin and evolution of the genetic code. 
A Principled Approach to Detecting Surprising Events in Video| Abstract Primates demonstrate unparalleled ability at rapidly orienting towards important events in complex dynamic environments.  During rapid guidance of attention and gaze towards potential objects of interest or threats, often there is no time for detailed visual analysis.  Thus, heuristic computations are necessary to locate the most interesting events in quasi real-time.  We present a new theory of sensory surprise, which provides a principled and computable shortcut to important information.  We develop a model that computes instantaneous low-level surprise at every location in video streams.  The algorithm significantly correlates with eye movements of two humans watching complex video clips, including television programs (17,936 frames, 2,152 saccadic gaze shifts).  The system allows more sophisticated and time-consuming image analysis to be efficiently focused onto the most surprising subsets of the incoming data. 
Exploiting the past and the future in protein secondary structure prediction| Abstract Motivation: Predicting the secondary structure of a protein (alpha-helix, beta-sheet, coil) is an important step towards elucidating its three dimensional structure, as well as its function.  Presently, the best predictors are based on machine learning approaches, in particular neural network architectures with a fixed, and relatively short, input window of amino acids, centered at the prediction site.  Although a fixed small window avoids overfitting problems, it does not permit to capture variable long-ranged information.  Results: We introduce a family of novel architectures which can learn to make predictions based on variable ranges of dependencies.  These architectures extend recurrent neural networks, introducing non-causal bidirectional dynamics to capture both upstream and downstream information.  The prediction algorithm is completed by the use of mixtures of estimators that leverage evolutionary information, expressed in terms of multiple alignments, both at the input and output levels.  While our system currently achieves an overall performance exceeding 75% correct prediction---at least comparable to the best existing systems---the main emphasis here is on the development of new algorithmic ideas.  Availability: The executable program for predicting protein secondary structure is available from the authors free of charge. 
Improving the Prediction of Protein Secondary Structure in Three and Eight Classes Using Recurrent Neural Networks and Profiles| Abstract Secondary structure predictions are increasingly becoming the workhorse for several methods aiming at predicting protein structure and function.  Here we use ensembles of bidirectional recurrent neural network architectures, PSIBLAST-derived profiles, and a large non-redundant training set to derivetwo new predictors: (1) the second version of the SSpro program for secondary structure classification into three categories; (2) the first version of the SSpro8 program for secondary structure classification into the eight classes produced by the DSSP program.  We describe the results of three different test sets on which SSpro achieves a sustained performance of about 78% correct prediction.  We report confusion matrices, compare PSI-BLAST- to BLAST-derived profiles, and assess the corresponding performance improvements.  SSpro and SSpro8 are implemented as web servers, available together with other structural feature predictors at: http://promoter. ics. uci. edu/BRNN-PRED/. 
Improved statistical inference from DNA microarray data using analysis of variance and a Bayesian statistical framework| Statistical analysis of DNA microarray data SUMMARY: We describe statistical methods, based on the t-test, which can be conveniently used on high-density array data to test for statistically significant differences between treatments.  These t-tests employ either the observed variance among replicates within treatments or a Bayesian estimate of the variance among replicates within treatments based on a prior estimate obtained from a local estimate of the standard deviation.  The Bayesian prior allows statistical inference to be made from microarray data even when experiments are only replicated at nominal levels.  We apply these new statistical tests to a data set which examined changes in gene expression in IHF + and IHF - E.  coli cells and identify a more biologically reasonable set of candidate genes than those identified using only fold-change or statistical tests not incorporating a Bayesian prior.  We also show that using statistical tests based on analysis of variance and a Bayesian prior identifies genes that are up- or down- regulated following an experimental manipulation more reliably than approaches based only on fold-change.  All the described tests are implemented in a simple-to-use Web interface called Cyber-T that is located on the University of California at Irvine genomics web site. 
On the Use of Bayesian Methods for Evaluating Compartmental Neural Models|
The Principled Design of Large-Scale Recursive Neural Network Architectures--DAG-RNNs and the Protein Structure Prediction Problem|
Linear Learning: Landscapes and Algorithms|
Neural networks, orientations of the hypercube, and algebraic threshold functions|
Computing with Arrays of Bell-Shaped and Sigmoid Functions|
Assessing the accuracy of prediction algorithms for classification: an overview|
On Properties of Networks of Neuron-Like Elements|
Prediction of contact maps by GIOHMMs and recurrent neural networks using lateral propagation from all four cardinal corners|
Hidden Markov Models of the G-Protein-Coupled Receptor Family|
Hidden Markov Models for Human Genes|
Matching Protein b-Sheet Partners by Feedforward and Recurrent Neural Networks|
Bidirectional Dynamics for Protein Secondary Structure Prediction|
Modeling the Internet and the Web: Probabilistic Method and Algorithms|
Characterization of Prokaryotic and Eukaryotic Promoters Using Hidden Markov Models|
Number of stable points for spin-glasses and neural networks of higher order|
Neural Networks, Acyclic Orientations of the Hypercube, and Sets of Orthogonal Vectors|
Hidden Markov Models in Molecular Biology: New Algorithms and Applications|
Inferring Ground Truth from Subjective Labelling of Venus Images|
Analysis of Yeast's ORF Upstream Regions by Parallel Processing, Microarrays, and Computational Methods|

Analytical Mean Squared Error Curves in Temporal Difference Learning| Abstract Wehave calculated analytical expressions for how the bias and variance of the estimators provided byvarious temporal difference value estimation algorithms change with offline updates over trials in absorbing Markovchains using lookup table representations.  We illustrate classes of learning curvebehavior in various chains, and show the manner in which TD is sensitive to the choice of its stepsize and eligibility trace parameters. 
Adaptation and Unsupervised Learning| Abstract Adaptation is a ubiquitous neural and psychological phenomenon, with a wealth of instantiations and implications.  Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theory of only one main variety.  In this paper, we study adaptation from the perspective of factor analysis, a paradigmatic technique of unsupervised learning.  We use factor analysis to re-interpret a standard view of adaptation, and apply our new model to some recent data on adaptation in the domain of face discrimination. 
Distributional Population Codes and Multiple Motion Models| Abstract Most theoretical and empirical studies of population codes make the assumption that underlying neuronal activities is a unique and unambiguous value of an encoded quantity.  However, population activities can contain additional information about such things as multiple values of or uncertainty about the quantity.  We have previously suggested a method to recover extra information by treating the activities of the population of cells as coding for a complete distribution over the coded quantity rather than just a single value.  We now show how this approach bears on psychophysical and neurophysiological studies of population codes for motion direction in tasks involving transparent motion stimuli.  We show that, unlike standard approaches, it is able to recover multiple motions from population responses, and also that its output is consistent with both correct and erroneous human performance on psychophysical tasks.  A population code can be defined as a set of units whose activities collectively encode some underlying variable (or variables).  The standard view is that population codes are useful for accurately encoding the underlying variable when the individual units are noisy.  Current statistical approaches to interpreting population activity reflect this view, in that they determine the optimal single value that explains the observed activity pattern given a particular model of the noise (and possibly a loss function).  In our work, we have pursued an alternative hypothesis, that the population encodes additional information about the underlying variable, including multiple values and uncertainty.  The Distributional Population Coding (DPC) framework finds the best probability distribution across values that fits the population activity (Zemel, Dayan, & Pouget, 1998).  The DPC framework is appealing since it makes clear how extra information can \Delta` : 30 ffi \Delta` : 60 ffi \Delta` : 90 ffi \Delta` : 120 ffi Figure 1: Each of the four plots depicts a single MT cell response (spikes per second) to a transparent motion stimulus of a fixed directional difference (\Delt)between the two motion directions.  The x-axis gives the average direction of stimulus motion relative to the cell's preferred direction (0 ffi ).  From Treue, personal communication.  be conveyed in a population code.  In this paper, we use it to address a particular body of experimental data on transparent motion perception, due to Treue and colleagues (Hol & Treue, 1997; Rauber & Treue, 1997).  These transparent motion experiments provide an ideal test of the DPC framework, in that the neurophysiological data reveal how the population responds to multiple values in the stimuli, and the psychophysical data describe how these values are actually decoded, putatively from the population response.  We investigate how standard methods fare on these data, and compare their performance to that of DPC. 
Estimation of Non-Normalized Statistical Models by Score Matching| Abstract One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant.  Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant.  Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data.  While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function.  The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model.  The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data. 
Combining Probabilistic Population Codes| Abstract We study the problem of statistically correct inference in networks whose basic representations are population codes.  Population codes are ubiquitous in the brain, and involvethesimultaneous activityofmany units coding for some low dimensional quantity.  A classic example are place cells in the rat hippocampus: these fire when the animal is at a particular place in an environment, so the underlying quantity has two dimensions of spatial location.  We showhowtointerpret the activity as encoding whole probability distributions over the underlying variable rather then just single values, and propose a method of inductively learning mappings between population codes that are computationally tractable and yet offer good approximations to statistically optimal inference.  Wesimulate the method on some simple examples to prove its competence.  In a population code, information about some lowdimensional quantity (such as the position of a visual feature) is represented in the activity of a collection of units, each responding to a limited range of stimuli within this low-dimensional space.  Strong evidence exists for this form of coding at the sensory input areas of the brain (eg retinotopic and tonotopic maps) as well as at the motor output level [ Georgopoulos et al. , 1986 ] .  Evidence is mounting that many other intermediate neural processing areas also use population codes [ Tanaka, 1996 ] .  Certain important questions about population codes have been extensively investigated, including howtoextract an optimal underlying value [ Salinas and Abbott, 1994; Snippe, 1996 ] and how to learn such representations [ Kohonen, 1982 ] .  However, two important issues have been almost ignored (with the important exception of [ Anderson, 1994 ] ).  One is the treatment of population codes as encoding whole probability density functions (PDFs) over the underlying quantities rather than just a single value.  PDFs can convey significant additional information, suchascertainty(eg in the existence in an image of the relevant object), as well as the mean and variance (eg in its position).  The other issue is howtoperform inference in networks whose basic representations are population codes.  Zemel, Dayan, and Pouget [1997] have recently presented a general framework for the probabilistic interpretation of population codes in terms of PDFs.  In this paper we apply this framework to all the population codes in a processing hierarchy, and suggest an inference method that approximates, in a quantifiable manner, Bayesian optimal methods of representing and combining the probability distributions.  We first discuss howtointerpret PDFs from population codes, and then introduce our framework for combining these codes.  We illustrate the techniques with an example based on a form of cue combination.  1 An Example Consider the case of a hunter attempting to shoot a pheasant as it flies out of a tree.  We'll assume that the hunter uses two cues, a visual cue concerning motion in the tree and an auditory cue based on rustling of the leaves, to estimate the pheasant's size and velocity.  Based on this estimate, he selects a time and place to fire his shotgun.  The combination problem concerns how the two inputs should be combined to produce the output.  In the simplest version of the combination problem for this example, visual motion is confined to one part of the tree, and the auditory signal directly corresponds to this visual signal.  Here these two single-valued inputs (which we will term v and a)give rise to a single output, and the hunter confidently aims his shotgun (to location s).  Evidence exists that the two inputs and the output information in this example are each represented in neural population codes in some animals.  That is, a fixed collection of neurons fire for each of the three variables of interest.  The relevant visual input is represented by the activity of a population of motion detectors: in monkeys, a particular cortical area (MT) contains cells that selectively respond to motion of a particular velocity within a small range of visual locations.  Similarly, the relevant auditory input is represented in a population of detectors tuned to particular frequencies and spatial locations in owl auditory cortex [ Knudsen and Konishi, 1978 ] ; the frequency maycontain important information about the bird's size and speed.  Directional motor output is also represented in a population code in monkey motor cortex [ Georgopoulos et al. , 1986 ] .  Therefore even in the simple version of the problem, the brain does not directly represent the values v, a, and s, but instead represents eachina separate population code.  The most straightforward waytosolve this problem is to perform an intermediate step of extracting separate single values from the input population codes, combine these values, and then encode these into the motor output population code.  However, this seems not to be the strategy actually implemented in the brain, where new population codes appear to be generated directly from old ones.  Another level of complexity is introduced into the problem when we consider that the inputs may be uncertain or ambiguous.  For example, if the wind is blowing, then leaves maybemoving all over the tree giving rise to multiple plausible motion hypotheses, while at the same time the auditory cues maybetoo fainttoconfidently estimate the motion.  The experienced hunter maythen be able to narrow down the set of candidate motions based on his knowledge of the combinations of auditory and visual cues, but he might not be able to confidently select a single value.  Two additional problems are introduced in this more general case.  First wemust interpret a population code as representing a whole probability distribution over the underlying variable.  And then the combination method must preserve the probabilistic information in the inputs.  Thus the aim of a combination network is to infer a population code for the motor action that preserves the statistical relationship between the input and output probability distributions.  2 Theory The basic theory underlying the combination of population codes is extremely simple.  Population codes use the explicit activities r = fr i g of multiple cells (as in area MT) to code information about the value of an implicit underlying variable x (such as the direction and speed of motion of the leaves).  We are interested in the case that the activities r code a whole probability distribution over the underlying variable: P [xjr]: (1) Consider the example of the hunter.  Activities fr v i g and fr a j g represent probability distributions over the motion position and velocity based on the visual and auditory signals respectively.  We will assume that afferent information in the different modalities is independent.  The activities fr s k g will represent a probability distribution over the corresponding required position s of the shotgun according to the equivalent of Equation 1.  Two computational operations are required to produce appropriate r s : information from the different modalities must be integrated and then expressed in appropriate coordinates.  These operations have to respect the statistical semantics of r v and r a .  We use an underlying analysis-by-synthesis statistical model as in the Helmholtz machine [ Hinton et al. , 1995 ] .  In such a model, inference is based on the analysis or recognition inverse to a probabilistic synthetic or generativemodel that specifies probability distributions P [v; ajs]over the visual motion signal v and auditory pattern a given the shotgun location s.  Given true probability distributions P [vj!]andP [aj!] over the visual and auditory information (here ! represents the underlying information available to the hunter), recognition requires calculating: P[sj!]= Z v;a P [vj!]P [aj!]P [sjv; a]dvda (2) /P[s] Z v;a P[vj!]P [aj!]P [v; ajs]dvda (3) where P[s] is the prior distribution over s.  Equation 3 establishes the standard by which inferences about the distribution over s should be judged.  We have therefore reduced the computational problem to one of mapping activities r v and r a into activities r s for which P [sjr s ] from Equation 1 is a good approximation to the integration in Equation 3, where P [vj!]is what r v represents (according to Equation 1) and P [aj!] is what r a represents.  Figure 1 illustrates the generative and recognition operations, showing the activities, the distributions that they represent, and the various probabilistic relationships.  The remaining questions concern how activities r specify distributions as in Equation 1, and how r v and r a are actually combined to produce r s .  We describe two models for Equation 1: a model based on a standard form of function approximation, kernel density estimation (KDE) [ Anderson, 1994 ] , and an extension to the conventional statistical population coding model that is designed to handle anyformofPDF [ Zemel et al. ,1997 ] .  Both models form estimates ^ P r (x)ofP [xj!] based on r.  2. 1 The KDE Model One way of treating population codes as distributions is in terms of kernel density estimates (KDEs) [ Anderson, 1994 ] .  Here, activities r represent distribution ^ P r (x)
Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex| Abstract In memory consolidation, declarative memories which initially require the hippocampus for their recall, ultimately become independent of it.  Consolidation has been the focus of numerous experimental and qualitative modeling studies, but only little quantitative exploration.  We present a consolidation model in which hierarchical connections in the cortex, that initially instantiate purely semantic information acquired through probabilistic unsupervised learning, come to instantiate episodic information as well.  The hippocampus is responsible for helping complete partial input patterns before consolidation is complete, while also training the cortex to perform appropriate completion by itself. 
To appear in:| Abstract Most theoretical and empirical studies of population codes make the assumption that underlying neuronal activities is a unique and unambiguous value of an encoded quantity.  However, population activities can contain additional information about such things as multiple values of or uncertainty about the quantity.  We have previously suggested a method to recover extra information by treating the activities of the population of cells as coding for a complete distribution over the coded quantity rather than just a single value.  We now show how this approach bears on psychophysical and neurophysiological studies of population codes for motion direction in tasks involving transparent motion stimuli.  We show that, unlike standard approaches, it is able to recover multiple motions from population responses, and also that its output is consistent with both correct and erroneous human performance on psychophysical tasks.  A population code can be defined as a set of units whose activities collectively encode some underlying variable (or variables).  The standard view is that population codes are useful for accurately encoding the underlying variable when the individual units are noisy.  Current statistical approaches to interpreting population activity reflect this view, in that they determine the optimal single value that explains the observed activity pattern given a particular model of the noise (and possibly a loss function).  In our work, we have pursued an alternative hypothesis, that the population encodes additional information about the underlying variable, including multiple values and uncertainty.  The Distributional Population Coding (DPC) framework finds the best probability distribution across values that fits the population activity (Zemel, Dayan, & Pouget, 1998).  The DPC framework is appealing since it makes clear how extra information can \Delta` :30 ffi \Delta` :60 ffi \Delta` :90 ffi \Delta` :120 ffi Figure 1: Each of the four plots depicts a single MT cell response (spikes per second) to a transparent motion stimulus of a fixed directional difference (\Delt)between the two motion directions.  The x-axis gives the average direction of stimulus motion relative to the cell's preferred direction (0 ffi ).  From Treue, personal communication.  be conveyed in a population code.  In this paper, we use it to address a particular body of experimental data on transparent motion perception, due to Treue and colleagues (Hol & Treue, 1997; Rauber & Treue, 1997).  These transparent motion experiments provide an ideal test of the DPC framework, in that the neurophysiological data reveal how the population responds to multiple values in the stimuli, and the psychophysical data describe how these values are actually decoded, putatively from the population response.  We investigate how standard methods fare on these data, and compare their performance to that of DPC.  1 RESPONSES TO MULTIPLE MOTIONS Many investigators have examined neural and behavioral responses to stimuli composed of two patterns sliding across each other.  These often create the impression of two separate surfaces moving in different directions.  The general neurophysiological finding is that an MT cell's response to these stimuli can be characterized as the average of its responses to the individual components (van Wezel et al. , 1996; Recanzone et al. , 1997).  As an example, Figure 1 shows data obtained from single-cell recordings in MT to random dot patterns consisting of two distinct motion directions (Treue, personal communication).  Each plot is for a different relative angle (\Delt)between the two directions.  A plot can equivalently be viewed as the response of an population of MT cells having different preferred directions to a single presentation of a stimulus containing two directions.  If \Delta` is large, the activity profile is bimodal, but as the directional difference shrinks, the profile becomes unimodal.  The population response to a \Delta` =30 ffi motion stimulus is merely a wider version of the response to a stimulus containing a single direction of motion.  However, this transition from a bimodal to unimodal profiles in MT does not apparently correspond to subjects' percepts; subjects can reliably perceive both motions in superimposed transparent random patterns down to an angle of 10 ffi (Mather & Moulden, 1983).  If these MT activities play a determining role in mo
Manuscript: 1646 Recurrent Sampling Models for the Helmholtz Machine| Abstract Many recent analysis-by-synthesis density estimation models of cortical learning and processing have made the crucial simplifying assumption that units within a single layer are mutually independent given the states of units in the layer below or the layer above.  In this paper, we suggest using either a Markov random field or an alternative stochastic sampling architecture to capture explicitly particular forms of dependence within each layer.  We develop the architecture in the context of real and binary Helmholtz machines.  Recurrent sampling can be used to capture correlations within layers in the generative or the recognition models, and we also show how these can be combined. 
Computational Differences between Asymmetrical and Symmetrical Networks| Abstract Symmetrically connected recurrent networks have recently been used as models of a
Inference, Attention, and Decision in a Bayesian Neural Architecture| Abstract We study the synthesis of neural coding, selective attention and perceptual decision making.  We build a hierarchical neural architecture that implements Bayesian integration of noisy sensory input and top-down attentional priors, leading to sound perceptual discrimination.  Many known psychophysical and neural consequences of attentional modulation can be captured within this framework.  The model offers some explicit explanations for the way that prior information about one feature (visual location) can dramatically alter the inferential performance about a completely independent feature (orientation), as well as the provenance of multiplicative modulation of neural tuning curves.  The model also illustrates a possible reconciliation of cortical and neuromodulatory representations of uncertainty. 
A Model of Hippocampally Dependent Navigation, Using the Temporal Difference Learning Rule| ABSTRACT: This paper presents a model of how hippocampal place cells might be used for spatial navigation in two watermaze tasks: the standard reference memory task and a delayed matching-to-place task.  In the reference memory task, the escape platform occupies a single location and rats gradually learn relatively direct paths to the goal over the course of days, in each of which they perform a fixed number of trials.  In the delayed matching-to-place task, the escape platform occupies a novel location on each day, and rats gradually acquire one-trial learning, i. e. , direct paths on the second trial of each day.  The model uses a local, incremental, and statistically efficient connectionist algorithm called temporal difference learning in two distinct components.  The first is a reinforcement-based "actor-critic" network that is a general model of classical and instrumental conditioning.  In this case, it is applied to navigation, using place cells to provide information about state.  By itself, the actor-critic can learn the reference memory task, but this learning is inflexible to changes to the platform location.  We argue that one-trial learning in the delayed matching-to-place task demands a goal-independent representation of space.  This is provided by the second component of the model: a network that uses temporal difference learning and selfmotion information to acquire consistent spatial coordinates in the environment.  Each component of the model is necessary at a different stage of the task; the actor-critic provides a way of transferring control to the component that performs best.  The model successfully captures gradual acquisition in both tasks, and, in particular, the ultimate development of one-trial learning in the delayed matching-to-place task.  Place cells report a form of stable, allocentric information that is well-suited to the various kinds of learning in the model. 
Probabilistic Interpretation of Population Codes| Abstract We present a general encoding-decoding framework for interpreting the activity of a population of units.  A standard population code interpretation method, the Poisson model, starts from a description as to how a single value of an underlying quantity can generate the activities of each unit in the population.  In casting it in the encoding-decoding framework, we find that this model is too restrictive to describe fully the activities of units in population codes in higher processing areas, such as the medial temporal area.  Under a more powerful model, the population activity can convey information not only about a single value of some quantity but also about its whole distribution, including its variance, and perhaps even the certainty the system has in the actual presence in the world of the entity generating this quantity.  We propose a novel method for forming such probabilistic interpretations of population codes and compare it to the existing method. 
TD( ) Converges with Probability| Abstract The methods of temporal differences (Samuel, 1959; Sutton 1984, 1988) allow agents to learn accurate predictions about stationary stochastic future outcomes.  The learning is effectively stochastic approximation based on samples extracted from the process generating the agent's future.  Sutton (1988) proved that for a special case of temporal differences, the expected values of the predictions converge to their correct values, as larger samples are taken, and Dayan (1992) extended his proof to the general case.  This paper proves the stronger result that the predictions of a slightly modified form of temporal difference learning converge with probability one, andshows how to quantify the rate of convergence. 
The Effect of Correlated Variability on the Accuracy of a Population Code| Abstract We study the impact of correlated neuronal firing rate variability on the accuracy with which an encoded quantity can be extracted from a population of neurons.  Contrary to a widespread belief, correlations in the variabilities of neuronal firing rates do not, in general, limit the increase in coding accuracy provided by using large populations of encoding neurons.  Furthermore, in some cases, but not all, correlations improve the accuracy of a population code. 
Neural Models for Part-Whole Hierarchies| Abstract We present a connectionist method for representing images that explicitly addresses their hierarchical nature.  It blends data from neuroscience about whole-object viewpoint sensitive cells in inferotemporal cortex 8 and attentional basis-field modulation in V4 3 with ideas about hierarchical descriptions based on microfeatures.  5, 11 The resulting model makes critical use of bottom-up and top-down pathways for analysis and synthesis.  6 We illustrate the model with a simple example of representing information about faces.  1 Hierarchical Models Images of objects constitute an important paradigm case of a representational hierarchy, in which `wholes', such as faces, consist of `parts', such as eyes, noses and mouths.  The representation and manipulation of part-whole hierarchical information in fixed hardware is a heavy millstone around connectionist necks, and has consequently been the inspiration for many interesting proposals, such as Pollack's RAAM.  11 We turned to the primate visual system for clues.  Anterior inferotemporal cortex (IT) appears to construct representations of visually presented objects.  Mouths and faces are both objects, and so require fully elaborated representations, presumably at the level of anterior IT, probably using different (or possibly partially overlapping) sets of cells.  The natural way to represent the part-whole relationship between mouths and faces is to have a neuronal hierarchy, with connections bottom-up from the mouth units to the face units so that information about the mouth can be used to help recognize or analyze the image of a face, and connections top-down from the face units to the mouth units expressing the generative or synthetic knowledge that if there is a face in a scene, then there is (usually) a mouth, too. 
Expected and Unexpected Uncertainty: ACh and NE in the Neocortex| Abstract Inference and adaptation in noisy and changing, rich sensory
Competition and Multiple Cause Models| Abstract If different causes can interact on any occasion to generate a set of patterns, then systems modelling the generation have to model the interaction too.  We discuss a way of combining multiple causes that is based on the Integrated Segmentation and Recognition architecture of Keeler, Rumelhart and Leow (1991).  It is more co-operative than the scheme embodied in the mixture of experts architecture, which insists that just one cause generate each output, and more competitive than the noisy-or combination function which was recently suggested by Saund (1994a;b).  Simulations confirm its efficacy. 
A simple algorithm that discovers efficient perceptual codes| Abstract We describe the "wake-sleep" algorithm that allows a multilayer, unsupervised, neural network to build a hierarchy of representations of sensory input.  The network has bottom-up "recognition" connections that are used to convert sensory input into underlying representations.  Unlike most artificial neural networks, it also has top-down "generative" connections that can be used to reconstruct the sensory input from the representations.  In the "wake" phase of the learning algorithm, the network is driven by the bottom-up recognition connections and the top-down generative connections are trained to be better at reconstructing the sensory input from the representation chosen by the recognition process.  In the "sleep" phase, the network is driven top-down by the generative connections to produce a fantasized representation and a fantasized sensory input.  The recognition connections are then trained to be better at recovering the fantasized representation from the fantasized sensory input.  In both phases, the synaptic learning rule is simple and local.  The combined effect of the two phases is to create representations of the sensory input that are efficient in the following sense: On average, it takes more bits to describe each sensory input vector directly than to first describe the representation of the sensory input chosen by the recognition process and then describe the difference between the sensory input and its reconstruction from the chosen representation. 
Arbitrary Elastic Topologies and Ocular Dominance| Abstract The elastic net, which has been used to produce accounts of the formation of topology preserving maps and ocular dominance columns (OD), embodies a nearest neighbour topology.  A Hebbian account of OD is not so restricted -- and indeed makes the prediction that the width of the columns depends on the nature of the (more general) neighbourhood relations.  Elastic and Hebbian accounts have recently been unified -- raising a question mark about their different determiners of column widths.  This paper considers this issue, and demonstrates theoretically that it is possible to use more general topologies in the elastic net, including those effectively adopted in the Hebbian model. 
Varieties of Helmholtz Machine| Abstract The Helmholtz machine is a new unsupervised learning architecture that uses topdown and bottom up connections to build probability density models of input and inverses to those models.  The wake-sleep learning algorithm for Helmholtz machines involves just the purely local delta rule.  This paper suggests a number of different varieties of Helmholtz machines, each with its own strengths and weaknesses, and draws conclusions from the hypothesis that cortical information processing can be viewed in their terms. 
Encyclopedia of Cognitive Science, in press Reinforcement Learning| Glossary Markov chain a model for a random process that evolves over time such that the states (like locations in a maze) occupied in the future are independent of the states in the past given the current state.  Markov decision problem (MDP) a model for a controlled random process in which an agent's choice of action determines the probabilities of transitions of a Markov chain and lead to rewards (or costs) that need to be maximised (or minimised).  policy a deterministic or stochastic scheme for choosing an action at every state or location.  reward an immediate, possibly stochastic, payoff that results from performing an action in a state.  In an MDP, the immediate reward depends on the current state and action only.  The agent's aim is to optimise a sum or average of (possibly discounted) future rewards.  value function a function defined over states, which gives an estimate of the total (possibly discounted) reward expected in the future, starting from each state, and following a particular policy.  discounting if rewards received in the far future are worth less than rewards received sooner, they are described as being discounted.  Humans and animals appear to discount future rewards hyperbolically; exponential discounting is common in engineering and finance.  dynamic programming a collection of calculation techniques (notably policy and value iteration) for finding a policy that maximises reward or minimises costs.  temporal difference prediction error a measure of the inconsistency between estimates of the value function at two successive states.  This prediction error can be used to improve the predictions and also to choose good actions.  Reinforcement Learning Secondary Computation dynamic programming#value function#policy#actor-critic#Q
ACh, Uncertainty, and Cortical Inference| Abstract Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity.  Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity.  We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, influenced by the input stimuli, and top-down inference, in#uenced by contextual information.  We illustrate our proposal using a hierarchical hidden Markov model. 
Representational Accuracy of Stochastic Neural Populations| Fisher information is used to analyze the accuracy with which a neural population encodes D stimulus features.  It turns out that the form of response variability has a major impact on the encoding capacity and therefore plays an important role in the selection of an appropriate neural model.  In particular, in the presence of baseline ring, the reconstruction error rapidly increases with D in the case of Poissonian noise but not for additive noise.  The existence of limited-range correlations of the type found in cortical tissue yields a saturation of the Fisher information content as a function of the population size only for an additive noise model.  We also show that random variability in the correlation coef cient within a neural population, as found empirically, considerably improves the average encoding quality.  Finally, the representational accuracy of populations with inhomogeneous tuning properties, either with variability in the tuning widths or fragmented into specialized subpopulations, is superior to the case of identical and radially symmetric tuning curves usually considered in the literature. 
Dopamine Bonuses| Abstract Substantial data support a temporal difference (TD) model of dopamine (DA) neuron activity in which the cells provide a global error signal for reinforcement learning.  However, in certain circumstances, DA activity seems anomalous under the TD model, responding to non-rewarding stimuli.  We address these anomalies by suggesting that DA cells multiplex information about reward bonuses, including Sutton's exploration bonuses and Ng et al's non-distorting shaping bonuses.  We interpret this additional role for DA in terms of the unconditional attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. 
Competition and Arbors in Ocular Dominance| Abstract Hebbian and competitive Hebbian algorithms are almost ubiquitous in modeling pattern formation in cortical development.  We analyse in theoretical detail a particular model (adapted from Piepenbrock & Obermayer, 1999) for the development of 1d stripe-like patterns, which places competitive and interactive cortical influences, and free and restricted initial arborisation onto a common footing. 
Unpublished technical report Feudal Q-Learning| Abstract One popular way of exorcising the d#mon of dimensionality in dynamic programming is to consider spatial and temporal hierarchies for representing the value functions and policies.  This paper develops a hierarchical method for Qlearning which is based on the familiar notion of a recursive feudal serfdom, with managers setting tasks and giving rewards and punishments to their juniors and in their turn receiving tasks and rewards and punishments from their superiors.  We showhow one such system performs in a navigation task, based on a manual division of state-space at successively coarser resolutions.  Links with other hierarchical systems are discussed. 
The final version of this article will appear| Abstract Perceptual inference fundamentally involves uncertainty, arising from noise in sensation and the ill-posed nature of many perceptual problems.  Accurate perception requires that this uncertainty be correctly represented, manipulated, and learned about.  The choices made by subjects in various psychophysical experiments suggest that they do indeed take such uncertainty into account when making perceptual inferences, posing the question as to how uncertainty is represented in the activities of neuronal populations.  Most theoretical investigations of population coding have ignored this issue altogether; the few existing proposals that address it, do so in such a way that it is fatally con#ated with another facet of perceptual problems that also needs correct handling, namely multiplicity (that is, the simultaneous presence of multiple distinct stimuli).  We present and validate a more powerful proposal for the way that population activity may encode uncertainty, both distinctly from, and simultaneously with, multiplicity. 
Does the Wake-sleep Algorithm Produce Good Density Estimators?| Abstract The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a relatively efficient method of fitting a multilayer stochastic generative model to high-dimensional data.  In addition to the top-down connections in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule.  We use a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit. 
2002 Special issue Dopamine: generalization and bonuses| Abstract In the temporal difference model of primate dopamine neurons, their phasic activity reports a prediction error for future reward.  This model is supported by a wealth of experimental data.  However, in certain circumstances, the activity of the dopamine cells seems anomalous under the model, as they respond in particular ways to stimuli that are not obviously related to predictions of reward.  In this paper, we address two important sets of anomalies, those having to do with generalization and novelty.  Generalization responses are treated as the natural consequence of partial information; novelty responses are treated by the suggestion that dopamine cells multiplex information about reward bonuses, including exploration bonuses and shaping bonuses.  We interpret this additional role for dopamine in terms of the mechanistic attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. 
Temporal Difference Learning of Position Evaluation in the Game of Go| Abstract The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult.  Development of conventional Go programs is hampered by their knowledge-intensive nature.  We demonstrate a viable alternative by training networks to evaluate Go positions via temporal difference (TD) learning.  Our approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play.  These techniques yield far better performance than undifferentiated networks trained by selfplay alone.  A network with less than 500 weights learned within 3,000 games of 9x9 Go a position evaluation function that enables a primitive one-ply search to defeat a commercial Go program at a low playing level. 
Long Term Potentiation, Navigation & Dynamic Progamming| Abstract Blum and Abbott (1995) recently proposed an algorithm for learned navigation that is based on Hebbian changes to adaptive connections between place cells in the hippocampus.  This paper suggests using a temporal difference rule (
Using EM for Reinforcement Learning| Abstract We discsus Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent.  We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters.  The proof is based on a mapping between the RPP and a form of the expectation-maximisation procedure of Dempster, Laird & Rubin (1976). 
Modelling the Manifolds of Images of Handwritten Digits| Abstract This paper describes two new methods for modelling the manifolds of digitised images of handwritten digits.  The models allow a priori information about the structure of the manifolds to be combined with empirical data.  Accurate modelling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models.  One of the methods is grounded in principal components analysis, the other in factor analysis.  Both methods are based on locally linear, low-dimensional approximations to the underlying data manifold.  Links with other methods that model the manifold are discussed. 
A Hierarchical Model of Binocular Rivalry| Abstract Binocular rivalry is the alternating percept that can result when the two eyes see different scenes.  Recent psychophysical evidence supports the notion that some aspects of binocular rivalry bear functional similarities to other bistable percepts.  We build a model based on the hypothesis (Logothetis & Schall, 1989; Leopold & Logothetis, 1996; Logothetis, Leopold & Sheinberg, 1996) that alternation can be generated by competition between top-down cortical explanations for the inputs, rather than by direct competition between the inputs.  Recent neurophysiological evidence shows that some binocular neurons are modulated with the changing percept; others are not, even if they are selective between the stimuli presented to the eyes.  We extend our model to a hierarchy to address these effects. 
Position Variance, Recurrence and Perceptual Learning| Abstract Stimulus arrays are inevitably presented at different positions on the retina in visual tasks, even those that nominally require fixation.  In particular, this applies to many perceptual learning tasks.  We show that perceptual inference or discrimination in the face of positional variance has a structurally different quality from inference about fixed position stimuli, involving a particular, quadratic, non-linearity rather than a purely linear discrimination.  We show the advantage taking this non-linearity into account has for discrimination, and suggest it as a role for recurrent connections in area V1, by demonstrating the superior discrimination performance of a recurrent network.  We propose that learning the feedforward and recurrent neural connections for these tasks corresponds to the fast and slow components of learning observed in perceptual learning tasks. 
Assignment of Multiplicative Mixtures in Natural Images| Abstract In the analysis of natural images, Gaussian scale mixtures (GSM) have been used to account for the statistics of filter responses, and to inspire hierarchical cortical representational learning schemes.  GSMs pose a critical assignment problem, working out which filter responses were generated by a common multiplicative factor.  We present a new approach to solving this assignment problem through a probabilistic extension to the basic GSM, and show how to perform inference in the model using Gibbs sampling.  We demonstrate the efficacy of the approach on both synthetic and image data. 
Improving Policies without Measuring Merits| Abstract Performing policy iteration in dynamic programming should only require knowledge of relative rather than absolute measures of the utility of actions -- what Baird (1993) calls the advantages of actions at states.  Nevertheless, existing methods in dynamic programming (including Baird's) compute some form of absolute utility function.  For smooth problems, advantages satisfy two differential consistency conditions (including the requirement that they be free of curl), and we show that enforcing these can lead to appropriate policy improvement solely in terms of advantages. 
Section: Neuroscience Preference: Oral Replay, Repair and Consolidation| Abstract A standard view of memory consolidation is that episodes are stored temporarily in the hippocampus, and are transferred to the neocortex through replay.  Various recent experimental challenges to the idea of transfer, particularly for human memory, are forcing its re-evaluation.  However, although there is independent neurophysiological evidence for replay, short of transfer, there are few theoretical ideas for what it might be doing.  We suggest and demonstrate two important computational roles associated with neocortical indices. 
LEARNING TO EVALUATE GO POSITIONS VIA TEMPORAL DIFFERENCE METHODS| The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult.  Development of conventional Go programs is hampered by their knowledge-intensive nature.  We demonstrate a viable alternative by training neural networks to evaluate Go positions via temporal difference (TD) learning.  Our approach is based on neural network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play.  These techniques yield far better performance than undifferentiated networks trained by self-play alone.  A network with less than 500 weights learned within 3 000 games of 9x9 Go a position evaluation function
How fast to work: Response vigor, motivation and tonic dopamine| Abstract Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior.  However, the bulk of data on animal conditioning comes from free-operant experiments measuring how hard animals will work for reinforcement.  Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor.  They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water.  Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding.  Motivational states such as hunger shift these factors, skewing the tradeoff.  This accounts normatively for the effects of motivation on productivity, as well as many other classic findings.  Finally, we suggest that tonic dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. 
The wake-sleep algorithm for unsupervised neural networks| Abstract We describe an unsupervised learning algorithm for a multilayer network of stochastic neurons.  Bottom-up "recognition" connections convert the input into representations in successive hidden layers and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above.  In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below.  In the "sleep" phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.  Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections.  The wake-sleep algorithm finesses both these problems.  When there is no teaching signal to be matched, some other goal is required to force the hidden units to extract underlying structure.  In the wake-sleep algorithm the goal is to learn representations that are economical to describe but allow the input to be reconstructed accurately.  Each input vector could be communicated to a receiver by first sending its hidden representation and then sending the difference between the input vector and its top-down reconstruction from the hidden representation.  The aim of learning is to minimize the "description length" which is the total number of bits that would be required to communicate the input vectors in this way [1].  No communication actually takes place, but minimizing the description length that would be required forces the network to learn economical representations that capture the underlying regularities in the data [2].  The neural network has two quite different sets of connections.  The bottom-up "recognition" connections are used to convert the input vector into a representation in one or more layers of hidden units.  The top-down "generative" connections are then used to reconstruct an approximation to the input vector from its underlying representation.  The training algorithm for these two sets of connections can be used with many different types of stochastic neuron, but for simplicity we use only stochastic binary units that have states of or # .  The state of unit # is ### and the probability that it is on is: ### # ### fiff # 25510 - ##### # ##### # ##### # # (1) where # # is the bias of the unit and # # # is the weight on a connection from unit # .  Sometimes the units are driven by the generative weights and other times by the recognition weights, but the same equation is used in both cases.  In the "wake" phase the units are driven bottom-up using the recognition weights, producing a representation of the input vector in the first hidden layer, a representation of this representation in the second hidden layer and so on.  All of these layers of representation combined are called the "total representation" of the input, and the binary state of each hidden unit, , in total representation ! is fiff" # .  This total representation could be used to communicate the input vector, $ , to a receiver.  According to Shannon's coding theorem, it requires #&%('*),+ bits to communicate an event that has probability + under a distribution agreed by the sender and receiver.  We assume that the receiver knows the top-down generative weights [3] so these can be used to create the agreed probability distributions required for communication.  First, the activity of each unit, - , in the top hidden layer is communicated using the distribution #. # "/10 ### "/ # which is obtained by applying Eq.  1 to the single generative bias weight of unit - .  Then the activities of the units in each lower layer are communicated using the distribution #(# " # 0 #2# " # # obtained by applying Eq.  1 to the already communicated activities in the layer above, # " / , and the generative weights, # / # .  The description length of the binary state of unit is: 3 # # " # # # # # " # %('*)4# " # #5# # # " # # %('*)6# #7# " # # (2) The description length for input vector $ using the total representation ! is simply the cost of describing all the hidden states in all the hidden layers plus the cost of describing the input vector given the hidden states 3 # ! 0 $ # # 3 # ! #8# 3 # $#9 ! # #;:<}=@?A: # =B< 3 # # " # #8# :DC 3 # #DE C 9F! # (3)
Feudal Reinforcement Learning| Abstract One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time.  This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them.  Sub-managers need not initially understand their managers' commands.  They simply learn to maximise their reinforcement in the context of the current command.  We illustrate the system using a simple maze task. .  As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map. 
In NIPS 12 Acquisition in Autoshaping| Abstract Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning.  However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude.  We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli play a crucial role. 
Statistical Models and Sensory Attention| Abstract Physiological investigations into the neural basis of sensory attention have led to puzzling and contradictory results.  Attention can seemingly lead to increased, decreased and unchanged neural activities, according to features of attentional experiments that are not well understood.  We take one particular case in which activities increase as a result of attention, model its possible statistical underpinning, and relate our model to other attentional suggestions.  Increased activities in population codes are associated with increased certainty about the encoded quantities.  This increased certainty has to come from somewhere { in our model it emerges from particular changes in the model's processing strategy. 
Acquisition and Extinction in Autoshaping| Abstract Gallistel and Gibbon (2000) have presented quantitative data on the speed with which animals acquire behavioral responses during autoshaping, together with a statistical model of learning intended to account for them.  Although this model captures the form of the dependencies amongst critical variables, its detailed predictions are substantially at variance with the data.  In the present article, further key data on the speed of acquisition are used to motivate an alternative model of learning, in which animals can be interpreted as paying different amounts of attention to stimuli according to estimates of their differential reliabilities as predictors. 
Acetylcholine, Norepinephrine, and Spatial Attention| Abstract Despite strong implication of the neuromodulators acetylcholine (
Probabilistic computation in spiking populations| Abstract As animals interact with their environments, they must constantly update estimates about their states.  Bayesian models combine prior probabilities, a dynamical model and sensory evidence to perform a statistically optimal updating of the states.  These models have proved to be consistent with the results of many diverse psychophysical studies.  However, little is known about the neural representation and manipulation of such Bayesian information, particularly in populations of spiking neurons.  We consider this issue, suggesting a model based on standard neural architecture and activations.  We illustrate the approach on a simple random walk example, and apply it to a sensorimotor integration task that provides a particularly compelling example of dynamic probabilistic computation.  Bayesian models have been used to explain a gamut of experimental results in tasks which require estimates to be derived from multiple sensory cues.  These include a wide range of psychophysical studies of perception; 13, 16 motor action; 7, 14 and decision-making.  3, 5 Central to Bayesian inference is that computations are sensitive to uncertainties about afferent and efferent quantities, arising from ignorance, noise, or inherent ambiguity (e. g. , the aperture problem), and that these uncertainties change over time as information accumulates and dissipates.  Understanding how neurons represent and manipulate uncertain quantities is therefore key to understanding the neural instantiation of these Bayesian inferences.  Most previous work on representing probabilistic inference in neural populations has focused on the representation of static information.  1, 12, 17 These approaches suggest varying strategies for encoding and decoding uncertain quantities, but they do not readily generalize to real-world dynamic information processing tasks, particularly the most interesting cases in which stimulus information changes over the same timescale as spiking itself.  11 Notable exceptions are the recent models proposed by Gold and Shadlen, 5 Rao, 10 and Deneve, 4 all of which make seminal contributions, but make use of probabilistic representations that we argue are comparatively restricted.  We make two contributions in this paper.  First, we show how probabilistic information varying over time can be represented in a spiking population code.  Second, we show the utility of this method by applying it to a temporal sensorimotor integration task.  1 MODEL FORMULATION We frame the problem and our approach in terms of a two-level network, connecting one population of neurons to another; this construction is intended to apply to any level of processing.  The network maps input population spikes R(t) to output population spikes S(t), where input and output evolve over time (we use R(t) and S(t) to indicate the input and output spike trains from time 0 to t).  The input spikes are assumed to arise stochastically in relation to a trajectory X(t) of an underlying (but hidden) relevant variable.  Therefore, via standard Bayesian inference, R(t) determines a distribution over the hidden variable at time t, P (X(t)jR(t)).  Similarly, output spikes are assumed to determine a distribution over a related hidden variable.  For the recurrent and feedforward computation in the network, we start with the deceptively simple goal 9 of producing output spikes in such a way that the distribution Q(X(t)jS(t)) they imply over the same hidden variable X(t) as the input, faithfully matches P (X(t)jR(t)).  This might seem a strange goal, since one could surely just listen to the input spikes.  However, in order for the output spikes to track the hidden variable, the dynamics of the interactions between the neurons must explicitly capture the dynamics of the process X(t).  Once this `identity mapping' problem has been solved, more general, complex computations can be performed with ease.  We illustrate this on a multisensory integration task, tracking a hidden variable that depends on multiple sensory cues.  We first consider two models of what the input spikes R(t) imply about X(t), and then discuss how the recurrently coupled network captures these dynamics.  Input Coding and Decoding The input spikes R(t) constitute the observations and are assumed to be probabilistically related to the signal by a tuning function f(X; # i ): P (R i (t)jX(t)) / f(X; # i ) (1) for the spike train of the ith input neuron, with parameters # i .  The model described in the following treats X(t) as a continuous variable (although it applies equally to the discrete case).  We consider a version of the dynamics and input coding that permits an analytical examination of the impact of spikes: Let X(t) follow a stationary Gaussian process such that the joint distribution P (X(t 1 ); X(t 2 ); : : : ; X(tm )) is Gaussian for any finite collection of times, with a covariance matrix which depends on the time difference: C ij = c(jt i t j j).  Depending on the function c(j#tj), this can capture random walks of varying forms of smoothness.  We can then write P (X(t)jR(t)) / p(X(t)) R X(t) dX(t)P (R(t)jX(t))P (X(t)jX(t)) (2) where P (X(t)jX(t)) is the distribution over the whole trajectory X(t) conditional on the value of X(t) at its end point.  If R(t) are a set of conditionally independent inhomogeneous Poisson processes, we have P (R(t)jX(t)) / Q ij f(X(t ij ); # i ) exp P i R # d# f(X(#); # i ) # ; (3) where t ij 8j are the spike times j of neuron i in R(t).  Let # = [X(t ij )] be the vector of stimulus positions at the times at which we observed a spike and # = [#(t ij )] be the vector of spike positions.  If the tuning functions are Gaussian f(X; # i ) / exp( (X # i ) 2 =2# 2 ) and sufficiently dense that P i R # d# f(X; # i ) is independent of X (a standard assumption in population coding), then P (R(t)jX(t)) / exp( (# #) T (# #)=2# 2 ) and in Equation 2, we can marginalize out X(t) except at the spike times t ij : P (
Recognition in Hierarchical Models| Abstract.  Various proposals have recently been made which cast cortical processing in terms of hierarchical statistical generative models (Mumford, 1994; Kawato, 1993; Hinton & Zemel, 1994; Zemel, 1994; Hinton et al , 1995; Dayan et al , 1995; Olshausen & Field, 1996; Rao & Ballard, 1995).  In the case of vision, these claim that top-down connections in the cortical hierarchy capture essential aspects of how the activities of neurons in primary sensory areas are generated by the contents of visually observed scenes.  The counterpart to a generative model is its statistical inverse, called a recognition model (Hinton & Zemel, 1994).  This takes low-level activities and produces probability distributions over the entities in the world that could have led to them, expressed as activities of neurons in higher visual areas that model the image generation process.  Even if a generative model is computationally tractable, its associated recognition model may not be.  In this paper, we study various different types of exact, sampling-based and approximate recognition models in the light of computational and cortical constraints. 
Uncertainty and Learning| Abstract It is a commonplace in statistics that uncertainty about parameters drives learning.  Indeed one of the most influential models of behavioural learning has uncertainty at its heart.  However, many popular theoretical models of learning focus exclusively on error, and ignore uncertainty.  Here we review the links between learning and uncertainty from three perspectives: statistical theories such as the Kalman filter, psychological models in which differential attention is paid to stimuli with an effect on the speed of learning associated with those stimuli, and neurobiological data on the influence of the neuromodulators acetylcholine and norepinephrine on learning and inference. 
Norepinephrine and Neural Interrupts| Abstract Experimental data indicate that norepinephrine is critically involved in aspects of vigilance and attention.  Previously, we considered the function of this neuromodulatory system on a time scale of minutes and longer, and suggested that it signals global uncertainty arising from gross changes in environmental contingencies.  However, norepinephrine is also known to be activated phasically by familiar stimuli in welllearned tasks.  Here, we extend our uncertainty-based treatment of norepinephrine to this phasic mode, proposing that it is involved in the detection and reaction to state uncertainty within a task.  This role of norepinephrine can be understood through the metaphor of neural interrupts. 
An Hierarchical Model of Visual Rivalry In NIPS 9| Abstract Binocular rivalry is the alternating percept that can result when the two eyes see different scenes.  Recent psychophysical evidence supports an account for one component of binocular rivalry similar to that for other bistable percepts.  We test the hypothesis 19, 16, 18 that alternation can be generated by competition between top-down cortical explanations for the inputs, rather than by direct competition between the inputs.  Recent neurophysiological evidence shows that some binocular neurons are modulated with the changing percept; others are not, even if they are selective between the stimuli presented to the eyes.  We extend our model to a hierarchy to address these effects. 
A Bayesian Framework for Tilt Perception and Confidence| Abstract The misjudgement of tilt in images lies at the heart of entertaining visual illusions and rigorous perceptual psychophysics.  A wealth of findings has attracted many mechanistic models, but few clear computational principles.  We adopt a Bayesian approach to perceptual tilt estimation, showing how a smoothness prior offers a powerful way of addressing much confusing data.  In particular, we faithfully model recent results showing that confidence in estimation can be systematically affected by the same aspects of images that affect bias.  Confidence is central to Bayesian modeling approaches, and is applicable in many other perceptual domains.  Perceptual anomalies and illusions, such as the misjudgements of motion and tilt evident in so many psychophysical experiments, have intrigued researchers for decades.  1--3 A Bayesian view 4--8 has been particularly influential in models of motion processing, treating such anomalies as the normative product of prior information (often statistically codifying Gestalt laws) with likelihood information from the actual scenes presented.  Here, we expand the range of statistically normative accounts to tilt estimation, for which there are classes of results (on estimation confidence) that are so far not available for motion.  The tilt illusion arises when the perceived tilt of a center target is misjudged (ie bias) in the presence of flankers.  Another phenomenon, called Crowding, refers to a loss in the confidence (ie sensitivity) of perceived target tilt in the presence of flankers.  Attempts have been made to formalize these phenomena quantitatively.  Crowding has been modeled as compulsory feature pooling (ie averaging of orientations), ignoring spatial positions.  9, 10 The tilt illusion has been explained by lateral interactions 11, 12 in populations of orientationtuned units; and by calibration.  13 However, most models of this form cannot explain a number of crucial aspects of the data.  First, the geometry of the positional arrangement of the stimuli affects attraction versus repulsion in bias, as emphasized by Kapadia et al 14 (figure 1A), and others.  15, 16 Second, Solomon et al.  recently measured bias and sensitivity simultaneously.  11 The rich and surprising range of sensitivities, far from flat as a function of flanker angles (figure 1B), are outside the reach of standard models.  Moreover, current explanations do not offer a computational account of tilt perception as the outcome of a normative inference process.  Here, we demonstrate that a Bayesian framework for orientation estimation, with a prior favoring smoothness, can naturally explain a range of seemingly puzzling tilt data.  We explicitly consider both the geometry of the stimuli, and the issue of confidence in the esti(A)
Explaining Away in Weight Space| Abstract Explaining away has mostly been considered in terms of inference of states in belief networks.  We show how it can also arise in a Bayesian context in inference about the weights governing relationships such as those between stimuli and reinforcers in conditioning experiments such as backward blocking.  We show how explaining away in weight space can be accounted for using an extension of a Kalman filter model; provide a new approximate way of looking at the Kalman gain matrix as a whitener for the correlation matrix of the observation process; suggest a network implementation of this whitener using an architecture due to Goodall; and show that the resulting model exhibits backward blocking. 
Replay, Repair and Consolidation| Abstract A standard view of memory consolidation is that episodes are stored temporarily in the hippocampus, and are transferred to the neocortex through replay.  Various recent experimental challenges to the idea of transfer, particularly for human memory, are forcing its re-evaluation.  However, although there is independent neurophysiological evidence for replay, short of transfer, there are few theoretical ideas for what it might be doing.  We suggest and demonstrate two important computational roles associated with neocortical indices. 
Section: Neuroscience Preference: Oral Adaptation and Unsupervised Learning| Abstract Adaptation is a ubiquitous neural and psychological phenomenon, with a wealth of instantiations and implications.  Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theory of only one main variety.  In this paper, we study adaptation from the perspective of factor analysis, a paradigmatic technique of unsupervised learning.  We use factor analysis to re-interpret a standard view of adaptation, and apply our new model to some recent data on adaptation in the domain of face discrimination. 
Acquisition in Autoshaping| Abstract Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning.  However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude.  We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli play a crucial role. 
Using Aperiodic Reinforcement for Directed Self-Organization During Development| Abstract We present a local learning rule in which Hebbian learning is conditional on an incorrect prediction of a reinforcement signal.  We propose a biological interpretation of such a framework and display its utility through examples in which the reinforcement signal is cast as the delivery of a neuromodulator to its target.  Three examples are presented which illustrate how this framework can be applied to the development of the oculomotor system. 
Improving Generalisation for Temporal Difference Learning: The Successor Representation| Abstract Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations.  Appropriate generalisation between states is determined by how similar their successors are, and representations should follow suit.  This paper shows howTD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result. 
The Helmholtz machine| Abstract Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning.  One fruitful approach is to build a parameterised stochastic generative model, independent draws from which are likely to produce the patterns.  For all but the simplest generative models, each pattern can be generated in exponentially many ways.  It is thus intractable to adjust the parameters to maximize the probability of the observed patterns, We describe a way of finessing this combinatorial explosion by maximising an easily computed lower bound on the probability of the observations.  Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways. 
Statistical Models of Conditioning| Abstract Conditioning experiments probe the ways that animals make predictions about rewards and punishments and use those predictions to control their behavior.  One standard model of conditioning paradigms which involve many conditioned stimuli suggests that individual predictions should be added together.  Various key results show that this model fails in some circumstances, and motivate an alternative model, in which there is attentional selection between different available stimuli.  The new model is a form of mixture of experts, has a close relationship with some other existing psychological suggestions, and is statistically well-founded. 
The involvement of recurrent connections in area CA3 in establishing the properties of place fields: A model| Abstract Strong constraints on the neural mechanisms underlying the formation of place fields in the rodent hippocampus come from the systematic changes in spatial activity patterns that are consequent on systematic environmental manipulations.  We describe an attractor network model of area CA3 in which local, recurrent, excitatory and inhibitory interactions generate appropriate place cell representations from location- and direction-specific activity in the entorhinal cortex.  The model has two modes of operation, learning and recall, which are switched under neuromodulatory control.  During learning, mossy fiber inputs impose activity patterns on CA3.  Then, through Hebbian plasticity in the recurrent excitatory connections, attractors in CA3 are sculpted appropriately, and through Hebbian plasticity in the perforant path inputs, entorhinal activity is associated with these attractors.  During recall, the spatial characteristics of the place fields are controlled by the way that the perforant path input selects amongst the attractors.  Depending on the training experience provided, the model generates place fields that are either directional or non-directional, and which change in accordance with experimental data when the environment undergoes simple geometric transformations.  Representations of multiple environments can be stored and recalled with little interference, and these have the appropriate degrees of similarity in visually similar environments. 
2002 Special issue Acetylcholine in cortical inference| Abstract Acetylcholine (ACh) plays an important role in a wide variety of cognitive tasks, such as perception, selective attention, associative learning, and memory.  Extensive experimental and theoretical work in tasks involving learning and memory has suggested that ACh reports on unfamiliarity and controls plasticity and effective network connectivity.  Based on these computational and implementational insights, we develop a theory of cholinergic modulation in perceptual inference.  We propose that ACh levels reflect the uncertainty associated with topdown information, and have the effect of modulating the interaction between top-down and bottom-up processing in determining the appropriate neural representations for inputs.  We illustrate our proposal by means of an hierarchical hidden Markov model, showing that cholinergic modulation of contextual information leads to appropriate perceptual inference. 
Computational Modelling| Computational modelling is playing an increasingly accepted and important role in neuroscience.  It is not a unitary enterprise, though, and the distinction between two different sorts of modelling, one interested in description and the other also in function, is illustrated in their application to activitydependent developmental plasticity and adult conditioning. 
Bayesian Retrieval in Associative Memories with Storage Errors| Abstract---It is well known that for finite-sized networks, onestep retrieval in the autoassociative Willshaw net is a suboptimal way to extract the information stored in the synapses.  Iterative retrieval strategies are much better, but have hitherto only had heuristic justification.  We show how they emerge naturally from considerations of probabilistic inference under conditions of noisy and partial input and a corrupted weight matrix.  We start from the conditional probability distribution over possible patterns for retrieval.  This contains all possible information that is available to an observer of the network and the initial input.  Since this distribution is over exponentially many patterns, we use it to develop two approximate, but tractable, iterative retrieval methods.  One performs maximum likelihood inference to find the single most likely pattern, using the (negative log of the) conditional probability as a Lyapunov function for retrieval.  In physics terms, if storage errors are present, then the modified iterative update equations contain an additional antiferromagnetic interaction term and site dependent threshold values.  The second method makes a mean field assumption to optimize a tractable estimate of the full conditional probability distribution.  This leads to iterative mean field equations which can be interpreted in terms of a network of neurons with sigmoidal responses but with the same interactions and thresholds as in the maximum likelihood update equations.  In the absence of storage errors, both models become very similiar to the Willshaw model, where standard retrieval is iterated using a particular form of linear threshold strategy. 
Conditions for Cognition| Abstract The application of Marr's notions of computational theory to data on classical and operant conditioning suggests computational and neural frameworks into which learning algorithms fit.  This allow connections to be made between these algorithms and theoretical notions of what behaviour is appropriate, coming in this case from dynamic programming and Kalman filtering.  It also helps to reconcile apparently conflicting results. 
Foraging through Uncertainty Using Predictive Hebbian learning| ABSTRACT To survive, an animal must use sensory events to predict the presence of mates, food, danger, and various other stimuli.  We present a model which utilizes diffuse neuromodulatory systems to implement a predictive version of a Hebbian rule, and embed this rule in a feasible neural architecture.  The predictive model suggests a unified way in which neuromodulatory influences are used to bias actions and control synaptic plasticity, and offers a simple framework which is nonetheless more powerful than standard correlational accounts.  When required to forage in a stochastic environment, the model captures the strategies seen in the behavior of bees and a number of other animals. 
Factor Analysis Using Delta-Rule Wake-Sleep Learning| We describe a linear network that models correlations between real-valued visible variables using one or more real-valued hidden variables --- a factor analysis model.  This model can be seen as a linear version of the "Helmholtz machine", and its parameters can be learned using the "wake-sleep" method, in which learning of the primary "generative" model is assisted by a "recognition" model, whose role is to fill in the values of hidden variables based on the values of visible variables.  The generative and recognition models are jointly learned in "wake" and "sleep" phases, using just the delta rule.  This learning procedure is comparable in simplicity to Oja's version of Hebbian learning, which produces a somewhat different representation of correlations in terms of principal components.  We argue that the simplicity of wake-sleep learning makes factor analysis a plausible alternative to Hebbian learning as a model of activity-dependent cortical plasticity. 
A familiarity-based learning procedure for the establishment of place fields in area CA3 of the rat hippocampus| Abstract That familiarity should gate learning is a neurobiological and computational commonplace, and has been particularly investigated in the context of the hippocampus.  We show its critical importance in continuous attractor models, using the formation of place fields in hippocampal area CA3 as an example.  Without it, biased sampling of places in an environment, coming from purely random exploration, leads to degenerate attractors and prevents the formation of place fields.  Conversely, appropriate use of familiarity information during learning can counteract the sampling bias, resulting in a normal place cell representation. 
Rate- and Phase-coded Autoassociative Memory| Abstract Areas of the brain involved in various forms of memory exhibit patterns of neural activity quite unlike those in canonical computational models.  We show how to use well-founded Bayesian probabilistic autoassociative recall to derive biologically reasonable neuronal dynamics in recurrently coupled models, together with appropriate values for parameters such as the membrane time constant and inhibition.  We explicitly treat two cases.  One arises from a standard Hebbian learning rule, and involves activity patterns that are coded by graded firing rates.  The other arises from a spike timing dependent learning rule, and involves patterns coded by the phase of spike times relative to a coherent local field potential oscillation.  Our model offers a new and more complete understanding of how neural dynamics may support autoassociation. 
Exploration Bonuses and Dual Control| Abstract Finding the Bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable.  This paper shows how to compute suboptimal estimates based on a certainty equivalence approximation arising from a form of dual control.  This systematizes and extends existing uses of exploration bonuses in reinforcement learning (Sutton, 1990).  The approach has two components: a statistical model of uncertainty in the world and a way of turning this into exploratory behaviour.  It is applied to two-dimensional mazes with moveable barriers. 
Acetylcholine in Cortical Inference| Abstract Acetylcholine (ACh) plays an important role in a wide variety of cognitive tasks, such as perception, selective attention, associative learning, and memory.  Extensive experimental and theoretical work in tasks involving learning and memory has suggested that ACh reports on unfamiliarity and controls plasticity and effective network connectivity.  Based on these computational and mechanistic insights, we develop a theory of cholinergic modulation in perceptual inference.  We propose that ACh levels reflect the uncertainty associated with top-down information, and have the effect of modulating the interaction between top-down and bottom-up processing in determining the appropriate neural representations for inputs.  We illustrate our proposal by means of an hierarchical hidden Markov model, showing that cholinergic modulation of contextual information leads to appropriate perceptual inference. 
Nonlinear Component Analysis as a Kernel Eigenvalue Problem| A new method for performing a nonlinear form of principal component analysis is proposed.  By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map---for instance, the space of all possible five-pixel products in 1616 images.  We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition. 
The Convergence of TD( ) for General| Abstract The method of temporal differences (TD) is one way of making consistent predictions about the future.  This paper uses some analysis of Watkins [19] to extend a convergence theorem due to Sutton [17] from the case which only uses information from adjacent time steps to that involving information from arbitrary ones.  It also considers how this version of TD behaves in the face of linearly dependent representations for states -- demonstrating that it still converges, but to a different answer from the least mean squares algorithm.  Finally, it adapts Watkins' theorem that Q-learning, his closely related prediction and action learning method, converges with probability one, to demonstrate this strong form of convergence for a slightly modified version of TD.  Running head : TD( ) for General 1 This paper is based on a chapter of my thesis [5]. 
An optimal two transmit antenna space-time code and its stacked extension",|
Distributional Population Codes and Multiple Motion Models|
Technical Note Q-Learning|
Recognizing Handwritten Digits Using Mixtures of Linear Models|
Analytical Mean Squared Error Curves for Temporal Dierence|
Doubly Distributional Population Codes: Simultaneous Representation of Uncertainty and Multiplicity|
Navigating Through Temporal Difference|
Using the td(lambda) algorithm to learn an evaluation function for the game of go|
Improving generalization for temporal difference learning: The successor representation|
Foraging in an Uncertain Environment Using Predictive Hebbian Learning|
Structure in the Space of Value Functions|
Recurrent Sampling Models for the Helmholtz Machine|
Algebraic space-time codes that achieve maximal diversity and/or capacity optimality with low peak-to mean power ratio,|
A Hierarchical Model of Visual Rivalry|
Analytical Mean Squared Error Curves for Temporal Difference Learning|
Plasticity Kernels and Temporal Statistics|
Learning to evaluate Go positions via temporal difference methods|

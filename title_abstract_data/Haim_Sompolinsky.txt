On-line Learning of Dichotomies| Abstract The performance of on-line algorithms for learning dichotomies is studied.  In on-line learning, the number of examples P is equivalent to the learning time, since each example is presented only once.  The learning curve, or generalization error as a function of P , depends on the schedule at which the learning rate is lowered.  For a target that is a perceptron rule, the learning curve of the perceptron algorithm can decrease as fast as P\Gamma 1 , if the schedule is optimized.  If the target is not realizable by a perceptron, the perceptron algorithm does not generally converge to the solution with lowest generalization error.  For the case of unrealizability due to a simple output noise, we propose a new on-line algorithm for a perceptron yielding a learning curve that can approach the optimal generalization error as fast as P \Gamma 1=2 .  We then generalize the perceptron algorithm to any class of thresholded smooth functions learning a target from that class.  For "well-behaved" input distributions, if this algorithm converges to the optimal solution, its learning curve can decrease as fast as P \Gamma 1 . 
Query by Committee| Abstract We propose an algorithm called query by committee, in which a committee of students is trained on the same data set.  The next query is chosen according to the principle of maximal disagreement.  The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron.  As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain.  This leads to generalization error that decreases exponentially with the number of examples.  This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law.  We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms. 
Correlation Codes in Neuronal Populations| Abstract Population codes often rely on the tuning of the mean responses to the stimulus parameters.  However, this information can be greatly suppressed by long range correlations.  Here we study the efficiency of coding information in the second order statistics of the population responses.  We show that the Fisher Information of this system grows linearly with the size of the system.  We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks.  It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses. 
Majority Voting in the Presence of Random Voters| Abstract The literature about computation in the presence of faulty elements usually deals with worst-case (Byzantine) faults or with fail-stop faults.  In this paper we address an intermediate failure mode, namely random behavior.  We limit the discussion to the effect that random behavior can have on a majority-voting process.  Analytic results for single rounds of votes and for iterative votes are presented; the iterative case is a simple extension of a single round of votes because it is a Markov process.  The concept of quasi-stable states in a Markov process is introduced to account for the characteristics of the consensus states.  Then conditions for reliable voting in the face of random participants are found, using an analogy with a physical system of spins; this eliminates the need for a complex analysis of the Markov matrix.  Indeed, an important contribution of this paper is that it provides an example of a physical model that helps solve a computer-science problem. 
Stable orientation tuning in the visual cortex| Abstract We study the behaviour of the Ben-Yishai hypercolumn model [2] under presentation of oriented stimuli, having extended this model by including plastic afferent (LGN to cortex) connections.  We find that Hebbian plasticity creates a self-organising map and show that constraining or modifying the standard Hebb rule in a particular way will lead to a contrast-insensitive tuning width, thus giving an explanation for persistent orientation tuning as observed in the visual cortex.  Our analytical results confirm those of simulations done by Von der Malsburg [4] and provide a starting point for further analytical treatment of less restricted stimuli. 
Learning Curves in Large Neural Networks|
Rate Models for Conductance-Based Cortical Neuronal Networks|
Chaotic Balanced State in a Model Of Cortical Circuits|
Traveling Waves and the Processing of Weakly Tuned Inputs in a Cortical Network Module|
Relaxational dynamics of the Edwards-Anderson model and the mean-field theory of spin glasses|
Learning a Continuous Hidden Variable Model for Binary Data|
The Effect of Correlations on the Fisher Information of Population Codes|

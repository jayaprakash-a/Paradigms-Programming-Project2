Linear-time inference in Hierarchical HMMs| Abstract The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98].  Unfortunately, the original inference algorithm is rather complicated, and takes ######### time, where # is the length of the sequence, making it impractical for many domains.  In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes ####### time.  Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. 
Sample Propagation| Abstract Rao--Blackwellization is an approximation technique for probabilistic inference that flexibly combines exact inference with sampling.  It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably.  This paper presents Sample Propagation, an efficient implementation of Rao--Blackwellized approximate inference for a large class of models.  Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster's variables and then passing a message to one of its neighbors.  We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems. 
Robotic Mapping with Polygonal Random Fields| Abstract Two types of probabilistic maps are popular in the mobile robotics literature: occupancy grids and geometric maps.  Occupancy grids have the advantages of simplicity and speed, but they represent only a restricted class of maps and they make incorrect independence assumptions.  On the other hand, current geometric approaches, which characterize the environment by features such as line segments, can represent complex environments compactly.  However, they do not reason explicitly about occupancy, a necessity for motion planning; and, they lack a complete probability model over environmental structures.  In this paper we present a probabilistic mapping technique based on polygonal random fields (PRF), which combines the advantages of both approaches.  Our approach explicitly represents occupancy using a geometric representation, and it is based upon a consistent probability distribution over environments which avoids the incorrect independence assumptions made by occupancy grids.  We show how sampling techniques for PRFs can be applied to localized laser and sonar data, and we demonstrate significant improvements in mapping performance over occupancy grids. 
A Robust Architecture for Distributed Inference in Sensor Networks| Abstract--- Many inference problems that arise in sensor networks require the computation of a global conclusion that is consistent with local information known to each node.  A large class of these problems--including probabilistic inference, regression, and control problems---can be solved by message passing on a data structure called a junction tree.  In this paper, we present a distributed architecture for solving these problems that is robust to unreliable communication and node failures.  In this architecture, the nodes of the sensor network assemble themselves into a junction tree and exchange messages between neighbors to solve the inference problem efficiently and exactly.  A key part of the architecture is an efficient distributed algorithm for optimizing the choice of junction tree to minimize the communication and computation required by inference.  We present experimental results from a prototype implementation on a 97-node Mica2 mote network, as well as simulation results for three applications: distributed sensor calibration, optimal control, and sensor field modeling.  These experiments demonstrate that our distributed architecture can solve many important inference problems exactly, efficiently, and robustly. 
Distributed Regression: an Efficient Framework for Modeling Sensor Network Data| ABSTRACT We present distributed regression, an efficient and general framework for in-network modeling of sensor data.  In this framework, the nodes of the sensor network collaborate to optimally fit a global function to each of their local measurements.  The algorithm is based upon kernel linear regression, where the model takes the form of a weighted sum of local basis functions; this provides an expressive yet tractable class of models for sensor network data.  Rather than transmitting data to one another or outside the network, nodes communicate constraints on the model parameters, drastically reducing the communication required.  After the algorithm is run, each node can answer queries for its local region, or the nodes can efficiently transmit the parameters of the model to a user outside the network.  We present an evaluation of the algorithm based upon data from a 48-node sensor network deployment at the Intel Research - Berkeley Lab, demonstrating that our distributed algorithm converges to the optimal solution at a fast rate and is very robust to packet losses. 
Copyright 2004, Intel Corporation, All rights reserved| Distributed Regression: an Efficient Framework for Modeling Sensor Network Data.  DISCLAIMER: THIS DOCUMENT IS PROVIDED TO YOU "AS IS" WITH NO WARRANTIES WHATSOEVER, INCLUDING ANY WARRANTY OF MERCHANTABILITY NON-INFRINGEMENT,
Thin Junction Tree Filters for Simultaneous Localization and Mapping| Abstract Simultaneous Localization and Mapping (SLAM) is a fundamental problem in mobile robotics: while a robot navigates in an unknown environment, it must incrementally build a map of its surroundings and localize itself within that map.  Traditional approaches to the problem are based upon Kalman filters, but suffer from complexity issues: the size of the belief state and the time complexity of the filtering operation grow quadratically in the size of the map.  This paper presents a filtering technique that maintains a tractable approximation of the filtered belief state as a thin junction tree.  The junction tree grows under measurement and motion updates and is periodically "thinned" to remain tractable via efficient maximum likelihood projections.  When applied to the SLAM problem, these thin junction tree filters have a linear-space belief state representation, and use a linear-time filtering operation.  Further approximation can yield a constant-time filtering operation, at the expense of delaying the incorporation of observations into the majority of the map.  Experiments on a suite of SLAM problems validate the approach. 
Grammatical Bigrams| Abstract Unsupervised learning algorithms have been derived for several statistical models of English grammar, but their computational complexity makes applying them to large data sets intractable.  This paper presents a probabilistic model of English grammar that is much simpler than conventional models, but which admits an ecient EM training algorithm.  The model is based upon grammatical bigrams, i. e. , syntactic relationships between pairs of words.  We present the results of experiments that quantify the representational adequacy of the grammatical bigram model, its ability to generalize from labelled data, and its ability to induce syntactic structure from large amounts of raw text. 
Maximum Entropy Probabilistic Logic| Abstract Recent research has shown there are two types of uncertainty that can be expressed in first-order logic--propositional and statistical uncertainty---and that both types can be represented in terms of probability spaces.  However, these efforts have fallen short of providing a general account of how to design probability measures for these spaces; as a result, we lack a crucial component of any system that reasons under these types of uncertainty.  In this paper, we describe an automatic procedure for defining such measures in terms of a probabilistic knowledge base.  In particular, we employ the principle of maximum entropy to select measures that are consistent with our knowledge and that make the fewest assumptions in doing so.  This approach yields models of first-order uncertainty that are principled, intuitive, and economical in their representation. 
PREDATOR: An OR-DBMS with Enhanced Data Types,|
PREDATOR: An OR-DBMS with Enhanced Data Types|
Simulation-Based Language Understanding in Embodied Construction Grammar|
Cubic-time parsing and learning algorithms for grammatical bigram models|
Distributed inference in sensor networks|
Mutation of a phenylalanine conserved in SH3-containing tyrosine kinases activates the transforming ability of cAbl|
The junction filters for simultaneous localization and mapping|
Building an OR-DBMS over the WWW: Design and Implementation Issues|
Junction tree algorithms for solving sparse linear systems|
Thin junction tree filters for simultaneous localization and mapping| Computer Science Division. 
Structure and promoter activity of the mouse CDC25A gene|
Atomistics of crack branching,|

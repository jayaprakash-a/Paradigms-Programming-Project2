Automatic Attention-Based Prioritization of Unconstrained Video for Compression| ABSTRACT We apply a biologically-motivated algorithm that selects visually-salient regions of interest in video streams to multiply-foveated video compression.  Regions of high encoding priority are selected based on nonlinear integration of low-level visual cues, mimicking processing in primate occipital and posterior parietal cortex.  A dynamic foveation filter then blurs (foveates) every frame, increasingly with distance from high-priority regions.  Two variants of the model (one with continuously-variable blur proportional to saliency at every pixel, and the other with blur proportional to distance from three independent foveation centers) are validated against eye fixations from 4-6 human observers on 50 video clips (synthetic stimuli, video games, outdoors day and night home video, television newscast, sports, talk-shows, etc).  Significant overlap is found between human and algorithmic foveations on every clip with one variant, and on 48 out of 50 clips with the other.  Substantial compressed file size reductions by a factor 0. 5 on average are obtained for foveated compared to unfoveated clips.  These results suggest a general-purpose usefulness of the algorithm in improving compression ratios of unconstrained video. 
Optimal cue selection strategy| Abstract Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments.  We investigate whether our visual system selects cues that guide search in an optimal manner.  We formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio (SNR) between a search target and surrounding distractors.  This optimal strategy successfully accounts for several phenomena in visual search behavior, including the effect of target-distractor discriminability, uncertainty in target's features, distractor heterogeneity, and linear separability.  Furthermore, the theory generates a new prediction, which we verify through psychophysical experiments with human subjects.  Our results provide direct experimental evidence that humans select visual cues so as to maximize SNR between the targets and surrounding clutter. 
CINNIC, a new computational algorithm for the modeling of early visual contour integration in humans| In order to gain a better understanding of visual saliency, we have developed and algorithm which simulates the phenomenon of contour integration for the purpose of visual saliency.  The model developed consists of the classical butterfly pattern of connection between orientation selective neurons in the primary visual cortex.  In addition, we also add a local group suppression gain control to eliminate extraneous noise and a fast plasticity term which helps to account for closure effect often observed in humans exposed to closed contour maps.  Results from real world images suggest that our algorithm is effective at picking out reasonable contours from a scene.  The results improved with the introduction of both the fast plasticity and group suppression.  An addition of multi scale analysis has also increased the effectiveness as well.  1 Introduction We have created an algorithm, which will integrate contours in real world images.  The idea is to emulate the way the human brain integrates contours for visual salience in early visual pre-processing.  To this end, our goal is to simulate saliency for a given contour.  The important components of this model are not only its abilities to find contours in an image, but for it to find contours that a human finds salient as well.  Such things include contour continuity, length, closure and the uniqueness of the contour when compared to its background.  Our approach has been to start with a simple model of neural connections.  We use here a standard butterfly shape for connections, which has been tried with success in the past (for instance Li[2]).  The elements of the butterfly pattern are connections that branch out like wings to co-linear neurons, which creates excitation on the neurons it connects to.  However, suppression is also added with a set of orthogonal wings that are used to suppress parallel contours.  The end result should be that the butterfly
A saliency-based search mechanism for overt and covert shifts of visual attention| Abstract Most models of visual search, whether involving overt eye movements or covert shifts of attention, are based on the concept of a saliency map, that is, an explicit two-dimensional map that encodes the saliency or conspicuity of objects in the visual environment.  Competition among neurons in this map gives rise to a single winning location that corresponds to the next attended target.  Inhibiting this location automatically allows the system to attend to the next most salient location.  We describe a detailed computer implementation of such a scheme, focusing on the problem of combining information across modalities, here orientation, intensity and color information, in a purely stimulus-driven manner.  The model is applied to common psychophysical stimuli as well as to a very demanding visual search task.  Its successful performance is used to address the extent to which the primate visual system carries out visual search via one or more such saliency maps and how this can be tested.  2000 Elsevier Science Ltd.  All rights reserved. 
Bayesian Surprise Attracts Human Attention| Abstract The concept of surprise is central to sensory processing, adaptation, learning, and attention.  Yet, no widely-accepted mathematical theory currently exists to quantitatively characterize surprise elicited by a stimulus or event, for observers that range from single neurons to complex natural or engineered systems.  We describe a formal Bayesian definition of surprise that is the only consistent formulation under minimal axiomatic assumptions.  Surprise quantifies how data affects a natural or artificial observer, by measuring the difference between posterior and prior beliefs of the observer.  Using this framework we measure the extent to which humans direct their gaze towards surprising items while watching television and video games.  We find that subjects are strongly attracted towards surprising locations, with 72% of all human gaze shifts directed towards locations more surprising than the average, a figure which rises to 84% when considering only gaze targets simultaneously selected by all subjects.  The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.  Life is full of surprises, ranging from a great christmas gift or a new magic trick, to wardrobe malfunctions, reckless drivers, terrorist attacks, and tsunami waves.  Key to survival is our ability to rapidly attend to, identify, and learn from surprising events, to decide on present and future courses of action [1].  Yet, little theoretical and computational understanding exists of the very essence of surprise, as evidenced by the absence from our everyday vocabulary of a quantitative unit of surprise: Qualities such as the "wow factor" have remained vague and elusive to mathematical analysis.  Informal correlates of surprise exist at nearly all stages of neural processing.  In sensory neuroscience, it has been suggested that only the unexpected at one stage is transmitted to the next stage [2].  Hence, sensory cortex may have evolved to adapt to, to predict, and to quiet down the expected statistical regularities of the world [3, 4, 5, 6], focusing instead on events that are unpredictable or surprising.  Electrophysiological evidence for this early sensory emphasis onto surprising stimuli exists from studies of adaptation in visual [7, 8, 4, 9], olfactory [10, 11], and auditory cortices [12], subcortical structures like the LGN [13], and even retinal ganglion cells [14, 15] and cochlear hair cells [16]: neural response greatly attenuates with repeated or prolonged exposure to an initially novel stimulus.  Surprise and novelty are also central to learning and memory formation [1], to the point that surprise is believed to be a necessary trigger for associative learning [17, 18], as supported by mounting evidence for a role of the hippocampus as a novelty detector [19, 20, 21].  Finally, seeking novelty is a well-identified human character trait, with possible association with the dopamine D4 receptor gene [22, 23, 24].  In the Bayesian framework, we develop the only consistent theory of surprise, in terms of the difference between the posterior and prior distributions of beliefs of an observer over the available class of models or hypotheses about the world.  We show that this definition derived from first principles presents key advantages over more ad-hoc formulations, typically relying on detecting outlier stimuli.  Armed with this new framework, we provide direct experimental evidence that surprise best characterizes what attracts human gaze in large amounts of natural video stimuli.  We here extend a recent pilot study [25], adding more comprehensive theory, large-scale human data collection, and additional analysis.  1 Theory Bayesian Definition of Surprise.  We propose that surprise is a general concept, which can be derived from first principles and formalized across spatio-temporal scales, sensory modalities, and, more generally, data types and data sources.  Two elements are essential for a principled definition of surprise.  First, surprise can exist only in the presence of uncertainty, which can arise from intrinsic stochasticity, missing information, or limited computing resources.  A world that is purely deterministic and predictable in real-time for a given observer contains no surprises.  Second, surprise can only be defined in a relative, subjective, manner and is related to the expectations of the observer, be it a single synapse, neuronal circuit, organism, or computer device.  The same data may carry different amount of surprise for different observers, or even for the same observer taken at different times.  In probability and decision theory it can be shown that the only consistent and optimal way for modeling and reasoning about uncertainty is provided by the Bayesian theory of probability [26, 27, 28].  Furthermore, in the Bayesian framework, probabilities correspond to subjective degrees of beliefs in hypotheses or models which are updated, as data is acquired, using Bayes' theorem as the fundamental tool for transforming prior belief distributions into posterior belief distributions.  Therefore, within the same optimal framework, the only consistent definition of surprise must involve: (1) probabilistic concepts to cope with uncertainty; and (2) prior and posterior distributions to capture subjective expectations.  Consistently with this Bayesian approach, the background information of an observer is captured by his/her/its prior probability distribution {P (M)}M2M over the hypotheses or models M in a model space M.  Given this prior distribution of beliefs, the fundamental effect of a new data observation D on the observer is to change the prior distribution {P (M)}M2M into the posterior distribution {P (M |D)}M2M via Bayes theorem, whereby 8M 2 M, P (M |D) = P (D|M) P (D)
motion planning using hardware-accelerated computation of generalized Voronoi diagrams," in Proc| IEEE Int.  Conf.  Robot.  Autom. 
TARGET DETECTION USING SALIENCY-BASED ATTENTION| Most models of visual search, whether involving overt eye movements or covert shifts of attention, are based on the concept of a "saliency map", that is, an explicit two-dimensional map that encodes the saliency or conspicuity of objects in the visual environment.  Competition among neurons in this map gives rise to a single winning location that corresponds to the next attended target.  Inhibiting this location automatically allows the system to attend to the next most salient location.  We describe a detailed computer implementation of such a scheme, focusing on the problem of combining information across modalities, here orientation, intensity and color information, in a purely stimulus-driven manner.  We have successfully applied this model to a wide range of target detection tasks, using synthetic and natural stimuli.  Performance has however remained difficult to objectively evaluate on natural scenes, because no objective reference was available for comparison.  We here present predicted search times for our model on the Search2 database of rural scenes containing a military vehicle.  Overall, we found a poor correlation between human and model search times.  Further analysis however revealed that in 3/4 of the images, the model appeared to detect the target faster than humans (for comparison, we calibrated the model's arbitrary internal time frame such that no more than 24 image locations were visited per second).  It hence seems that this model, which had originally been designed not to find small, hidden military vehicles, but rather to find the few most obviously conspicuous objects in an image, performed as an efficient target detector on the Search2 dataset. 
l Original Contribution CORRELATION OF REGIONAL CEREBRAL BLOOD FLOW FROM PERFUSION MRI AND SPECT IN NORMAL SUBJECTS| The objective of this study was to determine the relationship in regional cerebral blood flow (rCBF) as measured with perfusion magnetic resonance imaging (pMRI) and single photon emission computer tomography (SPECT).  rCBF was determined in 26 healthy subjects with pMRI and SPECT.  After co-registration of pMRI with SPECT, rCBF was determined in 10 brain regions relative to the whole slice value.  pMRI was evaluated with and without elimination of large vessels.  rCBF from pMRI correlates significantly with rCBF from SPECT (r 5 0. 69 with and r 5 0. 59 without elimination of large vessels; p < 0. 0001 for both).  Elimination of large vessels reduced the interindividual variance of the pMRI measurements in most regions.  rCBF from pMRI shows good correlation with rCBF from SPECT.  Because pMRI is sensitive to flow in large vessels while SPECT is not, elimination of large vessels in pMRI reduces the interindividual variability of pMRI and improves the correlation between the two methods.  pMRI is a reliable noninvasive method for rCBF measurements.  1999 Elsevier Science Inc. 
Quantitative Modeling of Perceptual Salience at Human Eye| Abstract: We investigate the extent to which a simple model of bottom-up attention and salience may be embedded within a broader computational framework, and compared with human eye movement data.  We focus on quantifying whether increased simulation realism significantly affects quantitative measures of how well the model may predict where in video clips humans direct their gaze.  We hence compare three variants of the model, tested with 15 video clips of natural scenes shown to three observers.  We measure modelpredicted salience at the locations gazed to by the observers, compared to random locations.  The first variant simply processes the raw video clips.  The second adds a gaze-contingent foveation filter.  The third further attempts to realistically simulate dynamic human vision by embedding the video frames within a larger background, and shifting them to eye position.  Our main finding is that increasing simulation realism significantly improves the predictive ability of the model.  Better emulating the details of how a visual stimulus is captured by a constantly rotating retina during active vision has a significant positive impact onto quantitative comparisons between model and human behavior. 
Modeling the influence of task on attention| Abstract We propose a computational model for the task-specific guidance of visual attention in real-world scenes.  Our model emphasizes four aspects that are important in biological vision: determining task-relevance of an entity, biasing attention for the low-level visual features of desired targets, recognizing these targets using the same low-level features, and incrementally building a visual map of task-relevance at every scene location.  Given a task definition in the form of keywords, the model first determines and stores the task-relevant entities in working memory, using prior knowledge stored in long-term memory. 
Visual attention and target detection in cluttered natural scenes| Abstract.  Rather than attempting to fully interpret visual scenes in a parallel fashion, biological systems appear to employ a serial strategy by which an attentional spotlight rapidly selects circumscribed regions in the scene for further analysis.  The spatiotemporal deployment of attention has been shown to be controlled by both bottom-up (image-based) and top-down (volitional) cues.  We describe a detailed neuromimetic computer implementation of a bottom-up scheme for the control of visual attention, focusing on the problem of combining information across modalities (orientation, intensity, and color information) in a purely stimulusdriven manner.  We have applied this model to a wide range of target detection tasks, using synthetic and natural stimuli.  Performance has, however, remained difficult to objectively evaluate on natural scenes, because no objective reference was available for comparison.  We present predicted search times for our model on the Search -- 2 database of rural scenes containing a military vehicle.  Overall, we found a poor correlation between human and model search times.  Further analysis, however, revealed that in 75% of the images, the model appeared to detect the target faster than humans (for comparison, we calibrated the model's arbitrary internal time frame such that 2 to 4 image locations were visited per second).  It seems that this model, which had originally been designed not to find small, hidden military vehicles, but rather to find the few most obviously conspicuous objects in an image, performed as an efficient target detector on the Search -- 2 dataset.  Further developments of the model are finally explored, in particular through a more formal treatment of the difficult problem of extracting suitable low-level features to be fed into the saliency map.  2001 Society of Photo-Optical Instrumentation Engineers. 
To appear in: Neurobiology of Attention,| Abstract We describe two models of attention that utilize probabilistic principles to compute task-relevant variables.  In the first model, objects and visual scenes are represented iconically using spatial filters at multiple scales.  A maximum likelihood-based approach is used to compute the location of a target in a given scene.  The eye movements generated by such a strategy are shown to be similar to human eye movement patterns elicited during visual search in naturalistic scenes.  The second model is based on the statistical concept of predictive coding.  It assumes that top-down feedback from higher cortical areas conveys predictions of expected activity at lower levels while the errors in prediction are conveyed through feedforward connections.  The model explains how multiple objects in a scene can be recognized sequentially without an explicit spotlight of attention.  An extension of the model provides an interpretation of object-based versus spatial attention in terms of interactions between "what" and "where" networks in the visual pathway. 
Biologically inspired feature based categorization of objects| ABSTRACT We have developed a method for clustering features into objects by taking those features which include intensity, orientations and colors from the most salient points in an image as determined by our biologically motivated saliency program.  We can train a program to cluster these features by only supplying as training input the number of objects that should appear in an image.  We do this by clustering from a technique that involves linking nodes in a minimum spanning tree by not only distance, but by a density metric as well.  We can then form classes over objects or object segmentation in a novel validation set by training over a set of seven soft and hard parameters.  We discus as well the uses of such a flexible method in landmark based navigation since a robot using such a method may have a better ability to generalize over the features and objects. 
Short Papers A Model of Saliency-Based Visual Attention for Rapid Scene Analysis| Abstract---A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented.  Multiscale image features are combined into a single topographical saliency map.  A dynamical neural network then selects attended locations in order of decreasing saliency.  The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. 
Utilization and viability of biologically-inspired algorithms in a dynamic multi agent camera surveillance system| ABSTRACT In view of the growing complexity of computational tasks and their design, we propose that certain interactive systems may be better designed by utilizing computational strategies based on the study of the human brain.  Compared with current engineering paradigms, brain theory offers the promise of improved self-organization and adaptation to the current environment, freeing the programmer from having to address those issues in a procedural manner when designing and implementing large-scale complex systems.  To advance this hypothesis, we discus a multi-agent surveillance system where 12 agent CPUs each with its own camera, compete and cooperate to monitor a large room.  To cope with the overload of image data streaming from 12 cameras, we take inspiration from the primate's visual system, which allows the animal to operate a real-time selection of the few most conspicuous locations in visual input.  This is accomplished by having each camera agent utilize the bottom-up, saliency-based visual attention algorithm of Itti and Koch (Vision Research 2000;40(10-12):1489-1506) to scan the scene for objects of interest.  Real time operation is achieved using a distributed version that runs on a 16-CPU Beowulf cluster composed of the agent computers.  The algorithm guides cameras to track and monitor salient objects based on maps of color, orientation, intensity, and motion.  To spread camera view points or create cooperation in monitoring highly salient targets, camera agents bias each other by increasing or decreasing the weight of different feature vectors in other cameras, using mechanisms similar to excitation and suppression that have been documented in electrophysiology, psychophysics and imaging studies of low-level visual processing.  In addition, if cameras need to compete for computing resources, allocation of computational time is weighed based upon the history of each camera.  A camera agent that has a history of seeing more salient targets is more likely to obtain computational resources.  The system demonstrates the viability of biologically inspired systems in a real time tracking.  In future work we plan on implementing additional biological mechanisms for cooperative management of both the sensor and processing resources in this system that include top down biasing for target specificity as well as novelty and the activity of the tracked object in relation to sensitive features of the environment. 
Robust Multimodality Registration for Brain Mapping| Abstract: We present a robust intrasubject registration method for the synergistic use of multiple neuroimaging modalities, with applications to magnetic resonance imaging (MRI), functional MRI, perfusion MRI, MR spectroscopy, and single-photon emission computed tomography (SPECT).  This method allows user-friendly processing of difficult examinations (low spatial resolution, advanced pathology, motion during acquisition, and large areas of focal activation).  Registration of three-dimensional (3D) brain scans is initially estimated by first-order moment matching, followed by iterative anisotrophic chamfer matching of brain surfaces.  Automatic brain surface extraction is performed in all imaging modalities.  A new generalized distance definition and new specific methodologies allow registration of scans that cover only a limited range of brain surface.  A new semiautomated supervision scheme allows fast and intuitive corrections of possible false automatic registration results.  The accuracy of the MRI/SPECT anatomical-functional correspondence obtained was evaluated using simulations and two difficult clinical populations (tumors and degenerative brain disorders).  The average discrimination capability of SPECT (12. 4 mm in-plane resolution, 20 mm slice thickness) was found to be better than 5 mm after registration with MRI (5 mm slice thickness).  Registration accuracy was always better than imaging resolution.  Complete 3D MRI and SPECT registration time ranged between 6--11 min, in which surface matching represented 2--3 min.  No registration failure occurred.  In conclusion, the application of several new image processing techniques allowed efficient and robust registration.  Hum.  Brain Mapping 5:3--17, 1997.  r 1997 Wiley-Liss, Inc. 
Improved 3D Correction for Partial Volume Effects in Brain SPECT| Abstract: An improved method for correction of partial volume effects (PVE) in brain SPECT is proposed.  It is fully three-dimensional, does not require particular patient positioning, and works with scans only partially covering the brain.  The location of functionally inactive brain regions (primarily cerebrospinal fluid) is extracted from high-resolution MRI.  An automatic 3D registration algorithm then determines the geometric transformation between MRI and SPECT.  Correction consists of: 1) counting the volumetric active/inactive ratio in each volume element of the functional scan using the measured SPECT point spread function; 2) correcting the functional measures according to these ratios; 3) fusing functional and anatomical information at the resolution of MRI.  Quantitative validation was performed using a phantom containing a test region in which multiple parallel acrylic plates thinner than SPECT resolution created high PVE, as well as a large reference region not suffering from PVE.  Reference activity was recovered in the test region with an accuracy of 1--3%.  The method was applied to clinical images demonstrating a combination of hypoperfusion and cortical atrophy.  The composite anatomical-functional corrected images, in which the main sulci are visible, yield better differentiation between decreased function and focal atrophy. 
Quantifying the Contribution of Low-Level Saliency to Human Eye Movements in Dynamic Scenes| Abstract: We investigated the contribution of low-level saliency to human eye movements in complex dynamic scenes.  Eye movements were recorded while naive observers viewed a heterogeneous collection of 50 video clips (46,489 frames; 4-6 subjects per clip), yielding 11,916 saccades of amplitude # 2 # .  A model of bottom-up visual attention computed instantaneous saliency at the instant each saccade started and at its future endpoint location.  Median model-predicted saliency was 45% the maximum saliency, a significant factor 2. 03 greater than expected by chance.  Motion and temporal change were stronger predictors of human saccades than color, intensity or orientation features, with the best predictor being the sum of all features.  There was no significant correlation between model-predicted saliency and duration of fixation.  A majority of saccades were directed to a minority of locations reliably marked as salient by the model, suggesting that bottom-up saliency may provide a set of candidate saccade target locations, with the final choice of which location to fixate more strongly determined top-down. 
z/x MDMA on cerebral blood flow: a co-registered SPECT and MRI study| Abstract Z.  3,4-Methylenedioxymethamphetamine MDMA , an illicit recreational drug, damages serotonergic nerve endings.  Since the cerebrovasculature is regulated partly by the serotonergic system, MDMA may affect cerebral blood flow Z.  CBF in humans.  We evaluated 21 abstinent recreational MDMA users and 21 age- and gender-matched healthy subjects with brain SPECT and MRI.  Ten of the MDMA subjects also had repeat SPECT and MRI after receiving Z.  two doses of MDMA.  Abstinent MDMA users showed no significantly different global or regional CBF rCBF compared to the control subjects.  However, within 3 weeks after MDMA administration, rCBF remained decreased in the visual cortex, the caudate, the superior parietal and dorsolateral frontal regions compared to baseline rCBF.  The decreased rCBF tended to be more pronounced in subjects who received the higher dosage of MDMA.  Two subjects who were scanned at 2]3 months after MDMA administration showed increased rather than decreased rCBF. 
A Principled Approach to Detecting Surprising Events in Video| Abstract Primates demonstrate unparalleled ability at rapidly orienting towards important events in complex dynamic environments.  During rapid guidance of attention and gaze towards potential objects of interest or threats, often there is no time for detailed visual analysis.  Thus, heuristic computations are necessary to locate the most interesting events in quasi real-time.  We present a new theory of sensory surprise, which provides a principled and computable shortcut to important information.  We develop a model that computes instantaneous low-level surprise at every location in video streams.  The algorithm significantly correlates with eye movements of two humans watching complex video clips, including television programs (17,936 frames, 2,152 saccadic gaze shifts).  The system allows more sophisticated and time-consuming image analysis to be efficiently focused onto the most surprising subsets of the incoming data. 
SALIENCY-BASED MULTI-FOVEATED MPEG COMPRESSION| ABSTRACT Most current foveation strategies are limited to foveating sequences based on a direct measurement or an implicit assumption of the gaze direction.  Such approaches often fail in unconstrained environments or when necessary equipment is absent.  Alternatively, a computational model of visual attention may be used to predict visually salient locations.  We describe such a neurobiological model of attention and its specific application to foveated video compression.  The algorithm is demonstrated to be successful in foveating to Regions Of human Interest in a variety of video segments, including synthetic as well as natural scenes, and also gives good compression ratios. 
Realistic Avatar Eye and Head Animation Using a Neurobiological Model of Visual Attention| ABSTRACT We describe a neurobiological model of visual attention and eye/head movements in primates, and its application to the automatic animation of a realistic virtual human head watching an unconstrained variety of visual inputs.  The bottom-up (image-based) attention model is based on the known neurophysiology of visual processing along the occipito-parietal pathway of the primate brain, while the eye/head movement model is derived from recordings in freely behaving Rhesus monkeys.  The system is successful at autonomously saccading towards and tracking salient targets in a variety of video clips, including synthetic stimuli, real outdoors scenes and gaming console outputs.  The resulting virtual human eye/head animation yields realistic rendering of the simulation results, both suggesting applicability of this approach to avatar animation and reinforcing the plausibility of the neural model. 
Automatic Foveation for Video Compression Using a Neurobiological Model of Visual Attention| Abstract We evaluate the applicability of a biologically-motivated algorithm to select visually-salient regions of interest in video streams for multiply-foveated video compression.  Regions are selected based on a nonlinear integration of low-level visual cues, mimicking processing in primate occipital and posterior parietal cortex.  A dynamic foveation filter then blurs every frame, increasingly with distance from salient locations.  Sixty-three variants of the algorithm (varying number and shape of virtual foveas, maximum blur, and saliency competition) are evaluated against an outdoor video scene, using MPEG-1 and constantquality MPEG-4 (DivX) encoding.  Additional compression radios of 1. 1 to 8. 5 are achieved by foveation.  Two variants of the algorithm are validated against eye fixations recorded from 4-6 human observers on a heterogeneous collection of 50 video clips (over 45,000 frames in total).  Significantly higher overlap than expected by chance is found between human and algorithmic foveations.  With both variants, foveated clips are on average approximately half the size of unfoveated clips, for both MPEG-1 and MPEG-4.  These results suggest a general-purpose usefulness of the algorithm in improving compression ratios of unconstrained video. 
Sharing Resources: Buy Attention, Get Object Recognition| Abstract Inspired by nature's policy of sharing resources, we have enhanced our attention model with minimal extra hardware to enable the twin powers of object detection and recognition.  With just the elementary information available at the preattentive stage in the form of low-level feature maps tuned to color, intensity and orientation, our model learns representations of objects in diverse, complex backgrounds.  The representation starts with simple vectors of low-level feature values computed at different locations on the object (views of the object).  We then recursively combine these views to form instances, in turn combined into simple objects, composite objects, and so on, taking into account feature values and their variance.  Given any new scene, our model uses the learnt representation of the target object to perform top-down biasing on the attention system such as to render this object more salient by enhancing those features which are characteristic of the object.  Experimental results verified the null hypothesis that the enhanced model would take half the number of fixations as that taken by the naive bottom-up model to detect all targets in a scene.  Our model is also able to recognize a wide variety of simple objects ranging from geometrical objects to soda cans, handicap signs, and many others under noisy conditions.  There are few false negatives and false positives.  The good performance of our lightweight model suggests that the human visual system may indeed be sharing resources extensively and attention and object recognition may be so intimately related that if we buy attention, we might get the other with minimal effort!
Attentional Selection for Object Recognition – A Gentle Way| Abstract.  Attentional selection of an object for recognition is often modeled using all-or-nothing switching of neuronal connection pathways from the attended region of the retinal input to the recognition units.  However, there is little physiological evidence for such all-or-none modulation in early areas.  We present a combined model for spatial attention and object recognition in which the recognition system monitors the entire visual field, but attentional modulation by as little as 20% at a high level is sufficient to recognize multiple objects.  To determine the size and shape of the region to be modulated, a rough segmentation is performed, based on pre-attentive features already computed to guide attention.  Testing with synthetic and natural stimuli demonstrates that our new approach to attentional selection for recognition yields encouraging results in addition to being biologically plausible. 
Models of Bottom-Up Attention and Saliency| Abstract: Visually conspicuous, or so-called salient, stimuli often have the capability of attracting focal visual attention towards their locations.  Several computational architectures subserving this bottom-up, stimulus-driven, spatiotemporal deployment of attention are reviewed in this article.  The resulting computational models have applications not only to the prediction of visual search psychophysics, but also, in the domain of machine vision, to the rapid selection of regions of interest in complex, cluttered visual environments.  We describe an unusal such application, to the objective evaluation of advertising designs.  One of the most important functions of selective visual attention is to rapidly direct our gaze towards objects of interest in our visual environment.  From an evolutionary standpoint, this rapid orienting capability is critical in allowing living systems to quickly become aware of possible preys, mates or predators in their cluttered visual world.  It has become clear that attention guides where to look next based on both bottom-up (image-based) and top-down (task-dependent) cues (James, 1890/1981).  As such, attention implements an information processing bottleneck, only allowing a small part of the incoming sensory information to reach short-
Biologically-Inspired Face Detection: Non-Brute-Force-Search Approach| Abstract We present a biologically-inspired face detection system.  The system applies notions such as saliency, gist, and gaze to localize a face without performing blind spatial search.  The saliency model consists of highly parallel low-level computations that operate in domains such as intensity, orientation, and color.  It is used to direct attention to a set of conspicuous locations in an image as starting points.  The gist model, computed in parallel with the saliency model, estimates holistic image characteristics such as dominant contours and magnitude in high and low spatial frequency bands.  We are limiting its use to predicting the likely head size based on the entire scene.  Also, instead of identifying face as a single entity, this system performs detection by parts and uses spatial configuration constraints to be robust against occlusion and perspective. 
Neuromorphic algorithms for computer vision and attention| ABSTRACT We describe an integrated vision system which reliably detects persons in static color natural scenes, or other targets among distracting objects.  The system is built upon the biologically-inspired synergy between two processing stages: A fast trainable visual attention front-end (\where"), which rapidly selects a restricted number of conspicuous image locations, and a computationally expensive object recognition back-end (\what"), which determines whether the selected locations are targets of interest.  We experimentwithtwo recognition back-ends: One uses a support vector machine algorithm and achieves highly reliable recognition of pedestrians in natural scenes, but is not particularly biologically plausible, while the other is directly inspired from the neurobiology of inferotemporal cortex, but is not yet as robust with natural images.  Integrating the attention and recognition algorithms yields substantial speedup over exhaustive search, while preserving detection rate.  The success of this approach demonstrates that using a biological attention-based strategy to guide an object recognition system may represent an ecient strategy for rapid scene analysis. 
Modeling the Modulatory Effect of Attention on Human Spatial Vision| Abstract We present new simulation results, in which a computational model of interacting visual neurons simultaneously predicts the modulation of spatial vision thresholds by focal visual attention, for five dual-task human psychophysics experiments.  This new study complements our previous findings that attention activates a winnertake-all competition among early visual neurons within one cortical hypercolumn.  This \intensified competition" hypothesis assumed that attention equally affects all neurons, and yielded two singleunit predictions: an increase in gain and a sharpening of tuning with attention.  While both effects have been separately observed in electrophysiology, no single-unit study has yet shown them simultaneously.  Hence, we here explore whether our model could still predict our data if attention might only modulate neuronal gain, but do so non-uniformly across neurons and tasks.  Specifically, we investigate whether modulating the gain of only the neurons that are loudest, best-tuned, or most informative about the stimulus, or of all neurons equally but in a task-dependent manner, may account for the data.  We find that none of these hypotheses yields predictions as plausible as the intensified competition hypothesis, hence providing additional support for our original findings. 
Computational modeling of visual attention|
A Model of Saliency-Based Visual Attention for Rapid Scene Analysis|
A neural model combining attentional orienting to object recognition: Preliminary explorations on the interplay between where and what|
Models of bottom-up and top-down visual attention|
Attention activates winner-take-all competition among visual filters|
A comparison of feature combination strategies for saliency-based visual attention systems|
Perceptual consequences ofmultilevel selection|
Computational modelling of visual attention|
Feature combination strategies for saliency-based visual attention systems|
Segmentation of progressive multifocal leuko-encephalopathy lesions in fluid-attenuated inversion recovery magnetic resonance imaging|
A Goal Oriented Attention Guidance Model|
Eye movements are influenced by short-range interactions among orientation channels,|
Modeling primate visual attention|
Towards visually-guided neuromorphic robots: Welcome to the Beobot project|
Real-time high-performance attention focusing in outdoors color video streams,|
\Comparing attention models with different types of behavior data,|
A quantitative model for human spatial vision threshold on the basis of non-linear interactions among spatial filters|
Improved 3-D correction for partial volume effects in brain SPECT| Human Brain Mapping 5,. 
Revisiting spatial vision: toward a unifying model,|
Visual attention|
Towards an integrated model of saliency-based attention and object recognition in the primate's visual system|
Cerebral blood flow correlates of apathy in alzheimer disease|

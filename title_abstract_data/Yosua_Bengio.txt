Bias Learning, Knowledge Sharing| Abstract Biasing the hypothesis space of a learner has been shown to improve generalisation performances.  Methods for achieving this goal have been proposed, that range from deriving and introducing a bias into a learner to automatically learning the bias.  In the latter case, most methods learn the bias by simultaneously training several related tasks derived from the same domain and imposing constraints on their parameters.  We extend some of the ideas presented in this field and describe a new model that parameterizes the parameters of each task as a function of an affine manifold defined in parameter space and a point lying on the manifold.  An analysis of variance on a class of learning tasks is performed that shows some significantly improved performances when using the model. 
Semi-supervised Learning by Entropy Minimization| Abstract We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data.  In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning.  Our approach includes other approaches to the semi-supervised problem as particular or limiting cases.  A series of experiments illustrates that the proposed solutions benefit from unlabeled data.  The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model.  The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the "cluster assumption".  Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces. 
A Connectionist Approach to Speech Recognition| Abstract The task discussed in this paper is that of learning to map input sequences to output sequences.  In particular, problems of phoneme recognition in continuous speech are considered, but most of the discussed techniques could be applied to other tasks, such as the recognition of sequences of handwritten characters.  The systems considered in this paper are based on connectionist models, or artificial neural networks, sometimes combined with statistical techniques for recognition of sequences of patterns, stressing the integration of prior knowledge and learning.  Different architectures for sequence and speech recognition are reviewed, including recurrent networks as well as hybrid systems involving hidden Markov models. 
High Quality Document Image Compression with DjVu| Abstract We present a new image compression technique called "DjVu " that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color.  This enables fast transmission of document images over low-speed connections, while faithfully reproducing the visual aspect of the document, including color, fonts, pictures, and paper texture.  The DjVu compressor separates the text and drawings, which needs a high spatial resolution, from the pictures and backgrounds, which are smoother and can be coded at a lower spatial resolution.  Then, several novel techniques are used to maximize the compression ratio: the bi-level foreground image is encoded with AT&T's proposal to the new JBIG2 fax standard, and a new wavelet-based compression method is used for the backgrounds and pictures.  Both techniques use a new adaptive binary arithmetic coder called the Z-coder.  A typical magazine page in color at 300dpi can be compressed down to between40to60KB,approximately 5 to 10 times better than JPEG for a similar level of subjective quality.  A real-time, memory efficientversion of the decoder was implemented, and is available as a plug-in for popular web browsers. 
Use of Genetic Programming for the Search of a New Learning Rule for Neural Networks| Abstract--- In previous work ([1, 2, 3]) we explained how to use standard optimization methods such as simulated annealing,
Learning Eigenfunctions Links Spectral Embedding and Kernel PCA| Abstract In this paper, we show a direct relation between spectral embedding methods and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of an operator defined from a kernel and the unknown data generating density.  Whereas spectral embedding methods only provided coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nystrom formula) for Multi-Dimensional Scaling, spectral clustering, Laplacian eigenmaps, Locally Linear Embedding (LLE) and Isomap.  The analysis provides, for all such spectral embedding methods, the definition of a loss function, whose empirical average is minimized by the traditional algorithms.  The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn.  Experiments with LLE, Isomap, spectral clustering and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding. 
On Out-of-Sample Statistics for Time-Series| This paper studies an out-of-sample statistic for time-series prediction that is analogous to the widely used R 2 in-sample statistic.  We propose and study methods to estimate the variance of this out-of-sample statistic.  We suggest that the out-ofsample statistic is more robust to distributional and asymptotic assumptions behind many tests for in-sample statistics.  Furthermore we argue that it may be more important in some cases to choose a model that generalizes as well as possible rather than choose the parameters that are closest to the true parameters.  Comparative experiments are performed on a financial time-series (daily and monthly returns of the TSE300 index).  The experiments are performed for varying prediction horizons and we study the relation between predictibility (out-of-sample R 2 ), variability of the out-of-sample R 2 statistic, and the prediction horizon. 
Brain Inspired Reinforcement Learning| Abstract Successful application of reinforcement learning algorithms often involves considerable hand-crafting of the necessary non-linear features to reduce the complexity of the value functions and hence to promote convergence of the algorithm.  In contrast, the human brain readily and autonomously finds the complex features when provided with sufficient training.  Recent work in machine learning and neurophysiology has demonstrated the role of the basal ganglia and the frontal cortex in mammalian reinforcement learning.  This paper develops and explores new reinforcement learning algorithms inspired by neurological evidence that provides potential new approaches to the feature construction problem.  The algorithms are compared and evaluated on the Acrobot task. 
Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering| {bengioy,vincentp,paiemeje,delallea,lerouxni,ouimema} @iro. umontreal. ca Abstract Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors.  This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering.  This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel.  Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data. 
Stochastic Learning of Strategic Equilibria for Auctions| Abstract This paper presents a new application of stochastic adaptive learning algorithms to the computation of strategic equilibria in auctions.  The proposed approach addresses the problems of tracking a moving target and balancing exploration (of action space) versus exploitation (of better modeled regions of action space).  Neural networks are used to represent a stochastic decision model for each bidder.  Experiments confirm the correctness and usefulness of the approach. 
An Input Output HMM Architecture| Abstract We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm.  The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation. 
Hierarchical Probabilistic Neural Network Language Model| Abstract In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e. g.  in the language modeling component of speech recognizers.  The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient.  However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition.  As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition.  The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy. 
Robust Regression with Asymmetric Heavy-Tail Noise Distributions| Abstract In the presence of a heavy-tail noise distribution, regression becomes much more difficult.  Traditional robust regression methods assume that the noise distribution is symmetric and they downweight the influence of so-called outliers.  When the noise distribution is asymmetric these methods yield strongly biased regression estimators.  Motivated by data-mining problems for the insurance industry, we propose in this paper a new approachtorobust regression that is tailored to deal with the case where the noise distribution is asymmetric.  The main idea is to learn most of the parameters of the model using conditional quantile estimators (which are biased but robust estimators of the regression), and to learn a few remaining parameters to combine and correct these estimators, to minimize the average squared error.  Theoretical analysis and experiments show the clear advantages of the approach.  Results are on artificial data as well as real insurance data, using both linear and neural-network predictors. 
Learning Long-Term Dependencies with Gradient Descent is Difficult| Abstract Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems.  However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals.  We showwhy gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases.  These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods.  Based on an understanding of this problem, alternatives to standard gradient descent are considered. 
Efficient Non-Parametric Function Induction in Semi-Supervised Learning| Abstract There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones.  This paper follows up on proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples.  It extends them to function induction algorithms that correspond to the minimization
EVALUATING LONG-TERM DEPENDENCY BENCHMARK PROBLEMS BY RANDOM GUESSING| Abstract Numerous recent papers focus on standard recurrent nets' problems with tasks involving long-term dependencies.  We solve such tasks by random weight guessing (RG).  Although RG cannot be viewed as a reasonable learning algorithm we find that it often outperforms previous, more complex methods on widely used benchmark problems.  One reason for RG's success is that the solutions to many of these benchmarks are dense in weight space.  An analysis of cases in which RG works well versus those in which it does not can serve to improve the quality of benchmarks for novel recurrent net algorithms. 
Shared Context Probabilistic Transducers| Abstract Recently, a model for supervised learning of probabilistic transducers represented by suffix trees was introduced.  However, this algorithm tends to build very large trees, requiring very large amounts of computer memory.  In this paper, we propose a new, more compact, transducer model in which one shares the parameters of distributions associated to contexts yielding similar conditional output distributions.  We illustrate the advantages of the proposed algorithm with comparative experiments on inducing a noun phrase recognizer. 
Discovering Shared Structure in Manifold Learning| Abstract We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local will suffer from at least four generic problems associated with (1) noise in the data, (2) curvature of the manifold, (3) dimensionality of the manifold, and (4) the presence of many manifolds with little data per manifold.  This analysis suggests non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions.  A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented.  The function has parameters that are shared across space rather than estimated based on the local neighborhood, as in current non-parametric manifold learning algorithms.  The results show clearly the advantages of this approach with respect to local manifold learning algorithms. 
Learning Simple Non-Stationarities with Hyper-Parameters| Abstract We consider sequential data that is sampled from an unknown process, so that the data are not necessarily i. i. d. .  Most approaches to machine learning assume that data points are i. i. d. .  Instead we consider a measure of generalization that does not make this assumption, and we consider in this context a recently proposed approach to optimizing hyper-parameters, based on the computation of the gradient of a model selection criterion with respect to hyper-parameters.  Here we use hyper-parameters that control a function that gives different weights to different time steps in the historical data sequence.  The approach is successfully applied to modeling the volatility of stock returns one month ahead.  Comparative experiments with more traditional methods are presented. 
Convex Neural Networks| Abstract Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks.  We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem.  This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors. 
Generalization of a Parametric Learning Rule \Lambda| Abstract In previous work (
LeRec: a NN/HMM hybrid for on-line handwriting recognition| Abstract We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style.  The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm.  Words are then coded into low resolution "annotated images" where each pixel contains information about trajectory direction and curvature.  The recognizer is a convolution network which can be spatially replicated.  From the network output, a hidden Markov model produces word scores.  The entire system is globally trained to minimize word-level errors. 
Using a Financial Training Criterion Rather than a Prediction Criterion| Abstract The application of this work is to decision taking with financial time-series, using learning algorithms.  The traditional approach is to train a model using a prediction criterion, such as minimizing the squared error between predictions and actual values of a dependent variable, or maximizing the likelihood of a conditional model of the dependent variable.  We find here with noisy time-series that better results can be obtained when the model is directly trained in order to optimize the financial criterion of interest.  Experiments were performed on portfolio selection with 35 Canadian stocks. 
Incorporating Second-Order Functional Knowledge for Better Option Pricing| Abstract Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance.  We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them.  For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties.  We apply this new class of functions to the task of modeling the price of call options.  Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints. 
Globally Trained Handwritten Word Recognizer Using Spatial Representation, Convolutional Neural Networks, and Hidden Markov Models| Abstract We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style.  The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm.  Words are then coded into low resolution "annotated images" where each pixel contains information about trajectory direction and curvature.  The recognizer is a convolution network which can be spatially replicated.  From the network output, a hidden Markov model produces word scores.  The entire system is globally trained to minimize word-level errors. 
On the Optimization of a Synaptic Learning Rule| Abstract This paper presents a new approach to neural modeling based on the idea of using an automated method to optimize the parameters of a synaptic learning rule.  The synaptic modification rule is considered as a parametric function.  This function has local inputs and is the same in many neurons.  We can use standard optimization methods to select appropriate parameters for a given type of task.  We also present a theoretical analysis permitting to study the generalization property of such parametric learning rules.  By generalization, we mean the possibility for the learning rule to learn to solve new tasks.  Experiments were performed on three types of problems: a biologically inspired circuit (for conditioning in Aplysia), Boolean functions (linearly separable as well as non linearly separable) and classification tasks.  The neural network architecture as well as the form and initial parameter values of the synaptic learning function can be designed using a priori knowledge. 
Gaussian Mixture Densities for Classification of Nuclear Power Plant Data| Abstract In this paper we are concerned with the application of learning algorithms to the classification of reactor states in nuclear plants.  Two aspects must be considered: (1) some types of events (e. g. , abnormal or rare) will not appear in the data set, but the system should be able to detect them, (2) not only classification of signals but also their interpretation are important for nuclear plant monitoring.  We address both issues with a mixture of mixtures of Gaussians in which some parameters are shared to reflect the similar signals observed in different states of the reactor.  An EM algorithm for these shared Gaussian mixtures is presented.  Experimental results on nuclear plant data demonstrate the advantages of the proposed approach with respect to the above two points. 
Non-Local Manifold Parzen Windows| Abstract In order to escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x.  With this objective, we present a non-local non-parametric density estimator.  It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold.  It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models. 
Probabilistic Neural Network Models for Sequential Data| Abstract It has already been shown how Artificial Neural Networks (ANNs) can be incorporated into probabilistic models.  In this paper we review some of the approaches which have been proposed to incorporate them into probabilistic models of sequential data, such as Hidden Markov Models (HMMs).  We also discuss new developments and new ideas in this area, in particular how ANNs can be used to model high-dimensional discrete and continuous data to deal with the curse of dimensionality, and how the ideas proposed in these models could be applied to statistical language modeling to represent longer-term context than allowed by trigram models, while keeping word-order information. 
Greedy Spectral Embedding| Abstract Spectral dimensionality reduction methods and spectral clustering methods require computation of the principal eigenvectors of an n n matrix where n is the number of examples.  Following up on previously proposed techniques to speed-up kernel methods by focusing on a subset of m examples, we study a greedy selection procedure for this subset, based on the featurespace distance between a candidate example and the span of the previously chosen ones.  In the case of kernel PCA or spectral clustering this reduces computation to O(m 2 n).  For the same computational complexity, we can also compute the feature space projection of the non-selected examples on the subspace spanned by the selected examples, to estimate the embedding function based on all the data, which yields considerably better estimation of the embedding function.  This algorithm can be formulated in an online setting and we can bound the error on the approximation of the Gram matrix. 
Kernel Matching Pursuit| Abstract Matching Pursuit algorithms learn a function that is a weighted sum of basis functions, by sequentially appending functions to an initially empty basis, to approximate a target function in the leastsquares sense.  We show how matching pursuit can be extended to use non-squared error loss functions, and how it can be used to build kernel-based solutions to machine learning problems, while keeping control of the sparsity of the solution.  We present a version of the algorithm that makes an optimal choice of both the next basis and the weights of all the previously chosen bases.  Finally, links to boosting algorithms and RBF training procedures, as well as an extensive experimental comparison with SVMs for classification are given, showing comparable results with typically much sparser models. 
Convergence Properties of the K-Means Algorithms| Abstract This paper studies the convergence properties of the well known K-Means clustering algorithm.  The K-Means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the EM algorithm to this hard threshold case.  We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm. 
Model Selection for Small Sample Regression| Abstract.  Model selection is an important ingredient of many machine learning algorithms, in particular when the sample size in small, in order to strike the right trade-off between overfitting and underfitting.  Previous classical results for linear regression are based on an asymptotic analysis.  We present a new penalization
Inference for the Generalization Error| Abstract In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better.  Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case.  This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not.  We perform a theoretical investigation of the variance of a cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples.  Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased.  This analysis allows us to propose new estimators of this variance.  We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in (Dietterich, 1998).  In particular, the new tests have correct size and good power.  That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false. 
An EM Algorithm for Asynchronous Input/Output Hidden Markov Models| Abstract--- In learning tasks in which input sequences are mapped to output sequences, it is often the case that the input and output sequences are not synchronous.  For example, in speech recognition, acoustic sequences are longer than phoneme sequences.  Input/Output Hidden Markov Models have already been proposed to represent the distribution of an output sequence given an input sequence of the same length.  We extend here this model to the case of asynchronous sequences, and show an Expectation-Maximization algorithm for training such models. 
Unsupervised Sense Disambiguation Using Bilingual Probabilistic Models| Abstract We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora.  The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework.  The second model, which we call the Concept model, is a hierarchical model that uses a concept latent variable to relate different language specific sense labels.  We show that both models improve performance on the word sense disambiguation task over previous unsupervised approaches, with the Concept model showing the largest improvement.  Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language. 
Manifold Parzen Windows| Abstract The similarity between objects is a fundamental element of many learning algorithms.  Most non-parametric methods take this similarity to be fixed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies.  We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigenvectors of regularized local covariance matrices.  Experiments in density estimation show significant improvements with respect to Parzen density estimators.  The density estimators can also be used within Bayes classifiers, yielding classification rates similar to SVMs and much superior to the Parzen classifier. 
Extended Semantic Tagging for Entity Extraction| Abstract We present results of a
Scaling Large Learning Problems with Hard Parallel Mixtures| A challenge for statistical learning is to deal with large data sets, e. g.  in data mining.  The training time of ordinary Support Vector Machines is at least quadratic, which raises a serious research challenge if we want to deal with data sets of millions of examples.  We propose a "hard parallelizable mixture" methodology which yields significantly reduced training time through modularization and parallelization: the training data is iteratively partitioned by a "gater" model in such a way that it becomes easy to learn an "expert" model separately in each region of the partition.  A probabilistic extension and the use of a set of generative models allows representing the gater so that all pieces of the model are locally trained.  For SVMs, time complexity appears empirically to locally grow linearly with the number of examples, while generalization performance can be enhanced.  For the probabilistic version of the algorithm, the iterative algorithm provably goes down in a cost function that is an upper bound on the negative log-likelihood. 
Topic Segmentation : A First Stage to Dialog-Based Information Extraction| Abstract We study the problem of topic segmentation of manually transcribed speech in order to facilitate information extraction from dialogs.  Our approach is based on a combination of multi-source knowledge modeled by hidden Markov models.  We experiment with different combinations of linguistic-level cues on dialogs dealing with search and rescue missions.  Results show the effectiveness of multi-source knowledge. 
Diffusion of Credit in Markovian Models| Abstract This paper studies the problem of diffusion in Markovian models, such as hidden Markov models (HMMs) and how it makes very difficult the task of learning of long-term dependencies in sequences.  Using results from Markov chain theory, we show that the problem of diffusion is reduced if the transition probabilities approach 0 or 1.  Under this condition, standard HMMs have very limited modeling capabilities, but input/output HMMs can still perform interesting computations. 
No Unbiased Estimator of the Variance of K-Fold Cross-Validation| In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates.  This paper studies the very commonly used K-fold cross-validation estimator of generalization performance.  The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation.  The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance.  This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance.  This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied. 
METRIC-BASED MODEL SELECTION FOR TIME-SERIES FORECASTING| Abstract.  Metric-based methods, which use unlabeled data to detect gross differences in behavior away from the training points, have recently been introduced for model selection, often yielding very significant improvements over alternatives (including cross-validation).  We introduce extensions that take advantage of the particular case of time-series data in which the task involves prediction with a horizon h.  The ideas are (i) to use at t the h unlabeled examples that precede t for model selection, and (ii) take advantage of the different error distributions of cross-validation and the metric methods.  Experimental results establish the effectiveness of these extensions in the context of feature subset selection.  MODEL SELECTION AND REGULARIZATION Supervised learning algorithms take input/output training pairs {(x 1 , y 1 ) (x l , y l )} sampled (usually independently) from an unknown joint distribution P (X, Y ) and attempt to infer a function f 2 F that minimizes the expected value of the loss L(f(X), Y ) (also called the generalization error).  In many cases one faces the dilemma that if F is too "rich" then the average training set loss (training error) will be low but the expected out-of-sample loss may be large (overfitting), and viceversa if F is not "rich" enough (underfitting).  In many cases one can define a collection of increasingly complex function classes F 0 # F 1 # # F (although some methods studied here work as well with a partial order).  Model selection methods attempt to choose one of these function classes to avoid both overfitting and underfitting.  For example, in the case of variable subset selection, these subsets may correspond to the number of input variables that are allowed (e. g.  F i is the set of linear regressions with i input variables).  One approach to model selection is based on complexity penalization [5, 3].  Another approach to model selection is based on held-out data: one selects the model with the lowest generalization error, estimated by repeatedly training on a subset of the data and testing on the rest, e. g.  using the bootstrap, leave-one-out or K-fold crossvalidation (XVT).  The metric-based methods introduced by Schuurmans [6, 7] are somewhat in between in that they take advantage of unlabeled data not used for training (but only the input part) in order to introduce a complexity penalty.  These methods take advantage of unlabeled data: the behavior of functions corresponding to different choices of complexity are compared on the training data and on the unlabeled data, and differences in behavior that would indicate overfitting are exploited to perform model selection.  An overview of advances in model selection and feature selection methods can be found in a recent Machine Learning special issue [1].  After a review of metric-based model selection methods, we introduce the extensions proposed in this paper that deal specifically with time-series data. 
Experiments on the Application of IOHMMs to Model Financial Returns Series| Abstract Input/Output Hidden Markov Models (IOHMMs) are conditional hidden Markov models in which the emission (and possibly the transition) probabilities can be conditionned on an input sequence.  For example, these conditional distributions can be linear, logistic, or non-linear (using for example multi-layer neural networks).  We compare the generalization performance of several models which are special cases of Input/Output Hidden Markov Models on financial time-series prediction tasks: an unconditional Gaussian, a conditional linear Gaussian, a mixture of Gaussians, a mixture of conditional linear Gaussians, a hidden Markov model, and various IOHMMs.  The experiments are performed on modeling the returns of market and sector indices.  Note that the unconditional Gaussian estimates the first moment with the historical average.  The results show that, although for the first moment the historical average gives the best results, for the higher moments, the IOHMMs yielded significantly better performance, as measured by the out-of-sample likelihood. 
The Challenge of Non-Linear Regression on Large Datasets with Asymmetric Heavy Tails| Regression becomes unstable under a heavy-tail error distribution due to dominant effects of outliers.  Traditional robust estimators are helpful under symmetric error, by reducing the effect of outliers equally from both sides of the distribution.  Under asymmetric error, however, those estimators are biased because the outliers appear only one side of the distribution.  Motivated by datamining problems for the insurance industry, we propose in this paper a new approach to robust regression that is tailored to deal with asymmetric error.  The main idea is to estimate a majority of the parameters using quantile regression (which may be biased but robust), and to estimate the few remaining using least squares estimator to correct the biases.  Theoretical analysis shows conditions when the conditional expectations can be recovered from the conditional quantiles and what can be gained asymptotically with the proposed algorithm.  Experiments confirm the clear advantages of the approach.  Results are on synthetic data as well as real insurance data, using both linear and neural-network predictors. 
Quick Training of Probabilistic Neural Nets by Importance Sampling| Abstract Our previous work on statistical language modeling introduced the use of probabilistic feedforward neural networks to help dealing with the curse of dimensionality.  Training this model by maximum likelihood however requires for each example to perform as many network passes as there are words in the vocabulary.  Inspired by the contrastive divergence model, we propose and evaluate sampling-based methods which require network passes only for the observed \positive example" and a few sampled negative example words.  A very significant speed-up is obtained with an adaptive importance sampling. 
A Neural Probabilistic Language Model| Abstract A goal of statistical language modeling is to learn the joint probability function of sequences of words.  This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed representation for each word (i. e.  a similarity between words) along with (2) the probability function for word sequences, expressed with these representations.  Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence.  We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model. 
Input/Output HMMs for Sequence Processing| Abstract We consider problems of sequence processing and propose a solution based on a discrete state model in order to represent past context.  We introduce a recurrent connectionist architecture having a modular structure that associates a subnetwork to each state.  The model has a statistical interpretation we call Input/Output Hidden Markov Model (IOHMM).  It can be trained by the EM or GEM algorithms, considering state trajectories as missing data, which decouples temporal credit assignment and actual parameter estimation.  The model presents similarities to hidden Markov models (HMMs), but allows us to map input sequences to output sequences, using the same processing style as recurrent neural networks.  IOHMMs are trained using a more discriminant learning paradigm than HMMs, while potentially taking advantage of the EM algorithm.  We demonstrate that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem.  Experimental results are presented for the seven Tomita grammars, showing that these adaptive models can attain excellent generalization. 
The Curse of Highly Variable Functions for Local Kernel Machines| Abstract We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior -- with similarity between examples expressed with a local kernel -- are sensitive to the curse of dimensionality, or more precisely to the variability of the target.  Our discussion covers supervised, semisupervised and unsupervised learning algorithms.  These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set.  This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning.  We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i. e.  their Kolmogorov complexity may be low.  This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge. 
Multi-Task Learning for Stock Selection| Abstract Artificial Neural Networks can be used to predict future returns of stocks in order to take financial decisions.  Should one build a separate network for each stock or share the same network for all the stocks? In this paper we also explore other alternatives, in which some layers are shared and others are not shared.  When the prediction of future returns for different stocks are viewed as different tasks, sharing some parameters across stocks is a form of multi-task learning.  In a series of experiments with Canadian stocks, we obtain yearly returns that are more than 14% above various benchmarks. 
Hierarchical Recurrent Neural Networks for Long-Term Dependencies| Abstract We have already shown that extracting long-term dependencies from sequential data is difficult, both for deterministic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs).  In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context.  In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically.  This implies that long-term dependencies are represented by variables with a long time scale.  This principle is applied to a recurrent network which includes delays and multiple time scales.  Experiments confirm the advantages of such structures.  A similar approach is proposed for HMMs and IOHMMs. 
Spectral Clustering and Kernel PCA are Learning Eigenfunctions| Abstract In this paper, we show a direct equivalence between spectral clustering and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of a kernel, when the functions are from a function space whose scalar product is defined with respect to a density model.  This defines a natural mapping for new data points, for methods that only provided an embedding, such as spectral clustering and Laplacian eigenmaps.  The analysis hinges on a notion of generalization for embedding algorithms based on the estimation of underlying eigenfunctions, and suggests ways to improve this generalization by smoothing the data empirical distribution. 
Gradient-Based Optimization of Hyperparameters| a Cholesky decomposition.  In the more general case, we show that the implicit function theorem can be used to derive a formula for the hyperparameter gradient involving second derivatives of the training criterion. 
Cost Functions and Model Combination for VaR--based Asset Allocation using Neural Networks| Abstract We introduce an asset-allocation framework based on the active control of the value-at-risk of the portfolio.  Within this framework, we compare two paradigms for making the allocation using neural networks.  The first one uses the network to make a forecast of asset behavior, in conjunction with a traditional mean-variance allocator for constructing the portfolio.  The second paradigm uses the network to directly make the portfolio allocation decisions. 
Recurrent Neural Networks for Missing or Asynchronous Data| Abstract In this paper we propose recurrent neural networks with feedback into the input units for handling two types of data analysis problems.  On the one hand, this scheme can be used for static data when some of the input variables are missing.  On the other hand, it can also be used for sequential data, when some of the input variables are missing or are available at different frequencies.  Unlike in the case of probabilistic models (e. g.  Gaussian) of the missing variables, the network does not attempt to model the distribution of the missing variables given the observed variables.  Instead it is a more "discriminant" approach that fills in the missing variables for the sole purpose of minimizing a learning criterion (e. g. , to minimize an output error). 
Non-Local Manifold Tangent Learning| Abstract We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local will suffer from at least four generic problems associated with noise in the data, curvature of the manifold, dimensionality of the manifold, and the presence of many manifolds with little data per manifold.  This suggests non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions.  A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data (on learning handwritten character image rotations), where a local non-parametric method fails. 
A Neural Probabilistic Language Model| Abstract A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language.  This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.  Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set.  We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences.  The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations.  Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence.  Training such large
ft#U"pf#|
Learning a synaptic learning rule| Abstract This paper presents an original approach to neural modeling based on the idea of
Gradient-Based Learning Applied to Document Recognition| Abstract--Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique.  Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing.  This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task.  Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. 
Locally Linear Embedding for dimensionality reduction in QSAR| Abstract.  Current practice in QSAR usually involves generating a great number of chemical descriptors and then cutting them back with variable selection techniques.  Variable selection is an effective method to reduce the dimensionality but may discard some valuable information.  This paper introduces Locally Linear Embedding (LLE),
Taking on the Curse of Dimensionality in Joint Distributions Using Neural Networks| Abstract|The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially.  In this paper we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow at most as the square of the number of variables, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions.  The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units.  The connectivity of the neural network can be pruned by using dependency tests between the variables (thus reducing significantly the number of parameters).  Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks, and show that significant improvements can be obtained by pruning the network. 
Efficient Non-Parametric Function Induction in Semi-Supervised Learning| Abstract There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones.  This paper follows up on proposed nonparametric algorithms which provide an estimated continuous label for the given unlabeled examples.  First, it extends them to function induction algorithms that minimize a regularization criterion applied to an outof-sample example, and happen to have the form of Parzen windows regressors.  This allows to predict test labels without solving again a linear system of dimension n (the number of unlabeled and labeled training examples), which can cost O(n 3 ).  Second, this function induction procedure gives rise to an efficient approximation of the training process, reducing the linear system to be solved to m # n unknowns, using only a subset of m examples.  An improvement of O(n 2 /m 2 ) in time can thus be obtained.  Comparative experiments are presented, showing the good performance of the induction formula and approximation algorithm. 
New Distributed Probabilistic Language Models| Abstract Our previous work on statistical language modeling introduced the use of probabilistic feedforward neural networks with shared parameters in order to help dealing with the curse of dimensionality.  This work started with the motivation to speed up the above model and to take advantage of prior knowledge e. g. , in WordNet or in syntactically labeled data sets, and to better deal with polysemy.  With the objective of reaching these goals, we present here a series of new statistical language models, most of which are yet untested. 
Boosting Neural Networks paper No 1806| Abstract ''Boosting" is a general method for improving the performance of learning algorithms.  A recently proposed boosting algorithm is AdaBoost.  It has been applied with great success to several benchmark machine learning problems using mainly decision trees as base classifiers.  In this paper we investigate whether AdaBoost also works as well with neural networks, and we discuss the advantages and drawbacks of different versions of the AdaBoost algorithm.  In particular, we compare training methods based on sampling the training set and weighting the cost function.  The results suggest that random resampling of the training data is not the main explanation of the success of the improvements brought by AdaBoost.  This is in contrast to Bagging which directly aims at reducing variance and for which random resampling is essential to obtain the reduction in generalization error.  Our system achieves about 1. 4% error on a data set of online handwritten digits from more than 200 writers.  A boosted multi-layer network achieved 1. 5% error on the UCI Letters and 8. 1% error on the UCI satellite data set, which is significantly better than boosted decision trees. 
Word Normalization for On-Line Handwritten Word Recognition| Abstract We introduce a new approach to normalizing words written with an electronic stylus that applies to all styles of handwriting (upper case, lower case, printed, cursive, or mixed).  A geometrical model of the word spatial structure is fitted to the pen trajectory using the EM algorithm.  The fitting process maximizes the likelihood of the trajectory given the model and a set a priors on its parameters.  The method was evaluated and integrated to a recognition system that combines neural networks and hidden Markov models. 
Browsing through High Quality Document Images with DjVu| Abstract We present a new image compression technique called "DjVu " that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color.  With DjVu , any screen connected to the Internet can access and display images of scannedpages while faithfully reproducing the font, color, drawing, pictures, and paper texture.  A typical magazine page in color at 300dpi can becompressed down to between 40 to 60 KB, approximately 5 to 10 times better than JPEG for a similar level of subjective quality.  B&W documents are typically 15 to 30 KBytes at 300dpi, or 4 to 8 times better than CCITT-G4.  Areal-time, memory efficient version of the decoder was implemented, and is available as a plug-in for popular web browsers. 
The Curse of Dimensionality for Local Kernel Machines| Abstract We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms based on local kernels are sensitive to the curse of dimensionality.  These include local manifold learning algorithms such as Isomap and LLE, support vector classifiers with Gaussian or other local kernels, and graph-based semisupervised learning algorithms using a local similarity function.  These algorithms are shown to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set.  This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning.  There is a large class of data distributions for which non-local solutions could be expressed compactly and potentially be learned with few examples, but which will require a large number of local bases and therefore a large number of training examples when using a local learning algorithm. 
Global optimization of a neural network-hidden Markov model hybrid|
Training a neural network with a financial criterion rather than a prediction criterion,|
A Parallel Mixture of SVMs for Very Large Scale Problems|
Robust regression with asymmetric heavy-tail noise|
AdaBoosting neural networks|
BPS: A learning algorithm for capturing the dynamic nature of speech|
Semi-supervised learning by entropy minimization|
Density-sensitive metrics and kernels|
Locally linear embedding for dimensionality reduction in QSAR|
\Pattern recognition and neural networks"|
AdaBoosting Neural Networks: Application to on-line Character Recognition|
Markovian Models for Sequential Data| Neural Computing Surveys 2,. 
Credit Assignment through Time: Alternatives to Backpropagation|
Gradient flow in recurrent nets: the diculty of learning long-term dependencies|
Convolutional Networks for Images, Speech, and Time-Series",|
Reading checks with graph transformer networks|
Extensions to Metric-Based Model Selection|
Boosting Neural Networks|
Out-of-Sample Extensions for LLE,|
Special Issue on New methods for model selection and model combination,"|
Kernels Matching Pursuit|
Learning the dynamic of speech with backpropagation for sequences|
Diffusion of Context and Credit Information in Markovian Models|
The problem of learning long-term dependencies in recurrent networks|
An EM algorithm for asynchronous input/output hidden Markov models,|
Aspects th'eoriques de l'optimisation d'une rgle d'apprentissage,|
Analyses empiriques sur des transactions d'options|
Support vector machines for improving the classification of brain pet images,|
Reading checks with multilayer graph transformer networks|
The Z-Coder Adaptive Binary Coder|
Extracting hidden sense probabilities from bitexts|
Global training of document processing systems using graph transformer networks,"|
P Frasconi, and J Schmidhuber| Gradient flow in recurrent nets: the difficuly of learning long term dependencies. 
Inference for the generalisation error|
Artificial Neural Networks and their Application to Sequence Recognition|
An em approach to learning sequential behavior|
Binary Pseudowavelets and Applications to Bilevel Image Processing|
Continuous Optimization of Hyper-Parameters|
Programmable Execution of Multi-Layered Networks for Automatic Speech Recognition|
Di+usion of credits in Markovain model|
Markovian Model for Sequential Data",|
Linear regression and the optimization of hyper-parameters|
Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks|
Neural Network - Gaussian Mixture Hybrid for Speech Recognition or Density Estimation|
On-line handwriting recognition with neural networks: spatial representation versus temporal representation|,. 
Discriminative feature and model design for automatic speech recognition,|
Special Issue on Neural Networks for Data Mining and Knowledge Discovery,|
Training asynchronous input/output hidden markov models|
Bias learning, knoweldge sharing|
On the search for new learning rules for ANNs|
Genmark: Parallel gene recognition for both DNA strands|
Use of neural networks for the recognition of place of articulation|
Robust Regression with Asymmetric Heavy-Tail Noise Distributions|
Gradient-based learning applied to document recognition|
"Phonetically-based multi-layered networks for acoustic property extraction and automatic speech recognition,"|
Phonetically motivated acoustic parameters for continuous speech recognition using artificial neural networks,|
Speaker Independent Speech Recognition with Neural Networks and Speech Knowledge|
"A new approach to estimating density functions with artificial neural networks,"|
"A hybrid coder for hidden Markov models using a recurrent neural network,|
"Representations based on articulatory dynamics for speech recognition",|
Learning eigenfunctions of similarity: linking spectral clustering and kernel PCA|

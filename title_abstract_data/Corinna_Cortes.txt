Context-Free Recognition with Weighted Automata| Abstract We introduce the definition of language recognition with weighted automata, a generalization of the classical definition of recognition with unweighted acceptors.  We show that, with our definition of recognition, weighted automata can be used to recognize a class of languages that strictly includes regular languages.  The class of languages accepted depends on the weight set which has the algebraic structure of a semiring.  We give a generic linear time algorithm for recognition with weighted automata and describe examples with various weight sets illustrating the recognition of several classes of context-free languages.  We prove, in particular, that the class of languages equivalent to the language of palindromes can be recognized by weighted automata over the (+; \Delta)-semiring, and that the class of languages equivalent to the Dyck language of first order D 0\Lambda 1 can be recognized by weighted automata over the real tropical semiring. 
Margin-Based Ranking Meets Boosting in the Middle| AdaBoost and RankBoost will produce the same result, explaining the empirical observations. 
Giga-Mining| Abstract We describe an industrial-strength data mining application in telecommunications.  The application requires building a short (7 byte) profile for all telephone numbers seen on a large telecom network.  By large, we mean very large: we maintain approximately 350 million profiles.  In addition, the procedure for updating these profiles is based on processing approximately 275 million call records per day.  We discuss the motivation for massive tracking and fully describe the definition and the computation of one of the more interesting bytes in the profile. 
Distribution kernels based on moments of counts| Abstract Many applications in text and speech processing require the analysis of distributions of variable-length sequences.  We recently introduced a general kernel framework, rational kernels,
Squashing Flat Files Flatter| Abstract A feature of data mining that distinguishes it from "classical" machine learning (ML) and statistical modeling (SM) is scale.  The community seems to agree on this yet progress to this point has been limited.  We present a methodology that addresses scale in a novel fashion that has the potential for revolutionizing the field.  While the methodology applies most directly to flat (row by column) data sets we believe that it can be adapted to other representations.  Our approach to the problem is not to scale up individual ML and SM methods.  Rather we prefer to leverage the entire collection of existing methods by scaling down the data set.  We call the method squashing.  Our method demonstrably outperforms random sampling and a theoretical argument suggests how and why it works well.  Squashing consists of three modular steps: grouping, momentizing, and generating (GMG).  These three steps describe the squashing pipeline whereby the original (very large data set) is sectioned off into mutually exclusive groups (or bins); within each group a series of low-order moments are computed; and finally these moments are passed off to a routine that generates pseudo data that accurately reproduce the moments.  The result of the GMG squashing pipeline is a squashed data set that has the same structure as the original data with the addition of a weight for each pseudo data point that reflects the distribution of the original data into the initial groups.  Any ML or SM method that accepts weights can be used to analyze the weighted pseudo data.  By construction the resulting analyses will mimic the corresponding analyses on the original data set.  Squashing should appeal to many of the sub-disciplines of KDD: statistics: squashing generalizes the sufficiency principle to apply across parameter space and across model space, database research: a squashed data set is a lossy materialized view of a large data set that is tuned for analyses that parallel how data cubes are tuned for fast aggregation queries, algorithms: the steps in the GMG pipeline can individually be investigated and improved for both speed and accuracy, machine learning: extending existing ML methods for weighted data and devising new methods that explicitly exploit this structure. 
Weighted Automata Kernels -- General Framework and Algorithms| Abstract Kernel methods have found in recent years wide use in statistical learning techniques due to their good performance and their computational efficiency in high-dimensional feature space.  However, text or speech data cannot always be represented by the fixed-length vectors that the traditional kernels handle.  We recently introduced a general kernel framework based on weighted transducers, rational kernels, to extend kernel methods to the analysis of variable-length sequences and weighted automata [5] and described their application to spoken-dialog applications.  We presented a constructive algorithm for ensuring that rational kernels are positive definite symmetric, a property which guarantees the convergence of discriminant classification algorithms such as Support Vector Machines, and showed that many string kernels previously introduced in the computational biology literature are special instances of such positive definite symmetric rational kernels [4].  This paper reviews the essential results given in [5, 3, 4] and presents them in the form of a short tutorial. 
Rational Kernels: Theory and Algorithms| Abstract Many classification algorithms were originally designed for fixed-size vectors.  Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and more generally weighted automata.  An approach widely used in statistical learning techniques such as Support Vector Machines (SVMs) is that of kernel methods, due to their computational efficiency in high-dimensional feature spaces.  We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that extend kernel methods to the analysis of variable-length sequences or more generally weighted automata.  We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm.  Not all rational kernels are positive definite and symmetric (PDS), or equivalently verify the Mercer condition, a condition that guarantees the convergence of training for discriminant classification algorithms such as SVMs.  We present several theoretical results related to PDS rational kernels.  We show that under some general conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We give the proof of several characterization results that can be used to guide the design of PDS rational kernels.  We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels.  Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  Rational kernels can be combined with SVMs to form efficient and powerful techniques for a variety of classification tasks in text and speech processing, or computational biology.  We describe examples of general families of PDS rational kernels that are useful in many of these applications and report the result of our experiments illustrating the use of
On the Effective VC Dimension| Abstract The very idea of an "Effective Vapnik Chervonenkis (VC) dimension" (Vapnik, Levin and Le Cun, 1993) relies on the hypothesis that the relation between the generalization error and the number of training examples can be expressed by a formula algebraically similar to the VC bound.  This hypothesis calls for a serious discussion since the traditional VC bound widely overestimates the generalization error.  In this paper we describe an algorithm and data dependent measure of capacity.  We derive a confidence interval on the difference between the training error and the generalization error.  This confidence interval is much tighter than the traditional VC bound.  A simple change of the formulation of the problem yields this extra accuracy: our confidence interval bounds the error difference between a training set and a test set, rather than the error difference between a training set and some hypothetical grand truth.  This "transductive" approach allows for deriving a data and algorithm dependent confidence interval. 
Communities of interest| Abstract.  We consider problems that can be characterized by large dynamic graphs.  Communication networks provide the prototypical example of such problems where nodes in the graph are network IDs and the edges represent communication between pairs of network IDs.  In such graphs, nodes and edges appear and disappear through time so that methods that apply to static graphs are not sufficient.  We introduce a data structure that captures, in an approximate sense, the graph and its evolution through time.  The data structure arises from a bottom-up representation of the large graph as the union of small subgraphs, called Communities of Interest (COI), centered on every node.  These subgraphs are interesting in their own right and we discuss two applications in the area of telecommunications fraud detection to help motivate the ideas. 
Limits in Learning Machine Accuracy Imposed by Data Quality| Abstract Random errors and insufficiencies in databases limit the performance of any classifier trained from and applied to the database.  In this paper we propose a method to estimate the limiting performance of classifiers imposed by the database.  We demonstrate this technique on the task of predicting failure in telecommunication paths. 
Confidence Intervals for the Area under the ROC Curve| Abstract In many applications, good ranking is a highly desirable performance for a classifier.  The criterion commonly used to measure the ranking quality of a classification algorithm is the area under the ROC curve (AUC).  To report it properly, it is crucial to determine an interval of confidence for its value.  This paper provides confidence intervals for the AUC based on a statistical and combinatorial analysis using only simple parameters such as the error rate and the number of positive and negative examples.  The analysis is distribution-independent, it makes no assumption about the distribution of the scores of negative or positive examples.  The results are of practical use and can be viewed as the equivalent for AUC of the standard confidence intervals given in the case of the error rate.  They are compared with previous approaches in several standard classification tasks demonstrating the benefits of our analysis.  1 Motivation In many machine learning applications, the ranking quality of a classifier is critical.  For example, the ordering of the list of relevant documents returned by a search engine or a document classification system is essential.  The criterion widely used to measure the ranking quality of a classification algorithm is the area under an ROC curve (AUC).  But, to measure and report the AUC properly, it is crucial to determine an interval of confidence for its value as it is customary for the error rate and other measures.  It is also important to make the computation of the confidence interval practical by relying only on a small and simple number of parameters.  In the case of the error rate, such intervals are often derived from just the sample size N .  We present an extensive theoretical analysis of the AUC and show that a similar confidence interval can be derived for its value using only simple parameters such as the error rate k/N , the number of positive examples m, and the number of negative examples n = N- m.  Thus, our results extend to AUC the computation of confidence intervals from a small number of readily available parameters.  Our analysis is distribution-independent in the sense that it makes no assumption about the distribution of the scores of negative or positive examples.  The use of the error rate helps determine tight confidence intervals.  This contrasts with existing approaches presented in the statistical literature [11, 5, 2] which are based either on weak distribution-independent assumptions resulting in too loose confidence intervals, or strong distribution-dependent assumptions leading to tight but unsafe confidence intervals.  We show that our results are of practical use.  We also compare them with previous approaches in several standard classification tasks demonstrating the benefits of our analysis.  Our results are also useful for testing the statistical significance of the difference of the AUC values of two classifiers.  The paper is organized as follows.  We first introduce the definition of the AUC, its connection with the Wilcoxon-Mann-Whitney statistic (Section 2), and briefly review some essential aspects of the existing literature related to the computation of confidence intervals for the AUC.  Our computation of the expected value and variance of the AUC for a fixed error rate requires establishing several combinatorial identities.  Section 4 presents some existing identities and gives the proof of novel ones useful for the computation of the variance.  Section 5 gives the reduced expressions for the expected value and variance of the AUC for a fixed error rate.  These can be efficiently computed and used to determine our confidence intervals for the AUC (Section 6).  Section 7 reports the result of the comparison of our method with previous approaches, including empirical results for several standard tasks. 
Rational Kernels| Abstract We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that can be used for analysis of variable-length sequences or more generally weighted automata, in applications such as computational biology or speech recognition.  We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm.  We also describe several general families of positive definite symmetric rational kernels.  These general kernels can be combined with Support Vector Machines to form efficient and powerful techniques for spoken-dialog classification: highly complex kernels become easy to design and implement and lead to substantial improvements in the classification accuracy.  We also show that the string kernels considered in applications to computational biology are all specific instances of rational kernels. 
Hancock: A language for analyzing transactional data streams| and FREDERICK SMITH AT&T Labs 3 3 A?# ######################################################### #!##"$###%##&'#####$########### ########&(###%)*#+###,#####!####-#######. #####/######10
Hancock: a language for extracting signatures from data streams| ABSTRACT Massive transaction streams presentanumber of opportunities for data mining techniques.  Transactions might represent calls on a telephone network, commercial credit card purchases, stock market trades, or HTTP requests to a web server.  While historically such data have been collected for billing or security purposes, they are now being used to discover how customers or their intermediaries (called transactors) use the underlying services.  For several years, wehave computed evolving profiles (called signatures) of the transactors in large data streams using handwritten C code.  The signature for each transactor captures the salient features of his transactions through time.  Programs for processing signatures must be highly optimized because of the size of the data stream (several gigabytes per day) and the numberofsignaturestomaintain (hundreds of millions).  C programs to compute signatures often sacrificed readability for performance.  Consequently, they are difficult to verify and maintain.  Hancock is a domain-specific language created to express computationally efficient signature programs cleanly. Inthis paper, we describe the obstacles to computing signatures from massive streams and explain how Hancock addresses these problems.  For expository purposes, we present Hancock using a running example from the telecommunications industry# however, the language itself is general and applies equally well to other data sources. 
Positive Definite Rational Kernels| Abstract.  Kernel methods are widely used in statistical learning techniques.  We recently introduced a general kernel framework based on weighted transducers or rational relations, rational kernels, to extend kernel methods to the analysis of variable-length sequences or more generally weighted automata.  These kernels are ecient to compute and have been successfully used in applications such as spoken-dialog classi#cation.  Not all rational kernels are positive definite and symmetric (PDS) however, a sucient property for guaranteeing the convergence of discriminant classification algorithms such as Support Vector Machines.  We present several theoretical results related to PDS rational kernels.  We show in particular that under some conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels.  Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  1 Motivation Many classification algorithms were originally designed for fixed-length vectors.  Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and even more generally weighted automata.  Indeed, the output of a large-vocabulary speech recognizer for a particular input speech utterance, or that of a complex information extraction system combining several information sources for a specific input query, is typically a weighted automaton compactly representing a large set of alternative sequences.  The weights assigned by the system to each sequence are used to rank different alternatives according to the models the system is based on.  The error rate of such complex systems is still too high in many tasks to rely only on their one-best output, thus it is preferable instead to use the full output weighted automata which contain the correct result in most cases.  Kernel methods [13] are widely used in statistical learning techniques such as Support Vector Machines (SVMs) [2, 4, 14] due to their computational eciency in high-dimensional feature spaces.  Recently, a general kernel framework Semiring Set # # 0 1 Boolean f0; 1g _ ^ 0 1 Probability R+ + # 0 1 Log R [f2 ; +1g # log + +1 0 Tropical R [f2 ; +1g min + +1 0 Table 1.  Semiring examples.  # log is defined by: x # log y = log(e x + e y ).  based on weighted transducers or rational relations, rational kernels, was introduced to extend kernel methods to the analysis of variable-length sequences or more generally weighted automata [3].  It was shown that there are general and ecient algorithms for computing rational kernels.  Rational kernels have been successfully used for applications such as spoken-dialog classification.  Not all rational kernels are positive definite and symmetric (PDS), or equivalently verify the Mercer condition [1], a condition that guarantees the convergence of discriminant classification algorithms such as SVMs.  This motivates the study undertaken in this paper.  We present several theoretical results related to PDS rational kernels.  In particular, we show that under some conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings.  We also study the relationship between rational kernels and some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler [6], and some string kernels used in the context of computational biology [8].  We show that these kernels are all specific instances of rational kernels.  In each case, we explicitly describe the corresponding weighted transducer.  These transducers are often simple and ecient for computing kernels.  Their diagram often provides more insight into the definition of kernels and can guide the design of new kernels.  Our results also include the proof of the fact that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before.  2 Preliminaries In this section, we present the algebraic definitions and notation necessary to introduce rational kernels.  Definition 1 ([7]).  A system (K ; #; #; 0; 1) is a semiring if: (K ; #; 0) is a commutative monoid with identity element 0; (K ; #; 1) is a monoid with identity element 1; # distributes over #; and 0 is an annihilator for #: for all a 2 K ; a # 0 = 0 # a = 0.  Thus, a semiring is a ring that may lack negation.  Table 1 lists some familiar semirings.  Definition 2.  A weighted finite-state transducer T over a semiring K is an 8-tuple T = (#;
Support-Vector Networks| Abstract.  The support-vector network is a new learning machine for two-group classification problems.  The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space.  In this feature space a linear decision surface is constructed.  Special properties of the decision surface ensures high generalization ability of the learning machine.  The idea behind the supportvector network was previously implemented for the restricted case where the training data can be separated without errors.  We here extend this result to non-separable training data.  High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated.  We also compare the performance of the supportvector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition. 
LEARNING ALGORITHMS FOR CLASSIFICATION: A COMPARISON ON HANDWRITTEN DIGIT RECOGNITION| ABSTRACT This paper compares the performance of several classifier algorithms on a standard database of handwritten digits.  We consider not only raw accuracy, but also training time, recognition time, and memory requirements.  When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold. 
AUC Optimization vs| Error Rate Minimization.  Abstract The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classification algorithm.  However, the objective function optimized in most of these algorithms is the error rate and not the AUC value.  We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate.  Our results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable.  Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values.  We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC.  We report the results of our experiments with RankBoost in several datasets demonstrating the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.  1 Motivation In many applications, the overall classification error rate is not the most pertinent performance measure, criteria such as ordering or ranking seem more appropriate.  Consider for example the list of relevant documents returned by a search engine for a specific query.  That list may contain several thousand documents, but, in practice, only the top fifty or so are examined by the user.  Thus, a search engine's ranking of the documents is more critical than the accuracy of its classification of all documents as relevant or not.  More generally, for a binary classifier assigning a real-valued score to each object, a better correlation between output scores and the probability of correct classification is highly desirable.  A natural criterion or summary statistic often used to measure the ranking quality of a classifier is the area under an ROC curve (AUC) [8].  1 However, the objective function optimized by most classification algorithms is the error rate and not the AUC.  Recently, several algorithms have been proposed for maximizing the AUC value locally [4] or maximizing some approximations of the global AUC value [9, 15], but, in general, these algorithms do not obtain AUC values significantly better than those obtained by an algorithm designed to minimize the error rates.  Thus, it is important to determine the relationship between the AUC values and the error rate.  # This author's new address is: Google Labs, 1440 Broadway, New York, NY 10018, corinna@google. com.  1 The AUC value is equivalent to the Wilcoxon-Mann-Whitney statistic [8] and closely related to the Gini index [1].  It has been re-invented under the name of L-measure by [11], as already pointed out by [2], and slightly modified under the name of Linear Ranking by [13, 14].  (1,1) (0,0) False positive rate True positive rate ROC Curve.  AUC=0. 718 True positive rate = correctly classified positive total positive False positive rate = incorrectly classified negative total negative Figure 1: An example of ROC curve.  The line connecting (0, 0) and (1, 1), corresponding to random classification, is drawn for reference.  The true positive (negative) rate is sometimes referred to as the sensitivity (resp.  specificity) in this context.  In the following sections, we give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate.  2 We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC.  We report the results of our experiments with RankBoost in several datasets and demonstrate the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.  2 Definition and properties of the AUC The Receiver Operating Characteristics (ROC) curves were originally developed in signal detection theory [3] in connection with radio signals, and have been used since then in many other applications, in particular for medical decision-making.  Over the last few years, they have found increased interest in the machine learning and data mining communities for model evaluation and selection [12, 10, 4, 9, 15, 2].  The ROC curve for a binary classification problem plots the true positive rate as a function of the false positive rate.  The points of the curve are obtained by sweeping the classification threshold from the most positive classification value to the most negative.  For a fully random classification, the ROC curve is a straight line connecting the origin to (1, 1).  Any improvement over random classification results in an ROC curve at least partially above this straight line.  Fig.  (1) shows an example of ROC curve.  The AUC is defined as the area under the ROC curve and is closely related to the ranking quality of the classification as shown more formally by Lemma 1 below.  Consider a binary classification task with m positive examples and n negative examples.  We will assume that a classifier outputs a strictly ordered list for these examples and will denote by 1X the indicator function of a set X .  Lemma 1 ([8]) Let c be a fixed classifier.  Let x 1 , .  .  .  , xm be the output of c on the positive examples and y 1 , .  .  .  , yn its output on the negative examples.  Then, the AUC, A, associated to c is given by: A = P m i=1 P n j=1 1 x i }y j mn (1) that is the value of the Wilcoxon-Mann-Whitney statistic [8].  Proof.  The proof is based on the observation that the AUC value is exactly the probability P (X } Y ) where X is the random variable corresponding to the distribution of the outputs for the positive examples and Y the one corresponding to the negative examples [7].  The Wilcoxon-Mann-Whitney statistic is clearly the expression of that probability in the discrete case, which proves the lemma [8].  Thus, the AUC can be viewed as a measure based on pairwise comparisons between classifications of the two classes.  With a perfect ranking, all positive examples are ranked higher than the negative ones and A = 1.  Any deviation from this ranking decreases the AUC. 
Boosting Decision Trees|
Information Mining Platforms: An Infrastructure for KDD Rapid Deployment|
Signature-Based Methods for Data Streams|
Learning Curves: Asymptotic Values and Rate of Convergence|

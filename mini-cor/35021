Learning Hierarchical Structures with Linear Relational Embedding
 Abstract We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts.  Its final goal is to be able to generalize, i. e.  infer new instances of these relations among the concepts.  On a task involving family relationships we show that LRE can generalize better than any previously published method.  We then show how LRE can be used effectively to find compact distributed representations for variable-sized recursive data structures, such as trees and lists.  1 Linear Relational Embedding Our aim is to take a large set of facts about a domain expressed as tuples of arbitrary symbols in a simple and rigid syntactic format and to be able to infer other "common-sense" facts without having any prior knowledge about the domain.  Let us imagine a situation in which we have a set of concepts and a set of relations among these concepts, and that our data consists of few instances of these relations that hold among the concepts.  We want to be able to infer other instances of these relations.  For example, if the concepts are the people in a certain family, the relations are kinship relations, and we are given the facts "Alberto has-father Pietro" and "Pietro has-brother Giovanni", we would like to be able to infer "Alberto has-uncle Giovanni".  Our approach is to learn appropriate distributed representations of the entities in the data, and then exploit the generalization properties of the distributed representations [2] to make the inferences.  In this paper we present a method, which we have called Linear Relational Embedding (LRE), which learns a distributed representation for the concepts by embedding them in a space where the relations between concepts are linear transformations of their distributed representations.  Let us consider the case in which all the relations are binary, i. e.  involve two concepts.  In this case our data consists of triplets (concept 1 ; relation; concept 2 ), and the problem we are trying to solve is to infer missing triplets when we are given only few of them.  Inferring a triplet is equivalent to being able to complete it, that is to come up with one of its elements, given the other two.  Here we shall always try to complete the third element of the triplets 1 .  LRE will then represent each concept in the data as a learned vector in a 1 Methods analogous to the ones presented here that can be used to complete any element of a triplet can be found in [4].  Euclidean space and each relationship between the two concepts as a learned matrix that maps the first concept into an approximation to the second concept.  Let us assume that our data consists of C such triplets containing N distinct concepts and M binary relations.  We shall call this set of triplets C; V = fv 1 ;:::;v N g will denote the set of n-dimensional vectors corresponding to the N concepts, and R = fR 1 ;:::;R M g the set of (n # n) matrices corresponding to the M relations.  Often we shall need to indicate the vectors and the matrix which correspond to the concepts and the relation in a certain triplet c. In this case we shall denote the vector corresponding to the first concept with a, the vector corresponding to the second concept with b and the matrix corresponding to the relation with R.  We shall therefore write the triplet c as (a c ;R c ; b c ) where a c ; b c 2VandR c 2R.  The operation that relates a pair (a c ;R c ) to a vector b c is the matrix-vector multiplication, R c # a c , which produces an approximation to b c .  If for every triplet (a c ;R c ; b c ) we think of R c # a c as a noisy version of one of the concept vectors, then one way to learn an embedding is to maximize the probability that it is a noisy version of the correct completion, b c . We imagine that a concept has an average location in the space, but that each "observation" of the concept is a noisy realization of this average location.  Assuming spherical Gaussian noise with a variance of 1=2 on each dimension, the discriminative goodness function that corresponds to the log probability of getting the right completion, summed over all training triplets is: D = C X c=1 1 k c log e 1 R c #a c b c jj 2 X v i 2V e32 R c #a c v i jj 2 (1) where k c is the number of triplets in C having the first two terms equal to the ones of c,but differing in the third term 2 .  Learning based on maximizing D with respect to all the vector and matrix components has given good results, and has proved successful in generalization as well [5].  However, when we learn an embedding by maximizing D, we are not making use of exactly the information that we have in the triplets.  For each triplet c, we are making the vector representing the correct completion b c more probable than any other concept vector given R c # a c , while the triplet states that R c # a c must be equal to b c .  The numerator of D does exactly this, but we also have the denominator, which is necessary in order to stay away from the trivial 0 solution 3 .  We noticed however that the denominator is critical at the beginning of the learning, but as the vectors and matrices differentiate we could gradually lift this burden, allowing P C c=1 kR c # a c b c k 2 to become the real goal of the learning.  To do this we modify the discriminative function to include a parameter #, which is annealed from 1 to 0 during learning 4 : G = C X c=1 1 k c log e3 R c #a c b c k 2 [ X v i 2V e3 R c #a c v i k 2 ] # (2) 2 We would like our system to assign equal probability to each of the correct completions.  The discrete probability distribution that we want to approximate is therefore: Px = 1 d P d i=1 (b i x) where is the discrete delta function and x ranges over the vectors in V .  Our system implements the discrete probability distribution: Qx = 1 Z exp( R # a xk 2 ) where Z is the normalization factor.  The 1=kc factor in eq. 1 ensures that we are minimizing the Kullback-Leibler divergence between P and Q.  3 The obvious approach to find an embedding would be to minimize the sum of squared distances between R c # a c and b c over all the triplets, with respect to all the vector and matrix components.  Unfortunately this minimization (almost) always causes all of the vectors and matrices to collapse to the trivial 0 solution.  4 For one-to-many relations we must not decrease the value of # all the way to 0, because this would cause some concept vectors to become coincident.  This is because the only way to make R c # a c equal to kc different vectors, is by collapsing them onto a unique vector.

Shortest Paths in a Dynamic Uncertain Domain
 Abstract This paper describes solutions to finding shortest paths in stochastic graphs with partially unknown topologies.  We consider graphs which are both static and dynamic.  We solve the static problem by reduction to a Markov decision process and solve the dynamic problem by reduction to a partially observable Markov decision process.  We show these solutions to be intractable and explore reinforcement learning as a method of approximation.  Finally, we present empirical results of a reinforcement learning approach in this framework.  Suppose we are trying to deliver an important package to a town in a cluster of small islands.  These islands have recently been struck by a terrible storm and we can't be sure of the status of each bridge.  Some of them are intact but many of them washed away or are otherwise unusable.  What can we do in such a situation? Suppose now that there are efforts to fix some of the bridges while other bridges continue to fall apart due to additional rains.  Now how can we plan to deliver the package? These planning problems are interesting and difficult.  We know something about the state of the world but need to observe it to be sure of that state.  Furthermore, we want to deliver the package to the town as soon as possible and avoid wasting valuable time and resources on improbable and long paths through the islands.

BAYESIAN LEARNING FOR MODELS OF HUMAN SPEECH PERCEPTION
 ABSTRACT Humans speech recognition error rates are 30 times lower than machine error rates.  Psychophysical experiments have pinpointed a number of specific human behaviors that may contribute to accurate speech recognition, but previous attempts to incorporate such behaviors into automatic speech recognition have often failed because the resulting models could not be easily trained from data.  This paper describes Bayesian learning methods for computational models of human speech perception.  Specifically, the linked computational models proposed in this paper seek to imitate the following human behaviors: independence of distinctive feature errors, perceptual magnet effect, the vowel sequence illusion, sensitivity to energy onsets and offsets, and redundant use of asynchronous acoustic correlates.  The proposed models differ from many previous computational psychological models in that the desired behavior is learned from data, using a constrained optimization algorithm (the EM algorithm), rather than being coded into the model as a series of fixed rules.  1.  INDEPENDENT FEATURE ERRORS This paper has two goals.  First, this paper seeks to introduce a large variety of recent speech psychology results to the statistical signal processing community.  Second, this paper proposes computational models of the human behaviors evidenced in these recent results.  It is, perhaps, necessary to discuss the reasons why an engineer might be interested in psychology.  There is no a priori need for automatic speech recognizers to imitate the processes of human speech perception: the experience of the past 30 years indicates that simple but trainable mathematical models consistently achieve lower error rates than psychologically motivated but untrainable models.  Despite this progress, however, automatic speech recognition error rates are typically 30 to 300 times worse than human speech recognition error rates [1].  In order to close the gap between human and machine performance, it may be useful to evaluate the performance advantages conferred by specific human speech perceptual behaviors, and to try to imitate the most apparently useful behaviors using trainable machine learning models.  In 1952, Jakobson, Fant and Halle suggested encoding each phoneme as a vector of binary "distinctive features:" voiced vs.  unvoiced, lowpass vs.  highpass, spectrally compact vs.  spectrally diffuse [2].  The idea that a phoneme This work was supported by NSF award number 0132900.  Statements in this paper reflect the opinions and conclusions of the authors, and are not endorsed by the NSF.  can be decomposed into independently manipulable dimensions is quite old: classical Greek, Hebrew, Arabic, and Japanese, for example, mark secondary distinctions such as voicing and aspiration by means of diacritics.  Jakobson's binary notation was important in part because, within three years after Jakobson's paper, Miller and Nicely were able to prove the psychological reality of a nearly binary distinctive feature notation similar to Jakobson's [3].  Miller and Nicely [3] asked listeners to transcribe noisy recordings of consonant-vowel syllables.  Human listeners rarely misunderstand nonsense syllables under quiet listening conditions, but with enough noise, it is possible to get listeners to make mistakes, and the mistakes they make are revealing.  First, some distinctive features are more susceptible to noise than others: place of articulation is reliably communicated only at SNR above -6dB, while sonorancy is reliably communicated even at -12dB SNR.  Second, errors in the perception of distinctive features are approximately independent, in the following sense: given that the true values of the N distinctive features are F = [f1 , .  .  .  , fN ] T , the SNR-dependent probability that a listener will perceive the vector ^ F = [ ^ f1 , .  .  .  , ^ fN ] T is given by p( ^ F 

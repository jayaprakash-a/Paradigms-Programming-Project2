Learning Theory and Language Modeling
 Abstract We consider some of our recent work on Good-Turing estimators in the larger context of learning theory and language modeling.  The Good-Turing estimators have played a significant role in natural language modeling for the past twenty years.  We have recently shown that these particular leave-one-out estimators converge rapidly.  We present these results and consider possible consequences for language modeling in general.  In particular, other leave-one-out estimators, such as for the cross entropy of various forms of language models, might also be shown to be rapidly converging using proof methods similar to those used for the Good-Turing estimators.  This could have broad ramification in the analysis and development of language modeling methods.  We suggest that, in language modeling at least, leave-one-out estimation may be more significant than Occam's razor.

Distribution-Dependent Vapnik-Chervonenkis Bounds
 Abstract.  Vapnik-Chervonenkis (VC) bounds play an important role in statistical learning theory as they are the fundamental result which explains the generalization ability of learning machines.  There have been consequent mathematical works on the improvement of VC rates of convergence of empirical means to their expectations over the years.  The result obtained by Talagrand in 1994 seems to provide more or less the final word to this issue as far as universal bounds are concerned.  Though for fixed distributions, this bound can be practically outperformed.  We show indeed that it is possible to replace the 2ffl 2 under the exponential of the deviation term by the corresponding Cram'er transform as shown by large deviations theorems.  Then, we formulate rigorous distributionsensitive VC bounds and we also explain why these theoretical results on such bounds can lead to practical estimates of the effective VC dimension of learning structures.

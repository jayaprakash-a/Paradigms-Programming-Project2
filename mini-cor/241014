Wishart Processes: A Statistical View of Reproducing Kernels and Its Applications to Kernel Learning
 Abstract Kernels are playing an increasingly important role in machine learning.  In this paper, we propose a statistical view of kernels and establish a new connection between reproducing kernels and Gaussian processes.  Specifically, we draw equivalence between two notions, that the reproducing kernel is a Wishart process and that the dimensions of the feature vectors in the kernel-induced feature space are mutually independent Gaussian processes.  This leads to a probabilistic generative model of the kernel matrix following the Wishart distribution.  We find that the degree of freedom parameter of the Wishart distribution has a clear physical meaning, which is equal to the dimensionality of the feature space.  Moreover, although the feature vectors follow the Gaussian distributions, they are no longer required to be mutually independent.  Based on the statistical view proposed, we address the kernel matrix learning problem as a specific application of this view.  In particular, we propose two possible methods for kernel matrix learning, both of which are based on maximizing some likelihood function.  The first method learns the hyperparameters of a kernel matrix in an inductive learning setting, using an optimization procedure such as a gradient method.  The second method uses an expectation-maximization (EM) iterative procedure to estimate the eigenvalues and the dimensionality of the feature space in a transductive learning setting.

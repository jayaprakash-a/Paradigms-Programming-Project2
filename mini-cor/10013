Video Input Driven Animation (VIDA)
 Abstract There are many challenges associated with the integration of synthetic and real imagery.  One particularly difficult problem is the automatic extraction of salient parameters of natural phenomena in real video footage for subsequent application to synthetic objects.  Can we ensure that the hair and clothing of a synthetic actor placed in a meadow of swaying grass will move consistently with the wind that moved that grass? The video footage can be seen as a controller for the motion of synthetic features, a concept we call video input driven animation (VIDA).  We propose a schema that analyzes an input video sequence, extracts parameters from the motion of objects in the video, and uses this information to drive the motion of synthetic objects.  To validate the principles of VIDA, we approximate the inverse problem to harmonic oscillation, which we use to extract parameters of wind and of regular water waves.  We observe the effect of wind on a tree in a video, estimate wind speed parameters from its motion, and then use this to make synthetic objects move.  We also extract water elevation parameters from the observed motion of boats and apply the resulting water waves to synthetic boats.

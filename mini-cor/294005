Probabilistic interpretations and Bayesian methods for Support Vector Machines
 Abstract Support Vector Machines (SVMs) can be interpreted as maximum a posteriori solutions to inference problems with Gaussian Process (GP) priors and appropriate likelihood functions.  Focussing on the case of classification, I show first that such an interpretation gives a clear intuitive meaning to SVM kernels, as covariance functions of GP priors; this can be used to guide the choice of kernel.  Second, a probabilitistic interpretation allows Bayesian methods to be used for SVMs: Using a local approximation of the posterior around its maximum (the standard SVM solution), I discuss how the evidence for a given kernel and noise parameter can be estimated, and how approximate error bars for the classification of test points can be calculated.  1 SVMs Support Vector Machines (SVMs) have been the subject of intense research activity from within the neural networks community over the last few years; for tutorial introductions and overviews of recent developments see [1, 2, 3].  One of the open questions that remains is how to set the `tunable' parameters of an SVM algorithm: While methods for choosing the width of the kernel function and the noise parameter (which controls how closely the training data are fitted) have recently been proposed [4, 5], the effect of the overall shape of the kernel function remains imperfectly understood [1].  In this paper I suggest that a probabilistic interpretation of SVMs might be useful in tackling this problem.  It has two main benefits: First, it clarifies the role of the kernel as specifying an SVM `prior' over functions on the input space, avoiding the need to think in terms of an abstract feature space.  I illustrate this with samples from some typical SVM priors and discuss general guidelines for the choice of kernels that emerge.  Second, a probabilistic interpretation also allows Bayesian methods to be applied to SVMs.  As an example, I sketch how the loglikelihood (or `evidence') for an SVM model -- specified by a kernel and noise parameter -can be evaluated approximately; this could be a useful alternative to generalization error bounds in guiding the search for optimal kernels.  I also outline how approximate error bars for the predictions of a trained SVM can be obtained.  These could be of use in safety-critical applications, for example, where the conventional purely deterministic SVM predictions are undesirable.  I will focus mainly on (two-class) classification problems.  Suppose we are given a set D of n training examples, each of the form (x; y x ) with a binary output y x = \Sigma1 classifying to which of the two possible classes the input x belongs.  The basic SVM idea is then to map the inputs x onto vectors OE(x) in some high-dimensional feature space; ideally, in this feature space, the problem should be linearly separable.  Suppose first that this is true.  Among all decision hyperplanes w \Delta OE(x) + b = 0 which separate the training examples (i. e. , which obey y x (w \Delta OE(x) + b) ? 0 for all x 2 X , X being the set of training inputs), the SVM solution is then chosen as the one with the largest margin, i. e. , the largest minimal distance from any of the training examples.  In practice, it is easier to specify the margin instead (the conventional value is unity) and minimize the squared length of the weight vector jjwjj 2 [1].  This leads to the following optimization problem: Find a weight vector w and an offset b such that 1 2 jjwjj 2 is minimized, subject to the constraint that y x (w\DeltaOE(x) +b) 1 for all training examples.  What happens if the problem is not linearly separable, even in the highdimensional feature space? Then `slack variables' x 0 are introduced which measure how much the margin constraint is violated for training input x; one writes y x (w \Delta OE(x) + b) 1 \Gamma x .  To control the amount of slack allowed (which determines how closely the training data are fitted), a penalty term must then be added to the objective function 1 2 jjwjj 2 .  The most widely used choice is C P x x , with a penalty coefficient C.  Training examples for which y x (w\DeltaOE(x) + b) 1 (and hence x = 0) then incur no penalty, while for the other examples there is a penalty of C[1\Gamma y x (w\DeltaOE(x)+b)] each.  Altogether, the SVM optimization problem can therefore be expressed as follows: Find w and b to minimize 1 2 jjwjj 2 + C X x2X l(y x [w \Delta OE(x) + b]) (1) where l(z) is the (shifted) `hinge loss', l(z) = (1 \Gamma z)\Theta(1\Gamma z).  The probabilistic interpretation of SVMs that I discuss below hinges on the fact that (1) can be regarded as defining a (negative) log-posterior probability for the parameters w and b of the SVM, given a training set D.  The first term gives us the prior P (w) exp(\Gamma 1 2 jjwjj 2 ).  This is a Gaussian prior on w; the components of w are uncorrelated with each other and have unit variance.  The prior on b is flat (uninformative, improper) and I will not write it explicitly.  The weights w occur in the second term of (1) -- to be identified as the log-likelihood shortly -- only through a(x) = w \Delta OE(x).  It is therefore useful to express the prior directly as a distribution over the functions a.  The function values a(x) have a joint Gaussian distribution because the components of w do, and their covariances are given by ha(x)a(x 0 )i = h(OE(x) \Delta w)(w \Delta OE(x 0 ))i = OE(x)\DeltaOE(x 0 ).  In other words, the SVM prior is simply a Gaussian process (GP) prior over the functions a, with covariance function K(x; x 0 ) = OE(x) \Delta OE(x 0 ) (and zero mean).  To interpret the second term in (1) as a (negative) log-likelihood, one only has to define the probability of obtaining output y for a given x (and a, b) as P (y = \Sigma1jx; a; b) = (C)e \Gamma Cl(y[a(x)+b]) (2) The proportionality constant (C) here needs to be chosen such that the probabilities for y = \Sigma1 never add up to a value larger than one; it is sensible to choose the largest value that achieves this, which is (C) = 1=[1 + exp(\Gamma 2C)]: (3) For a generic value of a(x) + b, however, the probabilities P (y = \Sigma1jx; a; b) then still add to a value ! 1.  It is therefore necessary to introduce a `don't-know' class (labelled by y = 0, say), with probability P (y = 0jx; a; b) = 1 \Gamma X y=\Sigma1 P (y = \Sigma1jx; a; b): (4) While this may seem surprising at first sight, it has the appealing feature that the probability for the `don't-know' class is largest in the `gap',\Gamma 1 ! a(x) + b ! 1, where one would intuitively expect the output of the SVM to be least certain 1 .  Up to the trivial additive constant n ln(1 + e \Gamma 2C ), the second term in (1) can now be identified as the negative log-likelihood of the observed data D.  To summarize, the task of training an SVM for classification can be reformulated as follows: Given a zero-mean Gaussian process prior over functions a with covariance function K(x; x 0 ), a flat prior over offsets b, and the likelihood function defined by (2--4), find the maximum a posteriori (MAP) values of a and b.  (These are the values that maximize P (a; bjD) P (a)P (Dja; b). ) The feature space has disappeared entirely from this formulation 2 ; all its relevant properties are encoded in the GP prior, specified by the kernel K(x; x 0 ).  There is nothing profound about this correspondence between SVMs and GPs.  It is related to the common link to reproducing kernel Hilbert spaces [7], and can be extended from SVMs to more general 1 This is based on the reasonable assumption that C is not too small (C 1, roughly).  Very small values of C would give a large range of values of a(x) + b where the probabilities for y \Sigma 1 are comparable, producing essentially random outputs; this is an unrealistic situation.  2 The case of SVM regression is completely analogous.  The only modification is in the likelihood function, which then determines the probability of a real-valued output, P (yjx; a; b) expf\Gamma Cl ffl (y \Gamma [a(x)+b])g, in terms of the ffl-insensitive loss function l ffl (z) = (jzj \Gamma ffl)\Theta(jzj \Gamma ffl).

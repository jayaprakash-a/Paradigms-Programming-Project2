ON PLANNING AND EXPLORATION IN NON-DISCRETE ENVIRONMENTS
 Abstract The application of reinforcement learning to control problems has received considerable attention in the last few years [And86, Bar89, Sut84].  In general there are two principles to solve reinforcement learning problems: direct and indirect techniques, both having their advantages and disadvantages.  We present a system that combines both methods [TML91, TML90].  By interaction with an unknown environment a world model is progressively constructed using the backpropagation algorithm.  For optimizing actions with respect to future reinforcement planning is applied in two steps: An experience network proposes a plan which is subsequently optimized by gradient descent with a chain of model networks.  While operating in a goal-oriented manner due to the planning process the experience network is trained.  Its accumulating experience is fed back into the planning process in form of initial plans, such that planning can be gradually reduced.  In order to ensure complete system identification, a competence network is trained to predict the accuracy of the model.  This network enables purposeful exploration of the world.  The appropriateness of this approach to reinforcement learning is demonstrated by three different control experiments, namely a target tracking, a robotics and a pole balancing task.

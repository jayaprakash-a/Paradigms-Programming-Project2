Algorithms for Inverse Reinforcement Learning
 Abstract This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behavior.  IRL may be useful for apprenticeship learning to acquire skilled behavior, and for ascertaining the reward function being optimized by a natural system.  We first characterize the set of all reward functions for which a given policy is optimal.  We then derive three algorithms for IRL.  The first two deal with the case where the entire policy is known; we handle tabulated reward functions on a finite state space and linear functional approximation of the reward function over a potentially infinite state space.  The third algorithm deals with the more realistic case in which the policy is known only through a finite set of observed trajectories.  In all cases, a key issue is degeneracy

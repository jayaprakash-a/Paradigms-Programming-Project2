Developing Population Codes by Minimizing Description Length
 Abstract The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately.  We show how MDL can be used to develop highly redundant population codes.  Each hidden unit has a location in a low-dimensional implicit space.  If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center of this bump.  So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump.  The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes.  Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs.  Most existing unsupervised learning algorithms can be understood using the Minimum Description Length (MDL) principle (Rissanen, 1989).  Given an ensemble of input vectors, the aim of the learning algorithm is to find a method of coding each input vector that minimizes the total cost, in bits, of communicating the input vectors to a receiver.  There are three terms in the total description length: ffl The code-cost is the number of bits required to communicate the code that the algorithm assigns to each input vector.  ffl The model-cost is the number of bits required to specify how to reconstruct input vectors from codes (e. g. , the hidden-to-output weights).  ffl The reconstruction-error is the number of bits required to fix up any errors that occur when the input vector is reconstructed from its code.  Formulating the problem in terms of a communication model allows us to derive an objective function for a network (note that we are not actually sending the bits).  For example, in competitive learning (vector quantization), the code is the identity of the winning hidden unit, so by limiting the system to H units we limit the average code-cost to at most log 2 H bits.  The reconstruction-error is proportional to the squared difference between the input vector and the weight-vector of the winner, and this is what competitive learning algorithms minimize.  The model-cost is usually ignored.  The representations produced by vector quantization contain very little information about the input (at most log 2 H bits).  To get richer representations we must allow many hidden units to be active at once and to have varying activity levels.  Principal components analysis (PCA) achieves this for linear mappings from inputs to codes.  It can be viewed as a version of MDL in which we limit the code-cost by only having a few hidden units, and ignoring the model-cost and the accuracy with which the hidden activities must be coded.  An autoencoder (see Figure 2) that tries to reconstruct the input vector on its output units will perform a version of PCA if the output units are linear.  We can obtain novel and interesting unsupervised learning algorithms using this MDL approach by considering various alternative methods of communicating the hidden activities.  The algorithms can all be implemented by backpropagating the derivative of the code-cost for the hidden units in addition to the derivative of the reconstruction-error backpropagated from the output units.  Any method that communicates each hidden activity separately and independently will tend to lead to factorial codes because any mutual information between hidden units will cause redundancy in the communicated message, so the pressure to keep the message short will squeeze out the redundancy.  In (Zemel, 1993) and (Hinton and Zemel, 1994), we present algorithms derived from this MDL approach aimed at developing factorial codes.  Although factorial codes are interesting, they are not robust against hardware failure nor do they resemble the population codes found in some parts of the brain.  Our aim in this paper is to show how the MDL approach can be used to develop population codes in which the activities of hidden units are highly correlated.  For a more complete discussion of the details of this algorithm, see (Zemel, 1993).  Unsupervised algorithms contain an implicit assumption about the nature of the structure or constraints underlying the input set.  For example, competitive learning algorithms are suited to datasets in which each input can be attributed to one of a set of possible causes.  In the algorithm we present here, we assume that each input can be described as a point in a low-dimensional continuous constraint space.  For instance, a complex shape may require a detailed representation, but a set of images of that shape from multiple viewpoints can be concisely represented by first describing the shape, and then encoding each instance as a point in the constraint space spanned by the viewing parameters.  Our goal is to find and represent the constraint space underlying high-dimensional data samples.

GENERALIZED OPTIMIZATION ALGORITHM FOR SPEECH RECOGNITION TRANSDUCERS
 ABSTRACT Weighted transducers provide a common representation for the components of a speech recognition system.  In previous work, we showed that these components can be combined off-line into a single compact recognition transducer that maps directly HMM state sequences to word sequences [11].  The construction of that recognition transducer and its efficiency of use critically depend on the use of a general optimization algorithm, determinization.  However, not all weighted automata and transducers used in largevocabulary speech recognition are determinizable.  We present a general algorithm that can make an arbitrary weighted transducer determinizable and generalize our previous optimization technique for building an integrated recognition transducer to deal with arbitrary weighted transducers used in speech recognition.  We report experimental results in a large-vocabulary speech recognition task, How May I Help You (HMIHY), showing that our generalized technique leads to a recognition transducer that performs as well as our original solution in the case of classical n-gram models while inserting less special symbols, and that it leads to a substantial improvement of the recognition speed, factor of 2:6, in the same task when using a class-based language model.  1.  MOTIVATION Weighted transducers are finite-state transducers in which each transition carries a weight in addition to the usual input and output symbols [14, 7].  They provide a common representation for several components of a speech recognition system: language models, pronunciation dictionaries, context-dependency and HMM models [11].  General weighted transducer algorithms can be used to combine and optimize these representations and to build off-line a single efficient recognition transducer that integrates all of these components, directly mapping from HMM state sequences to word sequences [11].  The size of that integrated recognition transducer is practical since it has been shown empirically to be close to that of the language model used.  The construction of that recognition transducer and its efficiency of use critically depend on the use of a general optimization algorithm, determinization [9, 12].  Determinization outputs a transducer equivalent to the input that is deterministic, i. e. , one that has a unique initial state and that has no two transitions leaving the same state with the same input label.  This considerably reduces the number of paths needed to be explored by the decoder and thus substantially improves the efficiency of recognition.  However, not all weighted automata and transducers used in speech recognition are determinizable.  A solution to this problem was provided in the special case where n-gram statistical language models are used [11].  But that solution does not cover some important cases such as that of class-based language models which can lead to non-determinizable weighted transducers.  Other language models created either directly or by approximation of weighted context-free grammars can also create similar non-determinism that could make our previous technique inapplicable.  We present a general algorithm that makes an arbitrary weighted transducer determinizable by inserting in it transitions labeled with special symbols just when needed and at the optimal positions to fully benefit from the application of determinization.  Those special symbols can be removed, or mapped to the empty string after application of determinization.  The algorithm generalizes our previous optimization technique for building an integrated recognition transducer to deal with arbitrary weighted transducers used in speech recognition.  Our experiments in a large-vocabulary speech recognition task, How May I Help You (HMIHY), show that our new and generalized technique leads to a recognition transducer that performs as well as our original solution in the case where classical n-gram models are used, while inserting less special symbols.  We also report experiments with a class-based language model in the same task using our generalized optimization technique.  The experiments show an improvement of the recognition speed by a factor of 2:6 over the system used without application of determinization.  We first introduce some preliminary definitions and notation necessary for the presentation of our symbol insertion algorithm and experimental results.

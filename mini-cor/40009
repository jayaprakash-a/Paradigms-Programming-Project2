Wide Coverage Natural Language Processing using Kernel Methods and Neural Networks for Structured Data
 Convolution kernels and recursive neural networks are both suitable approaches for supervised learning when the input is a discrete structure like a labeled tree or graph.  We compare these techniques in two natural language problems.  In both problems, the learning task consists in choosing the best alternative tree in a set of candidates.  We report about an empirical evaluation between the two methods on a large corpus of parsed sentences and speculate on the role played by the representation and the loss function.

Structured Language Modeling for Speech Recognition
 Abstract A new language model for speech recognition is presented.  The model develops hidden hierarchical syntactic-like structure incrementally and uses it to extract meaningful information from the word history, thus complementing the locality of currently used trigram models.  The structured language model (SLM) and its performance in a two-pass speech recognizer --- lattice decoding --- are presented.  Experiments on the WSJ corpus show an improvement in both perplexity (PPL) and word error rate (WER) over conventional trigram models.  1 Structured Language Model An extensive presentation of the SLM can be found in [1].  The model assigns a probability P (W; T ) to every sentence W and its every possible binary parse T .  The terminals of T are the words of W with POStags, and the nodes of T are annotated with phrase headwords and non-terminal labels.  Let W be a sentence of length n words to which we have prepended !s? and appended !/s? so that w 0 =!s? and w n+1 =!/s?.  Let W k be the word k-prefix w 0 : : : w k of the sentence and W k T k the word-parse k-prefix.  Figure 1 shows a word-parse k-prefix; h0 . .  h---m are the exposed heads, each head being a pair(headword, non-terminal label), or (word, POStag) in the case of a root-only tree.  h_0 = (h_0. word, h_0. tag) h_{-1} h_{-m} = (<s}, SB)

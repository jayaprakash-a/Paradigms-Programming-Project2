Confidence Intervals for the Area under the ROC Curve
 Abstract In many applications, good ranking is a highly desirable performance for a classifier.  The criterion commonly used to measure the ranking quality of a classification algorithm is the area under the ROC curve (AUC).  To report it properly, it is crucial to determine an interval of confidence for its value.  This paper provides confidence intervals for the AUC based on a statistical and combinatorial analysis using only simple parameters such as the error rate and the number of positive and negative examples.  The analysis is distribution-independent, it makes no assumption about the distribution of the scores of negative or positive examples.  The results are of practical use and can be viewed as the equivalent for AUC of the standard confidence intervals given in the case of the error rate.  They are compared with previous approaches in several standard classification tasks demonstrating the benefits of our analysis.  1 Motivation In many machine learning applications, the ranking quality of a classifier is critical.  For example, the ordering of the list of relevant documents returned by a search engine or a document classification system is essential.  The criterion widely used to measure the ranking quality of a classification algorithm is the area under an ROC curve (AUC).  But, to measure and report the AUC properly, it is crucial to determine an interval of confidence for its value as it is customary for the error rate and other measures.  It is also important to make the computation of the confidence interval practical by relying only on a small and simple number of parameters.  In the case of the error rate, such intervals are often derived from just the sample size N .  We present an extensive theoretical analysis of the AUC and show that a similar confidence interval can be derived for its value using only simple parameters such as the error rate k/N , the number of positive examples m, and the number of negative examples n = N- m.  Thus, our results extend to AUC the computation of confidence intervals from a small number of readily available parameters.  Our analysis is distribution-independent in the sense that it makes no assumption about the distribution of the scores of negative or positive examples.  The use of the error rate helps determine tight confidence intervals.  This contrasts with existing approaches presented in the statistical literature [11, 5, 2] which are based either on weak distribution-independent assumptions resulting in too loose confidence intervals, or strong distribution-dependent assumptions leading to tight but unsafe confidence intervals.  We show that our results are of practical use.  We also compare them with previous approaches in several standard classification tasks demonstrating the benefits of our analysis.  Our results are also useful for testing the statistical significance of the difference of the AUC values of two classifiers.  The paper is organized as follows.  We first introduce the definition of the AUC, its connection with the Wilcoxon-Mann-Whitney statistic (Section 2), and briefly review some essential aspects of the existing literature related to the computation of confidence intervals for the AUC.  Our computation of the expected value and variance of the AUC for a fixed error rate requires establishing several combinatorial identities.  Section 4 presents some existing identities and gives the proof of novel ones useful for the computation of the variance.  Section 5 gives the reduced expressions for the expected value and variance of the AUC for a fixed error rate.  These can be efficiently computed and used to determine our confidence intervals for the AUC (Section 6).  Section 7 reports the result of the comparison of our method with previous approaches, including empirical results for several standard tasks.

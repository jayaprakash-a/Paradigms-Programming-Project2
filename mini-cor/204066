Eligibility Traces for Off-Policy Policy Evaluation
 Abstract Eligibility traces have been shown to speed reinforcement learning, to make it more robust to hidden states, and to provide a link between Monte Carlo and temporal-difference methods.  Here we generalize eligibility traces to off-policy learning, in which one learns about a policy different from the policy that generates the data.  Off-policy methods can greatly multiply learning, as many policies can be learned about from the same data stream, and have been identified as particularly useful for learning about subgoals and temporally extended macro-actions.  In this paper we consider the off-policy version of the policy evaluation problem, for which only one eligibility trace algorithm is known, a Monte Carlo method.  We analyze and compare this and four new eligibility trace algorithms, emphasizing their relationships to the classical statistical technique known as importance sampling.  Our main results are 1) to establish the consistency and bias properties of the new methods and 2) to empirically rank the new methods, showing improvement over one-step and Monte Carlo methods.  Our results are restricted to model-free, table-lookup methods and to offline updating (at the end of each episode) although several of the algorithms could be applied more generally.  In reinforcement learning, we generally learn from experience, that is, from the sequence of states, actions, and rewards generated by the agent interacting with its environment.  This data is affected by the decision-making policy used by the agent to select its actions, and thus we often end up learning something that is a function of the agent's policy.  For example, the common subproblem of policy evaluation is to learn the value function for the agent's policy (the function giving the expected future reward available from each state--action pair).  In general, however, we might want to learn about policies other than that currently followed by the agent, a process known as off-policy learning.  For example, 1-step Q-learning is often used in an off-policy manner, learning about the greedy policy while the data is generated by a slightly randomized policy that ensures exploration.  Off-policy learning is especially important for research on the use of temporally extended actions in reinforcement learning (

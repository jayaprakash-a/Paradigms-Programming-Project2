Context-based policy search: transfer of experience across problems
 Abstract An important question in reinforcement learning is how generalization may be performed.  This problem is especially important if the learning agent receives only partial information about the state of the environment.  Typically, the bias required for generalization is chosen by the experimenter.  Here, we investigate a way for the learning method to extract bias from learning one problem and apply it in subsequent problems.  We use a gradient-based policy search method, and look for controllers that consist of a context component and an action component.  Empirical results on a two-agent coordination problem are reported.  It was found that learning a bias made it possible to address problems that were not solved otherwise.

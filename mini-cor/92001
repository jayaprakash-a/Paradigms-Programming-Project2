Tetris: Experiments with the LP Approach to Approximate DP
 Abstract We study the linear programming (LP) approach to approximate dynamic programming (DP) through experiments with the game of Tetris.  Our empirical results suggest that the performance of policies generated by the approach is highly sensitive to how the problem is formulated and the discount factor.  Furthermore, we find that, using a state-sampling scheme of the kind proposed in [7], the simulation time required to generate an adequate number of constraints far exceeds the time taken to solve the resulting LP.  As an extension to the standard approximate LP approach, we examine a bootstrapped version wherein a sequence of LPs is solved, with the policy generated by each solution being used to sample constraints for the next LP.  Our empirical results demonstrate that this bootstrapped approach can amplify performance.

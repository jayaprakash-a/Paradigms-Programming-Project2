Unsupervised efficient learning and representation of language structure
 Abstract We describe a linguistic pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of corpus data.  This is achieved by compactly coding recursively structured constituent patterns, and by placing strings that have an identical backbone and similar context structure into the same equivalence class.  The resulting representations constitute an efficient encoding of linguistic knowledge and support systematic generalization to unseen sentences.  Motivation Considerations of representational parsimony dictate that the explanation for the pattern of acceptable sentences in a language be as concise as possible.  A reduced representation of linguistic knowledge need not, however, take the form of a meta-language such as a prescriptive ruleset or grammar [1].  Instead, syntax may constitute an abstraction, emerging from a corpus of language [2], yet coexisting within the same representational mechanism that embodies the data.  The process of abstraction can be guided by principles such as complementarity of distributions: tokens that function similarly in some sense (phonological, morphological, syntactic or semantic) but represent systematic rather than free variation will form complementary distributions or classes (e. g. , [3, 4]).  In thinking about emergent regularities [2], or syntactic-semantic constructions [5], we adopt

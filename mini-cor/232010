Training conditional random fields via gradient tree boosting
 Abstract Conditional Random Fields (CRFs; Lafferty, McCallum, & Pereira, 2001) provide a flexible and powerful model for learning to assign labels to elements of sequences in such applications as part-of-speech tagging, textto-speech mapping, protein and DNA sequence analysis, and information extraction from web pages.  However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features.  This paper describes a new method for training CRFs by applying Friedman's (1999) gradient tree boosting method.  In tree boosting, the CRF potential functions are represented as weighted sums of regression trees.  Regression trees are learned by stage-wise optimizations similar to Adaboost, but with the objective of maximizing the conditional likelihood P (Y 

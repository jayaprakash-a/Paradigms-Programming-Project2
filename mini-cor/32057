Optimal Gradient-Based Learning Using Importance Weights
 Abstract--- We introduce a novel "importance weight" method (IW) to speed up gradient based learning.  The method is particularly useful for "difficult" data sets including features like unbalanced data, highly non-linear relationships between variables, or long-term dependencies in sequences.  An importance weight is assigned to every data point of the training set.  The weight controls the contribution of the data point to the total training error according to its informativeness for learning a good predictor.  It can also be interpreted as an individual learning step size for the local gradient at this particular data point.  The importance weights are obtained by solving a quadratic optimization problem which minimizes the absolute value of the change in the parameter vector during a learning step under the (soft) constraint, that the total error should be reduced by at least a given fixed value.  For linear classifiers we show, that the new method is equivalent to standard support vector learning.  We apply the IW method to feedforward multi-layer perceptrons and to recurrent neural networks (LSTM).  Benchmarks with QuickProp and standard gradient descent methods are provided for toy data as well as for "real world" protein datasets.  Results show that the new learning method is usually much faster in terms of epochs as well as in terms of absolute CPU time, and that it provides equal or better prediction results.  In the "latching benchmark" for sequence prediction, the new approach was able to extract and exploit dependencies between sites which are 1,000,000 sequence elements apart -- a new record.

Slow stochastic learning with global inhibition: a biological solution to the binary perceptron problem
 Abstract Networks of neurons connected by plastic all-or-none synapses tend to quickly forget previously acquired information when new patterns are learned.  This problem could be solved for random uncorrelated patterns by randomly selecting a small fraction of synapses to be modi-ed upon each stimulus presentation (slow stochastic learning).  Here we show that more complex, but still linearly separable patterns, can be learned by networks withbinary excitatory synapses in a -nite number of presentations provided that: (1) there is non-vanishing global inhibition, (2) the binary synapses are changed with small enough probability (slow learning) only when the output neuron does not give the desired response (as in the classical perceptron rule) and (3) the neuronal threshold separating the total synaptic inputs corresponding to di0erent classes is small enough.

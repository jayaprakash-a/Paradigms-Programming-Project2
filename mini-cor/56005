Distributional Phrase Structure Induction
 Abstract Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data.  Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts.  We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features.  The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.  1 Overview While early work showed that small, artificial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective.  For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality.  Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000).  It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text.  However, there are compelling motivations for unsupervised grammar induction.  Building supervised training data requires considerable resources, including time and linguistic expertise.  Furthermore, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within a supervised parser's supervisory information, and, therefore, often not explicitly modeled in such systems.  For example, our system and others have difficulty correctly attaching subjects to verbs above objects.  For a supervised CFG parser, this ordering is implicit in the given structure of VP and S constituents, however, it seems likely that to learn attachment order reliably, an unsupervised system will have to model it explicitly.  Our goal in this work is the induction of highquality, linguistically sensible grammars, not parsing accuracy.  We present two systems, one which does not do disambiguation well and one which does not do it at all.  Both take tagged but unparsed Penn treebank sentences as input.  1 To whatever degree our systems parse well, it can be taken as evidence that their grammars are sensible, but no effort was taken to improve parsing accuracy directly.  There is no claim that human language acquisition is in any way modeled by the systems described here.  However, any success of these methods is evidence of substantial cues present in the data, which could potentially be exploited by humans as well.  Furthermore, mistakes made by these systems could indicate points where human acquisition is likely not being driven by these kinds of statistics.  2 Approach At the heart of any iterative grammar induction system is a method, implicit or explicit, for deciding how to update the grammar.  Two linguistic criteria for constituency in natural language grammars form the basis of this work (Radford, 1988): 1.  External distribution: A constituent is a sequence of words which appears in various structural positions within larger constituents.  1 The Penn tag and category sets used in examples in this paper are documented in Manning and Schutze (1999, 413).  2.  Substitutability: A constituent is a sequence of words with (simple) variants which can be substituted for that sequence.  To make use of these intuitions, we use a distributional notion of context.  Let # be a part-of-speech tag sequence.  Every occurence of # will be in some context x # y, where x and y are the adjacent tags or sentence boundaries.  The distribution over contexts in which # occurs is called its signature, which we denote by #(#).  Criterion 1 regards constituency itself.  Consider the tag sequences IN DT NN and IN DT.  The former is a canonical example of a constituent (of category PP), while the later, though strictly more common, is, in general, not a constituent.  Frequency alone does not distinguish these two sequences, but Criterion 1 points to a distributional fact which does.  In particular, IN DT NN occurs in many environments.  It can follow a verb, begin a sentence, end a sentence, and so on.  On the other hand, IN DT is generally followed by some kind of a noun or adjective.  This example suggests that a sequence's constituency might be roughly indicated by the entropy of its signature, H(#(#)).  This turns out to be somewhat true, given a few qualifications.  Figure 1 shows the actual most frequent constituents along with their rankings by several other measures.  Tag entropy by itself gives a list that is not particularly impressive.  There are two primary causes for this.  One is that uncommon but possible contexts have little impact on the tag entropy value.  Given the skewed distribution of short sentences in the treebank, this is somewhat of a problem.  To correct for this, let # u (#) be the uniform distribution over the observed contexts for #.  Using H(# u (#)) would have the obvious effect of boosting rare contexts, and the more subtle effect of biasing the rankings slightly towards more common sequences.  However, while H(#(#)) presumably converges to some sensible limit given infinite data, H(# u (#)) will not, as noise eventually makes all or most counts non-zero.  Let u be the uniform distribution over all contexts.  The scaled entropy H s (#(#)) = H(#(#))[H(# u (#))=H(u)] turned out to be a useful quantity in practice.  Multiplying entropies is not theoretically meaningful, but this quantity does converge to H(#(#)) given infinite (noisy) data.  The list for scaled entropy still has notable flaws, mainly relatively low ranks for common NPs, which does not hurt system perforSequence Actual Freq Entropy Scaled Boundary GREEDY-RE DT NN

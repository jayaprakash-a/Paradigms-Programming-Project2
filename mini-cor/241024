Applying the Bayesian Evidence Framework to \nu -Support Vector Regression
 Abstract.  Following previous successes on applying the Bayesian evidence framework to support vector classifiers and the #-support vector regression algorithm, in this paper we extend the evidence framework also to the #-support vector regression (#-SVR) algorithm.  We show that #SVR training implies a prior on the size of the #-tube that is dependent on the number of training patterns.  Besides, this prior has properties that are in line with the error-regulating behavior of #.  Under the evidence framework, standard #-SVR training can then be regarded as performing level one inference, while levels two and three allow automatic adjustments of the regularization and kernel parameters respectively, without the need of a validation set.  Furthermore, this Bayesian extension allows computation of the prediction intervals, taking uncertainties of both the weight parameter and the #-tube width into account.  Performance of this method is illustrated on both synthetic and real-world data sets.

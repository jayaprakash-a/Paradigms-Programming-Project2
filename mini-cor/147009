One Microphone Source Separation
 Abstract Source separation, or computational auditory scene analysis, attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment.  Unmixing algorithms such as ICA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available.  I present a technique called refiltering which recovers sources by a nonstationary reweighting ("masking") of frequency sub-bands from a single recording, and argue for the application of statistical algorithms to learning this masking function.  I present results of a simple factorial HMM system which learns on recordings of single speakers and can then separate mixtures using only one observation signal by computing the masking function and then refiltering.  1 Learning from data in computational auditory scene analysis Imagine listening to many pianos being played simultaneously.  If each pianist were striking keys randomly it would be very difficult to tell which note came from which piano.  But if each were playing a coherent song, separation would be much easier because of the structure of music.  Now imagine teaching a computer to do the separation by showing it many musical scores as "training data".  Typical auditory perceptual input contains a mixture of sounds from different sources, altered by the acoustic environment.  Any biological or artificial hearing system must extract individual acoustic objects or streams in order to do successful localization, denoising and recognition.  Bregman [1] called this

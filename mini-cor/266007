Using Physical Theories to Infer Hidden Causal Structure
 Everyday reasoning draws on notions that go far beyond the observable world, just as modern science draws upon theoretical constructs beyond the limits of measurement.  The richness of our naive theories is a direct result of our ability to postulate hidden causal structure.  This capacity to reason about unobserved causes forms an essential part of cognition from early in life, whether we are reasoning about the forces involved in physical systems (e. g. , Shultz, 1982), the mental states of others (e. g. , Perner, 1991), or the essential properties of natural kinds (e. g. , Gelman & Wellman, 1991).  The central role of hidden causes in naive theories makes the question of how people infer hidden causal structure fundamental to understanding human reasoning.  Psychological research has shown that people can infer the existence of hidden causes from otherwise unexplained events (Ahn & Luhmann, 2003), and determine hidden causal structure from very little data (Kushnir, Gopnik, Schulz, & Danks, 2003).  This work has parallels in computer science, where the development of a formalism for reasoning about causality -- causal graphical models -- has led to algorithms that use patterns of dependency to identify causal relationships (Pearl, 2000; Spirtes, Glymour, & Scheines, 1993).  It has recently been proposed, chiefly by Gopnik, Glymour, and their colleagues (Glymour, 2001; Gopnik, Glymour, Sobel, Schulz, Kushnir, & Danks, 2004), that these algorithms may also explain how people infer causal structure.  A fundamental issue in explaining how people infer causal relationships is accounting for the interaction between abstract causal knowledge and statistical inference.  The classic debate between approaches that emphasize cause-effect covariation and those that emphasize mechanism knowledge (e. g. , Newsome, 2003) turns on this issue.  Causal graphical models provide a language in which the problem of causal induction can be formally expressed.  However, conventional algorithms for inducing causal structure (e. g. , Pearl, 2000; Spirtes et al. , 1993) do not provide a satisfying account of either the roles of causal knowledge or statistical inference, or their interaction.  These algorithms use tests of statistical independence to establish constraints that must be satisfied by causal structures consistent with the observed data.  No knowledge of how causal mechanisms operate, or the functional form of relationships between cause and effect, enters into the inference process.  As we argue below, such knowledge is necessary to explain how people are able to infer causal structure from very small samples, and to infer hidden causes from purely observational data.  Constraint-based methods are also unable to explain people's graded sensitivity to the strength of evidence for a causal structure, because they reason deductively from constraints to consistent structures.  We will present a rational account of human inference, Theory-Based Causal Induction, which emphasizes the interaction between causal knowledge and statistical learning.  Causal knowledge appears in the form of causal theories, specifying the principles by which causal relationships operate in a given domain.  These theories are used to generate hypothesis spaces of causal models -- some with hidden causes, some without -- that can be evaluated by domain-general statistical inference.  We will use this framework to develop models of people's inferences about hidden causes in two physical systems: a mechanical system called the stick-ball machine (Kushnir et al. , 2003), and a dynamical system involving an explosive compound called Nitro X.  Theory-based causal induction Our account of causal induction builds on causal graphical models, extending the formalism to incorporate the abstract knowledge about causal mechanism that plays an essential role in human inferences.  We will briefly introduce causal graphical models, consider how prior knowledge influences causal induction, and describe how we formalize the contribution of causal theories.  Causal graphical models Graphical models represent the dependency structure of a joint probability distribution using a graph in which nodes are variables and edges indicate dependence.  The graphical structure supports efficient computation of the probabilities of events involving these variables.  In a causal graphical model the edges indicate causal dependencies, with the direction of the arrow indicating the direction of causation, and they support inferences about the effects of interventions (Pearl, 2000).  An intervention is an event in which a variable is forced to hold a value, independent of any other variables on which it might depend.  Intervention on a variable A is denoted do(A).  Probabilistic inference on a modifified graph, in which incoming edges to A are removed, can be used to assess the consequences of intervening on A.  The structure of a causal graphical model implies a pattern of dependency among variables under observation and intervention.  Conventional algorithms for inferring causal structure use standard statistical tests, such as Pearson's # 2 test, to find the pattern of dependencies among variables, and then deductively identify the structure(s) consistent with that pattern (e. g. , Spirtes et al. , 1993).  These "constraint-based" algorithms can also exploit the results of interventions, and often require both observations and interventions in order to identify the hidden causal structure.  Gopnik, Glymour, and colleagues have suggested that this kind of constraint-based reasoning may underlie human causal induction (Glymour, 2001; Gopnik et al. , 2004; Kushnir et al. , 2003).  The role of causal theories Constraint-based algorithms for causal induction make relatively little use of prior knowledge.  While particular causal relationships can be ruled out a priori, there is no way to represent the belief that one structure may be more likely than another.  Furthermore, the use of statistical tests like # 2 makes only weak assumptions about the form of causal relationships: these tests simply assess dependency, regardless of whether a relationship is positive or negative, deterministic or probabilistic, strong or weak.  Several researchers (e. g. , Shultz, 1982) have argued that knowledge of causal mechanism plays a central role in human causal induction.  Mechanism knowledge is usually cited in arguments against statistical causal induction, but we view it as critical to explaining how statistical inferences about causal structure are possible from sparse data.  Knowledge about causal mechanisms provides two kinds of restrictions on possible causal models: restrictions on which relationships are plausible, and restrictions on the functional form of those relationships.  Restrictions on plausibility might indicate that one causal structure is more likely than another, while restrictions on functional form might indicate that a particular relationship should be positive and strong.  These restrictions have important implications for causal induction algorithms.  If all structures are possible, both observations and interventions are typically required to identify hidden causes, and without strong assumptions about the functional form of causal relationships, samples must be relatively large.  With limitations on the set of possible causal structures and expectations about functional form, however, it is possible to make causal inferences from just observations and from small samples -- important properties of human causal induction.  Using causal theories in causal induction The causal mechanism knowledge that is relevant for statistical causal inference may be quite abstract, and may also vary across domains.  Much of this knowledge may be represented in intuitive domain theories.  In contrast to Gopnik et al.  (2004), who suggest that causal graphical models are the primary substrate for intuitive theories, we emphasize the role of intuitive theories at a more abstract level, providing restrictions on the set of causal models under consideration.  Such restrictions cannot be represented as part of a causal graphical model: causal graphical models express the relations that hold among a finite set of propositions, while causal theories involve statements about all relations that could hold among entities in a given domain.  Formally, we view causal theories as hypothesis space generators: a theory is a set of principles that can be used to generate a hypothesis space of causal models, which are compared via Bayesian inference.  The principles that comprise a theory specify which relations are plausible and the functional form of those relations.  These principles articulate how causal relationships operate in a given domain, but need not identify the mechanisms underlying such relationships: all that is necessary for causal induction is the possibility that some mechanism exists, and expectations about the functional form associated with that mechanism.  This vague and abstract mechanism knowledge is consistent with the finding that people's understanding of causal mechanism is surprisingly shallow (Rozenblit & Keil, 2002).  In the remainder of the paper, we will demonstrate how Theory-Based Causal Induction can be used to explain human inferences about hidden causes in physical systems.  Different systems require different causal theories.  We will examine inferences in a mechanical system, the stick-ball machine (Kushnir et al. , 2003), and in a dynamical system, Nitro X, which we explore in a new experiment.  When reasoning about these systems, people infer hidden causal structure from very few observations, and are sensitive to graded degrees of evidence.  The stick-ball machine Kushnir et al.  (2003) conducted two experiments in which participants had to infer the causal structure

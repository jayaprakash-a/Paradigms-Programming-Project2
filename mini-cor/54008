Learning with Scope, with Application to Information Extraction and Classification
 Abstract In probabilistic approaches to text classi#cation and information extraction, one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data.  In many text data sets however, there are additional, scope-limited features whose predictive power is applicable only to a certain subset of the data.  For example, in information extraction from web pages, word formatting may be indicative of extraction category in different ways on different web pages.  The central diculty with using such features is capturing and exploiting the new regularities encountered in previously unseen data.  In this work, we propose a hierarchical probabilistic model that uses both local/scopelimited features (e. g. , formatting) and global features (e. g. , word content).  The local regularities are represented as an unobserved random parameter for each local data set, and these regularities are captured in the inference process.  This process is akin to automatically retuning our classifier to the local regularities on each newly encountered web page.  Exact inference is intractable, and we present approximations via point estimates and variational methods.  Empirical results on large collections of web data show this method significantly improving performance over traditional models of global features alone.

Alpha seeding for support vector machines
 ABSTRACT A key practical obstacle in applying support vector machines to many large-scale data mining tasks is that SVM training time generally scales quadratically (or worse) in the number of examples or support vectors.  This complexity is further compounded when a specific SVM training is but one of many, such as in Leave-One-Out-Cross-Validation (LOOCV) for determining optimal SVM parameters or as in wrapper-based feature selection.  In this paper we explore new techniques for reducing the amortized cost of each such SVM training, by seeding successive SVM trainings with the results of previous similar trainings.

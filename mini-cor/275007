Optimizing Average Reward Using Discounted Rewards
 Abstract.  In many reinforcement learning problems, it is appropriate to optimize the average reward.  In practice, this is often done by solving the Bellman equations using a discount factor close to 1.  In this paper, we provide a bound on the average reward of the policy obtained by solving the Bellman equations which depends on the relationship between the discount factor and the mixing time of the Markov chain.  We extend this result to the direct policy gradient of Baxter and Bartlett, in which a discount parameter is used to find a biased estimate of the gradient of the average reward with respect to the parameters of a policy.  We show that this biased gradient is an exact gradient of a related discounted problem and provide a bound on the optima found by following these biased gradients of the average reward.  Further, we show that the exact Hessian in this related discounted problem is an approximate Hessian of the average reward, with equality in the limit the discount factor tends to 1.  We then provide an algorithm to estimate the Hessian from a sample path of the underlying Markov chain, which converges with probability 1.

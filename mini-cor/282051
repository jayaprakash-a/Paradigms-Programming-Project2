Smooth "-Insensitive Regression by Loss Symmetrization
 Abstract We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions.  These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss.  The resulting symmetric logistic loss can be viewed as a smooth approximation to the "-insensitive hinge loss used in support vector regression.  We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses.  The first family employs an iterative log-additive update which provides a regression counterpart for recent boosting algorithms.  The second family utilizes an iterative additive update step.  We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the logistic loss.  A byproduct of our work is a new simple form of regularization for boosting-based classification and regression algorithms.  Our regression framework also has implications on classification algorithms, namely, a new additive batch algorithm for the log-loss and exp-loss used in boosting.  We demonstrate the merits of our algorithms in a series of experiments including an experiment that boosts the accuracy of support vector regressors on a benchmark dataset.

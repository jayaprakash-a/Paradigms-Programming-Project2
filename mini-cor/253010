Robust Value Function Approximation by Working Backwards
 Abstract In this paper, we examine the intuition that TD( ) is meant to operate by approximating asynchronous value iteration.  We note that on the important class of discrete acyclic stochastic tasks, value iteration is inefficient compared with the DAG-SP algorithm, which essentially performs only one sweep instead of many by working backwards from the goal.  The question we address in this paper is whether there is an analogous algorithm that can be used in large stochastic state spaces requiring function approximation.  We present such an algorithm, analyze it, and give comparative results to TD on several domains.  LEARNING CONTROL BACKWARDS Computing an accurate value function is the key to dynamic-programming-based algorithms for optimal sequential control in Markov Decision Processes.  The optimal value function V \Lambda (x) specifies, for each state x in the state space X, the expected cumulative reward when starting in state x and acting optimally thereafter.  It is also the unique solution to the Bellman equations (using the notation of [11]): 8x 2 X, V (x) = n R(x) if x is a terminal state max a2A(x) \Gamma R(x; a) + fl P y2X Prob(x a ! y)V (y) \Delta otherwise (1) The Bellman equation at x also reveals the optimal control from x: any action which instantiates the max is an optimal choice [2].  For small discrete problems, the value function can be stored in a lookup table and computed by iterative algorithms such as value iteration (VI) [2].  VI computes V \Lambda by repeatedly sweeping over the state space, applying Equation 1 as an assignment statement (this is called a "one-step backup") at each state in parallel.  If the lookup table is initialized with all 0's, then after i sweeps of VI, the table will represent the maximum expected return of a path of length i from each state.  For certain goal-oriented domains, this corresponds to the intuition that VI works by propagating correct V \Lambda values backwards, by one step per iteration, from the terminal states.  More precisely, there are two classes of MDPs for which correct V \Lambda values can be assigned by working strictly backwards from terminal states: 1.  deterministic domains with no positive-

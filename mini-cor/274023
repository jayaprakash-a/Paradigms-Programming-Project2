Maximum Margin Bayesian Networks
 Abstract We consider the problem of learning Bayesian network classifiers that maximize the margin over a set of classification variables.  We find that this problem is harder for Bayesian networks than for undirected graphical models like maximum margin Markov networks (M 3 N), since the parameters in a Bayesian network must satisfy additional normalization constraints that an undirected graphical model need not respect.  Unfortunately, these normalization constraints eliminate the convexity properties of the training problem, and significantly complicate the optimization task.  Nevertheless, we derive an effective training algorithm that solves the maximum margin training problem for a range of network topologies, and otherwise converges to a locally optimal set of parameters for arbitrary network topologies.  Experimental results show that the method can demonstrate improved generalization performance when the directed graphical structure encodes relevant knowledge.  Our intent is to pose and investigate what we believe is a natural machine learning approach, while also pointing out its difficulties.

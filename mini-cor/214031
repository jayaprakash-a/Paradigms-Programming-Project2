Belief Propagation and Revision in Networks with Loops
 Abstract Local belief propagation rules of the sort proposed byPearl (1988) are guaranteed to converge to the optimal beliefs for singly connected networks.  Recently,anumber of researchers have empirically demonstrated good performance of these same algorithms on networks with loops, but a theoretical understanding of this performance has yet to be achieved.  Here welay a foundation for an understanding of belief propagation in networks with loops.  For networks with a single loop, we derive an analytical relationship between the steady state beliefs in the loopy network and the true posterior probability.  Using this relationship we show a category of networks for which the MAP estimate obtained by belief update and by belief revision can be proven to be optimal (although the beliefs will be incorrect).  We showhow nodes can use local information in the messages they receive in order to correct the steady state beliefs.  Furthermore we prove that for all networks with a single loop, the MAP estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states.  The result is independent of the length of the cycle and the size of the state space.  For networks with multiple loops, weintroduce the concept of a \balanced network" and show simulation results comparing belief revision and update in such networks.  We show that the Turbo code structure is balanced and present simulations on a toyTurbo code problem indicating the decoding obtained by belief revision at convergence is significantly more likely to be correct.

Near Shannon Limit Performance of Low Density Parity Check Codes
 A linear code may be described in terms of a generator matrix G or in terms of a parity check matrix H, which satisfies Hx = 0 for all codewords x.  In 1962, Gallager reported work on binary codes defined in terms of low density parity check matrices (abbreviated `GL codes') [5, 6].  The matrix H was defined in a non-systematic form; each column of H had a small weight (e. g. , 3) and the weight per row was also uniform; the matrix H was constructed at random subject to these constraints.  Gallager proved distance properties of these codes and described a probability-based decoding algorithm with promising empirical performance.  However it appears that GL codes have been generally forgotten, the assumption perhaps being that concatenated codes [4] were superior for practical purposes (R. G.  Gallager, personal communication).  During our work on MN codes [8] we realised that it is possible to create `good' codes from very sparse random matrices, and to decode them (even beyond their minimum distance) using approximate probabilistic algorithms.  We eventually reinvented Gallager's decoding algorithm and GL codes.  In this paper we report the empirical performance of these codes on Gaussian channels.  We have proved theoretical properties of GL codes (essentially, that the channel coding theorem holds for them) elsewhere [9].  GL codes can also be defined over GF (q).  We are currently implementing this generalization.  We created sparse random parity check matrices in the following ways.  Construction 1A.  An M by N matrix (M rows, N columns) is created at random with weight per column t (e. g. , t = 3), and weight per row as uniform as possible, and overlap between any two columns no greater than 1.  (The weight of a column is the number of non-zero elements; the overlap between two columns is their inner product. ) Construction 2A.  Up to M/2 of the columns are designated weight 2 columns, and these are constructed such that there is zero overlap between any pair of columns.  The remaining columns are made at random with weight 3, with the weight per row as uniform as possible, and overlap between any two columns of the entire matrix no greater than 1.  Constructions 1B and 2B.  A small number of columns are deleted from a matrix produced by constructions 1A and 2A, respectively, so that the bipartite graph corresponding to the matrix has no short cycles of length less than some length l.  The above constructions do not ensure that all the rows of the matrix are linearly independent, so the M N matrix created is the parity matrix of a linear code with rate at least R # K/N , where K = N - M .  We report results on the assumption that the rate is R.  The generator matrix of the code can be created by Gaussian elimination.  We simulated a Gaussian channel with binary input a and additive noise of variance # 2 = 1.  If one communicates using a code of rate R then it is conventional to describe the signal to noise ratio by E b N0 = a 2 2R# 2 and to report this number in decibels as 10 log 10 E b /N 0 .  Decoding.  The decoding problem is to find the most probable vector x such that Hxmod 2 = 0, with the likelihood of x given by Q n f xn n where f 1 n = 1/(1 + exp(- 2ay n /# 2 )) and f 0 n = 1 - f 1 n , and y n is the channel's output at time n.  Gallager's algorithm (reviewed in detail in [9]) may be viewed as an approximate belief propagation algorithm [10].  (The Turbo decoding algorithm may also be viewed as a belief propagation algorithm (R. J. McEliece and D. J. C. MacKay, unpublished). ) We refer to the elements of x as bits and to the rows of H as checks.  We denote the set of bits n that participate in check m by N (m) # {n : Hmn = 1}.  Similarly we define the set of checks in which bit n participates, M(n) # {m : Hmn = 1}.  We denote a set N (m) with bit n excluded by N (m)\n.  The algorithm has two alternating parts, in which quantities q mn and r mn associated with each non-zero element in the H matrix are iteratively updated.  The quantity q x mn is meant to be the probability that bit n of x is x, given the information obtained via checks other than check m.  The quantity r x mn is meant to be the probability of check m being satisfied if bit n of x is considered fixed at x and the other bits have a separable distribution given by the probabilities {q mn 0 : n 0 2 N (m)\n}.  The algorithm would produce the exact posterior probabilities of all the bits if the bipartite graph defined by the matrix H contained no cycles [10].  Initialization.  The variables q 0 mn and q 1 mn are initialized to the values f 0 n and f 1 n respectively.  Horizontal step.  We define #q mn # q 0 mn- q 1 mn and compute for each m, n: fir mn = Y n 0 2N (m)\n #q mn 0 (1) then set r 0 mn = 1 2 (1 + fir mn ) and r 1 mn = 1 2 (1- fir mn ).  Vertical step.  For each n and m and for x = 0, 1 we update: q x mn = #mn f x n Y m 0 2M(n)\m r x m 0 n (2) where #mn is chosen such that q 0 mn +q 1 mn = 1.  We can also update the `pseudoposterior probabilities' q 0 n and q 1 n , given by: q x n = # n f x n Y m2M(n) r x mn .

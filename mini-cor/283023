Networks of Spiking Neurons: The Third Generation of Neural Network Models
 Abstract The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i. e.  threshold gates) respectively sigmoidal gates.  In particular it is shown that networks of spiking neurons are computationally more powerful than these other neural network models.  A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net.  This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.  1 Definitions and Motivations If one classifies neural network models according to their computational units, one can distinguish three different generations.  The first generation is based on McCulloch-Pitts neurons as computational units.  These are also referred to as perceptrons or threshold-gates.  They give rise to a variety of neural network models such as multilayer perceptrons (also called threshold circuits), Hopfield nets, and Boltzmann machines.  A characteristic feature of these models is that they can only give digital output.  In fact they are universal for computations with digital input and output, and every boolean function can be computed by some multi-layer perceptron with a single hidden layer.  The second generation is based on computational units that apply to a weighted sum (or polynomial) of the inputs an "activation function" with a continuous set of possible output values, such as the sigmoid function oe(y) = 1=(1+ e\Gamma y ) or the linear saturated function with (y) = y for 0 y 1; (y) = 0 for y ! 0; (y) = 1 for y ? 1 .  Besides piecewise polynomial activation functions we consider in this paper also "piecewise exponential" activation functions, whose pieces can be defined by expressions involving exponentiation (such as the definition of oe).  Typical examples for networks from this second generation are feedforward and recurrent sigmoidal neural nets, as well as networks of radial basis function units.  These nets are also able to compute (with the help of rounding at the network output) arbitrary boolean functions.  Actually it has been shown that neural nets from the second generation can compute certain boolean functions with fewer gates than neural nets from the first generation ([41], [11]).  In addition, neural nets from the second generation are able to compute functions with analog input and output.  In fact they are universal for analog computations in the sense that any continuous function with a compact domain and range can be approximated arbitrarily well (with regard to uniform convergence, i. e.  the L1 -norm) by a network of this type with a single hidden layer.  Another characteristic feature of this second generation of neural network models is that they support learning algorithms that are based on gradient

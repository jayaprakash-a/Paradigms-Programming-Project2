EM Algorithms for PCA and Sensible PCA
 Abstract I present an expectation-maximization (EM) algorithm for principal component analysis (PCA).  The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data.  It is computationally ecient in space and time and does not require computing the sample covariance of the data.  It also naturally accommodates missing information.  I introduce a new variation of PCA known as sensible principal component analysis (SPCA) which defines a proper density model in the data space.  Learning for SPCA is also done with an EM algorithm.  I include results of simulations showing that, in only a few iterations, these EM algorithms correctly and eciently find the leading eigenvectors of the covariance of large datasets (having up to half a million datapoints in tens of thousands of dimensions).  1 Why EM for PCA? Principal component analysis (PCA) is a widely used dimensionality reduction technique in data analysis.  Its popularity comes from three important properties.  First, it is the optimal (in terms of mean squared error) linear scheme for compressing a set of high dimensional vectors into a set of lower dimensional vectors and then reconstructing.  Second, the model parameters can be computed directly from the data { for example by diagonalizing the sample covariance of the data.  Third, compression and decompression are easy operations to perform given the model parameters { they require only matrix multiplications.  Despite these attractive features, however, PCA models have several shortcomings.  One is that naive methods for finding the principal component directions have trouble with high dimensional data.  Consider attempting to diagonalize the sample covariance matrix of n vectors in a space of p dimensions when n and p are several hundred or several thousand.  Diculties can arise both in the form of computational complexity and also data scarcity.  On the data scarcity front, we often do not have enough data in high dimensions for the sample covariance to be of full rank and so we must be careful to employ techniques such as the singular value decomposition which do not require full rank matrices.  On the complexity front, direct diagonalization of a symmetric matrix thousands of rows in size can be extremely costly since this operation is O(p 3 ) for p # p inputs.  Fortunately, several techniques exist for reducing this cost when only the first few leading eigenvectors and eigenvalues are required (for example the power method [9] which is only O(p 2 )).  However, even computing the sample covariance itself is very costly, requiring O(np 2 ) operations.  In general it is best to avoid altogether computing the sample covariance explicitly.  Methods such as the snap-shot algorithm [8] do this by assuming that the eigenvectors being searched for are linear combinations of the datapoints; their complexity is O(n 3 ).  In this note, I present a version of the expectation-maximization (EM) algorithm [1] for learning the principal components of a dataset.  The algorithm does not require computing the sample covariance and has a complexity limited by O(knp) operations where k is the number of leading eigenvectors to be learned.  For large n and p I am not aware of any other methods which possess this attractive scaling.  Another shortcoming of standard approaches to PCA is that it is not obvious how to deal properly with missing data.  Most of the methods discussed above cannot accommodate missing values and so incomplete points must either be discarded or completed using a variety of ad-hoc interpolation methods.  (For example, a standard solution is to replace missing coordinates with the mean of the known values in that coordinate or with a linear regression of the unknown value from the known values. ) On the other hand, the EM algorithm for PCA enjoys all the benefits [4] of other EM algorithms in terms of estimating the maximum likelihood values for missing information directly at each iteration.  Finally, the PCA model itself suffers from a critical flaw which is independent of the technique used to compute its parameters: it does not define a proper probability model in the space of inputs.  This is because the density is not normalized within the principal subspace.  In other words, if we perform PCA on some data and then ask how well new data are fit by the model, the only criterion available is the squared distance of the new data from their projections into the principal subspace.  A datapoint far away from the training data but nonetheless near the principal subspace will be assigned a high \pseudo-likelihood" or low error (see figure 1).  Similarly, it is not possible to generate \fantasy" data from a PCA model.  In this note I introduce a new model called sensible principal component analysis (SPCA), an obvious modification of PCA which does define a proper covariance structure in the data space.  Its parameters can also be learned with an EM algorithm, given below.  Figure 1: PCA does not define a proper density model in the data space.  The only measure of how well new data fits the model is the distance from the principal subspace; thus points far away from the bulk of the data but nonetheless near the principal subspace will have low reconstruction error.  In the figure, the two points denoted with X have the same reconstruction error.  2 Whence EM for PCA? Principal component analysis can be viewed as a limiting case of a particular class of linear-Gaussian models.  The goal of such models is to capture the covariance structure of an observed p-dimensional variable y using fewer than the p(p + 1)=2 free parameters required in a full covariance matrix.  Linear-Gaussian models do this by assuming that y was produced as a linear transformation of some k-dimensional latent variable x plus additive Gaussian noise.  Denoting the transformation by

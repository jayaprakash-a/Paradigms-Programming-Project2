A Novel Temporal Generative Model of Natural Video as an Internal Model in Early Vision
 Abstract--- In computational neuroscience one application of generative models is to examine how the statistics sensory input data are related to the properties corresponding sensory neural networks.  In this approach it is assumed that neural networks are tuned to the properties of input data, that is, that they have learned ecient internal representations of their environment.  In this paper we present a hypothetical internal representation for natural video at the level of early vision, more precisely, the level of simple and complex cells.  We define two-layer generative model for natural video, based on temporal relationships between simple cell outputs.  Preliminary results of estimating the parameters of the model from natural data suggest that the learned temporal interactions between cell outputs are similar to complex cell pooling of simple cell outputs.  This unsupervised learning pooling separates our experimental results from empirical work based on other advanced self-organizing models early vision.

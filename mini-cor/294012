Finite size effects in on-line learning of multi-layer neural networks
 Abstract We complement recent advances in thermodynamic limit analyses of mean on-line gradient descent learning dynamics in multi-layer networks by calculating fluctuations possessed by finite dimensional systems.  Fluctuations from the mean dynamics are largest at the onset of specialisation as student hidden unit weight vectors begin to imitate specific teacher vectors, increasing with the degree of symmetry of the initial conditions.  In light of this, we include a term to stimulate asymmetry in the learning process, which typically also leads to a significant decrease in training time.  An attractive feature of neural networks is their ability to learn a parametrised rule from a set of input/output training examples, by which the parameters of the network are adapted to minimise an error measuring the misfit of the network mapping on the training examples.  Different approaches to the learning process are typically evaluated by the expected error that the network will make on a randomly presented input example.  In on-line learning, statistical mechanics plays a strong role in calculating this generalisation error (see [1, 2, 4] and refs.  within) through self-averaging in the thermodynamic limit, for which an understanding of finite size effects would benefit further advances.  Connections to alternative finite dimensional methods (see [3] and refs.  within) will be pointed to in the course of our analysis.  In on-line learning, the weights parametrising the student network are successively updated according to the error incurred on a single example from a stream of input/output examples, f; ()g, generated by a teacher network (\Delta).  We assume that the teacher network the student attempts to learn is a soft committee machine[1, 4] of N inputs, and M hidden units, this being a one hidden layer network with weights connecting each hidden to output unit set to +1, and with each hidden unit n connected to all input units by B n (n = 1::M).  Explicitly, for the N dimensional training input vector , the output of the teacher is given by, i = M X n=1 g(B n \Delta ); (1) where g(x) is the activation function of the hidden units, and we take g(x) = erf(x= p 2).  The teacher generates a stream of training examples (; i ), with input components drawn from a normal distribution of zero mean, unit variance.  The student network that attempts to learn the teacher, by fitting the training examples, is also a soft committee machine, but with K hidden units.  For input , the student output is, oe(J; ) = K X i=1 g(J i \Delta ); (2) where the student weights J = fJ i g(i = 1::K) are sequentially modified to reduce the error that the student makes on an input , ffl(J; ) = 1 2 (oe(J; ) \Gamma i ) 2 = 1 2 / K X i=1 g(x i ) \Gamma M X n=1 g(y n ) ! 2 ; (3) where the activations are defined x i = J i \Delta , and y n = B n \Delta .  Gradient descent on the error(3) results in an update of the student weight vectors, J +1 = J \Gamma j N ffi i ; (4) where, ffi i = g 0 (x i ) 2 4 M X n=1 g(y n ) \Gamma K X j=1 g(x j ) 3 5 ; (5) and g 0 is the derivative of the activation function g.  The typical performance of the student on a randomly selected input example is given by the generalisation error, ffl g = hffl(J; )i, where h::i represents an average over the gaussian input distribution.  One finds that ffl g depends only on the order-parameters, R in = J i \Delta Bn , Q ij = J i \Delta J j , and Tnm = Bn \Delta Bm (i; j = 1::K; n; m = 1::M)[4], for which, using (4), we derive (stochastic) update equations, R +1 in\Gamma R in = j N ffi i y n ; (6) Q +1 ik\Gamma Q ik = j N \Gamma ffi i x j + ffi k x i \Delta + j 2 N 2 ffi i ffi k \Delta : (7) We average over the input distribution to obtain deterministic equations for the mean values of the order parameters, which are self-averaging in the thermodynamic limit, N!1.  The order-parameter approach contrasts with approaches which analyze the dynamics of the individual weight components, based upon approximate Fokker-Plank equations (see [3] and refs.  within).  The advantage of the order-parameter approach is that the system is modelled exactly in the thermodynamic limit, with only a small number of equations.  In this work we present a more realistic treatment by calculating the dynamic fluctuations induced

What if cameras could see themselves?
 Abstract We propose a camera design where the camera records images through an attached aperture, which can be seen in the image.  In such a design camera calibration is reduced to the problem of camera localization, with 3 degrees of freedom remaining which correspond to the unknown location of the camera.  9 degrees of freedom which correspond to the unknown camera's internal parameters and orientation are eliminated by the registration of the aperture.  We discuss two applications: pointing target detection when pointing towards the camera, and depth estimation from two or more uncalibrated cameras.  We conclude with experimental results which demonstrate the usefulness and robustness of our approach.

Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering
 Abstract Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space.  The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering.  Several applications are considered.  In many areas of artificial intelligence, information retrieval and data mining, one is often confronted with intrinsically low dimensional data lying in a very high dimensional space.  For example, gray scale n \Theta n images of a fixed object taken with a moving camera yield data points in R n 2 .  However, the intrinsic dimensionality of the space of all images of the same object is the number of degrees of freedom of the camera -- in fact the space has the natural structure of a manifold embedded in R n 2 .  While there is a large body of work on dimensionality reduction in general, most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside.  Recently, there has been some interest (Tenenbaum et al, 2000; Roweis and Saul, 2000) in the problem of developing low dimensional representations of data in this particular context.  In this paper, we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction.  The core algorithm is very simple, has a few local computations and one sparse eigenvalue problem.  The solution reflects the intrinsic geometric structure of the manifold.  The justification comes from the role of the Laplacian operator in providing an optimal embedding.  The Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace-Beltrami operator defined on the manifold.  The embedding maps for the data come from approximations to a natural map that is defined on the entire manifold.  The framework of analysis presented here makes this connection explicit.  While this connection is known to geometers and specialists in spectral graph theory (for example, see [1, 2]) to the best of our knowledge we do not know of any application to data representation yet.  The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner.  The locality preserving character of the Laplacian Eigenmap algorithm makes it relatively insensitive to outliers and noise.  A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data.  Connections to spectral clustering algorithms developed in learning and computer vision (see Shi and Malik, 1997) become very clear.  Following the discussion of Roweis and Saul (2000), and Tenenbaum et al (2000), we note that the biological perceptual apparatus is confronted with high dimensional stimuli from which it must recover low dimensional structure.  One might argue that if the approach to recovering such low-dimensional structure is inherently local, then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception.  1 The Algorithm Given k points x 1 ; : : : ; x k in R l , we construct a weighted graph with k nodes, one for each point, and the set of edges connecting neighboring points to each other.  1.  Step 1.  [Constructing the Graph] We put an edge between nodes i and j if x i and x j are "close".  There are two variations: (a) ffl-neighborhoods.  [parameter ffl 2 R] Nodes i and j are connected by an edge if jjx i \Gamma x j jj 2 ! ffl.  Advantages: geometrically motivated, the relationship is naturally symmetric.  Disadvantages: often leads to graphs with several connected components, difficult to choose ffl.  (b) n nearest neighbors.  [parameter n 2 N] Nodes i and j are connected by an edge if i is among n nearest neighbors of j or j is among n nearest neighbors of i.  Advantages: simpler to choose, tends to lead to connected graphs.  Disadvantages: less geometrically intuitive.  2.  Step 2.  [Choosing the weights] Here as well we have two variations for weighting the edges: (a) Heat kernel.  [parameter t 2 R].  If nodes i and j are connected, put W ij = e\Gamma jjx i \Gamma x j jj 2 t The justification for this choice of weights will be provided later.  (b) Simple-minded.  [No parameters].  W ij = 1 if and only if vertices i and j are connected by an edge.  A simplification which avoids the necessity of choosing t.  3.  Step 3.  [Eigenmaps] Assume the graph G, constructed above, is connected, otherwise proceed with Step 3 for each connected component.

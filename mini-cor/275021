Approximately Optimal Approximate Reinforcement Learning
 Abstract In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used.  In this paper, we present the conservative policy iteration algorithm which finds an #approximately# optimal policy, given access to a restart distribution (which draws the next state from a particular distribution) and an approximate greedy policy chooser.  Crudely, the greedy policy chooser outputs a policy that usually chooses actions with the largest state-action values of the current policy, ie it outputs an #approximate# greedy policy.  This greedy policy chooser can be implemented using standard value function approximation techniques.  Under these assumptions, our algorithm: (1) is guaranteed to improve a performance metric (2) is guaranteed to terminate in a #small# number of timesteps and (3) returns an #approximately# optimal policy.  The quantifled statements of (2) and (3) depend on the quality of the greedy policy chooser, but not explicitly on the the size of the state space.

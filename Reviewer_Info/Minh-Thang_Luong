Effective Approaches to Attention-based Neural Machine Translation.
Compression of Neural Machine Translation Models via Pruning.
A Hierarchical Neural Autoencoder for Paragraphs and Documents.
Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning.
ForeCite: towards a reader-centric scholarly digital library.
A Trajectory-based Parallel Model Combination with a unified static and dynamic parameter compensation for noisy speech recognition.
A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages.
Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models.
Logical Structure Recovery in Scholarly Articles with Rich Document Features.
Enhancing Morphological Alignment for Translating Highly Inflected Languages.
WINGNUS: Keyphrase Extraction Utilizing Document Logical Structure.
Models and Inference for Prefix-Constrained Machine Translation.
Multi-task Sequence to Sequence Learning.

An adaptive low dimensional quasi-Newton sum of functions optimizer.
Building a better probabilistic model of images by factorization.
Training sparse natural image models with a fast Gibbs sampler of an extended state space.
The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use
Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods.
Hamiltonian Monte Carlo with Reduced Momentum Flips
On the expressive power of deep neural networks.
Deep Unsupervised Learning using Nonequilibrium Thermodynamics.
Hamiltonian Monte Carlo Without Detailed Balance.
Efficient Methods for Unsupervised Learning of Probabilistic Models
Controlled experiments on millions of students to personalize learning.
A Device for Human Ultrasonic Echolocation.
Analyzing noise in autoencoders and deep networks.
Deep Knowledge Tracing.
Minimum Probability Flow Learning
Hamiltonian Annealed Importance Sampling for partition function estimation
An Unsupervised Algorithm For Learning Lie Group Transformations
A universal tradeoff between power, precision and speed in physical communication.
Modeling Higher-Order Correlations within Cortical Microcolumns.
Technical Note on Equivalence Between Recurrent Neural Network Time Series Models and Variational Bayesian Models.
Measurably Increasing Motivation in MOOCs.
Lie Group Transformation Models for Predictive Video Coding.
Exponential expressivity in deep neural networks through transient chaos.
Density estimation using Real NVP.
Minimum Probability Flow Learning.
